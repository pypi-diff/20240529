# Comparing `tmp/geonode_importer-1.0.8-py3-none-any.whl.zip` & `tmp/geonode_importer-1.0.9-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,41 +1,41 @@
-Zip file size: 100669 bytes, number of entries: 87
--rw-r--r--  2.0 unx     1132 b- defN 24-Mar-27 10:40 importer/__init__.py
+Zip file size: 102152 bytes, number of entries: 87
+-rw-r--r--  2.0 unx     1132 b- defN 24-May-23 07:55 importer/__init__.py
 -rw-r--r--  2.0 unx      894 b- defN 24-Feb-14 10:56 importer/apps.py
 -rw-r--r--  2.0 unx      301 b- defN 24-Jan-09 15:22 importer/celery_app.py
--rw-r--r--  2.0 unx    25978 b- defN 24-Mar-27 10:40 importer/celery_tasks.py
+-rw-r--r--  2.0 unx    25880 b- defN 24-May-23 07:49 importer/celery_tasks.py
 -rw-r--r--  2.0 unx     1137 b- defN 24-Jan-09 15:22 importer/datastore.py
 -rw-r--r--  2.0 unx     2137 b- defN 24-Jan-09 15:22 importer/db_router.py
 -rw-r--r--  2.0 unx     1574 b- defN 24-Feb-14 10:56 importer/models.py
 -rw-r--r--  2.0 unx    14508 b- defN 24-Mar-27 10:40 importer/orchestrator.py
--rw-r--r--  2.0 unx     6088 b- defN 24-Mar-27 10:40 importer/publisher.py
+-rw-r--r--  2.0 unx     6858 b- defN 24-May-23 07:49 importer/publisher.py
 -rw-r--r--  2.0 unx      399 b- defN 24-Jan-09 15:22 importer/settings.py
 -rw-r--r--  2.0 unx        0 b- defN 24-Jan-09 15:22 importer/urls.py
 -rw-r--r--  2.0 unx     1998 b- defN 24-Mar-27 10:40 importer/utils.py
 -rw-r--r--  2.0 unx        0 b- defN 24-Jan-09 15:22 importer/views.py
 -rw-r--r--  2.0 unx        0 b- defN 24-Jan-09 15:22 importer/api/__init__.py
 -rw-r--r--  2.0 unx     1675 b- defN 24-Jan-09 15:22 importer/api/exception.py
--rw-r--r--  2.0 unx     1010 b- defN 24-Mar-27 10:40 importer/api/serializer.py
+-rw-r--r--  2.0 unx     1011 b- defN 24-May-23 07:49 importer/api/serializer.py
 -rw-r--r--  2.0 unx     5844 b- defN 24-Feb-14 10:56 importer/api/tests.py
 -rw-r--r--  2.0 unx      486 b- defN 24-Feb-14 10:56 importer/api/urls.py
--rw-r--r--  2.0 unx    10559 b- defN 24-Mar-27 10:40 importer/api/views.py
+-rw-r--r--  2.0 unx    10559 b- defN 24-May-23 07:49 importer/api/views.py
 -rw-r--r--  2.0 unx        0 b- defN 24-Jan-09 15:22 importer/handlers/__init__.py
 -rw-r--r--  2.0 unx     2769 b- defN 24-Mar-27 10:40 importer/handlers/apps.py
--rw-r--r--  2.0 unx     6628 b- defN 24-Mar-27 10:40 importer/handlers/base.py
+-rw-r--r--  2.0 unx     8029 b- defN 24-May-23 07:49 importer/handlers/base.py
 -rw-r--r--  2.0 unx     2786 b- defN 24-Jan-09 15:22 importer/handlers/tests.py
 -rw-r--r--  2.0 unx     5260 b- defN 24-Feb-14 10:56 importer/handlers/utils.py
 -rw-r--r--  2.0 unx        0 b- defN 24-Jan-09 15:22 importer/handlers/common/__init__.py
--rw-r--r--  2.0 unx     3611 b- defN 24-Mar-27 10:40 importer/handlers/common/metadata.py
--rw-r--r--  2.0 unx    22971 b- defN 24-Mar-27 10:40 importer/handlers/common/raster.py
+-rw-r--r--  2.0 unx     3317 b- defN 24-May-23 07:49 importer/handlers/common/metadata.py
+-rw-r--r--  2.0 unx    22345 b- defN 24-May-23 07:49 importer/handlers/common/raster.py
 -rw-r--r--  2.0 unx     3162 b- defN 24-Jan-09 15:22 importer/handlers/common/tests_raster.py
--rw-r--r--  2.0 unx    11388 b- defN 24-Mar-27 10:40 importer/handlers/common/tests_vector.py
--rw-r--r--  2.0 unx    36968 b- defN 24-Mar-27 10:40 importer/handlers/common/vector.py
+-rw-r--r--  2.0 unx    13587 b- defN 24-May-23 07:49 importer/handlers/common/tests_vector.py
+-rw-r--r--  2.0 unx    37133 b- defN 24-May-23 07:49 importer/handlers/common/vector.py
 -rw-r--r--  2.0 unx        0 b- defN 24-Jan-09 15:22 importer/handlers/csv/__init__.py
 -rw-r--r--  2.0 unx      284 b- defN 24-Jan-09 15:22 importer/handlers/csv/exceptions.py
--rw-r--r--  2.0 unx     9665 b- defN 24-Mar-27 10:40 importer/handlers/csv/handler.py
+-rw-r--r--  2.0 unx     9841 b- defN 24-May-23 07:50 importer/handlers/csv/handler.py
 -rw-r--r--  2.0 unx     7151 b- defN 24-Mar-27 10:40 importer/handlers/csv/tests.py
 -rw-r--r--  2.0 unx        0 b- defN 24-Jan-09 15:22 importer/handlers/geojson/__init__.py
 -rw-r--r--  2.0 unx      296 b- defN 24-Jan-09 15:22 importer/handlers/geojson/exceptions.py
 -rw-r--r--  2.0 unx     3542 b- defN 24-Feb-14 10:56 importer/handlers/geojson/handler.py
 -rw-r--r--  2.0 unx     5585 b- defN 24-Mar-27 10:40 importer/handlers/geojson/tests.py
 -rw-r--r--  2.0 unx        0 b- defN 24-Jan-09 15:22 importer/handlers/geotiff/__init__.py
 -rw-r--r--  2.0 unx      290 b- defN 24-Jan-09 15:22 importer/handlers/geotiff/exceptions.py
@@ -49,41 +49,41 @@
 -rw-r--r--  2.0 unx        0 b- defN 24-Jan-09 15:22 importer/handlers/kml/__init__.py
 -rw-r--r--  2.0 unx      284 b- defN 24-Jan-09 15:22 importer/handlers/kml/exceptions.py
 -rw-r--r--  2.0 unx     4829 b- defN 24-Mar-27 10:40 importer/handlers/kml/handler.py
 -rw-r--r--  2.0 unx     4251 b- defN 24-Jan-09 15:22 importer/handlers/kml/tests.py
 -rw-r--r--  2.0 unx        0 b- defN 24-Jan-09 15:22 importer/handlers/shapefile/__init__.py
 -rw-r--r--  2.0 unx      302 b- defN 24-Jan-09 15:22 importer/handlers/shapefile/exceptions.py
 -rw-r--r--  2.0 unx     6673 b- defN 24-Mar-27 10:40 importer/handlers/shapefile/handler.py
--rw-r--r--  2.0 unx     1201 b- defN 24-Mar-27 10:40 importer/handlers/shapefile/serializer.py
+-rw-r--r--  2.0 unx     1202 b- defN 24-May-23 07:49 importer/handlers/shapefile/serializer.py
 -rw-r--r--  2.0 unx     6435 b- defN 24-Mar-27 10:40 importer/handlers/shapefile/tests.py
 -rw-r--r--  2.0 unx        0 b- defN 24-Mar-27 10:40 importer/handlers/sld/__init__.py
 -rw-r--r--  2.0 unx      284 b- defN 24-Mar-27 10:40 importer/handlers/sld/exceptions.py
 -rw-r--r--  2.0 unx     2004 b- defN 24-Mar-27 10:40 importer/handlers/sld/handler.py
 -rw-r--r--  2.0 unx     3111 b- defN 24-Mar-27 10:40 importer/handlers/sld/tests.py
 -rw-r--r--  2.0 unx        0 b- defN 24-Mar-27 10:40 importer/handlers/xml/__init__.py
 -rw-r--r--  2.0 unx      284 b- defN 24-Mar-27 10:40 importer/handlers/xml/exceptions.py
 -rw-r--r--  2.0 unx     1937 b- defN 24-Mar-27 10:40 importer/handlers/xml/handler.py
--rw-r--r--  2.0 unx      556 b- defN 24-Mar-27 10:40 importer/handlers/xml/serializer.py
+-rw-r--r--  2.0 unx      556 b- defN 24-May-23 07:49 importer/handlers/xml/serializer.py
 -rw-r--r--  2.0 unx     3096 b- defN 24-Mar-27 10:40 importer/handlers/xml/tests.py
 -rw-r--r--  2.0 unx     1030 b- defN 24-Jan-09 15:22 importer/migrations/0001_initial.py
 -rw-r--r--  2.0 unx      501 b- defN 24-Jan-09 15:22 importer/migrations/0002_resourcehandlerinfo_kwargs.py
 -rw-r--r--  2.0 unx      673 b- defN 24-Jan-09 15:22 importer/migrations/0003_resourcehandlerinfo_execution_id.py
 -rw-r--r--  2.0 unx      408 b- defN 24-Jan-09 15:22 importer/migrations/0004_rename_execution_id_resourcehandlerinfo_execution_request.py
 -rw-r--r--  2.0 unx     1056 b- defN 24-Feb-14 10:56 importer/migrations/0005_fixup_dynamic_shema_table_names.py
 -rw-r--r--  2.0 unx     1571 b- defN 24-Jan-09 15:22 importer/migrations/0006_dataset_migration.py
 -rw-r--r--  2.0 unx        0 b- defN 24-Jan-09 15:22 importer/migrations/__init__.py
 -rw-r--r--  2.0 unx        0 b- defN 24-Jan-09 15:22 importer/tests/__init__.py
 -rw-r--r--  2.0 unx     1814 b- defN 24-Jan-09 15:22 importer/tests/utils.py
 -rw-r--r--  2.0 unx        0 b- defN 24-Jan-09 15:22 importer/tests/end2end/__init__.py
--rw-r--r--  2.0 unx    10785 b- defN 24-Feb-14 10:56 importer/tests/end2end/test_end2end.py
+-rw-r--r--  2.0 unx    16065 b- defN 24-May-23 07:50 importer/tests/end2end/test_end2end.py
 -rw-r--r--  2.0 unx     8000 b- defN 24-Feb-14 10:56 importer/tests/end2end/test_end2end_copy.py
 -rw-r--r--  2.0 unx        0 b- defN 24-Jan-09 15:22 importer/tests/unit/__init__.py
 -rw-r--r--  2.0 unx     1672 b- defN 24-Feb-14 10:56 importer/tests/unit/test_models.py
 -rw-r--r--  2.0 unx    14036 b- defN 24-Mar-27 10:40 importer/tests/unit/test_orchestrator.py
 -rw-r--r--  2.0 unx     3632 b- defN 24-Mar-27 10:40 importer/tests/unit/test_publisher.py
 -rw-r--r--  2.0 unx    26252 b- defN 24-Mar-27 10:40 importer/tests/unit/test_task.py
--rw-r--r--  2.0 unx     1069 b- defN 24-Mar-27 10:41 geonode_importer-1.0.8.dist-info/LICENSE
--rw-r--r--  2.0 unx      891 b- defN 24-Mar-27 10:41 geonode_importer-1.0.8.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Mar-27 10:41 geonode_importer-1.0.8.dist-info/WHEEL
--rw-r--r--  2.0 unx        9 b- defN 24-Mar-27 10:41 geonode_importer-1.0.8.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     7703 b- defN 24-Mar-27 10:41 geonode_importer-1.0.8.dist-info/RECORD
-87 files, 333277 bytes uncompressed, 88309 bytes compressed:  73.5%
+-rw-r--r--  2.0 unx     1069 b- defN 24-May-29 13:27 geonode_importer-1.0.9.dist-info/LICENSE
+-rw-r--r--  2.0 unx      883 b- defN 24-May-29 13:27 geonode_importer-1.0.9.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-May-29 13:27 geonode_importer-1.0.9.dist-info/WHEEL
+-rw-r--r--  2.0 unx        9 b- defN 24-May-29 13:27 geonode_importer-1.0.9.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     7703 b- defN 24-May-29 13:27 geonode_importer-1.0.9.dist-info/RECORD
+87 files, 342244 bytes uncompressed, 89792 bytes compressed:  73.8%
```

## zipnote {}

```diff
@@ -240,23 +240,23 @@
 
 Filename: importer/tests/unit/test_publisher.py
 Comment: 
 
 Filename: importer/tests/unit/test_task.py
 Comment: 
 
-Filename: geonode_importer-1.0.8.dist-info/LICENSE
+Filename: geonode_importer-1.0.9.dist-info/LICENSE
 Comment: 
 
-Filename: geonode_importer-1.0.8.dist-info/METADATA
+Filename: geonode_importer-1.0.9.dist-info/METADATA
 Comment: 
 
-Filename: geonode_importer-1.0.8.dist-info/WHEEL
+Filename: geonode_importer-1.0.9.dist-info/WHEEL
 Comment: 
 
-Filename: geonode_importer-1.0.8.dist-info/top_level.txt
+Filename: geonode_importer-1.0.9.dist-info/top_level.txt
 Comment: 
 
-Filename: geonode_importer-1.0.8.dist-info/RECORD
+Filename: geonode_importer-1.0.9.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## importer/__init__.py

```diff
@@ -16,13 +16,13 @@
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 #########################################################################
 import os
 
 project_dir = os.path.dirname(os.path.abspath(__file__))
 
-VERSION = (1, 0, 8)
+VERSION = (1, 0, 9)
 __version__ = ".".join([str(i) for i in VERSION])
 __author__ = "geosolutions-it"
 __email__ = "info@geosolutionsgroup.com"
 __url__ = "https://github.com/GeoNode/geonode-importer"
 default_app_config = "importer.apps.ImporterConfig"
```

## importer/celery_tasks.py

```diff
@@ -217,15 +217,14 @@
             step=gettext_lazy("importer.publish_resource"),
             celery_task_request=self.request,
         )
         _exec = orchestrator.get_execution_object(execution_id)
         _files = _exec.input_params.get("files")
         _overwrite = _exec.input_params.get("overwrite_existing_layer")
 
-        # for now we dont heve the overwrite option in GS, skipping will we talk with the GS team
         _publisher = DataPublisher(handler_module_path)
 
         # extracting the crs and the resource name, are needed for publish the resource
         data = _publisher.extract_resource_to_publish(
             _files, action, layer_name, alternate, **kwargs
         )
         if data:
```

## importer/publisher.py

```diff
@@ -25,14 +25,16 @@
         _user, _password = ogc_server_settings.credentials
 
         self.cat = Catalog(
             service_url=ogc_server_settings.rest, username=_user, password=_password
         )
         self.workspace = self._get_default_workspace(create=True)
 
+        self.store = None
+
         if handler_module_path is not None:
             self.handler = import_string(handler_module_path)()
 
     def extract_resource_to_publish(
         self, files: dict, action: str, layer_name, alternate=None, **kwargs
     ):
         """
@@ -46,43 +48,43 @@
         """
 
         return self.handler.extract_resource_to_publish(
             files, action, layer_name, alternate, **kwargs
         )
 
     def get_resource(self, resource_name, return_bool=True) -> bool:
-        self.get_or_create_store()
+        self.get_or_create_store(default=resource_name)
         _res = self.cat.get_resource(
             resource_name, store=self.store, workspace=self.workspace
         )
         if return_bool:
             return True if _res else False
         return _res
 
     def publish_resources(self, resources: List[str]):
         """
         Given a list of strings (which rappresent the table on geoserver)
         Will publish the resorces on geoserver
         """
-        self.get_or_create_store()
+        self.get_or_create_store(default=resources[0]["name"])
         result = self.handler.publish_resources(
             resources=resources,
             catalog=self.cat,
             store=self.store,
             workspace=self.workspace,
         )
         self.sanity_checks(resources)
         return result
 
     def overwrite_resources(self, resources: List[str]):
         """
         We dont need to do anything for now. The data is replaced via ogr2ogr
         """
-        self.get_or_create_store()
         for _resource in resources:
+            self.get_or_create_store(default=_resource["name"])
             result = self.handler.overwrite_geoserver_resource(
                 resource=_resource,
                 catalog=self.cat,
                 store=self.store,
                 workspace=self.workspace,
             )
         self.sanity_checks(resources)
@@ -104,33 +106,48 @@
                 workspace=os.getenv(
                     "DEFAULT_WORKSPACE", os.getenv("CASCADE_WORKSPACE", "geonode")
                 ),
             )
         if store:
             self.cat.delete(store, purge="all", recurse=True)
 
-    def get_or_create_store(self):
+    def get_or_create_store(self, default=None):
         """
         Evaluate if the store exists. if not is created
         """
-        geodatabase = os.environ.get("GEONODE_GEODATABASE", "geonode_data")
-        self.store = self.cat.get_store(name=geodatabase, workspace=self.workspace)
+        store_name, to_be_created = self.handler.get_geoserver_store_name(
+            default=default
+        )
+
+        if self.store and self.store.name == store_name:
+            # if we already initialize the store, we can skip the checks
+            return self.store
+
+        if store_name is not None and not to_be_created:
+            # If the store name is provided by the handler, we retrieve the store
+            # from geoserver. This is usually used for raster layers
+            # for raster we dont want to create the store upfront since the pulishing
+            # is going to create it
+            self.store = self.cat.get_store(name=store_name, workspace=self.workspace)
+            return
+
+        self.store = self.cat.get_store(name=store_name, workspace=self.workspace)
         if not self.store:
-            logger.warning(f"The store does not exists: {geodatabase} creating...")
+            logger.warning(f"The store does not exists: {store_name} creating...")
             self.store = create_geoserver_db_featurestore(
-                store_name=geodatabase, workspace=self.workspace.name
+                store_name=store_name, workspace=self.workspace.name
             )
 
     def publish_geoserver_view(
         self, layer_name, crs, view_name, sql=None, geometry=None
     ):
         """
         Let the handler create a geoserver view given the input parameters
         """
-        self.get_or_create_store()
+        self.get_or_create_store(default=layer_name)
 
         return self.handler.publish_geoserver_view(
             catalog=self.cat,
             workspace=self.workspace,
             datastore=self.store,
             layer_name=layer_name,
             crs=crs,
```

## importer/api/serializer.py

```diff
@@ -11,17 +11,17 @@
         fields = (
             "base_file",
             "xml_file",
             "sld_file",
             "store_spatial_files",
             "overwrite_existing_layer",
             "skip_existing_layers",
-            "source"
+            "source",
         )
 
     base_file = serializers.FileField()
     xml_file = serializers.FileField(required=False)
     sld_file = serializers.FileField(required=False)
     store_spatial_files = serializers.BooleanField(required=False, default=True)
     overwrite_existing_layer = serializers.BooleanField(required=False, default=False)
     skip_existing_layers = serializers.BooleanField(required=False, default=False)
-    source = serializers.CharField(required=False, default='upload')
+    source = serializers.CharField(required=False, default="upload")
```

## importer/api/views.py

```diff
@@ -144,15 +144,15 @@
                     input_params={
                         **{"files": files, "handler_module_path": str(handler)},
                         **extracted_params,
                     },
                     legacy_upload_name=_file.name,
                     action=action,
                     name=_file.name,
-                    source=extracted_params.get('source'),
+                    source=extracted_params.get("source"),
                 )
 
                 sig = import_orchestrator.s(
                     files, str(execution_id), handler=str(handler), action=action
                 )
                 sig.apply_async()
                 return Response(data={"execution_id": execution_id}, status=201)
```

## importer/handlers/base.py

```diff
@@ -1,14 +1,16 @@
 from abc import ABC
 import logging
 from typing import List
 
 from geonode.resource.enumerator import ExecutionRequestAction as exa
 from geonode.layers.models import Dataset
 from importer.utils import ImporterRequestAction as ira
+from django_celery_results.models import TaskResult
+from django.db.models import Q
 
 logger = logging.getLogger(__name__)
 
 
 class BaseHandler(ABC):
     """
     Base abstract handler object
@@ -113,14 +115,46 @@
     def extract_params_from_data(_data):
         """
         Remove from the _data the params that needs to save into the executionRequest object
         all the other are returned
         """
         return []
 
+    @staticmethod
+    def perform_last_step(execution_id):
+        """
+        Override this method if there is some extra step to perform
+        before considering the execution as completed.
+        For example can be used to trigger an email-send to notify
+        that the execution is completed
+        """
+        from importer.orchestrator import orchestrator
+        from importer.models import ResourceHandlerInfo
+
+        # as last step, we delete the celery task to keep the number of rows under control
+        lower_exec_id = execution_id.replace("-", "_").lower()
+        TaskResult.objects.filter(
+            Q(task_args__icontains=lower_exec_id)
+            | Q(task_kwargs__icontains=lower_exec_id)
+            | Q(result__icontains=lower_exec_id)
+            | Q(task_args__icontains=execution_id)
+            | Q(task_kwargs__icontains=execution_id)
+            | Q(result__icontains=execution_id)
+        ).delete()
+
+        _exec = orchestrator.get_execution_object(execution_id)
+
+        resource_output_params = [
+            {"detail_url": x.resource.detail_url, "id": x.resource.pk}
+            for x in ResourceHandlerInfo.objects.filter(execution_request=_exec)
+        ]
+        _exec.output_params.update({"resources": resource_output_params})
+        _exec.save()
+        return _exec
+
     def fixup_name(self, name):
         """
         Emulate the LAUNDER option in ogr2ogr which will normalize the string.
         This is enriched with additional transformation for parentesis.
         The basic normalized function can be found here
         https://github.com/OSGeo/gdal/blob/0fc262675051b63f96c91ca920d27503655dfb7b/ogr/ogrsf_frmts/pgdump/ogrpgdumpdatasource.cpp#L130  # noqa
```

## importer/handlers/common/metadata.py

```diff
@@ -51,25 +51,15 @@
             "overwrite_existing_layer": _data.pop("overwrite_existing_layer", "False"),
             "store_spatial_file": _data.pop("store_spatial_files", "True"),
             "source": _data.pop("source", "resource_file_upload"),
         }, _data
 
     @staticmethod
     def perform_last_step(execution_id):
-        _exec = orchestrator.get_execution_object(execution_id)
-        
-        _exec.output_params.update(
-            **{
-                "detail_url": [
-                    x.resource.detail_url
-                    for x in ResourceHandlerInfo.objects.filter(execution_request=_exec)
-                ]
-            }
-        )
-        _exec.save()
+        BaseHandler.perform_last_step(execution_id=execution_id)
 
     def import_resource(self, files: dict, execution_id: str, **kwargs):
         _exec = orchestrator.get_execution_object(execution_id)
         # getting the dataset
         alternate = _exec.input_params.get("dataset_title")
         resource_id = _exec.input_params.get("resource_id")
         if resource_id:
@@ -77,15 +67,15 @@
         elif alternate:
             dataset = get_object_or_404(Dataset, alternate=alternate)
 
         # retrieving the handler used for the dataset
         original_handler = orchestrator.load_handler(
             dataset.resourcehandlerinfo_set.first().handler_module_path
         )()
-        
+
         ResourceHandlerInfo.objects.create(
             handler_module_path=dataset.resourcehandlerinfo_set.first().handler_module_path,
             resource=dataset,
             execution_request=_exec,
             kwargs=kwargs.get("kwargs", {}) or kwargs,
         )
```

## importer/handlers/common/raster.py

```diff
@@ -5,15 +5,14 @@
 import logging
 from pathlib import Path
 from subprocess import PIPE, Popen
 from typing import List
 
 from django.conf import settings
 from django.db.models import Q
-from django_celery_results.models import TaskResult
 from geonode.base.models import ResourceBase
 from geonode.layers.models import Dataset
 from geonode.resource.enumerator import ExecutionRequestAction as exa
 from geonode.resource.manager import resource_manager
 from geonode.resource.models import ExecutionRequest
 from importer.api.exception import ImportException
 from importer.celery_tasks import ErrorBaseTaskClass, import_orchestrator
@@ -42,14 +41,24 @@
         return "geometry"
 
     @property
     def supported_file_extension_config(self):
         return NotImplementedError
 
     @staticmethod
+    def get_geoserver_store_name(default=None):
+        """
+        Method that return the base store name where to save the data in geoserver
+        and a boolean to know if the store should be created.
+        For raster, the store is created during the geoserver publishing
+        so we dont want to created it upfront
+        """
+        return default, False
+
+    @staticmethod
     def is_valid(files, user):
         """
         Define basic validation steps
         """
         result = Popen("gdal_translate --version", stdout=PIPE, stderr=PIPE, shell=True)
         _, stderr = result.communicate()
         if stderr:
@@ -172,43 +181,15 @@
         # it should delete the image from the geoserver data dir
         # for now we can rely on the geonode delete behaviour
         # since the file is stored on local
         pass
 
     @staticmethod
     def perform_last_step(execution_id):
-        """
-        Override this method if there is some extra step to perform
-        before considering the execution as completed.
-        For example can be used to trigger an email-send to notify
-        that the execution is completed
-        """
-        # as last step, we delete the celery task to keep the number of rows under control
-        lower_exec_id = execution_id.replace("-", "_").lower()
-        TaskResult.objects.filter(
-            Q(task_args__icontains=lower_exec_id)
-            | Q(task_kwargs__icontains=lower_exec_id)
-            | Q(result__icontains=lower_exec_id)
-            | Q(task_args__icontains=execution_id)
-            | Q(task_kwargs__icontains=execution_id)
-            | Q(result__icontains=execution_id)
-        ).delete()
-
-        _exec = orchestrator.get_execution_object(execution_id)
-
-        _exec.save()
-
-        _exec.output_params.update(
-            **{
-                "detail_url": [
-                    x.resource.detail_url
-                    for x in ResourceHandlerInfo.objects.filter(execution_request=_exec)
-                ]
-            }
-        )
+        BaseHandler.perform_last_step(execution_id=execution_id)
 
     def extract_resource_to_publish(
         self, files, action, layer_name, alternate, **kwargs
     ):
         if action == exa.COPY.value:
             return [
                 {
@@ -294,15 +275,18 @@
                 user_datasets = Dataset.objects.filter(
                     owner=_exec.user, alternate=f"{workspace.name}:{layer_name}"
                 )
 
                 dataset_exists = user_datasets.exists()
 
                 if dataset_exists and should_be_overwritten:
-                    layer_name, alternate = layer_name, user_datasets.first().alternate
+                    layer_name, alternate = (
+                        layer_name,
+                        user_datasets.first().alternate.split(":")[-1],
+                    )
                 elif not dataset_exists:
                     alternate = layer_name
                 else:
                     alternate = create_alternate(layer_name, execution_id)
 
                 import_orchestrator.apply_async(
                     (
```

## importer/handlers/common/tests_vector.py

```diff
@@ -3,14 +3,15 @@
 from celery.canvas import Signature
 from celery import group
 from django.test import TestCase
 from mock import MagicMock, patch
 from importer.handlers.common.vector import BaseVectorFileHandler, import_with_ogr2ogr
 from django.contrib.auth import get_user_model
 from importer import project_dir
+from importer.handlers.gpkg.handler import GPKGFileHandler
 from importer.orchestrator import orchestrator
 from geonode.base.populate_test_data import create_single_dataset
 from geonode.resource.models import ExecutionRequest
 from dynamic_models.models import ModelSchema
 from osgeo import ogr
 
 
@@ -19,14 +20,15 @@
 
     @classmethod
     def setUpClass(cls):
         super().setUpClass()
         cls.handler = BaseVectorFileHandler()
         cls.valid_gpkg = f"{project_dir}/tests/fixture/valid.gpkg"
         cls.invalid_gpkg = f"{project_dir}/tests/fixture/invalid.gpkg"
+        cls.no_crs_gpkg = f"{project_dir}/tests/fixture/noCrsTable.gpkg"
         cls.user, _ = get_user_model().objects.get_or_create(username="admin")
         cls.invalid_files = {"base_file": cls.invalid_gpkg}
         cls.valid_files = {"base_file": cls.valid_gpkg}
         cls.owner = get_user_model().objects.first()
         cls.layer = create_single_dataset(
             name="stazioni_metropolitana", owner=cls.owner
         )
@@ -152,17 +154,23 @@
             exec_id = orchestrator.create_execution_request(
                 user=get_user_model().objects.first(),
                 func_name="funct1",
                 step="step",
                 input_params={"files": self.valid_files, "skip_existing_layer": True},
             )
 
-            # start the resource import
-            self.handler.import_resource(
-                files=self.valid_files, execution_id=str(exec_id)
+            with self.assertRaises(Exception) as exception:
+                # start the resource import
+                self.handler.import_resource(
+                    files=self.valid_files, execution_id=str(exec_id)
+                )
+            self.assertIn(
+                "No valid layers found",
+                exception.exception.args[0],
+                "No valid layers found.",
             )
 
             celery_chord.assert_not_called()
         finally:
             if exec_id:
                 ExecutionRequest.objects.filter(exec_id=exec_id).delete()
 
@@ -292,7 +300,53 @@
 
         _open.assert_called_once()
         _call_as_string = _open.mock_calls[0][1][0]
 
         self.assertTrue("-f PGDump /vsistdout/" in _call_as_string)
         self.assertTrue("psql -d" in _call_as_string)
         self.assertFalse("-f PostgreSQL PG" in _call_as_string)
+
+    def test_select_valid_layers(self):
+        """
+        The function should return only the datasets with a geometry
+        The other one are discarded
+        """
+        all_layers = GPKGFileHandler().get_ogr2ogr_driver().Open(self.no_crs_gpkg)
+
+        with self.assertLogs(level="ERROR") as _log:
+            valid_layer = GPKGFileHandler()._select_valid_layers(all_layers)
+
+        self.assertIn(
+            "The following layer layer_styles does not have a Coordinate Reference System (CRS) and will be skipped.",
+            [x.message for x in _log.records],
+        )
+        self.assertEqual(1, len(valid_layer))
+        self.assertEqual("mattia_test", valid_layer[0].GetName())
+
+    def test_perform_last_step(self):
+        """
+        Output params in perform_last_step should return the detail_url and the ID
+        of the resource created
+        """
+        # creating exec_id for the import
+        exec_id = orchestrator.create_execution_request(
+            user=get_user_model().objects.first(),
+            func_name="funct1",
+            step="step",
+            input_params={"files": self.valid_files, "store_spatial_file": True},
+        )
+
+        # create_geonode_resource
+        resource = self.handler.create_geonode_resource(
+            "layer_name",
+            "layer_alternate",
+            str(exec_id),
+        )
+        exec_obj = orchestrator.get_execution_object(str(exec_id))
+        self.handler.create_resourcehandlerinfo(str(self.handler), resource, exec_obj)
+        # calling the last_step
+        self.handler.perform_last_step(str(exec_id))
+        expected_output = {
+            "resources": [{"id": resource.pk, "detail_url": resource.detail_url}]
+        }
+        exec_obj.refresh_from_db()
+        self.assertDictEqual(expected_output, exec_obj.output_params)
```

## importer/handlers/common/vector.py

```diff
@@ -7,15 +7,14 @@
 import logging
 import os
 from subprocess import PIPE, Popen
 from typing import List
 from celery import chord, group
 
 from django.conf import settings
-from django_celery_results.models import TaskResult
 from dynamic_models.models import ModelSchema
 from dynamic_models.schema import ModelSchemaEditor
 from geonode.base.models import ResourceBase
 from geonode.resource.enumerator import ExecutionRequestAction as exa
 from geonode.layers.models import Dataset
 from importer.celery_tasks import ErrorBaseTaskClass, create_dynamic_structure
 from importer.handlers.base import BaseHandler
@@ -52,14 +51,23 @@
         return "geometry"
 
     @property
     def supported_file_extension_config(self):
         return NotImplementedError
 
     @staticmethod
+    def get_geoserver_store_name(default=None):
+        """
+        Method that return the base store name where to save the data in geoserver
+        and a boolean to know if the store should be created.
+        For vector, the store must be created
+        """
+        return os.environ.get("GEONODE_GEODATABASE", "geonode_data"), True
+
+    @staticmethod
     def is_valid(files, user):
         """
         Define basic validation steps
         """
         result = Popen("ogr2ogr --version", stdout=PIPE, stderr=PIPE, shell=True)
         _, stderr = result.communicate()
         if stderr:
@@ -207,36 +215,15 @@
     def perform_last_step(execution_id):
         """
         Override this method if there is some extra step to perform
         before considering the execution as completed.
         For example can be used to trigger an email-send to notify
         that the execution is completed
         """
-        # as last step, we delete the celery task to keep the number of rows under control
-        lower_exec_id = execution_id.replace("-", "_").lower()
-        TaskResult.objects.filter(
-            Q(task_args__icontains=lower_exec_id)
-            | Q(task_kwargs__icontains=lower_exec_id)
-            | Q(result__icontains=lower_exec_id)
-            | Q(task_args__icontains=execution_id)
-            | Q(task_kwargs__icontains=execution_id)
-            | Q(result__icontains=execution_id)
-        ).delete()
-
-        _exec = orchestrator.get_execution_object(execution_id)
-
-        _exec.output_params.update(
-            **{
-                "detail_url": [
-                    x.resource.detail_url
-                    for x in ResourceHandlerInfo.objects.filter(execution_request=_exec)
-                ]
-            }
-        )
-        _exec.save()
+        _exec = BaseHandler.perform_last_step(execution_id=execution_id)
         if _exec and not _exec.input_params.get("store_spatial_file", False):
             resources = ResourceHandlerInfo.objects.filter(execution_request=_exec)
             # getting all files list
             resources_files = list(set(chain(*[x.resource.files for x in resources])))
             # better to delete each single file since it can be a remove storage service
             list(map(storage_manager.delete, resources_files))
 
@@ -301,27 +288,32 @@
 
     def import_resource(self, files: dict, execution_id: str, **kwargs) -> str:
         """
         Main function to import the resource.
         Internally will call the steps required to import the
         data inside the geonode_data database
         """
-        layers = self.get_ogr2ogr_driver().Open(files.get("base_file"))
+        all_layers = self.get_ogr2ogr_driver().Open(files.get("base_file"))
+        layers = self._select_valid_layers(all_layers)
         # for the moment we skip the dyanamic model creation
         layer_count = len(layers)
         logger.info(f"Total number of layers available: {layer_count}")
         _exec = self._get_execution_request_object(execution_id)
         _input = {**_exec.input_params, **{"total_layers": layer_count}}
         orchestrator.update_execution_request_status(
             execution_id=str(execution_id), input_params=_input
         )
         dynamic_model = None
         celery_group = None
         try:
+            if len(layers) == 0:
+                raise Exception("No valid layers found")
+
             # start looping on the layers available
+
             for index, layer in enumerate(layers, start=1):
                 layer_name = self.fixup_name(layer.GetName())
 
                 should_be_overwritten = _exec.input_params.get(
                     "overwrite_existing_layer"
                 )
                 # should_be_imported check if the user+layername already exists or not
@@ -393,14 +385,28 @@
                 In case of fail, we want to delete the dynamic_model schema and his field
                 to keep the DB in a consistent state
                 """
                 drop_dynamic_model_schema(dynamic_model)
             raise e
         return
 
+    def _select_valid_layers(self, all_layers):
+        layers = []
+        for layer in all_layers:
+            try:
+                self.identify_authority(layer)
+                layers.append(layer)
+            except Exception as e:
+                logger.error(e)
+                logger.error(
+                    f"The following layer {layer.GetName()} does not have a Coordinate Reference System (CRS) and will be skipped."
+                )
+                pass
+        return layers
+
     def find_alternate_by_dataset(self, _exec_obj, layer_name, should_be_overwritten):
         workspace = DataPublisher(None).workspace
         dataset_available = Dataset.objects.filter(
             alternate__iexact=f"{workspace.name}:{layer_name}"
         )
 
         dataset_exists = dataset_available.exists()
```

## importer/handlers/csv/handler.py

```diff
@@ -240,13 +240,20 @@
         layers = self.get_ogr2ogr_driver().Open(files.get("base_file"), 0)
         if not layers:
             return []
         return [
             {
                 "name": alternate or layer_name,
                 "crs": (
-                    self.identify_authority(_l) if _l.GetSpatialRef() else "EPSG:4326"
+                    self.identify_authority(_l)
                 ),
             }
             for _l in layers
             if self.fixup_name(_l.GetName()) == layer_name
         ]
+
+    def identify_authority(self, layer):
+        try:
+            authority_code = super().identify_authority(layer=layer)
+            return authority_code
+        except Exception:
+            return "EPSG:4326"
```

## importer/handlers/shapefile/serializer.py

```diff
@@ -14,20 +14,20 @@
             "shx_file",
             "prj_file",
             "xml_file",
             "sld_file",
             "store_spatial_files",
             "overwrite_existing_layer",
             "skip_existing_layers",
-            "source"
+            "source",
         )
 
     base_file = serializers.FileField()
     dbf_file = serializers.FileField()
     shx_file = serializers.FileField()
     prj_file = serializers.FileField()
     xml_file = serializers.FileField(required=False)
     sld_file = serializers.FileField(required=False)
     store_spatial_files = serializers.BooleanField(required=False, default=True)
     overwrite_existing_layer = serializers.BooleanField(required=False, default=False)
     skip_existing_layers = serializers.BooleanField(required=False, default=False)
-    source = serializers.CharField(required=False, default='upload')
+    source = serializers.CharField(required=False, default="upload")
```

## importer/handlers/xml/serializer.py

```diff
@@ -8,8 +8,8 @@
         ref_name = "MetadataFileSerializer"
         model = Upload
         view_name = "importer_upload"
         fields = ("dataset_title", "base_file", "source")
 
     base_file = serializers.FileField()
     dataset_title = serializers.CharField(required=True)
-    source = serializers.CharField(required=False, default='resource_file_upload')
+    source = serializers.CharField(required=False, default="resource_file_upload")
```

## importer/tests/end2end/test_end2end.py

```diff
@@ -22,23 +22,26 @@
 
 class BaseImporterEndToEndTest(ImporterBaseTestSupport):
     @classmethod
     def setUpClass(cls) -> None:
         super().setUpClass()
         cls.valid_gkpg = f"{project_dir}/tests/fixture/valid.gpkg"
         cls.valid_geojson = f"{project_dir}/tests/fixture/valid.geojson"
+        cls.no_crs_gpkg = f"{project_dir}/tests/fixture/noCrsTable.gpkg"
         file_path = gisdata.VECTOR_DATA
         filename = os.path.join(file_path, "san_andres_y_providencia_highway.shp")
         cls.valid_shp = {
             "base_file": filename,
             "dbf_file": f"{file_path}/san_andres_y_providencia_highway.dbf",
             "prj_file": f"{file_path}/san_andres_y_providencia_highway.prj",
             "shx_file": f"{file_path}/san_andres_y_providencia_highway.shx",
         }
         cls.valid_kml = f"{project_dir}/tests/fixture/valid.kml"
+        cls.valid_tif = f"{project_dir}/tests/fixture/test_grid.tif"
+        cls.valid_csv = f"{project_dir}/tests/fixture/valid.csv"
 
         cls.url = reverse("importer_upload")
         ogc_server_settings = OGC_Servers_Handler(settings.OGC_SERVER)["default"]
 
         _user, _password = ogc_server_settings.credentials
 
         cls.cat = Catalog(
@@ -97,16 +100,16 @@
 
             # check if the geonode resource exists
             dataset = Dataset.objects.filter(
                 alternate__icontains=f"geonode:{initial_name}"
             )
             self.assertTrue(dataset.exists())
 
-            # check if the resource is in geoserver
             resources = self.cat.get_resources()
+            # check if the resource is in geoserver
             self.assertTrue(dataset.first().title in [y.name for y in resources])
             if overwrite:
                 self.assertTrue(dataset.first().last_updated > last_update)
         finally:
             dataset = Dataset.objects.filter(
                 alternate__icontains=f"geonode:{initial_name}"
             )
@@ -150,14 +153,74 @@
             payload, initial_name, overwrite=True, last_update=prev_dataset.last_updated
         )
         layer = self.cat.get_layer("geonode:stazioni_metropolitana")
         if layer:
             self.cat.delete(layer)
 
 
+class ImporterNoCRSImportTest(BaseImporterEndToEndTest):
+    @override_settings(ASYNC_SIGNALS=False)
+    @mock.patch.dict(os.environ, {"GEONODE_GEODATABASE": "test_geonode_data"})
+    @override_settings(
+        GEODATABASE_URL=f"{geourl.split('/geonode_data')[0]}/test_geonode_data"
+    )
+    def test_import_geopackage_with_no_crs_table(self):
+        layer = self.cat.get_layer("geonode:mattia_test")
+        if layer:
+            self.cat.delete(layer)
+        payload = {
+            "base_file": open(self.no_crs_gpkg, "rb"),
+        }
+        initial_name = "mattia_test"
+        with self.assertLogs(level="ERROR") as _log:
+            self._assertimport(payload, initial_name)
+
+        self.assertIn(
+            "The following layer layer_styles does not have a Coordinate Reference System (CRS) and will be skipped.",
+            [x.message for x in _log.records],
+        )
+        layer = self.cat.get_layer("geonode:mattia_test")
+        if layer:
+            self.cat.delete(layer)
+
+    @override_settings(ASYNC_SIGNALS=False)
+    @mock.patch.dict(os.environ, {"GEONODE_GEODATABASE": "test_geonode_data"})
+    @override_settings(
+        GEODATABASE_URL=f"{geourl.split('/geonode_data')[0]}/test_geonode_data"
+    )
+    @mock.patch(
+        "importer.handlers.common.vector.BaseVectorFileHandler._select_valid_layers"
+    )
+    def test_import_geopackage_with_no_crs_table_should_raise_error_if_all_layer_are_invalid(
+        self, _select_valid_layers
+    ):
+        _select_valid_layers.return_value = []
+        layer = self.cat.get_layer("geonode:mattia_test")
+        if layer:
+            self.cat.delete(layer)
+
+        payload = {
+            "base_file": open(self.no_crs_gpkg, "rb"),
+        }
+
+        with self.assertLogs(level="ERROR") as _log:
+            self.client.force_login(self.admin)
+
+            response = self.client.post(self.url, data=payload)
+            self.assertEqual(500, response.status_code)
+
+        self.assertIn(
+            "No valid layers found",
+            [x.message for x in _log.records],
+        )
+        layer = self.cat.get_layer("geonode:mattia_test")
+        if layer:
+            self.cat.delete(layer)
+
+
 class ImporterGeoJsonImportTest(BaseImporterEndToEndTest):
     @mock.patch.dict(os.environ, {"GEONODE_GEODATABASE": "test_geonode_data"})
     @override_settings(
         GEODATABASE_URL=f"{geourl.split('/geonode_data')[0]}/test_geonode_data"
     )
     def test_import_geojson(self):
         layer = self.cat.get_layer("geonode:valid")
@@ -192,14 +255,56 @@
             payload, initial_name, overwrite=True, last_update=prev_dataset.last_updated
         )
         layer = self.cat.get_layer("geonode:valid")
         if layer:
             self.cat.delete(layer)
 
 
+class ImporterGCSVImportTest(BaseImporterEndToEndTest):
+    @mock.patch.dict(os.environ, {"GEONODE_GEODATABASE": "test_geonode_data"})
+    @override_settings(
+        GEODATABASE_URL=f"{geourl.split('/geonode_data')[0]}/test_geonode_data"
+    )
+    def test_import_geojson(self):
+        layer = self.cat.get_layer("geonode:valid")
+        if layer:
+            self.cat.delete(layer)
+
+        payload = {
+            "base_file": open(self.valid_csv, "rb"),
+        }
+        initial_name = "valid"
+        self._assertimport(payload, initial_name)
+        layer = self.cat.get_layer("geonode:valid")
+        if layer:
+            self.cat.delete(layer)
+
+    @mock.patch.dict(os.environ, {"GEONODE_GEODATABASE": "test_geonode_data"})
+    @override_settings(
+        GEODATABASE_URL=f"{geourl.split('/geonode_data')[0]}/test_geonode_data"
+    )
+    def test_import_csv_overwrite(self):
+        prev_dataset = create_single_dataset(name="valid")
+
+        layer = self.cat.get_layer("geonode:valid")
+        if layer:
+            self.cat.delete(layer)
+        payload = {
+            "base_file": open(self.valid_csv, "rb"),
+        }
+        initial_name = "valid"
+        payload["overwrite_existing_layer"] = True
+        self._assertimport(
+            payload, initial_name, overwrite=True, last_update=prev_dataset.last_updated
+        )
+        layer = self.cat.get_layer("geonode:valid")
+        if layer:
+            self.cat.delete(layer)
+
+
 class ImporterKMLImportTest(BaseImporterEndToEndTest):
     @mock.patch.dict(os.environ, {"GEONODE_GEODATABASE": "test_geonode_data"})
     @override_settings(
         GEODATABASE_URL=f"{geourl.split('/geonode_data')[0]}/test_geonode_data"
     )
     def test_import_kml(self):
         layer = self.cat.get_layer("geonode:sample_point_dataset")
@@ -273,7 +378,49 @@
         payload["overwrite_existing_layer"] = True
         self._assertimport(
             payload, initial_name, overwrite=True, last_update=prev_dataset.last_updated
         )
         layer = self.cat.get_layer("geonode:san_andres_y_providencia_highway")
         if layer:
             self.cat.delete(layer)
+
+
+class ImporterRasterImportTest(BaseImporterEndToEndTest):
+    @mock.patch.dict(os.environ, {"GEONODE_GEODATABASE": "test_geonode_data"})
+    @override_settings(
+        GEODATABASE_URL=f"{geourl.split('/geonode_data')[0]}/test_geonode_data"
+    )
+    def test_import_raster(self):
+        layer = self.cat.get_layer("test_grid")
+        if layer:
+            self.cat.delete(layer)
+
+        payload = {
+            "base_file": open(self.valid_tif, "rb"),
+        }
+        initial_name = "test_grid"
+        self._assertimport(payload, initial_name)
+        layer = self.cat.get_layer("test_grid")
+        if layer:
+            self.cat.delete(layer)
+
+    @mock.patch.dict(os.environ, {"GEONODE_GEODATABASE": "test_geonode_data"})
+    @override_settings(
+        GEODATABASE_URL=f"{geourl.split('/geonode_data')[0]}/test_geonode_data"
+    )
+    def test_import_raster_overwrite(self):
+        prev_dataset = create_single_dataset(name="test_grid")
+
+        layer = self.cat.get_layer("test_grid")
+        if layer:
+            self.cat.delete(layer)
+        payload = {
+            "base_file": open(self.valid_tif, "rb"),
+        }
+        initial_name = "test_grid"
+        payload["overwrite_existing_layer"] = True
+        self._assertimport(
+            payload, initial_name, overwrite=True, last_update=prev_dataset.last_updated
+        )
+        layer = self.cat.get_layer("test_grid")
+        if layer:
+            self.cat.delete(layer)
```

## Comparing `geonode_importer-1.0.8.dist-info/LICENSE` & `geonode_importer-1.0.9.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `geonode_importer-1.0.8.dist-info/METADATA` & `geonode_importer-1.0.9.dist-info/METADATA`

 * *Files 23% similar despite different names*

```diff
@@ -1,23 +1,23 @@
 Metadata-Version: 2.1
 Name: geonode-importer
-Version: 1.0.8
+Version: 1.0.9
 Home-page: https://github.com/GeoNode/geonode-importer
 Author: geosolutions-it
 Author-email: info@geosolutionsgroup.com
 Project-URL: Bug Tracker, https://github.com/geosolutions-it/geonode-importer/issues
 Platform: any
 Classifier: Environment :: Web Environment
 Classifier: Framework :: Django :: 3.2
 Classifier: Framework :: Django :: 4.2
 Classifier: Programming Language :: Python
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Programming Language :: Python :: 3.10
 Description-Content-Type: text/markdown
 License-File: LICENSE
-Requires-Dist: setuptools (>=59)
-Requires-Dist: gdal (<=3.4.3)
-Requires-Dist: pdok-geopackage-validator (==0.8.5)
-Requires-Dist: geonode-django-dynamic-model (==0.4.0)
+Requires-Dist: setuptools >=59
+Requires-Dist: gdal <=3.4.3
+Requires-Dist: pdok-geopackage-validator ==0.8.5
+Requires-Dist: geonode-django-dynamic-model ==0.4.0
 
 A GeoNode 4.0+ app that implements a brand new upload/import flow
```

## Comparing `geonode_importer-1.0.8.dist-info/RECORD` & `geonode_importer-1.0.9.dist-info/RECORD`

 * *Files 4% similar despite different names*

```diff
@@ -1,40 +1,40 @@
-importer/__init__.py,sha256=J2Zlr4Vy4pV24vzXQBv-0m2dAp2eqw-hns-fX_Qc9RQ,1132
+importer/__init__.py,sha256=cl-pyP9C5cCdJLLvGHHSPbjogyIea9hAUqvhbGflu9c,1132
 importer/apps.py,sha256=bgxz5mWaIPbIC_jfXKKijHR6vGVBttJ4_RrOTSDyMWI,894
 importer/celery_app.py,sha256=vXVNHq_WntdNstgbKbVVJsOhTllf__ZWr-skmDohJvE,301
-importer/celery_tasks.py,sha256=zZb5XFnLglJD3vgDdsaxzdsZQU9wSWhwzlTJgrvla0Q,25978
+importer/celery_tasks.py,sha256=BdXFONkyfX9Lbkedg1q8BMrNDsDp9QdUirZz8gtJHHc,25880
 importer/datastore.py,sha256=m2jAY2_sgewRyJ5_E1RpVHAK2vcOKb2AZJng4bwixRc,1137
 importer/db_router.py,sha256=zY65wik8Z6ptJ3MggYxp4isFt11ohmk_5t2pIS9MBeA,2137
 importer/models.py,sha256=fFFzCr7uReM0DIx5apSsH5Mc1uKZsLPMmcjwl1AxQHQ,1574
 importer/orchestrator.py,sha256=d7mtOJ2_a1jwBewBlrIEsDSrkCzXJoia8yGgSrM9vX8,14508
-importer/publisher.py,sha256=l6UmIbKHDZqrqcuzbXqtrSY7Plx3vWpYfY_898JKwB0,6088
+importer/publisher.py,sha256=cl_ZmKcT2_tqBZT3RyrwNdRp8x9ironnLQNu5ZfNYEM,6858
 importer/settings.py,sha256=LNbTsJn01-yf162a9F3LSVWefKlbEgXpRiSyrpo34j0,399
 importer/urls.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 importer/utils.py,sha256=PFAnOMnojHgZMhLatew2fO1dQNRAS6RDEvXTzvTVAcU,1998
 importer/views.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 importer/api/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 importer/api/exception.py,sha256=cvkC0bnGK0JFzHg8yLhmoniB3Vr3XU65TcU_zLrPZn4,1675
-importer/api/serializer.py,sha256=mOJfN3EPV8EBLMJOpNiT3lDKnf5jEikhFs3ZtTtrL4I,1010
+importer/api/serializer.py,sha256=RXqgsAmiEUIDcjL1oiZwTtbb12ByTg5pGw5HN8ehfc0,1011
 importer/api/tests.py,sha256=g7A0i0OWdKgVp-9r8aMQUUjFdrtibx08Ala093JvwIc,5844
 importer/api/urls.py,sha256=0CusAv5v8sM1j6SKKSoeajp1W4fkBcTRCiOcY9L-r68,486
-importer/api/views.py,sha256=7ITmqeCGJPcHPUNCBT11J_jD6YqdFJJth74ezsoXMSM,10559
+importer/api/views.py,sha256=QGL_DLAN8gLT6aHSHEMUHjAKXZVH7dzmYVngEmeVfWk,10559
 importer/handlers/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 importer/handlers/apps.py,sha256=QUXcTIP5D9CzQulkOT6MCW9m1kB1iJoGMbZek8EMj8I,2769
-importer/handlers/base.py,sha256=w6556FSs899DOqFVnzAZKj6o0WLUREF1_cqKk5SbTGY,6628
+importer/handlers/base.py,sha256=mqEV7F_hy6BlfC1AjW7QQtIUQjBbzQJhwnu8OkpeVzo,8029
 importer/handlers/tests.py,sha256=ov4UXqVw7r1FzKn356-fXqukLWRNI1Eya9fPpKGGmsE,2786
 importer/handlers/utils.py,sha256=pXT4IJQcJnekqbrqV4VdJxFL_zyyRfgyrzw7Mv5DFiw,5260
 importer/handlers/common/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-importer/handlers/common/metadata.py,sha256=KRpoMlbT-ldKt2fN7z4TJbUBuE1skAXvpyoEeanUZgU,3611
-importer/handlers/common/raster.py,sha256=K6_f_9JUprUOI9Vhp1X7jfjKcmOnPW6Dm84SBTHt3mg,22971
+importer/handlers/common/metadata.py,sha256=uuFuzzyMy_s5Su01sZCgMO9A9DHmmq2Oe05HTiLgmjU,3317
+importer/handlers/common/raster.py,sha256=yVQPKmOU6LYHOJ6P8xHcXWQWCFXJUlU9leaa94U2WYY,22345
 importer/handlers/common/tests_raster.py,sha256=a6d-cWz0_SYT0l2-51xUus7GJ-nPeavQbQXHq_Jv98U,3162
-importer/handlers/common/tests_vector.py,sha256=g1VC35g2wDXviDDptEPMglqciwNZztOq0K8iCl8Z_7g,11388
-importer/handlers/common/vector.py,sha256=Nr4ZF8Px6eueVap7Mb-AIHFyBcQqvpzn6hhb7WwpvQI,36968
+importer/handlers/common/tests_vector.py,sha256=f_M0OMlR07wtd2uhzFpXavgDiOl_Kdqy5nChZFaL_Yw,13587
+importer/handlers/common/vector.py,sha256=enHBk5zmTpk4_BU7tDy2D_8KZteVo9XD6YihQGKh8WI,37133
 importer/handlers/csv/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 importer/handlers/csv/exceptions.py,sha256=WKpt-dN1O1OroyUJx7jztuJPxe0FRks5qGpoZlKVucM,284
-importer/handlers/csv/handler.py,sha256=L07pWR-3S91qLCM0SfvsQ7IhAVviIBWmglrK9pAuQx0,9665
+importer/handlers/csv/handler.py,sha256=PZSABTwTHgLrw5RDCOVLErCZfmcvMwRDQhwo2G8UlAI,9841
 importer/handlers/csv/tests.py,sha256=HORbaZne1mIVfH_K9gByTFD3Ox2dJZlpIJPGOl9EnWk,7151
 importer/handlers/geojson/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 importer/handlers/geojson/exceptions.py,sha256=M4JAZguS2u-75i3TAcTyA_biGJW3O6BSW782mAQcI8k,296
 importer/handlers/geojson/handler.py,sha256=8xZqeipDiCL3nC-j0JeL3izaLLeVql6mA_O_lGM3FdE,3542
 importer/handlers/geojson/tests.py,sha256=s-10V3HeS1gRBnofgVdZ7Q8Cog8Zu4nvvXD8OCJC8n0,5585
 importer/handlers/geotiff/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 importer/handlers/geotiff/exceptions.py,sha256=2KGd-1rIJxDNcD4vkRoUdwk6QyA0ML__TivrGQWr8oA,290
@@ -48,40 +48,40 @@
 importer/handlers/kml/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 importer/handlers/kml/exceptions.py,sha256=eCF-oSy3Xk-zKveLpBQtoLQfsHk5gG_9vIufMdDU234,284
 importer/handlers/kml/handler.py,sha256=_a2cvO5rgrOWhwpv_eQXTLND-GllcfXJ_U6ArtHKxe8,4829
 importer/handlers/kml/tests.py,sha256=opipzkoRsWbjHl7GZKNd1AcZ09nuki3ZFXoUCK5fZ5c,4251
 importer/handlers/shapefile/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 importer/handlers/shapefile/exceptions.py,sha256=3zOLlarTh9_xS8cOdf_HdA3mHzeopKSmsSG8AAwJpOg,302
 importer/handlers/shapefile/handler.py,sha256=31L3Tfo-eGxXLjg_2v_L_GHhfNr1JlchxnzTYifa3VE,6673
-importer/handlers/shapefile/serializer.py,sha256=6IinNffWqR5E_vH127caVdezpcOCr1gPlCJzOqizdCM,1201
+importer/handlers/shapefile/serializer.py,sha256=sUY02WELXixO0mi_QiZgIXCP7KGNaBbKU-eR3qoCYsg,1202
 importer/handlers/shapefile/tests.py,sha256=Z2J6KFNYqFiDE3gthalCjnkuPHFPWb9cEsQl8ucn9rs,6435
 importer/handlers/sld/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 importer/handlers/sld/exceptions.py,sha256=beplt1EXGFiF3_IAgePlvo-G7HOYb2Di1ycHY3ssr9U,284
 importer/handlers/sld/handler.py,sha256=_Rz4T_XsPcV1JVBsXig6t3Gs-1HDJARW6qvIr_sgGCg,2004
 importer/handlers/sld/tests.py,sha256=AVBB_ietSon7neMn3hD9MiCyqjLf9sMbT7RwXZdmdj8,3111
 importer/handlers/xml/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 importer/handlers/xml/exceptions.py,sha256=66fKWc1EPrVI1vmeb0EBZYXBTIs14lv0mkTYJfpuwuU,284
 importer/handlers/xml/handler.py,sha256=DxeyrEosHUfpFwF4BGPzxEmu-j10S6jZvWdjvckU3o0,1937
-importer/handlers/xml/serializer.py,sha256=FosBRc47gI7OVcL-EcXYwTQGyqPJd4mro24pquDP6Bg,556
+importer/handlers/xml/serializer.py,sha256=V6bmP27okOqGtZuIuhzPVTwguH8pJ70cYTxN2YEYqRk,556
 importer/handlers/xml/tests.py,sha256=P_AYDuVPe_VCVal4Y-Tub9ucAe4a9ET8VTuXu6a8fDA,3096
 importer/migrations/0001_initial.py,sha256=imLf8_-4c0ntRjtAPH_35Zd3ziiwlNZKtMgkgABY7hM,1030
 importer/migrations/0002_resourcehandlerinfo_kwargs.py,sha256=caoCKkdqcypGnxO6opArpUbFBZ_PBORODlIWzt5p0jA,501
 importer/migrations/0003_resourcehandlerinfo_execution_id.py,sha256=Ya6oRdwZIAQ3IIuGdI3nSl_vTGLrVmZaXql71hmmcgI,673
 importer/migrations/0004_rename_execution_id_resourcehandlerinfo_execution_request.py,sha256=z5Uh7ELS6nTyyFAx4lHzoSOiRegLWZCmyQWzPYTTinQ,408
 importer/migrations/0005_fixup_dynamic_shema_table_names.py,sha256=ngvtC_VZ9cBXE8v_nEENVpLhz98xtREM9WMfKHmnMpY,1056
 importer/migrations/0006_dataset_migration.py,sha256=bzQfT2-XJ0VME6EoClQbjyMDp4AUxmL__8Ig3VgsXTc,1571
 importer/migrations/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 importer/tests/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 importer/tests/utils.py,sha256=6ipagaatvK-sltI_SVTe-aByucvS3sYMtDN4vlo1YZY,1814
 importer/tests/end2end/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-importer/tests/end2end/test_end2end.py,sha256=6FhQyRKCk6NZT3wPFKsOvEevokfnQoXQaQ2GxyoAkBc,10785
+importer/tests/end2end/test_end2end.py,sha256=9cpt-HOLVnw95oYNPSvxx88XEBTyhonuvH3jpC8mumg,16065
 importer/tests/end2end/test_end2end_copy.py,sha256=NQBLLhNWxiSS0kysoPmXJms8ma5mFf6KJN4B__maxSE,8000
 importer/tests/unit/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 importer/tests/unit/test_models.py,sha256=5GTOfa29VblIXkmzyxW-8NbmnYDnNtDQJzOKWpoR7Dk,1672
 importer/tests/unit/test_orchestrator.py,sha256=KF0VBe_ciBlH1apWLK1GMUvl1jENIZze_JOaiVX1zkQ,14036
 importer/tests/unit/test_publisher.py,sha256=EcASIdzzMLbyS4wzKLtCw_ascGwC12DI5dSimUEXUqQ,3632
 importer/tests/unit/test_task.py,sha256=T6SJBUbTRopBcinwHR2uuOc731ovLUozYH5ip9yF6RE,26252
-geonode_importer-1.0.8.dist-info/LICENSE,sha256=gMS2YAekulHmzbeQAk-e6U1_unOcCszd6aKY5bQJXO0,1069
-geonode_importer-1.0.8.dist-info/METADATA,sha256=yolHFnLcEwVlPSMzNI_wXhLX2lq40FHTUZt2sJvIJUw,891
-geonode_importer-1.0.8.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-geonode_importer-1.0.8.dist-info/top_level.txt,sha256=MRQ0MhtKdXF90IDYri2Z5uPJ8A9O0ITe5bEXA2ZLjM4,9
-geonode_importer-1.0.8.dist-info/RECORD,,
+geonode_importer-1.0.9.dist-info/LICENSE,sha256=gMS2YAekulHmzbeQAk-e6U1_unOcCszd6aKY5bQJXO0,1069
+geonode_importer-1.0.9.dist-info/METADATA,sha256=1qkUwbKfcX0BSK6Q10WRU41B0x06e5OvX5P8wQEcij4,883
+geonode_importer-1.0.9.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+geonode_importer-1.0.9.dist-info/top_level.txt,sha256=MRQ0MhtKdXF90IDYri2Z5uPJ8A9O0ITe5bEXA2ZLjM4,9
+geonode_importer-1.0.9.dist-info/RECORD,,
```

