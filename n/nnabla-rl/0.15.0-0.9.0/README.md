# Comparing `tmp/nnabla_rl-0.15.0-py3-none-any.whl.zip` & `tmp/nnabla_rl-0.9.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,375 +1,239 @@
-Zip file size: 694236 bytes, number of entries: 373
--rw-r--r--  2.0 unx      836 b- defN 24-May-29 02:52 nnabla_rl/__init__.py
--rw-r--r--  2.0 unx    12189 b- defN 24-May-29 02:52 nnabla_rl/algorithm.py
--rw-r--r--  2.0 unx     2938 b- defN 24-May-29 02:52 nnabla_rl/configuration.py
--rw-r--r--  2.0 unx     6509 b- defN 24-May-29 02:52 nnabla_rl/environment_explorer.py
--rw-r--r--  2.0 unx     1060 b- defN 24-May-29 02:52 nnabla_rl/exceptions.py
--rw-r--r--  2.0 unx    28446 b- defN 24-May-29 02:52 nnabla_rl/functions.py
--rw-r--r--  2.0 unx     2484 b- defN 24-May-29 02:52 nnabla_rl/hook.py
--rw-r--r--  2.0 unx     5482 b- defN 24-May-29 02:52 nnabla_rl/initializers.py
--rw-r--r--  2.0 unx     1804 b- defN 24-May-29 02:52 nnabla_rl/logger.py
--rw-r--r--  2.0 unx    15213 b- defN 24-May-29 02:52 nnabla_rl/parametric_functions.py
--rw-r--r--  2.0 unx      752 b- defN 24-May-29 02:52 nnabla_rl/random.py
--rw-r--r--  2.0 unx     6414 b- defN 24-May-29 02:52 nnabla_rl/replay_buffer.py
--rw-r--r--  2.0 unx      935 b- defN 24-May-29 02:52 nnabla_rl/scopes.py
--rw-r--r--  2.0 unx     3886 b- defN 24-May-29 02:52 nnabla_rl/typing.py
--rw-r--r--  2.0 unx     1461 b- defN 24-May-29 02:52 nnabla_rl/writer.py
--rw-r--r--  2.0 unx     5737 b- defN 24-May-29 02:52 nnabla_rl/algorithms/__init__.py
--rw-r--r--  2.0 unx    23862 b- defN 24-May-29 02:52 nnabla_rl/algorithms/a2c.py
--rw-r--r--  2.0 unx    70086 b- defN 24-May-29 02:52 nnabla_rl/algorithms/amp.py
--rw-r--r--  2.0 unx    21067 b- defN 24-May-29 02:52 nnabla_rl/algorithms/atrpo.py
--rw-r--r--  2.0 unx    17479 b- defN 24-May-29 02:52 nnabla_rl/algorithms/bcq.py
--rw-r--r--  2.0 unx    21163 b- defN 24-May-29 02:52 nnabla_rl/algorithms/bear.py
--rw-r--r--  2.0 unx     5105 b- defN 24-May-29 02:52 nnabla_rl/algorithms/categorical_ddqn.py
--rw-r--r--  2.0 unx    18081 b- defN 24-May-29 02:52 nnabla_rl/algorithms/categorical_dqn.py
--rw-r--r--  2.0 unx    28406 b- defN 24-May-29 02:52 nnabla_rl/algorithms/common_utils.py
--rw-r--r--  2.0 unx    13634 b- defN 24-May-29 02:52 nnabla_rl/algorithms/ddp.py
--rw-r--r--  2.0 unx    18541 b- defN 24-May-29 02:52 nnabla_rl/algorithms/ddpg.py
--rw-r--r--  2.0 unx     6704 b- defN 24-May-29 02:52 nnabla_rl/algorithms/ddqn.py
--rw-r--r--  2.0 unx    15594 b- defN 24-May-29 02:52 nnabla_rl/algorithms/decision_transformer.py
--rw-r--r--  2.0 unx    34277 b- defN 24-May-29 02:52 nnabla_rl/algorithms/demme_sac.py
--rw-r--r--  2.0 unx    17447 b- defN 24-May-29 02:52 nnabla_rl/algorithms/dqn.py
--rw-r--r--  2.0 unx     6199 b- defN 24-May-29 02:52 nnabla_rl/algorithms/drqn.py
--rw-r--r--  2.0 unx     2188 b- defN 24-May-29 02:52 nnabla_rl/algorithms/dummy.py
--rw-r--r--  2.0 unx    28752 b- defN 24-May-29 02:52 nnabla_rl/algorithms/gail.py
--rw-r--r--  2.0 unx    18321 b- defN 24-May-29 02:52 nnabla_rl/algorithms/her.py
--rw-r--r--  2.0 unx    34671 b- defN 24-May-29 02:52 nnabla_rl/algorithms/hyar.py
--rw-r--r--  2.0 unx    13836 b- defN 24-May-29 02:52 nnabla_rl/algorithms/icml2015_trpo.py
--rw-r--r--  2.0 unx    23292 b- defN 24-May-29 02:52 nnabla_rl/algorithms/icml2018_sac.py
--rw-r--r--  2.0 unx    14613 b- defN 24-May-29 02:52 nnabla_rl/algorithms/icra2018_qtopt.py
--rw-r--r--  2.0 unx     4391 b- defN 24-May-29 02:52 nnabla_rl/algorithms/ilqr.py
--rw-r--r--  2.0 unx    18230 b- defN 24-May-29 02:52 nnabla_rl/algorithms/iqn.py
--rw-r--r--  2.0 unx     6822 b- defN 24-May-29 02:52 nnabla_rl/algorithms/lqr.py
--rw-r--r--  2.0 unx     6695 b- defN 24-May-29 02:52 nnabla_rl/algorithms/mme_sac.py
--rw-r--r--  2.0 unx    18553 b- defN 24-May-29 02:52 nnabla_rl/algorithms/mppi.py
--rw-r--r--  2.0 unx     6567 b- defN 24-May-29 02:52 nnabla_rl/algorithms/munchausen_dqn.py
--rw-r--r--  2.0 unx     6288 b- defN 24-May-29 02:52 nnabla_rl/algorithms/munchausen_iqn.py
--rw-r--r--  2.0 unx    33820 b- defN 24-May-29 02:52 nnabla_rl/algorithms/ppo.py
--rw-r--r--  2.0 unx    16951 b- defN 24-May-29 02:52 nnabla_rl/algorithms/qrdqn.py
--rw-r--r--  2.0 unx    20909 b- defN 24-May-29 02:52 nnabla_rl/algorithms/qrsac.py
--rw-r--r--  2.0 unx     9344 b- defN 24-May-29 02:52 nnabla_rl/algorithms/rainbow.py
--rw-r--r--  2.0 unx     9167 b- defN 24-May-29 02:52 nnabla_rl/algorithms/redq.py
--rw-r--r--  2.0 unx    12762 b- defN 24-May-29 02:52 nnabla_rl/algorithms/reinforce.py
--rw-r--r--  2.0 unx    20767 b- defN 24-May-29 02:52 nnabla_rl/algorithms/sac.py
--rw-r--r--  2.0 unx    10694 b- defN 24-May-29 02:52 nnabla_rl/algorithms/sacd.py
--rw-r--r--  2.0 unx    21231 b- defN 24-May-29 02:52 nnabla_rl/algorithms/srsac.py
--rw-r--r--  2.0 unx    19864 b- defN 24-May-29 02:52 nnabla_rl/algorithms/td3.py
--rw-r--r--  2.0 unx    19875 b- defN 24-May-29 02:52 nnabla_rl/algorithms/trpo.py
--rw-r--r--  2.0 unx    22113 b- defN 24-May-29 02:52 nnabla_rl/algorithms/xql.py
--rw-r--r--  2.0 unx     1089 b- defN 24-May-29 02:52 nnabla_rl/builders/__init__.py
--rw-r--r--  2.0 unx     2071 b- defN 24-May-29 02:52 nnabla_rl/builders/explorer_builder.py
--rw-r--r--  2.0 unx     1849 b- defN 24-May-29 02:52 nnabla_rl/builders/lr_scheduler_builder.py
--rw-r--r--  2.0 unx     1949 b- defN 24-May-29 02:52 nnabla_rl/builders/model_builder.py
--rw-r--r--  2.0 unx     1987 b- defN 24-May-29 02:52 nnabla_rl/builders/preprocessor_builder.py
--rw-r--r--  2.0 unx     1785 b- defN 24-May-29 02:52 nnabla_rl/builders/replay_buffer_builder.py
--rw-r--r--  2.0 unx     1713 b- defN 24-May-29 02:52 nnabla_rl/builders/solver_builder.py
--rw-r--r--  2.0 unx     1144 b- defN 24-May-29 02:52 nnabla_rl/distributions/__init__.py
--rw-r--r--  2.0 unx     3338 b- defN 24-May-29 02:52 nnabla_rl/distributions/bernoulli.py
--rw-r--r--  2.0 unx      949 b- defN 24-May-29 02:52 nnabla_rl/distributions/common_utils.py
--rw-r--r--  2.0 unx     5051 b- defN 24-May-29 02:52 nnabla_rl/distributions/distribution.py
--rw-r--r--  2.0 unx     7627 b- defN 24-May-29 02:52 nnabla_rl/distributions/gaussian.py
--rw-r--r--  2.0 unx     9466 b- defN 24-May-29 02:52 nnabla_rl/distributions/gmm.py
--rw-r--r--  2.0 unx     2798 b- defN 24-May-29 02:52 nnabla_rl/distributions/one_hot_softmax.py
--rw-r--r--  2.0 unx     3390 b- defN 24-May-29 02:52 nnabla_rl/distributions/softmax.py
--rw-r--r--  2.0 unx     5015 b- defN 24-May-29 02:52 nnabla_rl/distributions/squashed_gaussian.py
--rw-r--r--  2.0 unx     1279 b- defN 24-May-29 02:52 nnabla_rl/environment_explorers/__init__.py
--rw-r--r--  2.0 unx     7207 b- defN 24-May-29 02:52 nnabla_rl/environment_explorers/epsilon_greedy_explorer.py
--rw-r--r--  2.0 unx     3507 b- defN 24-May-29 02:52 nnabla_rl/environment_explorers/gaussian_explorer.py
--rw-r--r--  2.0 unx     2154 b- defN 24-May-29 02:52 nnabla_rl/environment_explorers/raw_policy_explorer.py
--rw-r--r--  2.0 unx     3945 b- defN 24-May-29 02:52 nnabla_rl/environments/__init__.py
--rw-r--r--  2.0 unx     2970 b- defN 24-May-29 02:52 nnabla_rl/environments/amp_env.py
--rw-r--r--  2.0 unx    22059 b- defN 24-May-29 02:52 nnabla_rl/environments/dummy.py
--rw-r--r--  2.0 unx     9036 b- defN 24-May-29 02:52 nnabla_rl/environments/environment_info.py
--rw-r--r--  2.0 unx    10838 b- defN 24-May-29 02:52 nnabla_rl/environments/factored_envs.py
--rw-r--r--  2.0 unx     3176 b- defN 24-May-29 02:52 nnabla_rl/environments/gym_utils.py
--rw-r--r--  2.0 unx     1330 b- defN 24-May-29 02:52 nnabla_rl/environments/wrappers/__init__.py
--rw-r--r--  2.0 unx     4973 b- defN 24-May-29 02:52 nnabla_rl/environments/wrappers/atari.py
--rw-r--r--  2.0 unx    10292 b- defN 24-May-29 02:52 nnabla_rl/environments/wrappers/common.py
--rw-r--r--  2.0 unx     6548 b- defN 24-May-29 02:52 nnabla_rl/environments/wrappers/goal_conditioned.py
--rw-r--r--  2.0 unx     3275 b- defN 24-May-29 02:52 nnabla_rl/environments/wrappers/gymnasium.py
--rw-r--r--  2.0 unx     8078 b- defN 24-May-29 02:52 nnabla_rl/environments/wrappers/hybrid_env.py
--rw-r--r--  2.0 unx     2055 b- defN 24-May-29 02:52 nnabla_rl/environments/wrappers/mujoco.py
--rw-r--r--  2.0 unx      612 b- defN 24-May-29 02:52 nnabla_rl/external/__init__.py
--rw-r--r--  2.0 unx    11253 b- defN 24-May-29 02:52 nnabla_rl/external/atari_wrappers.py
--rw-r--r--  2.0 unx     5301 b- defN 24-May-29 02:52 nnabla_rl/external/dmc_env.py
--rw-r--r--  2.0 unx     3707 b- defN 24-May-29 02:52 nnabla_rl/external/goal_env.py
--rw-r--r--  2.0 unx     1207 b- defN 24-May-29 02:52 nnabla_rl/hooks/__init__.py
--rw-r--r--  2.0 unx     2562 b- defN 24-May-29 02:52 nnabla_rl/hooks/computational_graph_hook.py
--rw-r--r--  2.0 unx     1252 b- defN 24-May-29 02:52 nnabla_rl/hooks/epoch_num_hook.py
--rw-r--r--  2.0 unx     2846 b- defN 24-May-29 02:52 nnabla_rl/hooks/evaluation_hook.py
--rw-r--r--  2.0 unx     1155 b- defN 24-May-29 02:52 nnabla_rl/hooks/iteration_num_hook.py
--rw-r--r--  2.0 unx     1968 b- defN 24-May-29 02:52 nnabla_rl/hooks/iteration_state_hook.py
--rw-r--r--  2.0 unx     1332 b- defN 24-May-29 02:52 nnabla_rl/hooks/progress_bar_hook.py
--rw-r--r--  2.0 unx     1133 b- defN 24-May-29 02:52 nnabla_rl/hooks/save_snapshot_hook.py
--rw-r--r--  2.0 unx     1290 b- defN 24-May-29 02:52 nnabla_rl/hooks/time_measuring_hook.py
--rw-r--r--  2.0 unx     1313 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/__init__.py
--rw-r--r--  2.0 unx    17885 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/model_trainer.py
--rw-r--r--  2.0 unx      863 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/decision_transformer/__init__.py
--rw-r--r--  2.0 unx     9699 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/decision_transformer/decision_transformer_trainer.py
--rw-r--r--  2.0 unx      711 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/dynamics/__init__.py
--rw-r--r--  2.0 unx     6705 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/dynamics/mppi_dynamics_trainer.py
--rw-r--r--  2.0 unx      882 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/encoder/__init__.py
--rw-r--r--  2.0 unx     6861 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/encoder/hyar_vae_trainer.py
--rw-r--r--  2.0 unx     5491 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/encoder/kld_variational_auto_encoder_trainer.py
--rw-r--r--  2.0 unx      722 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/hybrid/__init__.py
--rw-r--r--  2.0 unx    12788 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/hybrid/srsac_actor_critic_trainer.py
--rw-r--r--  2.0 unx      728 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/perturbator/__init__.py
--rw-r--r--  2.0 unx     5053 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/perturbator/bcq_perturbator_trainer.py
--rw-r--r--  2.0 unx     2139 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/policy/__init__.py
--rw-r--r--  2.0 unx     4839 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/policy/a2c_policy_trainer.py
--rw-r--r--  2.0 unx     8404 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/policy/amp_policy_trainer.py
--rw-r--r--  2.0 unx    12355 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/policy/bear_policy_trainer.py
--rw-r--r--  2.0 unx     9904 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/policy/demme_policy_trainer.py
--rw-r--r--  2.0 unx     7168 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/policy/dpg_policy_trainer.py
--rw-r--r--  2.0 unx     3040 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/policy/her_policy_trainer.py
--rw-r--r--  2.0 unx     6931 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/policy/hyar_policy_trainer.py
--rw-r--r--  2.0 unx     5872 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/policy/ppo_policy_trainer.py
--rw-r--r--  2.0 unx     2988 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/policy/reinforce_policy_trainer.py
--rw-r--r--  2.0 unx    10535 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/policy/soft_policy_trainer.py
--rw-r--r--  2.0 unx     5450 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/policy/spg_policy_trainer.py
--rw-r--r--  2.0 unx    14028 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/policy/trpo_policy_trainer.py
--rw-r--r--  2.0 unx     9932 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/policy/xql_forward_policy_trainer.py
--rw-r--r--  2.0 unx     9602 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/policy/xql_reverse_policy_trainer.py
--rw-r--r--  2.0 unx     2737 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/q_value/__init__.py
--rw-r--r--  2.0 unx     5288 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/q_value/bcq_q_trainer.py
--rw-r--r--  2.0 unx     4452 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/q_value/categorical_ddqn_q_trainer.py
--rw-r--r--  2.0 unx     4796 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/q_value/categorical_dqn_q_trainer.py
--rw-r--r--  2.0 unx     4256 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/q_value/clipped_double_q_trainer.py
--rw-r--r--  2.0 unx     4577 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/q_value/ddpg_q_trainer.py
--rw-r--r--  2.0 unx     3910 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/q_value/ddqn_q_trainer.py
--rw-r--r--  2.0 unx     3293 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/q_value/dqn_q_trainer.py
--rw-r--r--  2.0 unx     4893 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/q_value/her_q_trainer.py
--rw-r--r--  2.0 unx     8756 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/q_value/hyar_q_trainer.py
--rw-r--r--  2.0 unx     4043 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/q_value/iqn_q_trainer.py
--rw-r--r--  2.0 unx     4224 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/q_value/multi_step_trainer.py
--rw-r--r--  2.0 unx    11056 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/q_value/munchausen_rl_q_trainer.py
--rw-r--r--  2.0 unx     3416 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/q_value/qrdqn_q_trainer.py
--rw-r--r--  2.0 unx     5595 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/q_value/qrsac_q_trainer.py
--rw-r--r--  2.0 unx     9210 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/q_value/quantile_distribution_function_trainer.py
--rw-r--r--  2.0 unx     5103 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/q_value/redq_q_trainer.py
--rw-r--r--  2.0 unx     5269 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/q_value/soft_q_decomposition_trainer.py
--rw-r--r--  2.0 unx     4737 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/q_value/soft_q_trainer.py
--rw-r--r--  2.0 unx     9502 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/q_value/squared_td_q_function_trainer.py
--rw-r--r--  2.0 unx     8359 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/q_value/state_action_quantile_function_trainer.py
--rw-r--r--  2.0 unx     5189 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/q_value/td3_q_trainer.py
--rw-r--r--  2.0 unx     4152 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/q_value/v_targeted_q_trainer.py
--rw-r--r--  2.0 unx     9134 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/q_value/value_distribution_function_trainer.py
--rw-r--r--  2.0 unx      896 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/reward/__init__.py
--rw-r--r--  2.0 unx    12282 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/reward/amp_reward_function_trainer.py
--rw-r--r--  2.0 unx     6346 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/reward/gail_reward_function_trainer.py
--rw-r--r--  2.0 unx     1154 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/v_value/__init__.py
--rw-r--r--  2.0 unx     4273 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/v_value/demme_v_trainer.py
--rw-r--r--  2.0 unx     3121 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/v_value/extreme_v_function_trainer.py
--rw-r--r--  2.0 unx     4942 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/v_value/mme_v_trainer.py
--rw-r--r--  2.0 unx     2738 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/v_value/monte_carlo_v_trainer.py
--rw-r--r--  2.0 unx     4311 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/v_value/soft_v_trainer.py
--rw-r--r--  2.0 unx     2450 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/v_value/squared_td_v_function_trainer.py
--rw-r--r--  2.0 unx     6869 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/v_value/v_function_trainer.py
--rw-r--r--  2.0 unx     4304 b- defN 24-May-29 02:52 nnabla_rl/model_trainers/v_value/xql_v_trainer.py
--rw-r--r--  2.0 unx     5781 b- defN 24-May-29 02:52 nnabla_rl/models/__init__.py
--rw-r--r--  2.0 unx     2288 b- defN 24-May-29 02:52 nnabla_rl/models/decision_transformer.py
--rw-r--r--  2.0 unx    26165 b- defN 24-May-29 02:52 nnabla_rl/models/distributional_function.py
--rw-r--r--  2.0 unx     2548 b- defN 24-May-29 02:52 nnabla_rl/models/dynamics.py
--rw-r--r--  2.0 unx     2727 b- defN 24-May-29 02:52 nnabla_rl/models/encoder.py
--rw-r--r--  2.0 unx     7096 b- defN 24-May-29 02:52 nnabla_rl/models/model.py
--rw-r--r--  2.0 unx     1090 b- defN 24-May-29 02:52 nnabla_rl/models/perturbator.py
--rw-r--r--  2.0 unx     1927 b- defN 24-May-29 02:52 nnabla_rl/models/policy.py
--rw-r--r--  2.0 unx     3857 b- defN 24-May-29 02:52 nnabla_rl/models/q_function.py
--rw-r--r--  2.0 unx     1557 b- defN 24-May-29 02:52 nnabla_rl/models/reward_function.py
--rw-r--r--  2.0 unx     1217 b- defN 24-May-29 02:52 nnabla_rl/models/v_function.py
--rw-r--r--  2.0 unx      627 b- defN 24-May-29 02:52 nnabla_rl/models/atari/__init__.py
--rw-r--r--  2.0 unx     8994 b- defN 24-May-29 02:52 nnabla_rl/models/atari/decision_transformers.py
--rw-r--r--  2.0 unx    11094 b- defN 24-May-29 02:52 nnabla_rl/models/atari/distributional_functions.py
--rw-r--r--  2.0 unx     4311 b- defN 24-May-29 02:52 nnabla_rl/models/atari/policies.py
--rw-r--r--  2.0 unx     7777 b- defN 24-May-29 02:52 nnabla_rl/models/atari/q_functions.py
--rw-r--r--  2.0 unx     3244 b- defN 24-May-29 02:52 nnabla_rl/models/atari/shared_functions.py
--rw-r--r--  2.0 unx     2580 b- defN 24-May-29 02:52 nnabla_rl/models/atari/v_functions.py
--rw-r--r--  2.0 unx      622 b- defN 24-May-29 02:52 nnabla_rl/models/classic_control/__init__.py
--rw-r--r--  2.0 unx     2262 b- defN 24-May-29 02:52 nnabla_rl/models/classic_control/dynamics.py
--rw-r--r--  2.0 unx     3843 b- defN 24-May-29 02:52 nnabla_rl/models/classic_control/policies.py
--rw-r--r--  2.0 unx      587 b- defN 24-May-29 02:52 nnabla_rl/models/hybrid_env/__init__.py
--rw-r--r--  2.0 unx     8361 b- defN 24-May-29 02:52 nnabla_rl/models/hybrid_env/encoders.py
--rw-r--r--  2.0 unx     2235 b- defN 24-May-29 02:52 nnabla_rl/models/hybrid_env/policies.py
--rw-r--r--  2.0 unx     2146 b- defN 24-May-29 02:52 nnabla_rl/models/hybrid_env/q_functions.py
--rw-r--r--  2.0 unx      627 b- defN 24-May-29 02:52 nnabla_rl/models/mujoco/__init__.py
--rw-r--r--  2.0 unx     6532 b- defN 24-May-29 02:52 nnabla_rl/models/mujoco/decision_transformers.py
--rw-r--r--  2.0 unx     1499 b- defN 24-May-29 02:52 nnabla_rl/models/mujoco/distributional_functions.py
--rw-r--r--  2.0 unx     5397 b- defN 24-May-29 02:52 nnabla_rl/models/mujoco/encoders.py
--rw-r--r--  2.0 unx     1706 b- defN 24-May-29 02:52 nnabla_rl/models/mujoco/perturbators.py
--rw-r--r--  2.0 unx    17227 b- defN 24-May-29 02:52 nnabla_rl/models/mujoco/policies.py
--rw-r--r--  2.0 unx     8552 b- defN 24-May-29 02:52 nnabla_rl/models/mujoco/q_functions.py
--rw-r--r--  2.0 unx     1868 b- defN 24-May-29 02:52 nnabla_rl/models/mujoco/reward_functions.py
--rw-r--r--  2.0 unx     5903 b- defN 24-May-29 02:52 nnabla_rl/models/mujoco/v_functions.py
--rw-r--r--  2.0 unx      587 b- defN 24-May-29 02:52 nnabla_rl/models/pybullet/__init__.py
--rw-r--r--  2.0 unx     7612 b- defN 24-May-29 02:52 nnabla_rl/models/pybullet/policy.py
--rw-r--r--  2.0 unx     5778 b- defN 24-May-29 02:52 nnabla_rl/models/pybullet/q_functions.py
--rw-r--r--  2.0 unx     2659 b- defN 24-May-29 02:52 nnabla_rl/models/pybullet/reward_functions.py
--rw-r--r--  2.0 unx     4639 b- defN 24-May-29 02:52 nnabla_rl/models/pybullet/v_functions.py
--rw-r--r--  2.0 unx      587 b- defN 24-May-29 02:52 nnabla_rl/numpy_model_trainers/__init__.py
--rw-r--r--  2.0 unx      998 b- defN 24-May-29 02:52 nnabla_rl/numpy_model_trainers/numpy_model_trainer.py
--rw-r--r--  2.0 unx      587 b- defN 24-May-29 02:52 nnabla_rl/numpy_model_trainers/distribution_parameters/__init__.py
--rw-r--r--  2.0 unx     3953 b- defN 24-May-29 02:52 nnabla_rl/numpy_model_trainers/distribution_parameters/gmm_parameter_trainer.py
--rw-r--r--  2.0 unx      587 b- defN 24-May-29 02:52 nnabla_rl/numpy_models/__init__.py
--rw-r--r--  2.0 unx     6825 b- defN 24-May-29 02:52 nnabla_rl/numpy_models/cost_function.py
--rw-r--r--  2.0 unx      866 b- defN 24-May-29 02:52 nnabla_rl/numpy_models/distribution_parameter.py
--rw-r--r--  2.0 unx     4330 b- defN 24-May-29 02:52 nnabla_rl/numpy_models/dynamics.py
--rw-r--r--  2.0 unx     1442 b- defN 24-May-29 02:52 nnabla_rl/numpy_models/numpy_model.py
--rw-r--r--  2.0 unx      587 b- defN 24-May-29 02:52 nnabla_rl/numpy_models/distribution_parameters/__init__.py
--rw-r--r--  2.0 unx     2917 b- defN 24-May-29 02:52 nnabla_rl/numpy_models/distribution_parameters/gmm_parameter.py
--rw-r--r--  2.0 unx      865 b- defN 24-May-29 02:52 nnabla_rl/preprocessors/__init__.py
--rw-r--r--  2.0 unx     3060 b- defN 24-May-29 02:52 nnabla_rl/preprocessors/her_preprocessor.py
--rw-r--r--  2.0 unx      831 b- defN 24-May-29 02:52 nnabla_rl/preprocessors/preprocessor.py
--rw-r--r--  2.0 unx     5826 b- defN 24-May-29 02:52 nnabla_rl/preprocessors/running_mean_normalizer.py
--rw-r--r--  2.0 unx     1812 b- defN 24-May-29 02:52 nnabla_rl/replay_buffers/__init__.py
--rw-r--r--  2.0 unx     2355 b- defN 24-May-29 02:52 nnabla_rl/replay_buffers/buffer_iterator.py
--rw-r--r--  2.0 unx     1347 b- defN 24-May-29 02:52 nnabla_rl/replay_buffers/decorable_replay_buffer.py
--rw-r--r--  2.0 unx     4876 b- defN 24-May-29 02:52 nnabla_rl/replay_buffers/hindsight_replay_buffer.py
--rw-r--r--  2.0 unx    11160 b- defN 24-May-29 02:52 nnabla_rl/replay_buffers/memory_efficient_atari_buffer.py
--rw-r--r--  2.0 unx    27478 b- defN 24-May-29 02:52 nnabla_rl/replay_buffers/prioritized_replay_buffer.py
--rw-r--r--  2.0 unx     1590 b- defN 24-May-29 02:52 nnabla_rl/replay_buffers/replacement_sampling_replay_buffer.py
--rw-r--r--  2.0 unx    11220 b- defN 24-May-29 02:52 nnabla_rl/replay_buffers/trajectory_replay_buffer.py
--rw-r--r--  2.0 unx      627 b- defN 24-May-29 02:52 nnabla_rl/utils/__init__.py
--rw-r--r--  2.0 unx     1418 b- defN 24-May-29 02:52 nnabla_rl/utils/context.py
--rw-r--r--  2.0 unx     9925 b- defN 24-May-29 02:52 nnabla_rl/utils/data.py
--rw-r--r--  2.0 unx     3839 b- defN 24-May-29 02:52 nnabla_rl/utils/debugging.py
--rw-r--r--  2.0 unx     3931 b- defN 24-May-29 02:52 nnabla_rl/utils/evaluator.py
--rw-r--r--  2.0 unx     1954 b- defN 24-May-29 02:52 nnabla_rl/utils/files.py
--rw-r--r--  2.0 unx     1824 b- defN 24-May-29 02:52 nnabla_rl/utils/matrices.py
--rw-r--r--  2.0 unx     4170 b- defN 24-May-29 02:52 nnabla_rl/utils/misc.py
--rw-r--r--  2.0 unx     3415 b- defN 24-May-29 02:52 nnabla_rl/utils/multiprocess.py
--rw-r--r--  2.0 unx     1955 b- defN 24-May-29 02:52 nnabla_rl/utils/optimization.py
--rw-r--r--  2.0 unx     5670 b- defN 24-May-29 02:52 nnabla_rl/utils/reproductions.py
--rw-r--r--  2.0 unx     5630 b- defN 24-May-29 02:52 nnabla_rl/utils/serializers.py
--rw-r--r--  2.0 unx     4744 b- defN 24-May-29 02:52 nnabla_rl/utils/solver_wrappers.py
--rw-r--r--  2.0 unx      761 b- defN 24-May-29 02:52 nnabla_rl/writers/__init__.py
--rw-r--r--  2.0 unx     2523 b- defN 24-May-29 02:52 nnabla_rl/writers/file_writer.py
--rw-r--r--  2.0 unx     1561 b- defN 24-May-29 02:52 nnabla_rl/writers/monitor_writer.py
--rw-r--r--  2.0 unx     1433 b- defN 24-May-29 02:52 nnabla_rl/writers/writing_distributor.py
--rwxr-xr-x  2.0 unx     2114 b- defN 24-May-29 02:53 nnabla_rl-0.15.0.data/scripts/check_best_iteration
--rwxr-xr-x  2.0 unx     8100 b- defN 24-May-29 02:53 nnabla_rl-0.15.0.data/scripts/compile_results
--rwxr-xr-x  2.0 unx     6386 b- defN 24-May-29 02:52 nnabla_rl-0.15.0.data/scripts/evaluate_algorithm
--rwxr-xr-x  2.0 unx     8891 b- defN 24-May-29 02:53 nnabla_rl-0.15.0.data/scripts/insert_copyright
--rwxr-xr-x  2.0 unx     4888 b- defN 24-May-29 02:53 nnabla_rl-0.15.0.data/scripts/plot_result
--rwxr-xr-x  2.0 unx     7628 b- defN 24-May-29 02:52 nnabla_rl-0.15.0.data/scripts/test_reproductions
--rwxr-xr-x  2.0 unx     1332 b- defN 24-May-29 02:52 nnabla_rl-0.15.0.data/scripts/train_with_seeds
--rw-r--r--  2.0 unx      627 b- defN 24-May-29 02:52 tests/algorithms/__init__.py
--rw-r--r--  2.0 unx     3210 b- defN 24-May-29 02:52 tests/algorithms/test_a2c.py
--rw-r--r--  2.0 unx    18200 b- defN 24-May-29 02:52 tests/algorithms/test_amp.py
--rw-r--r--  2.0 unx     4165 b- defN 24-May-29 02:52 tests/algorithms/test_atrpo.py
--rw-r--r--  2.0 unx     4303 b- defN 24-May-29 02:52 tests/algorithms/test_bcq.py
--rw-r--r--  2.0 unx     4283 b- defN 24-May-29 02:52 tests/algorithms/test_bear.py
--rw-r--r--  2.0 unx     3587 b- defN 24-May-29 02:52 tests/algorithms/test_categorical_ddqn.py
--rw-r--r--  2.0 unx     8462 b- defN 24-May-29 02:52 tests/algorithms/test_categorical_dqn.py
--rw-r--r--  2.0 unx    12600 b- defN 24-May-29 02:52 tests/algorithms/test_common_utils.py
--rw-r--r--  2.0 unx     7789 b- defN 24-May-29 02:52 tests/algorithms/test_ddp.py
--rw-r--r--  2.0 unx     9776 b- defN 24-May-29 02:52 tests/algorithms/test_ddpg.py
--rw-r--r--  2.0 unx     4413 b- defN 24-May-29 02:52 tests/algorithms/test_ddqn.py
--rw-r--r--  2.0 unx     5317 b- defN 24-May-29 02:52 tests/algorithms/test_decision_transformer.py
--rw-r--r--  2.0 unx    15196 b- defN 24-May-29 02:52 tests/algorithms/test_demme_sac.py
--rw-r--r--  2.0 unx     5861 b- defN 24-May-29 02:52 tests/algorithms/test_dqn.py
--rw-r--r--  2.0 unx     4485 b- defN 24-May-29 02:52 tests/algorithms/test_drqn.py
--rw-r--r--  2.0 unx     2675 b- defN 24-May-29 02:52 tests/algorithms/test_dummy.py
--rw-r--r--  2.0 unx     5509 b- defN 24-May-29 02:52 tests/algorithms/test_gail.py
--rw-r--r--  2.0 unx    11220 b- defN 24-May-29 02:52 tests/algorithms/test_her.py
--rw-r--r--  2.0 unx     5942 b- defN 24-May-29 02:52 tests/algorithms/test_hyar.py
--rw-r--r--  2.0 unx     4157 b- defN 24-May-29 02:52 tests/algorithms/test_icml2015_trpo.py
--rw-r--r--  2.0 unx    14313 b- defN 24-May-29 02:52 tests/algorithms/test_icml2018_sac.py
--rw-r--r--  2.0 unx     9052 b- defN 24-May-29 02:52 tests/algorithms/test_icra2018_qtopt.py
--rw-r--r--  2.0 unx     7487 b- defN 24-May-29 02:52 tests/algorithms/test_ilqr.py
--rw-r--r--  2.0 unx     1808 b- defN 24-May-29 02:52 tests/algorithms/test_init_py.py
--rw-r--r--  2.0 unx     9854 b- defN 24-May-29 02:52 tests/algorithms/test_iqn.py
--rw-r--r--  2.0 unx     6351 b- defN 24-May-29 02:52 tests/algorithms/test_lqr.py
--rw-r--r--  2.0 unx    14100 b- defN 24-May-29 02:52 tests/algorithms/test_mme_sac.py
--rw-r--r--  2.0 unx     6309 b- defN 24-May-29 02:52 tests/algorithms/test_mppi.py
--rw-r--r--  2.0 unx     5140 b- defN 24-May-29 02:52 tests/algorithms/test_munchausen_dqn.py
--rw-r--r--  2.0 unx     5282 b- defN 24-May-29 02:52 tests/algorithms/test_munchausen_iqn.py
--rw-r--r--  2.0 unx     8879 b- defN 24-May-29 02:52 tests/algorithms/test_ppo.py
--rw-r--r--  2.0 unx     8968 b- defN 24-May-29 02:52 tests/algorithms/test_qrdqn.py
--rw-r--r--  2.0 unx    10824 b- defN 24-May-29 02:52 tests/algorithms/test_qrsac.py
--rw-r--r--  2.0 unx     3875 b- defN 24-May-29 02:52 tests/algorithms/test_rainbow.py
--rw-r--r--  2.0 unx    10880 b- defN 24-May-29 02:52 tests/algorithms/test_redq.py
--rw-r--r--  2.0 unx     2636 b- defN 24-May-29 02:52 tests/algorithms/test_reinforce.py
--rw-r--r--  2.0 unx    10621 b- defN 24-May-29 02:52 tests/algorithms/test_sac.py
--rw-r--r--  2.0 unx    11244 b- defN 24-May-29 02:52 tests/algorithms/test_sacd.py
--rw-r--r--  2.0 unx    15241 b- defN 24-May-29 02:52 tests/algorithms/test_srsac.py
--rw-r--r--  2.0 unx    10718 b- defN 24-May-29 02:52 tests/algorithms/test_td3.py
--rw-r--r--  2.0 unx     3903 b- defN 24-May-29 02:52 tests/algorithms/test_trpo.py
--rw-r--r--  2.0 unx     4244 b- defN 24-May-29 02:52 tests/algorithms/test_xql.py
--rw-r--r--  2.0 unx      627 b- defN 24-May-29 02:52 tests/distributions/__init__.py
--rw-r--r--  2.0 unx     5791 b- defN 24-May-29 02:52 tests/distributions/test_bernoulli.py
--rw-r--r--  2.0 unx     1731 b- defN 24-May-29 02:52 tests/distributions/test_common_utils.py
--rw-r--r--  2.0 unx    13649 b- defN 24-May-29 02:52 tests/distributions/test_gaussian.py
--rw-r--r--  2.0 unx     5492 b- defN 24-May-29 02:52 tests/distributions/test_gmm.py
--rw-r--r--  2.0 unx     4182 b- defN 24-May-29 02:52 tests/distributions/test_one_hot_softmax.py
--rw-r--r--  2.0 unx     8259 b- defN 24-May-29 02:52 tests/distributions/test_softmax.py
--rw-r--r--  2.0 unx     9258 b- defN 24-May-29 02:52 tests/distributions/test_squashed_gaussian.py
--rw-r--r--  2.0 unx      627 b- defN 24-May-29 02:52 tests/environment_explorers/__init__.py
--rw-r--r--  2.0 unx     5512 b- defN 24-May-29 02:52 tests/environment_explorers/test_epsilon_greedy.py
--rw-r--r--  2.0 unx     2614 b- defN 24-May-29 02:52 tests/environment_explorers/test_gaussian.py
--rw-r--r--  2.0 unx      627 b- defN 24-May-29 02:52 tests/environments/__init__.py
--rw-r--r--  2.0 unx     5781 b- defN 24-May-29 02:52 tests/environments/test_amp_env.py
--rw-r--r--  2.0 unx     8719 b- defN 24-May-29 02:52 tests/environments/test_env_info.py
--rw-r--r--  2.0 unx     3754 b- defN 24-May-29 02:52 tests/environments/test_gym_utils.py
--rw-r--r--  2.0 unx      627 b- defN 24-May-29 02:52 tests/environments/wrappers/__init__.py
--rw-r--r--  2.0 unx     1638 b- defN 24-May-29 02:52 tests/environments/wrappers/test_atari.py
--rw-r--r--  2.0 unx     5500 b- defN 24-May-29 02:52 tests/environments/wrappers/test_common.py
--rw-r--r--  2.0 unx     1415 b- defN 24-May-29 02:52 tests/environments/wrappers/test_goal_conditioned.py
--rw-r--r--  2.0 unx     2314 b- defN 24-May-29 02:52 tests/environments/wrappers/test_gymnasium.py
--rw-r--r--  2.0 unx      627 b- defN 24-May-29 02:52 tests/hooks/__init__.py
--rw-r--r--  2.0 unx     2235 b- defN 24-May-29 02:52 tests/hooks/test_computational_graph_hook.py
--rw-r--r--  2.0 unx     1370 b- defN 24-May-29 02:52 tests/hooks/test_evaluation_hook.py
--rw-r--r--  2.0 unx     1175 b- defN 24-May-29 02:52 tests/hooks/test_iteration_num_hook.py
--rw-r--r--  2.0 unx     1947 b- defN 24-May-29 02:52 tests/hooks/test_iteration_state_hook.py
--rw-r--r--  2.0 unx      627 b- defN 24-May-29 02:52 tests/model_trainers/__init__.py
--rw-r--r--  2.0 unx     8212 b- defN 24-May-29 02:52 tests/model_trainers/test_model_trainer.py
--rw-r--r--  2.0 unx    15963 b- defN 24-May-29 02:52 tests/model_trainers/test_policy_trainers.py
--rw-r--r--  2.0 unx    13045 b- defN 24-May-29 02:52 tests/model_trainers/test_q_value_trainers.py
--rw-r--r--  2.0 unx      830 b- defN 24-May-29 02:52 tests/model_trainers/test_v_value_trainers.py
--rw-r--r--  2.0 unx      627 b- defN 24-May-29 02:52 tests/models/__init__.py
--rw-r--r--  2.0 unx     1805 b- defN 24-May-29 02:52 tests/models/test_distributional_function.py
--rw-r--r--  2.0 unx     6230 b- defN 24-May-29 02:52 tests/models/test_model.py
--rw-r--r--  2.0 unx      627 b- defN 24-May-29 02:52 tests/models/atari/__init__.py
--rw-r--r--  2.0 unx     9231 b- defN 24-May-29 02:52 tests/models/atari/test_distributional_functions.py
--rw-r--r--  2.0 unx     4142 b- defN 24-May-29 02:52 tests/models/atari/test_q_functions.py
--rw-r--r--  2.0 unx      627 b- defN 24-May-29 02:52 tests/models/classic_control/__init__.py
--rw-r--r--  2.0 unx     2331 b- defN 24-May-29 02:52 tests/models/classic_control/test_policies.py
--rw-r--r--  2.0 unx      627 b- defN 24-May-29 02:52 tests/models/mujoco/__init__.py
--rw-r--r--  2.0 unx     2394 b- defN 24-May-29 02:52 tests/models/mujoco/test_policies.py
--rw-r--r--  2.0 unx     1516 b- defN 24-May-29 02:52 tests/models/mujoco/test_q_functions.py
--rw-r--r--  2.0 unx      587 b- defN 24-May-29 02:52 tests/numpy_model_trainers/__init__.py
--rw-r--r--  2.0 unx      587 b- defN 24-May-29 02:52 tests/numpy_models/__init__.py
--rw-r--r--  2.0 unx      587 b- defN 24-May-29 02:52 tests/numpy_models/distribution_parameters/__init__.py
--rw-r--r--  2.0 unx     1491 b- defN 24-May-29 02:52 tests/numpy_models/distribution_parameters/test_gmm_parameter.py
--rw-r--r--  2.0 unx      627 b- defN 24-May-29 02:52 tests/preprocessors/__init__.py
--rw-r--r--  2.0 unx     5257 b- defN 24-May-29 02:52 tests/preprocessors/test_running_mean_normalizer.py
--rw-r--r--  2.0 unx      627 b- defN 24-May-29 02:52 tests/replay_buffers/__init__.py
--rw-r--r--  2.0 unx     2869 b- defN 24-May-29 02:52 tests/replay_buffers/test_buffer_iterator.py
--rw-r--r--  2.0 unx     1454 b- defN 24-May-29 02:52 tests/replay_buffers/test_decorable_replay_buffer.py
--rw-r--r--  2.0 unx     5914 b- defN 24-May-29 02:52 tests/replay_buffers/test_hindsight_replay_buffer.py
--rw-r--r--  2.0 unx    17691 b- defN 24-May-29 02:52 tests/replay_buffers/test_memory_efficient_atari_buffer.py
--rw-r--r--  2.0 unx    31989 b- defN 24-May-29 02:52 tests/replay_buffers/test_prioritized_replay_buffer.py
--rw-r--r--  2.0 unx     1768 b- defN 24-May-29 02:52 tests/replay_buffers/test_replacement_sampling_replay_buffer.py
--rw-r--r--  2.0 unx     6520 b- defN 24-May-29 02:52 tests/replay_buffers/test_trajectory_replay_buffer.py
--rw-r--r--  2.0 unx      627 b- defN 24-May-29 02:52 tests/utils/__init__.py
--rw-r--r--  2.0 unx     4182 b- defN 24-May-29 02:52 tests/utils/test_copy.py
--rw-r--r--  2.0 unx    12132 b- defN 24-May-29 02:52 tests/utils/test_data.py
--rw-r--r--  2.0 unx     1339 b- defN 24-May-29 02:52 tests/utils/test_debugging.py
--rw-r--r--  2.0 unx     2899 b- defN 24-May-29 02:52 tests/utils/test_evaluator.py
--rw-r--r--  2.0 unx     3012 b- defN 24-May-29 02:52 tests/utils/test_files.py
--rw-r--r--  2.0 unx     2179 b- defN 24-May-29 02:52 tests/utils/test_matrices.py
--rw-r--r--  2.0 unx     2018 b- defN 24-May-29 02:52 tests/utils/test_misc.py
--rw-r--r--  2.0 unx     4928 b- defN 24-May-29 02:52 tests/utils/test_multiprocess.py
--rw-r--r--  2.0 unx     1237 b- defN 24-May-29 02:52 tests/utils/test_optimization.py
--rw-r--r--  2.0 unx     2204 b- defN 24-May-29 02:52 tests/utils/test_reproductions.py
--rw-r--r--  2.0 unx     1762 b- defN 24-May-29 02:52 tests/utils/test_serializers.py
--rw-r--r--  2.0 unx     1771 b- defN 24-May-29 02:52 tests/utils/test_solver_wrappers.py
--rw-r--r--  2.0 unx      627 b- defN 24-May-29 02:52 tests/writers/__init__.py
--rw-r--r--  2.0 unx     3931 b- defN 24-May-29 02:52 tests/writers/test_file_writer.py
--rw-r--r--  2.0 unx     2123 b- defN 24-May-29 02:52 tests/writers/test_monitor_writer.py
--rw-r--r--  2.0 unx     1753 b- defN 24-May-29 02:52 tests/writers/test_writing_distributor.py
--rw-r--r--  2.0 unx    11358 b- defN 24-May-29 02:53 nnabla_rl-0.15.0.dist-info/LICENSE
--rw-r--r--  2.0 unx     1898 b- defN 24-May-29 02:53 nnabla_rl-0.15.0.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-May-29 02:53 nnabla_rl-0.15.0.dist-info/WHEEL
--rw-r--r--  2.0 unx       16 b- defN 24-May-29 02:53 nnabla_rl-0.15.0.dist-info/top_level.txt
--rw-rw-r--  2.0 unx    35834 b- defN 24-May-29 02:53 nnabla_rl-0.15.0.dist-info/RECORD
-373 files, 2436630 bytes uncompressed, 636652 bytes compressed:  73.9%
+Zip file size: 349549 bytes, number of entries: 237
+-rw-rw-r--  2.0 unx      783 b- defN 21-Jun-14 03:17 nnabla_rl/__init__.py
+-rw-rw-r--  2.0 unx     8747 b- defN 21-Jun-14 03:17 nnabla_rl/algorithm.py
+-rw-rw-r--  2.0 unx     2730 b- defN 21-Jun-14 03:17 nnabla_rl/configuration.py
+-rw-rw-r--  2.0 unx     5371 b- defN 21-Jun-14 03:17 nnabla_rl/environment_explorer.py
+-rw-rw-r--  2.0 unx     1072 b- defN 21-Jun-14 03:17 nnabla_rl/exceptions.py
+-rw-rw-r--  2.0 unx    11991 b- defN 21-Jun-14 03:17 nnabla_rl/functions.py
+-rw-rw-r--  2.0 unx     1565 b- defN 21-Jun-14 03:17 nnabla_rl/hook.py
+-rw-rw-r--  2.0 unx     5425 b- defN 21-Jun-14 03:17 nnabla_rl/initializers.py
+-rw-rw-r--  2.0 unx     1310 b- defN 21-Jun-14 03:17 nnabla_rl/logger.py
+-rw-rw-r--  2.0 unx     4723 b- defN 21-Jun-14 03:17 nnabla_rl/replay_buffer.py
+-rw-rw-r--  2.0 unx      935 b- defN 21-Jun-14 03:17 nnabla_rl/scopes.py
+-rw-rw-r--  2.0 unx      889 b- defN 21-Jun-14 03:17 nnabla_rl/typing.py
+-rw-rw-r--  2.0 unx     1451 b- defN 21-Jun-14 03:17 nnabla_rl/writer.py
+-rw-rw-r--  2.0 unx     3480 b- defN 21-Jun-14 03:17 nnabla_rl/algorithms/__init__.py
+-rw-rw-r--  2.0 unx    23479 b- defN 21-Jun-14 03:17 nnabla_rl/algorithms/a2c.py
+-rw-rw-r--  2.0 unx    16494 b- defN 21-Jun-14 03:17 nnabla_rl/algorithms/bcq.py
+-rw-rw-r--  2.0 unx    20147 b- defN 21-Jun-14 03:17 nnabla_rl/algorithms/bear.py
+-rw-rw-r--  2.0 unx    13639 b- defN 21-Jun-14 03:17 nnabla_rl/algorithms/categorical_dqn.py
+-rw-rw-r--  2.0 unx     5094 b- defN 21-Jun-14 03:17 nnabla_rl/algorithms/common_utils.py
+-rw-rw-r--  2.0 unx    13559 b- defN 21-Jun-14 03:17 nnabla_rl/algorithms/ddpg.py
+-rw-rw-r--  2.0 unx    14045 b- defN 21-Jun-14 03:17 nnabla_rl/algorithms/dqn.py
+-rw-rw-r--  2.0 unx     1993 b- defN 21-Jun-14 03:17 nnabla_rl/algorithms/dummy.py
+-rw-rw-r--  2.0 unx    27632 b- defN 21-Jun-14 03:17 nnabla_rl/algorithms/gail.py
+-rw-rw-r--  2.0 unx    12293 b- defN 21-Jun-14 03:17 nnabla_rl/algorithms/icml2015_trpo.py
+-rw-rw-r--  2.0 unx    17986 b- defN 21-Jun-14 03:17 nnabla_rl/algorithms/icml2018_sac.py
+-rw-rw-r--  2.0 unx    14962 b- defN 21-Jun-14 03:17 nnabla_rl/algorithms/iqn.py
+-rw-rw-r--  2.0 unx    14340 b- defN 21-Jun-14 03:17 nnabla_rl/algorithms/munchausen_dqn.py
+-rw-rw-r--  2.0 unx    16372 b- defN 21-Jun-14 03:17 nnabla_rl/algorithms/munchausen_iqn.py
+-rw-rw-r--  2.0 unx    30004 b- defN 21-Jun-14 03:17 nnabla_rl/algorithms/ppo.py
+-rw-rw-r--  2.0 unx    13918 b- defN 21-Jun-14 03:17 nnabla_rl/algorithms/qrdqn.py
+-rw-rw-r--  2.0 unx    11233 b- defN 21-Jun-14 03:17 nnabla_rl/algorithms/reinforce.py
+-rw-rw-r--  2.0 unx    16553 b- defN 21-Jun-14 03:17 nnabla_rl/algorithms/sac.py
+-rw-rw-r--  2.0 unx    15463 b- defN 21-Jun-14 03:17 nnabla_rl/algorithms/td3.py
+-rw-rw-r--  2.0 unx    18463 b- defN 21-Jun-14 03:17 nnabla_rl/algorithms/trpo.py
+-rw-rw-r--  2.0 unx      918 b- defN 21-Jun-14 03:17 nnabla_rl/builders/__init__.py
+-rw-rw-r--  2.0 unx     1949 b- defN 21-Jun-14 03:17 nnabla_rl/builders/model_builder.py
+-rw-rw-r--  2.0 unx     1987 b- defN 21-Jun-14 03:17 nnabla_rl/builders/preprocessor_builder.py
+-rw-rw-r--  2.0 unx     1785 b- defN 21-Jun-14 03:17 nnabla_rl/builders/replay_buffer_builder.py
+-rw-rw-r--  2.0 unx     1713 b- defN 21-Jun-14 03:17 nnabla_rl/builders/solver_builder.py
+-rw-rw-r--  2.0 unx      899 b- defN 21-Jun-14 03:17 nnabla_rl/distributions/__init__.py
+-rw-rw-r--  2.0 unx      949 b- defN 21-Jun-14 03:17 nnabla_rl/distributions/common_utils.py
+-rw-rw-r--  2.0 unx     4589 b- defN 21-Jun-14 03:17 nnabla_rl/distributions/distribution.py
+-rw-rw-r--  2.0 unx     3707 b- defN 21-Jun-14 03:17 nnabla_rl/distributions/gaussian.py
+-rw-rw-r--  2.0 unx     2962 b- defN 21-Jun-14 03:17 nnabla_rl/distributions/softmax.py
+-rw-rw-r--  2.0 unx     4964 b- defN 21-Jun-14 03:17 nnabla_rl/distributions/squashed_gaussian.py
+-rw-rw-r--  2.0 unx     1075 b- defN 21-Jun-14 03:17 nnabla_rl/environment_explorers/__init__.py
+-rw-rw-r--  2.0 unx     3065 b- defN 21-Jun-14 03:17 nnabla_rl/environment_explorers/epsilon_greedy_explorer.py
+-rw-rw-r--  2.0 unx     1957 b- defN 21-Jun-14 03:17 nnabla_rl/environment_explorers/gaussian_explorer.py
+-rw-rw-r--  2.0 unx     1434 b- defN 21-Jun-14 03:17 nnabla_rl/environment_explorers/raw_policy_explorer.py
+-rw-rw-r--  2.0 unx     1120 b- defN 21-Jun-14 03:17 nnabla_rl/environments/__init__.py
+-rw-rw-r--  2.0 unx     4343 b- defN 21-Jun-14 03:17 nnabla_rl/environments/dummy.py
+-rw-rw-r--  2.0 unx     3406 b- defN 21-Jun-14 03:17 nnabla_rl/environments/environment_info.py
+-rw-rw-r--  2.0 unx      804 b- defN 21-Jun-14 03:17 nnabla_rl/environments/wrappers/__init__.py
+-rw-rw-r--  2.0 unx     3528 b- defN 21-Jun-14 03:17 nnabla_rl/environments/wrappers/atari.py
+-rw-rw-r--  2.0 unx     2725 b- defN 21-Jun-14 03:17 nnabla_rl/environments/wrappers/common.py
+-rw-rw-r--  2.0 unx      612 b- defN 21-Jun-14 03:17 nnabla_rl/external/__init__.py
+-rw-rw-r--  2.0 unx    11029 b- defN 21-Jun-14 03:17 nnabla_rl/external/atari_wrappers.py
+-rw-rw-r--  2.0 unx      989 b- defN 21-Jun-14 03:17 nnabla_rl/hooks/__init__.py
+-rw-rw-r--  2.0 unx     2841 b- defN 21-Jun-14 03:17 nnabla_rl/hooks/evaluation_hook.py
+-rw-rw-r--  2.0 unx     1150 b- defN 21-Jun-14 03:17 nnabla_rl/hooks/iteration_num_hook.py
+-rw-rw-r--  2.0 unx     1959 b- defN 21-Jun-14 03:17 nnabla_rl/hooks/iteration_state_hook.py
+-rw-rw-r--  2.0 unx     1128 b- defN 21-Jun-14 03:17 nnabla_rl/hooks/save_snapshot_hook.py
+-rw-rw-r--  2.0 unx     1285 b- defN 21-Jun-14 03:17 nnabla_rl/hooks/time_measuring_hook.py
+-rw-rw-r--  2.0 unx     1071 b- defN 21-Jun-14 03:17 nnabla_rl/model_trainers/__init__.py
+-rw-rw-r--  2.0 unx     8123 b- defN 21-Jun-14 03:17 nnabla_rl/model_trainers/model_trainer.py
+-rw-rw-r--  2.0 unx      758 b- defN 21-Jun-14 03:17 nnabla_rl/model_trainers/encoder/__init__.py
+-rw-rw-r--  2.0 unx     4340 b- defN 21-Jun-14 03:17 nnabla_rl/model_trainers/encoder/kld_variational_auto_encoder_trainer.py
+-rw-rw-r--  2.0 unx      728 b- defN 21-Jun-14 03:17 nnabla_rl/model_trainers/perturbator/__init__.py
+-rw-rw-r--  2.0 unx     3962 b- defN 21-Jun-14 03:17 nnabla_rl/model_trainers/perturbator/bcq_perturbator_trainer.py
+-rw-rw-r--  2.0 unx     1399 b- defN 21-Jun-14 03:17 nnabla_rl/model_trainers/policy/__init__.py
+-rw-rw-r--  2.0 unx     4052 b- defN 21-Jun-14 03:17 nnabla_rl/model_trainers/policy/a2c_policy_trainer.py
+-rw-rw-r--  2.0 unx    10506 b- defN 21-Jun-14 03:17 nnabla_rl/model_trainers/policy/bear_policy_trainer.py
+-rw-rw-r--  2.0 unx     3303 b- defN 21-Jun-14 03:17 nnabla_rl/model_trainers/policy/dpg_policy_trainer.py
+-rw-rw-r--  2.0 unx     4833 b- defN 21-Jun-14 03:17 nnabla_rl/model_trainers/policy/ppo_policy_trainer.py
+-rw-rw-r--  2.0 unx     2677 b- defN 21-Jun-14 03:17 nnabla_rl/model_trainers/policy/reinforce_policy_trainer.py
+-rw-rw-r--  2.0 unx     6372 b- defN 21-Jun-14 03:17 nnabla_rl/model_trainers/policy/soft_policy_trainer.py
+-rw-rw-r--  2.0 unx     4855 b- defN 21-Jun-14 03:17 nnabla_rl/model_trainers/policy/spg_policy_trainer.py
+-rw-rw-r--  2.0 unx    13804 b- defN 21-Jun-14 03:17 nnabla_rl/model_trainers/policy/trpo_policy_trainer.py
+-rw-rw-r--  2.0 unx     2029 b- defN 21-Jun-14 03:17 nnabla_rl/model_trainers/q_value/__init__.py
+-rw-rw-r--  2.0 unx     3483 b- defN 21-Jun-14 03:17 nnabla_rl/model_trainers/q_value/bcq_q_trainer.py
+-rw-rw-r--  2.0 unx     3800 b- defN 21-Jun-14 03:17 nnabla_rl/model_trainers/q_value/categorical_dqn_q_trainer.py
+-rw-rw-r--  2.0 unx     2748 b- defN 21-Jun-14 03:17 nnabla_rl/model_trainers/q_value/clipped_double_q_trainer.py
+-rw-rw-r--  2.0 unx     2934 b- defN 21-Jun-14 03:17 nnabla_rl/model_trainers/q_value/ddpg_q_trainer.py
+-rw-rw-r--  2.0 unx     2550 b- defN 21-Jun-14 03:17 nnabla_rl/model_trainers/q_value/ddqn_q_trainer.py
+-rw-rw-r--  2.0 unx     2297 b- defN 21-Jun-14 03:17 nnabla_rl/model_trainers/q_value/dqn_q_trainer.py
+-rw-rw-r--  2.0 unx     2623 b- defN 21-Jun-14 03:17 nnabla_rl/model_trainers/q_value/iqn_q_trainer.py
+-rw-rw-r--  2.0 unx     7513 b- defN 21-Jun-14 03:17 nnabla_rl/model_trainers/q_value/munchausen_rl_q_trainer.py
+-rw-rw-r--  2.0 unx     2420 b- defN 21-Jun-14 03:17 nnabla_rl/model_trainers/q_value/qrdqn_q_trainer.py
+-rw-rw-r--  2.0 unx     6768 b- defN 21-Jun-14 03:17 nnabla_rl/model_trainers/q_value/quantile_distribution_function_trainer.py
+-rw-rw-r--  2.0 unx     3094 b- defN 21-Jun-14 03:17 nnabla_rl/model_trainers/q_value/soft_q_trainer.py
+-rw-rw-r--  2.0 unx     6674 b- defN 21-Jun-14 03:17 nnabla_rl/model_trainers/q_value/squared_td_q_function_trainer.py
+-rw-rw-r--  2.0 unx     5858 b- defN 21-Jun-14 03:17 nnabla_rl/model_trainers/q_value/state_action_quantile_function_trainer.py
+-rw-rw-r--  2.0 unx     3542 b- defN 21-Jun-14 03:17 nnabla_rl/model_trainers/q_value/td3_q_trainer.py
+-rw-rw-r--  2.0 unx     2621 b- defN 21-Jun-14 03:17 nnabla_rl/model_trainers/q_value/v_targeted_q_trainer.py
+-rw-rw-r--  2.0 unx     6287 b- defN 21-Jun-14 03:17 nnabla_rl/model_trainers/q_value/value_distribution_function_trainer.py
+-rw-rw-r--  2.0 unx      736 b- defN 21-Jun-14 03:17 nnabla_rl/model_trainers/reward/__init__.py
+-rw-rw-r--  2.0 unx     6024 b- defN 21-Jun-14 03:17 nnabla_rl/model_trainers/reward/gail_reward_function_trainer.py
+-rw-rw-r--  2.0 unx      823 b- defN 21-Jun-14 03:17 nnabla_rl/model_trainers/v_value/__init__.py
+-rw-rw-r--  2.0 unx     2505 b- defN 21-Jun-14 03:17 nnabla_rl/model_trainers/v_value/monte_carlo_v_trainer.py
+-rw-rw-r--  2.0 unx     2739 b- defN 21-Jun-14 03:17 nnabla_rl/model_trainers/v_value/soft_v_trainer.py
+-rw-rw-r--  2.0 unx     4542 b- defN 21-Jun-14 03:17 nnabla_rl/model_trainers/v_value/squared_td_v_function_trainer.py
+-rw-rw-r--  2.0 unx     3854 b- defN 21-Jun-14 03:17 nnabla_rl/models/__init__.py
+-rw-rw-r--  2.0 unx    22109 b- defN 21-Jun-14 03:17 nnabla_rl/models/distributional_function.py
+-rw-rw-r--  2.0 unx     2762 b- defN 21-Jun-14 03:17 nnabla_rl/models/encoder.py
+-rw-rw-r--  2.0 unx     4041 b- defN 21-Jun-14 03:17 nnabla_rl/models/model.py
+-rw-rw-r--  2.0 unx     1084 b- defN 21-Jun-14 03:17 nnabla_rl/models/perturbator.py
+-rw-rw-r--  2.0 unx     1921 b- defN 21-Jun-14 03:17 nnabla_rl/models/policy.py
+-rw-rw-r--  2.0 unx     3104 b- defN 21-Jun-14 03:17 nnabla_rl/models/q_function.py
+-rw-rw-r--  2.0 unx     1551 b- defN 21-Jun-14 03:17 nnabla_rl/models/reward_function.py
+-rw-rw-r--  2.0 unx     1210 b- defN 21-Jun-14 03:17 nnabla_rl/models/v_function.py
+-rw-rw-r--  2.0 unx      627 b- defN 21-Jun-14 03:17 nnabla_rl/models/atari/__init__.py
+-rw-rw-r--  2.0 unx     6524 b- defN 21-Jun-14 03:17 nnabla_rl/models/atari/distributional_functions.py
+-rw-rw-r--  2.0 unx     4309 b- defN 21-Jun-14 03:17 nnabla_rl/models/atari/policies.py
+-rw-rw-r--  2.0 unx     3374 b- defN 21-Jun-14 03:17 nnabla_rl/models/atari/q_functions.py
+-rw-rw-r--  2.0 unx     3244 b- defN 21-Jun-14 03:17 nnabla_rl/models/atari/shared_functions.py
+-rw-rw-r--  2.0 unx     2570 b- defN 21-Jun-14 03:17 nnabla_rl/models/atari/v_functions.py
+-rw-rw-r--  2.0 unx      622 b- defN 21-Jun-14 03:17 nnabla_rl/models/classic_control/__init__.py
+-rw-rw-r--  2.0 unx     3723 b- defN 21-Jun-14 03:17 nnabla_rl/models/classic_control/policies.py
+-rw-rw-r--  2.0 unx      627 b- defN 21-Jun-14 03:17 nnabla_rl/models/mujoco/__init__.py
+-rw-rw-r--  2.0 unx     5387 b- defN 21-Jun-14 03:17 nnabla_rl/models/mujoco/encoders.py
+-rw-rw-r--  2.0 unx     1696 b- defN 21-Jun-14 03:17 nnabla_rl/models/mujoco/perturbators.py
+-rw-rw-r--  2.0 unx    11836 b- defN 21-Jun-14 03:17 nnabla_rl/models/mujoco/policies.py
+-rw-rw-r--  2.0 unx     3967 b- defN 21-Jun-14 03:17 nnabla_rl/models/mujoco/q_functions.py
+-rw-rw-r--  2.0 unx     1858 b- defN 21-Jun-14 03:17 nnabla_rl/models/mujoco/reward_functions.py
+-rw-rw-r--  2.0 unx     3860 b- defN 21-Jun-14 03:17 nnabla_rl/models/mujoco/v_functions.py
+-rw-rw-r--  2.0 unx      788 b- defN 21-Jun-14 03:17 nnabla_rl/preprocessors/__init__.py
+-rw-rw-r--  2.0 unx      831 b- defN 21-Jun-14 03:17 nnabla_rl/preprocessors/preprocessor.py
+-rw-rw-r--  2.0 unx     3011 b- defN 21-Jun-14 03:17 nnabla_rl/preprocessors/running_mean_normalizer.py
+-rw-rw-r--  2.0 unx     1104 b- defN 21-Jun-14 03:17 nnabla_rl/replay_buffers/__init__.py
+-rw-rw-r--  2.0 unx     2279 b- defN 21-Jun-14 03:17 nnabla_rl/replay_buffers/buffer_iterator.py
+-rw-rw-r--  2.0 unx     1328 b- defN 21-Jun-14 03:17 nnabla_rl/replay_buffers/decorable_replay_buffer.py
+-rw-rw-r--  2.0 unx     4171 b- defN 21-Jun-14 03:17 nnabla_rl/replay_buffers/memory_efficient_atari_buffer.py
+-rw-rw-r--  2.0 unx     6061 b- defN 21-Jun-14 03:17 nnabla_rl/replay_buffers/prioritized_replay_buffer.py
+-rw-rw-r--  2.0 unx     1472 b- defN 21-Jun-14 03:17 nnabla_rl/replay_buffers/replacement_sampling_replay_buffer.py
+-rw-rw-r--  2.0 unx      627 b- defN 21-Jun-14 03:17 nnabla_rl/utils/__init__.py
+-rw-rw-r--  2.0 unx     1418 b- defN 21-Jun-14 03:17 nnabla_rl/utils/context.py
+-rw-rw-r--  2.0 unx     2380 b- defN 21-Jun-14 03:17 nnabla_rl/utils/data.py
+-rw-rw-r--  2.0 unx     2161 b- defN 21-Jun-14 03:17 nnabla_rl/utils/debugging.py
+-rw-rw-r--  2.0 unx     2646 b- defN 21-Jun-14 03:17 nnabla_rl/utils/evaluator.py
+-rw-rw-r--  2.0 unx     1960 b- defN 21-Jun-14 03:17 nnabla_rl/utils/files.py
+-rw-rw-r--  2.0 unx     1815 b- defN 21-Jun-14 03:17 nnabla_rl/utils/matrices.py
+-rw-rw-r--  2.0 unx     1501 b- defN 21-Jun-14 03:17 nnabla_rl/utils/misc.py
+-rw-rw-r--  2.0 unx     3402 b- defN 21-Jun-14 03:17 nnabla_rl/utils/multiprocess.py
+-rw-rw-r--  2.0 unx     1944 b- defN 21-Jun-14 03:17 nnabla_rl/utils/optimization.py
+-rw-rw-r--  2.0 unx     4041 b- defN 21-Jun-14 03:17 nnabla_rl/utils/reproductions.py
+-rw-rw-r--  2.0 unx     5494 b- defN 21-Jun-14 03:17 nnabla_rl/utils/serializers.py
+-rw-rw-r--  2.0 unx      689 b- defN 21-Jun-14 03:17 nnabla_rl/writers/__init__.py
+-rw-rw-r--  2.0 unx     2499 b- defN 21-Jun-14 03:17 nnabla_rl/writers/file_writer.py
+-rw-rw-r--  2.0 unx     1433 b- defN 21-Jun-14 03:17 nnabla_rl/writers/writing_distributor.py
+-rwxrwxr-x  2.0 unx     2114 b- defN 21-Jun-14 03:18 nnabla_rl-0.9.0.data/scripts/check_best_iteration
+-rwxrwxr-x  2.0 unx     8100 b- defN 21-Jun-14 03:18 nnabla_rl-0.9.0.data/scripts/compile_results
+-rwxrwxr-x  2.0 unx     4640 b- defN 21-Jun-14 03:17 nnabla_rl-0.9.0.data/scripts/evaluate_algorithm
+-rwxrwxr-x  2.0 unx     6433 b- defN 21-Jun-14 03:18 nnabla_rl-0.9.0.data/scripts/insert_copyright
+-rwxrwxr-x  2.0 unx     4233 b- defN 21-Jun-14 03:18 nnabla_rl-0.9.0.data/scripts/plot_result
+-rwxrwxr-x  2.0 unx     2169 b- defN 21-Jun-14 03:17 nnabla_rl-0.9.0.data/scripts/test_reproductions
+-rwxrwxr-x  2.0 unx      815 b- defN 21-Jun-14 03:17 nnabla_rl-0.9.0.data/scripts/train_with_seeds
+-rw-rw-r--  2.0 unx      627 b- defN 21-Jun-14 03:17 tests/algorithms/__init__.py
+-rw-rw-r--  2.0 unx     3241 b- defN 21-Jun-14 03:17 tests/algorithms/test_a2c.py
+-rw-rw-r--  2.0 unx     4385 b- defN 21-Jun-14 03:17 tests/algorithms/test_bcq.py
+-rw-rw-r--  2.0 unx     4365 b- defN 21-Jun-14 03:17 tests/algorithms/test_bear.py
+-rw-rw-r--  2.0 unx     3698 b- defN 21-Jun-14 03:17 tests/algorithms/test_categorical_dqn.py
+-rw-rw-r--  2.0 unx     4323 b- defN 21-Jun-14 03:17 tests/algorithms/test_common_utils.py
+-rw-rw-r--  2.0 unx     3538 b- defN 21-Jun-14 03:17 tests/algorithms/test_ddpg.py
+-rw-rw-r--  2.0 unx     4500 b- defN 21-Jun-14 03:17 tests/algorithms/test_dqn.py
+-rw-rw-r--  2.0 unx     2701 b- defN 21-Jun-14 03:17 tests/algorithms/test_dummy.py
+-rw-rw-r--  2.0 unx     5148 b- defN 21-Jun-14 03:17 tests/algorithms/test_gail.py
+-rw-rw-r--  2.0 unx     4224 b- defN 21-Jun-14 03:17 tests/algorithms/test_icml2015_trpo.py
+-rw-rw-r--  2.0 unx     4874 b- defN 21-Jun-14 03:17 tests/algorithms/test_icml2018_sac.py
+-rw-rw-r--  2.0 unx     1808 b- defN 21-Jun-14 03:17 tests/algorithms/test_init_py.py
+-rw-rw-r--  2.0 unx     4473 b- defN 21-Jun-14 03:17 tests/algorithms/test_iqn.py
+-rw-rw-r--  2.0 unx     4721 b- defN 21-Jun-14 03:17 tests/algorithms/test_munchausen_dqn.py
+-rw-rw-r--  2.0 unx     4844 b- defN 21-Jun-14 03:17 tests/algorithms/test_munchausen_iqn.py
+-rw-rw-r--  2.0 unx     3418 b- defN 21-Jun-14 03:17 tests/algorithms/test_ppo.py
+-rw-rw-r--  2.0 unx     4354 b- defN 21-Jun-14 03:17 tests/algorithms/test_qrdqn.py
+-rw-rw-r--  2.0 unx     2710 b- defN 21-Jun-14 03:17 tests/algorithms/test_reinforce.py
+-rw-rw-r--  2.0 unx     4044 b- defN 21-Jun-14 03:17 tests/algorithms/test_sac.py
+-rw-rw-r--  2.0 unx     4484 b- defN 21-Jun-14 03:17 tests/algorithms/test_td3.py
+-rw-rw-r--  2.0 unx     3993 b- defN 21-Jun-14 03:17 tests/algorithms/test_trpo.py
+-rw-rw-r--  2.0 unx     2315 b- defN 21-Jun-14 03:17 tests/algorithms/testing_utils.py
+-rw-rw-r--  2.0 unx      627 b- defN 21-Jun-14 03:17 tests/distributions/__init__.py
+-rw-rw-r--  2.0 unx     1731 b- defN 21-Jun-14 03:17 tests/distributions/test_common_utils.py
+-rw-rw-r--  2.0 unx     8744 b- defN 21-Jun-14 03:17 tests/distributions/test_gaussian.py
+-rw-rw-r--  2.0 unx     3680 b- defN 21-Jun-14 03:17 tests/distributions/test_softmax.py
+-rw-rw-r--  2.0 unx     9258 b- defN 21-Jun-14 03:17 tests/distributions/test_squashed_gaussian.py
+-rw-rw-r--  2.0 unx      627 b- defN 21-Jun-14 03:17 tests/environment_explorers/__init__.py
+-rw-rw-r--  2.0 unx     4161 b- defN 21-Jun-14 03:17 tests/environment_explorers/test_epsilon_greedy.py
+-rw-rw-r--  2.0 unx     1844 b- defN 21-Jun-14 03:17 tests/environment_explorers/test_gaussian.py
+-rw-rw-r--  2.0 unx      627 b- defN 21-Jun-14 03:17 tests/environments/__init__.py
+-rw-rw-r--  2.0 unx     3461 b- defN 21-Jun-14 03:17 tests/environments/test_env_info.py
+-rw-rw-r--  2.0 unx      627 b- defN 21-Jun-14 03:17 tests/environments/wrappers/__init__.py
+-rw-rw-r--  2.0 unx     1664 b- defN 21-Jun-14 03:17 tests/environments/wrappers/test_common.py
+-rw-rw-r--  2.0 unx      627 b- defN 21-Jun-14 03:17 tests/hooks/__init__.py
+-rw-rw-r--  2.0 unx     1370 b- defN 21-Jun-14 03:17 tests/hooks/test_evaluation_hook.py
+-rw-rw-r--  2.0 unx     1175 b- defN 21-Jun-14 03:17 tests/hooks/test_iteration_num_hook.py
+-rw-rw-r--  2.0 unx     1947 b- defN 21-Jun-14 03:17 tests/hooks/test_iteration_state_hook.py
+-rw-rw-r--  2.0 unx      627 b- defN 21-Jun-14 03:17 tests/model_trainers/__init__.py
+-rw-rw-r--  2.0 unx     9367 b- defN 21-Jun-14 03:17 tests/model_trainers/test_policy_trainers.py
+-rw-rw-r--  2.0 unx     1545 b- defN 21-Jun-14 03:17 tests/model_trainers/test_q_value_trainers.py
+-rw-rw-r--  2.0 unx      830 b- defN 21-Jun-14 03:17 tests/model_trainers/test_v_value_trainers.py
+-rw-rw-r--  2.0 unx      627 b- defN 21-Jun-14 03:17 tests/models/__init__.py
+-rw-rw-r--  2.0 unx     1805 b- defN 21-Jun-14 03:17 tests/models/test_distributional_function.py
+-rw-rw-r--  2.0 unx     5064 b- defN 21-Jun-14 03:17 tests/models/test_model.py
+-rw-rw-r--  2.0 unx      627 b- defN 21-Jun-14 03:17 tests/models/atari/__init__.py
+-rw-rw-r--  2.0 unx     9231 b- defN 21-Jun-14 03:17 tests/models/atari/test_distributional_functions.py
+-rw-rw-r--  2.0 unx     4142 b- defN 21-Jun-14 03:17 tests/models/atari/test_q_functions.py
+-rw-rw-r--  2.0 unx      627 b- defN 21-Jun-14 03:17 tests/models/classic_control/__init__.py
+-rw-rw-r--  2.0 unx     2331 b- defN 21-Jun-14 03:17 tests/models/classic_control/test_policies.py
+-rw-rw-r--  2.0 unx      627 b- defN 21-Jun-14 03:17 tests/models/mujoco/__init__.py
+-rw-rw-r--  2.0 unx     2394 b- defN 21-Jun-14 03:17 tests/models/mujoco/test_policies.py
+-rw-rw-r--  2.0 unx     1516 b- defN 21-Jun-14 03:17 tests/models/mujoco/test_q_functions.py
+-rw-rw-r--  2.0 unx      627 b- defN 21-Jun-14 03:17 tests/preprocessors/__init__.py
+-rw-rw-r--  2.0 unx     2721 b- defN 21-Jun-14 03:17 tests/preprocessors/test_running_mean_normalizer.py
+-rw-rw-r--  2.0 unx      627 b- defN 21-Jun-14 03:17 tests/replay_buffers/__init__.py
+-rw-rw-r--  2.0 unx     2869 b- defN 21-Jun-14 03:17 tests/replay_buffers/test_buffer_iterator.py
+-rw-rw-r--  2.0 unx     1454 b- defN 21-Jun-14 03:17 tests/replay_buffers/test_decorable_replay_buffer.py
+-rw-rw-r--  2.0 unx     4253 b- defN 21-Jun-14 03:17 tests/replay_buffers/test_memory_efficient_atari_buffer.py
+-rw-rw-r--  2.0 unx    12298 b- defN 21-Jun-14 03:17 tests/replay_buffers/test_prioritized_replay_buffer.py
+-rw-rw-r--  2.0 unx     1768 b- defN 21-Jun-14 03:17 tests/replay_buffers/test_replacement_sampling_replay_buffer.py
+-rw-rw-r--  2.0 unx      627 b- defN 21-Jun-14 03:17 tests/utils/__init__.py
+-rw-rw-r--  2.0 unx     4173 b- defN 21-Jun-14 03:17 tests/utils/test_copy.py
+-rw-rw-r--  2.0 unx     1826 b- defN 21-Jun-14 03:17 tests/utils/test_data.py
+-rw-rw-r--  2.0 unx     1339 b- defN 21-Jun-14 03:17 tests/utils/test_debugging.py
+-rw-rw-r--  2.0 unx     2899 b- defN 21-Jun-14 03:17 tests/utils/test_evaluator.py
+-rw-rw-r--  2.0 unx     3012 b- defN 21-Jun-14 03:17 tests/utils/test_files.py
+-rw-rw-r--  2.0 unx     2179 b- defN 21-Jun-14 03:17 tests/utils/test_matrices.py
+-rw-rw-r--  2.0 unx     4928 b- defN 21-Jun-14 03:17 tests/utils/test_multiprocess.py
+-rw-rw-r--  2.0 unx     1237 b- defN 21-Jun-14 03:17 tests/utils/test_optimization.py
+-rw-rw-r--  2.0 unx     2204 b- defN 21-Jun-14 03:17 tests/utils/test_reproductions.py
+-rw-rw-r--  2.0 unx     1421 b- defN 21-Jun-14 03:17 tests/utils/test_serializers.py
+-rw-rw-r--  2.0 unx      627 b- defN 21-Jun-14 03:17 tests/writers/__init__.py
+-rw-rw-r--  2.0 unx     2861 b- defN 21-Jun-14 03:17 tests/writers/test_file_writer.py
+-rw-rw-r--  2.0 unx     1753 b- defN 21-Jun-14 03:17 tests/writers/test_writing_distributor.py
+-rw-rw-r--  2.0 unx    11358 b- defN 21-Jun-14 03:18 nnabla_rl-0.9.0.dist-info/LICENSE
+-rw-rw-r--  2.0 unx     1144 b- defN 21-Jun-14 03:18 nnabla_rl-0.9.0.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 21-Jun-14 03:18 nnabla_rl-0.9.0.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       16 b- defN 21-Jun-14 03:18 nnabla_rl-0.9.0.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx    22468 b- defN 21-Jun-14 03:18 nnabla_rl-0.9.0.dist-info/RECORD
+237 files, 1036924 bytes uncompressed, 313459 bytes compressed:  69.8%
```

## zipnote {}

```diff
@@ -21,20 +21,14 @@
 
 Filename: nnabla_rl/initializers.py
 Comment: 
 
 Filename: nnabla_rl/logger.py
 Comment: 
 
-Filename: nnabla_rl/parametric_functions.py
-Comment: 
-
-Filename: nnabla_rl/random.py
-Comment: 
-
 Filename: nnabla_rl/replay_buffer.py
 Comment: 
 
 Filename: nnabla_rl/scopes.py
 Comment: 
 
 Filename: nnabla_rl/typing.py
@@ -45,143 +39,74 @@
 
 Filename: nnabla_rl/algorithms/__init__.py
 Comment: 
 
 Filename: nnabla_rl/algorithms/a2c.py
 Comment: 
 
-Filename: nnabla_rl/algorithms/amp.py
-Comment: 
-
-Filename: nnabla_rl/algorithms/atrpo.py
-Comment: 
-
 Filename: nnabla_rl/algorithms/bcq.py
 Comment: 
 
 Filename: nnabla_rl/algorithms/bear.py
 Comment: 
 
-Filename: nnabla_rl/algorithms/categorical_ddqn.py
-Comment: 
-
 Filename: nnabla_rl/algorithms/categorical_dqn.py
 Comment: 
 
 Filename: nnabla_rl/algorithms/common_utils.py
 Comment: 
 
-Filename: nnabla_rl/algorithms/ddp.py
-Comment: 
-
 Filename: nnabla_rl/algorithms/ddpg.py
 Comment: 
 
-Filename: nnabla_rl/algorithms/ddqn.py
-Comment: 
-
-Filename: nnabla_rl/algorithms/decision_transformer.py
-Comment: 
-
-Filename: nnabla_rl/algorithms/demme_sac.py
-Comment: 
-
 Filename: nnabla_rl/algorithms/dqn.py
 Comment: 
 
-Filename: nnabla_rl/algorithms/drqn.py
-Comment: 
-
 Filename: nnabla_rl/algorithms/dummy.py
 Comment: 
 
 Filename: nnabla_rl/algorithms/gail.py
 Comment: 
 
-Filename: nnabla_rl/algorithms/her.py
-Comment: 
-
-Filename: nnabla_rl/algorithms/hyar.py
-Comment: 
-
 Filename: nnabla_rl/algorithms/icml2015_trpo.py
 Comment: 
 
 Filename: nnabla_rl/algorithms/icml2018_sac.py
 Comment: 
 
-Filename: nnabla_rl/algorithms/icra2018_qtopt.py
-Comment: 
-
-Filename: nnabla_rl/algorithms/ilqr.py
-Comment: 
-
 Filename: nnabla_rl/algorithms/iqn.py
 Comment: 
 
-Filename: nnabla_rl/algorithms/lqr.py
-Comment: 
-
-Filename: nnabla_rl/algorithms/mme_sac.py
-Comment: 
-
-Filename: nnabla_rl/algorithms/mppi.py
-Comment: 
-
 Filename: nnabla_rl/algorithms/munchausen_dqn.py
 Comment: 
 
 Filename: nnabla_rl/algorithms/munchausen_iqn.py
 Comment: 
 
 Filename: nnabla_rl/algorithms/ppo.py
 Comment: 
 
 Filename: nnabla_rl/algorithms/qrdqn.py
 Comment: 
 
-Filename: nnabla_rl/algorithms/qrsac.py
-Comment: 
-
-Filename: nnabla_rl/algorithms/rainbow.py
-Comment: 
-
-Filename: nnabla_rl/algorithms/redq.py
-Comment: 
-
 Filename: nnabla_rl/algorithms/reinforce.py
 Comment: 
 
 Filename: nnabla_rl/algorithms/sac.py
 Comment: 
 
-Filename: nnabla_rl/algorithms/sacd.py
-Comment: 
-
-Filename: nnabla_rl/algorithms/srsac.py
-Comment: 
-
 Filename: nnabla_rl/algorithms/td3.py
 Comment: 
 
 Filename: nnabla_rl/algorithms/trpo.py
 Comment: 
 
-Filename: nnabla_rl/algorithms/xql.py
-Comment: 
-
 Filename: nnabla_rl/builders/__init__.py
 Comment: 
 
-Filename: nnabla_rl/builders/explorer_builder.py
-Comment: 
-
-Filename: nnabla_rl/builders/lr_scheduler_builder.py
-Comment: 
-
 Filename: nnabla_rl/builders/model_builder.py
 Comment: 
 
 Filename: nnabla_rl/builders/preprocessor_builder.py
 Comment: 
 
 Filename: nnabla_rl/builders/replay_buffer_builder.py
@@ -189,32 +114,23 @@
 
 Filename: nnabla_rl/builders/solver_builder.py
 Comment: 
 
 Filename: nnabla_rl/distributions/__init__.py
 Comment: 
 
-Filename: nnabla_rl/distributions/bernoulli.py
-Comment: 
-
 Filename: nnabla_rl/distributions/common_utils.py
 Comment: 
 
 Filename: nnabla_rl/distributions/distribution.py
 Comment: 
 
 Filename: nnabla_rl/distributions/gaussian.py
 Comment: 
 
-Filename: nnabla_rl/distributions/gmm.py
-Comment: 
-
-Filename: nnabla_rl/distributions/one_hot_softmax.py
-Comment: 
-
 Filename: nnabla_rl/distributions/softmax.py
 Comment: 
 
 Filename: nnabla_rl/distributions/squashed_gaussian.py
 Comment: 
 
 Filename: nnabla_rl/environment_explorers/__init__.py
@@ -228,152 +144,83 @@
 
 Filename: nnabla_rl/environment_explorers/raw_policy_explorer.py
 Comment: 
 
 Filename: nnabla_rl/environments/__init__.py
 Comment: 
 
-Filename: nnabla_rl/environments/amp_env.py
-Comment: 
-
 Filename: nnabla_rl/environments/dummy.py
 Comment: 
 
 Filename: nnabla_rl/environments/environment_info.py
 Comment: 
 
-Filename: nnabla_rl/environments/factored_envs.py
-Comment: 
-
-Filename: nnabla_rl/environments/gym_utils.py
-Comment: 
-
 Filename: nnabla_rl/environments/wrappers/__init__.py
 Comment: 
 
 Filename: nnabla_rl/environments/wrappers/atari.py
 Comment: 
 
 Filename: nnabla_rl/environments/wrappers/common.py
 Comment: 
 
-Filename: nnabla_rl/environments/wrappers/goal_conditioned.py
-Comment: 
-
-Filename: nnabla_rl/environments/wrappers/gymnasium.py
-Comment: 
-
-Filename: nnabla_rl/environments/wrappers/hybrid_env.py
-Comment: 
-
-Filename: nnabla_rl/environments/wrappers/mujoco.py
-Comment: 
-
 Filename: nnabla_rl/external/__init__.py
 Comment: 
 
 Filename: nnabla_rl/external/atari_wrappers.py
 Comment: 
 
-Filename: nnabla_rl/external/dmc_env.py
-Comment: 
-
-Filename: nnabla_rl/external/goal_env.py
-Comment: 
-
 Filename: nnabla_rl/hooks/__init__.py
 Comment: 
 
-Filename: nnabla_rl/hooks/computational_graph_hook.py
-Comment: 
-
-Filename: nnabla_rl/hooks/epoch_num_hook.py
-Comment: 
-
 Filename: nnabla_rl/hooks/evaluation_hook.py
 Comment: 
 
 Filename: nnabla_rl/hooks/iteration_num_hook.py
 Comment: 
 
 Filename: nnabla_rl/hooks/iteration_state_hook.py
 Comment: 
 
-Filename: nnabla_rl/hooks/progress_bar_hook.py
-Comment: 
-
 Filename: nnabla_rl/hooks/save_snapshot_hook.py
 Comment: 
 
 Filename: nnabla_rl/hooks/time_measuring_hook.py
 Comment: 
 
 Filename: nnabla_rl/model_trainers/__init__.py
 Comment: 
 
 Filename: nnabla_rl/model_trainers/model_trainer.py
 Comment: 
 
-Filename: nnabla_rl/model_trainers/decision_transformer/__init__.py
-Comment: 
-
-Filename: nnabla_rl/model_trainers/decision_transformer/decision_transformer_trainer.py
-Comment: 
-
-Filename: nnabla_rl/model_trainers/dynamics/__init__.py
-Comment: 
-
-Filename: nnabla_rl/model_trainers/dynamics/mppi_dynamics_trainer.py
-Comment: 
-
 Filename: nnabla_rl/model_trainers/encoder/__init__.py
 Comment: 
 
-Filename: nnabla_rl/model_trainers/encoder/hyar_vae_trainer.py
-Comment: 
-
 Filename: nnabla_rl/model_trainers/encoder/kld_variational_auto_encoder_trainer.py
 Comment: 
 
-Filename: nnabla_rl/model_trainers/hybrid/__init__.py
-Comment: 
-
-Filename: nnabla_rl/model_trainers/hybrid/srsac_actor_critic_trainer.py
-Comment: 
-
 Filename: nnabla_rl/model_trainers/perturbator/__init__.py
 Comment: 
 
 Filename: nnabla_rl/model_trainers/perturbator/bcq_perturbator_trainer.py
 Comment: 
 
 Filename: nnabla_rl/model_trainers/policy/__init__.py
 Comment: 
 
 Filename: nnabla_rl/model_trainers/policy/a2c_policy_trainer.py
 Comment: 
 
-Filename: nnabla_rl/model_trainers/policy/amp_policy_trainer.py
-Comment: 
-
 Filename: nnabla_rl/model_trainers/policy/bear_policy_trainer.py
 Comment: 
 
-Filename: nnabla_rl/model_trainers/policy/demme_policy_trainer.py
-Comment: 
-
 Filename: nnabla_rl/model_trainers/policy/dpg_policy_trainer.py
 Comment: 
 
-Filename: nnabla_rl/model_trainers/policy/her_policy_trainer.py
-Comment: 
-
-Filename: nnabla_rl/model_trainers/policy/hyar_policy_trainer.py
-Comment: 
-
 Filename: nnabla_rl/model_trainers/policy/ppo_policy_trainer.py
 Comment: 
 
 Filename: nnabla_rl/model_trainers/policy/reinforce_policy_trainer.py
 Comment: 
 
 Filename: nnabla_rl/model_trainers/policy/soft_policy_trainer.py
@@ -381,29 +228,20 @@
 
 Filename: nnabla_rl/model_trainers/policy/spg_policy_trainer.py
 Comment: 
 
 Filename: nnabla_rl/model_trainers/policy/trpo_policy_trainer.py
 Comment: 
 
-Filename: nnabla_rl/model_trainers/policy/xql_forward_policy_trainer.py
-Comment: 
-
-Filename: nnabla_rl/model_trainers/policy/xql_reverse_policy_trainer.py
-Comment: 
-
 Filename: nnabla_rl/model_trainers/q_value/__init__.py
 Comment: 
 
 Filename: nnabla_rl/model_trainers/q_value/bcq_q_trainer.py
 Comment: 
 
-Filename: nnabla_rl/model_trainers/q_value/categorical_ddqn_q_trainer.py
-Comment: 
-
 Filename: nnabla_rl/model_trainers/q_value/categorical_dqn_q_trainer.py
 Comment: 
 
 Filename: nnabla_rl/model_trainers/q_value/clipped_double_q_trainer.py
 Comment: 
 
 Filename: nnabla_rl/model_trainers/q_value/ddpg_q_trainer.py
@@ -411,44 +249,26 @@
 
 Filename: nnabla_rl/model_trainers/q_value/ddqn_q_trainer.py
 Comment: 
 
 Filename: nnabla_rl/model_trainers/q_value/dqn_q_trainer.py
 Comment: 
 
-Filename: nnabla_rl/model_trainers/q_value/her_q_trainer.py
-Comment: 
-
-Filename: nnabla_rl/model_trainers/q_value/hyar_q_trainer.py
-Comment: 
-
 Filename: nnabla_rl/model_trainers/q_value/iqn_q_trainer.py
 Comment: 
 
-Filename: nnabla_rl/model_trainers/q_value/multi_step_trainer.py
-Comment: 
-
 Filename: nnabla_rl/model_trainers/q_value/munchausen_rl_q_trainer.py
 Comment: 
 
 Filename: nnabla_rl/model_trainers/q_value/qrdqn_q_trainer.py
 Comment: 
 
-Filename: nnabla_rl/model_trainers/q_value/qrsac_q_trainer.py
-Comment: 
-
 Filename: nnabla_rl/model_trainers/q_value/quantile_distribution_function_trainer.py
 Comment: 
 
-Filename: nnabla_rl/model_trainers/q_value/redq_q_trainer.py
-Comment: 
-
-Filename: nnabla_rl/model_trainers/q_value/soft_q_decomposition_trainer.py
-Comment: 
-
 Filename: nnabla_rl/model_trainers/q_value/soft_q_trainer.py
 Comment: 
 
 Filename: nnabla_rl/model_trainers/q_value/squared_td_q_function_trainer.py
 Comment: 
 
 Filename: nnabla_rl/model_trainers/q_value/state_action_quantile_function_trainer.py
@@ -462,59 +282,35 @@
 
 Filename: nnabla_rl/model_trainers/q_value/value_distribution_function_trainer.py
 Comment: 
 
 Filename: nnabla_rl/model_trainers/reward/__init__.py
 Comment: 
 
-Filename: nnabla_rl/model_trainers/reward/amp_reward_function_trainer.py
-Comment: 
-
 Filename: nnabla_rl/model_trainers/reward/gail_reward_function_trainer.py
 Comment: 
 
 Filename: nnabla_rl/model_trainers/v_value/__init__.py
 Comment: 
 
-Filename: nnabla_rl/model_trainers/v_value/demme_v_trainer.py
-Comment: 
-
-Filename: nnabla_rl/model_trainers/v_value/extreme_v_function_trainer.py
-Comment: 
-
-Filename: nnabla_rl/model_trainers/v_value/mme_v_trainer.py
-Comment: 
-
 Filename: nnabla_rl/model_trainers/v_value/monte_carlo_v_trainer.py
 Comment: 
 
 Filename: nnabla_rl/model_trainers/v_value/soft_v_trainer.py
 Comment: 
 
 Filename: nnabla_rl/model_trainers/v_value/squared_td_v_function_trainer.py
 Comment: 
 
-Filename: nnabla_rl/model_trainers/v_value/v_function_trainer.py
-Comment: 
-
-Filename: nnabla_rl/model_trainers/v_value/xql_v_trainer.py
-Comment: 
-
 Filename: nnabla_rl/models/__init__.py
 Comment: 
 
-Filename: nnabla_rl/models/decision_transformer.py
-Comment: 
-
 Filename: nnabla_rl/models/distributional_function.py
 Comment: 
 
-Filename: nnabla_rl/models/dynamics.py
-Comment: 
-
 Filename: nnabla_rl/models/encoder.py
 Comment: 
 
 Filename: nnabla_rl/models/model.py
 Comment: 
 
 Filename: nnabla_rl/models/perturbator.py
@@ -531,17 +327,14 @@
 
 Filename: nnabla_rl/models/v_function.py
 Comment: 
 
 Filename: nnabla_rl/models/atari/__init__.py
 Comment: 
 
-Filename: nnabla_rl/models/atari/decision_transformers.py
-Comment: 
-
 Filename: nnabla_rl/models/atari/distributional_functions.py
 Comment: 
 
 Filename: nnabla_rl/models/atari/policies.py
 Comment: 
 
 Filename: nnabla_rl/models/atari/q_functions.py
@@ -552,41 +345,20 @@
 
 Filename: nnabla_rl/models/atari/v_functions.py
 Comment: 
 
 Filename: nnabla_rl/models/classic_control/__init__.py
 Comment: 
 
-Filename: nnabla_rl/models/classic_control/dynamics.py
-Comment: 
-
 Filename: nnabla_rl/models/classic_control/policies.py
 Comment: 
 
-Filename: nnabla_rl/models/hybrid_env/__init__.py
-Comment: 
-
-Filename: nnabla_rl/models/hybrid_env/encoders.py
-Comment: 
-
-Filename: nnabla_rl/models/hybrid_env/policies.py
-Comment: 
-
-Filename: nnabla_rl/models/hybrid_env/q_functions.py
-Comment: 
-
 Filename: nnabla_rl/models/mujoco/__init__.py
 Comment: 
 
-Filename: nnabla_rl/models/mujoco/decision_transformers.py
-Comment: 
-
-Filename: nnabla_rl/models/mujoco/distributional_functions.py
-Comment: 
-
 Filename: nnabla_rl/models/mujoco/encoders.py
 Comment: 
 
 Filename: nnabla_rl/models/mujoco/perturbators.py
 Comment: 
 
 Filename: nnabla_rl/models/mujoco/policies.py
@@ -597,68 +369,17 @@
 
 Filename: nnabla_rl/models/mujoco/reward_functions.py
 Comment: 
 
 Filename: nnabla_rl/models/mujoco/v_functions.py
 Comment: 
 
-Filename: nnabla_rl/models/pybullet/__init__.py
-Comment: 
-
-Filename: nnabla_rl/models/pybullet/policy.py
-Comment: 
-
-Filename: nnabla_rl/models/pybullet/q_functions.py
-Comment: 
-
-Filename: nnabla_rl/models/pybullet/reward_functions.py
-Comment: 
-
-Filename: nnabla_rl/models/pybullet/v_functions.py
-Comment: 
-
-Filename: nnabla_rl/numpy_model_trainers/__init__.py
-Comment: 
-
-Filename: nnabla_rl/numpy_model_trainers/numpy_model_trainer.py
-Comment: 
-
-Filename: nnabla_rl/numpy_model_trainers/distribution_parameters/__init__.py
-Comment: 
-
-Filename: nnabla_rl/numpy_model_trainers/distribution_parameters/gmm_parameter_trainer.py
-Comment: 
-
-Filename: nnabla_rl/numpy_models/__init__.py
-Comment: 
-
-Filename: nnabla_rl/numpy_models/cost_function.py
-Comment: 
-
-Filename: nnabla_rl/numpy_models/distribution_parameter.py
-Comment: 
-
-Filename: nnabla_rl/numpy_models/dynamics.py
-Comment: 
-
-Filename: nnabla_rl/numpy_models/numpy_model.py
-Comment: 
-
-Filename: nnabla_rl/numpy_models/distribution_parameters/__init__.py
-Comment: 
-
-Filename: nnabla_rl/numpy_models/distribution_parameters/gmm_parameter.py
-Comment: 
-
 Filename: nnabla_rl/preprocessors/__init__.py
 Comment: 
 
-Filename: nnabla_rl/preprocessors/her_preprocessor.py
-Comment: 
-
 Filename: nnabla_rl/preprocessors/preprocessor.py
 Comment: 
 
 Filename: nnabla_rl/preprocessors/running_mean_normalizer.py
 Comment: 
 
 Filename: nnabla_rl/replay_buffers/__init__.py
@@ -666,29 +387,23 @@
 
 Filename: nnabla_rl/replay_buffers/buffer_iterator.py
 Comment: 
 
 Filename: nnabla_rl/replay_buffers/decorable_replay_buffer.py
 Comment: 
 
-Filename: nnabla_rl/replay_buffers/hindsight_replay_buffer.py
-Comment: 
-
 Filename: nnabla_rl/replay_buffers/memory_efficient_atari_buffer.py
 Comment: 
 
 Filename: nnabla_rl/replay_buffers/prioritized_replay_buffer.py
 Comment: 
 
 Filename: nnabla_rl/replay_buffers/replacement_sampling_replay_buffer.py
 Comment: 
 
-Filename: nnabla_rl/replay_buffers/trajectory_replay_buffer.py
-Comment: 
-
 Filename: nnabla_rl/utils/__init__.py
 Comment: 
 
 Filename: nnabla_rl/utils/context.py
 Comment: 
 
 Filename: nnabla_rl/utils/data.py
@@ -717,197 +432,122 @@
 
 Filename: nnabla_rl/utils/reproductions.py
 Comment: 
 
 Filename: nnabla_rl/utils/serializers.py
 Comment: 
 
-Filename: nnabla_rl/utils/solver_wrappers.py
-Comment: 
-
 Filename: nnabla_rl/writers/__init__.py
 Comment: 
 
 Filename: nnabla_rl/writers/file_writer.py
 Comment: 
 
-Filename: nnabla_rl/writers/monitor_writer.py
-Comment: 
-
 Filename: nnabla_rl/writers/writing_distributor.py
 Comment: 
 
-Filename: nnabla_rl-0.15.0.data/scripts/check_best_iteration
+Filename: nnabla_rl-0.9.0.data/scripts/check_best_iteration
 Comment: 
 
-Filename: nnabla_rl-0.15.0.data/scripts/compile_results
+Filename: nnabla_rl-0.9.0.data/scripts/compile_results
 Comment: 
 
-Filename: nnabla_rl-0.15.0.data/scripts/evaluate_algorithm
+Filename: nnabla_rl-0.9.0.data/scripts/evaluate_algorithm
 Comment: 
 
-Filename: nnabla_rl-0.15.0.data/scripts/insert_copyright
+Filename: nnabla_rl-0.9.0.data/scripts/insert_copyright
 Comment: 
 
-Filename: nnabla_rl-0.15.0.data/scripts/plot_result
+Filename: nnabla_rl-0.9.0.data/scripts/plot_result
 Comment: 
 
-Filename: nnabla_rl-0.15.0.data/scripts/test_reproductions
+Filename: nnabla_rl-0.9.0.data/scripts/test_reproductions
 Comment: 
 
-Filename: nnabla_rl-0.15.0.data/scripts/train_with_seeds
+Filename: nnabla_rl-0.9.0.data/scripts/train_with_seeds
 Comment: 
 
 Filename: tests/algorithms/__init__.py
 Comment: 
 
 Filename: tests/algorithms/test_a2c.py
 Comment: 
 
-Filename: tests/algorithms/test_amp.py
-Comment: 
-
-Filename: tests/algorithms/test_atrpo.py
-Comment: 
-
 Filename: tests/algorithms/test_bcq.py
 Comment: 
 
 Filename: tests/algorithms/test_bear.py
 Comment: 
 
-Filename: tests/algorithms/test_categorical_ddqn.py
-Comment: 
-
 Filename: tests/algorithms/test_categorical_dqn.py
 Comment: 
 
 Filename: tests/algorithms/test_common_utils.py
 Comment: 
 
-Filename: tests/algorithms/test_ddp.py
-Comment: 
-
 Filename: tests/algorithms/test_ddpg.py
 Comment: 
 
-Filename: tests/algorithms/test_ddqn.py
-Comment: 
-
-Filename: tests/algorithms/test_decision_transformer.py
-Comment: 
-
-Filename: tests/algorithms/test_demme_sac.py
-Comment: 
-
 Filename: tests/algorithms/test_dqn.py
 Comment: 
 
-Filename: tests/algorithms/test_drqn.py
-Comment: 
-
 Filename: tests/algorithms/test_dummy.py
 Comment: 
 
 Filename: tests/algorithms/test_gail.py
 Comment: 
 
-Filename: tests/algorithms/test_her.py
-Comment: 
-
-Filename: tests/algorithms/test_hyar.py
-Comment: 
-
 Filename: tests/algorithms/test_icml2015_trpo.py
 Comment: 
 
 Filename: tests/algorithms/test_icml2018_sac.py
 Comment: 
 
-Filename: tests/algorithms/test_icra2018_qtopt.py
-Comment: 
-
-Filename: tests/algorithms/test_ilqr.py
-Comment: 
-
 Filename: tests/algorithms/test_init_py.py
 Comment: 
 
 Filename: tests/algorithms/test_iqn.py
 Comment: 
 
-Filename: tests/algorithms/test_lqr.py
-Comment: 
-
-Filename: tests/algorithms/test_mme_sac.py
-Comment: 
-
-Filename: tests/algorithms/test_mppi.py
-Comment: 
-
 Filename: tests/algorithms/test_munchausen_dqn.py
 Comment: 
 
 Filename: tests/algorithms/test_munchausen_iqn.py
 Comment: 
 
 Filename: tests/algorithms/test_ppo.py
 Comment: 
 
 Filename: tests/algorithms/test_qrdqn.py
 Comment: 
 
-Filename: tests/algorithms/test_qrsac.py
-Comment: 
-
-Filename: tests/algorithms/test_rainbow.py
-Comment: 
-
-Filename: tests/algorithms/test_redq.py
-Comment: 
-
 Filename: tests/algorithms/test_reinforce.py
 Comment: 
 
 Filename: tests/algorithms/test_sac.py
 Comment: 
 
-Filename: tests/algorithms/test_sacd.py
-Comment: 
-
-Filename: tests/algorithms/test_srsac.py
-Comment: 
-
 Filename: tests/algorithms/test_td3.py
 Comment: 
 
 Filename: tests/algorithms/test_trpo.py
 Comment: 
 
-Filename: tests/algorithms/test_xql.py
+Filename: tests/algorithms/testing_utils.py
 Comment: 
 
 Filename: tests/distributions/__init__.py
 Comment: 
 
-Filename: tests/distributions/test_bernoulli.py
-Comment: 
-
 Filename: tests/distributions/test_common_utils.py
 Comment: 
 
 Filename: tests/distributions/test_gaussian.py
 Comment: 
 
-Filename: tests/distributions/test_gmm.py
-Comment: 
-
-Filename: tests/distributions/test_one_hot_softmax.py
-Comment: 
-
 Filename: tests/distributions/test_softmax.py
 Comment: 
 
 Filename: tests/distributions/test_squashed_gaussian.py
 Comment: 
 
 Filename: tests/environment_explorers/__init__.py
@@ -918,59 +558,38 @@
 
 Filename: tests/environment_explorers/test_gaussian.py
 Comment: 
 
 Filename: tests/environments/__init__.py
 Comment: 
 
-Filename: tests/environments/test_amp_env.py
-Comment: 
-
 Filename: tests/environments/test_env_info.py
 Comment: 
 
-Filename: tests/environments/test_gym_utils.py
-Comment: 
-
 Filename: tests/environments/wrappers/__init__.py
 Comment: 
 
-Filename: tests/environments/wrappers/test_atari.py
-Comment: 
-
 Filename: tests/environments/wrappers/test_common.py
 Comment: 
 
-Filename: tests/environments/wrappers/test_goal_conditioned.py
-Comment: 
-
-Filename: tests/environments/wrappers/test_gymnasium.py
-Comment: 
-
 Filename: tests/hooks/__init__.py
 Comment: 
 
-Filename: tests/hooks/test_computational_graph_hook.py
-Comment: 
-
 Filename: tests/hooks/test_evaluation_hook.py
 Comment: 
 
 Filename: tests/hooks/test_iteration_num_hook.py
 Comment: 
 
 Filename: tests/hooks/test_iteration_state_hook.py
 Comment: 
 
 Filename: tests/model_trainers/__init__.py
 Comment: 
 
-Filename: tests/model_trainers/test_model_trainer.py
-Comment: 
-
 Filename: tests/model_trainers/test_policy_trainers.py
 Comment: 
 
 Filename: tests/model_trainers/test_q_value_trainers.py
 Comment: 
 
 Filename: tests/model_trainers/test_v_value_trainers.py
@@ -1005,26 +624,14 @@
 
 Filename: tests/models/mujoco/test_policies.py
 Comment: 
 
 Filename: tests/models/mujoco/test_q_functions.py
 Comment: 
 
-Filename: tests/numpy_model_trainers/__init__.py
-Comment: 
-
-Filename: tests/numpy_models/__init__.py
-Comment: 
-
-Filename: tests/numpy_models/distribution_parameters/__init__.py
-Comment: 
-
-Filename: tests/numpy_models/distribution_parameters/test_gmm_parameter.py
-Comment: 
-
 Filename: tests/preprocessors/__init__.py
 Comment: 
 
 Filename: tests/preprocessors/test_running_mean_normalizer.py
 Comment: 
 
 Filename: tests/replay_buffers/__init__.py
@@ -1032,29 +639,23 @@
 
 Filename: tests/replay_buffers/test_buffer_iterator.py
 Comment: 
 
 Filename: tests/replay_buffers/test_decorable_replay_buffer.py
 Comment: 
 
-Filename: tests/replay_buffers/test_hindsight_replay_buffer.py
-Comment: 
-
 Filename: tests/replay_buffers/test_memory_efficient_atari_buffer.py
 Comment: 
 
 Filename: tests/replay_buffers/test_prioritized_replay_buffer.py
 Comment: 
 
 Filename: tests/replay_buffers/test_replacement_sampling_replay_buffer.py
 Comment: 
 
-Filename: tests/replay_buffers/test_trajectory_replay_buffer.py
-Comment: 
-
 Filename: tests/utils/__init__.py
 Comment: 
 
 Filename: tests/utils/test_copy.py
 Comment: 
 
 Filename: tests/utils/test_data.py
@@ -1068,53 +669,44 @@
 
 Filename: tests/utils/test_files.py
 Comment: 
 
 Filename: tests/utils/test_matrices.py
 Comment: 
 
-Filename: tests/utils/test_misc.py
-Comment: 
-
 Filename: tests/utils/test_multiprocess.py
 Comment: 
 
 Filename: tests/utils/test_optimization.py
 Comment: 
 
 Filename: tests/utils/test_reproductions.py
 Comment: 
 
 Filename: tests/utils/test_serializers.py
 Comment: 
 
-Filename: tests/utils/test_solver_wrappers.py
-Comment: 
-
 Filename: tests/writers/__init__.py
 Comment: 
 
 Filename: tests/writers/test_file_writer.py
 Comment: 
 
-Filename: tests/writers/test_monitor_writer.py
-Comment: 
-
 Filename: tests/writers/test_writing_distributor.py
 Comment: 
 
-Filename: nnabla_rl-0.15.0.dist-info/LICENSE
+Filename: nnabla_rl-0.9.0.dist-info/LICENSE
 Comment: 
 
-Filename: nnabla_rl-0.15.0.dist-info/METADATA
+Filename: nnabla_rl-0.9.0.dist-info/METADATA
 Comment: 
 
-Filename: nnabla_rl-0.15.0.dist-info/WHEEL
+Filename: nnabla_rl-0.9.0.dist-info/WHEEL
 Comment: 
 
-Filename: nnabla_rl-0.15.0.dist-info/top_level.txt
+Filename: nnabla_rl-0.9.0.dist-info/top_level.txt
 Comment: 
 
-Filename: nnabla_rl-0.15.0.dist-info/RECORD
+Filename: nnabla_rl-0.9.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## nnabla_rl/__init__.py

```diff
@@ -1,20 +1,19 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-__version__ = '0.15.0'
+__version__ = '0.9.0'
 
 from nnabla_rl.logger import enable_logging, disable_logging  # noqa
 from nnabla_rl.scopes import eval_scope, is_eval_scope  # noqa
-from nnabla_rl.random import seed  # noqa
```

## nnabla_rl/algorithm.py

```diff
@@ -1,62 +1,60 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import sys
 from abc import ABCMeta, abstractmethod
 from dataclasses import dataclass
-from typing import Any, Callable, Dict, Optional, Sequence, Tuple, TypeVar, Union, cast
+from typing import Any, Dict, Sequence, Union
 
 import gym
 import numpy as np
 
 import nnabla as nn
+import nnabla.solvers
 import nnabla_rl as rl
 from nnabla_rl.configuration import Configuration
 from nnabla_rl.environments.environment_info import EnvironmentInfo
-from nnabla_rl.exceptions import UnsupportedEnvironmentException, UnsupportedTrainingException
+from nnabla_rl.exceptions import UnsupportedTrainingException
 from nnabla_rl.hook import Hook
 from nnabla_rl.logger import logger
-from nnabla_rl.model_trainers.model_trainer import ModelTrainer
 from nnabla_rl.replay_buffer import ReplayBuffer
 
-F = TypeVar('F', bound=Callable[..., Any])
 
-
-def eval_api(f: F) -> F:
+def eval_api(f):
     def wrapped_with_eval_scope(*args, **kwargs):
         with rl.eval_scope():
             return f(*args, **kwargs)
-    return cast(F, wrapped_with_eval_scope)
+    return wrapped_with_eval_scope
 
 
 @dataclass
 class AlgorithmConfig(Configuration):
-    """List of algorithm common configuration.
+    """
+    List of algorithm common configuration
 
     Args:
         gpu_id (int): id of the gpu to use. If negative, the training will run on cpu. Defaults to -1.
     """
     gpu_id: int = -1
 
 
 class Algorithm(metaclass=ABCMeta):
-    """Base Algorithm class.
+    """Base Algorithm class
 
     Args:
         env_or_env_info\
         (gym.Env or :py:class:`EnvironmentInfo <nnabla_rl.environments.environment_info.EnvironmentInfo>`)
             : environment or environment info
         config (:py:class:`AlgorithmConfig <nnabla_rl.algorithm.AlgorithmConfig>`):
             configuration of the algorithm
@@ -68,182 +66,157 @@
 
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _env_info: EnvironmentInfo
     _config: AlgorithmConfig
     _iteration_num: int
+    _max_iterations: int
     _hooks: Sequence[Hook]
 
     def __init__(self, env_info, config=AlgorithmConfig()):
         if isinstance(env_info, gym.Env):
             env_info = EnvironmentInfo.from_env(env_info)
         self._env_info = env_info
         self._config = config
         self._iteration_num = 0
+        self._max_iterations = 0
         self._hooks = []
 
-        if not self.is_supported_env(env_info):
-            raise UnsupportedEnvironmentException("{} does not support the enviroment. \
-                See the algorithm catalog (https://github.com/sony/nnabla-rl/tree/master/nnabla_rl/algorithms) \
-                and confirm what kinds of enviroments are supported".format(self.__name__))
-
         if self._config.gpu_id < 0:
-            logger.info('algorithm will run on cpu.')
+            logger.info('algorithm will run on cpu')
         else:
             logger.info('algorithm will run on gpu: {}'.format(self._config.gpu_id))
 
     @property
     def __name__(self):
         return self.__class__.__name__
 
     @property
     def latest_iteration_state(self) -> Dict[str, Any]:
-        """Return latest iteration state that is composed of items of training
-        process state. You can use this state for debugging (e.g. plot loss
-        curve). See [IterationStateHook](./hooks/iteration_state_hook.py) for
-        getting more details.
+        '''
+        Return latest iteration state that is composed of items of training process state.
+        You can use this state for debugging (e.g. plot loss curve).
+        See [IterationStateHook](./hooks/iteration_state_hook.py) for getting more details.
 
         Returns:
             Dict[str, Any]: Dictionary with items of training process state.
-        """
+        '''
         latest_iteration_state: Dict[str, Any] = {}
         latest_iteration_state['scalar'] = {}
         latest_iteration_state['histogram'] = {}
         latest_iteration_state['image'] = {}
         return latest_iteration_state
 
     @property
     def iteration_num(self) -> int:
-        """Current iteration number.
+        '''
+        Current iteration number.
 
         Returns:
             int: Current iteration number of running training.
-        """
+        '''
         return self._iteration_num
 
-    def train(self, env_or_buffer: Union[gym.Env, ReplayBuffer], total_iterations: int = sys.maxsize):
-        """Train the policy with reinforcement learning algorithm.
+    @property
+    def max_iterations(self) -> int:
+        '''
+        Maximum iteration number of running training.
+
+        Returns:
+            int: Maximum iteration number of running training.
+        '''
+        return self._max_iterations
+
+    def train(self, env_or_buffer: Union[gym.Env, ReplayBuffer], total_iterations: int):
+        '''
+        Train the policy with reinforcement learning algorithm
 
         Args:
             env_or_buffer (Union[gym.Env, ReplayBuffer]): Target environment to
                 train the policy online or reply buffer to train the policy offline.
             total_iterations (int): Total number of iterations to train the policy.
 
         Raises:
             UnsupportedTrainingException: Raises if this algorithm does not
                 support the training method for given parameter.
-        """
+        '''
         if self._is_env(env_or_buffer):
-            env_or_buffer = cast(gym.Env, env_or_buffer)
             self.train_online(env_or_buffer, total_iterations)
         elif self._is_buffer(env_or_buffer):
-            env_or_buffer = cast(ReplayBuffer, env_or_buffer)
             self.train_offline(env_or_buffer, total_iterations)
         else:
             raise UnsupportedTrainingException
 
-    def train_online(self, train_env: gym.Env, total_iterations: int = sys.maxsize):
-        """Train the policy by interacting with given environment.
+    def train_online(self, train_env: gym.Env, total_iterations: int):
+        '''
+        Train the policy by interacting with given environment.
 
         Args:
             train_env (gym.Env): Target environment to train the policy.
             total_iterations (int): Total number of iterations to train the policy.
 
         Raises:
             UnsupportedTrainingException:
                 Raises if the algorithm does not support online training
-        """
-        if self._has_rnn_models():
-            self._assert_rnn_is_supported()
+        '''
+        self._max_iterations = self._iteration_num + total_iterations
         self._before_training_start(train_env)
-        self._setup_hooks(total_iterations)
-        for _ in range(total_iterations):
+        while self._iteration_num < self.max_iterations:
             self._iteration_num += 1
             self._run_online_training_iteration(train_env)
             self._invoke_hooks()
-        self._teardown_hooks(total_iterations)
         self._after_training_finish(train_env)
 
-    def train_offline(self, replay_buffer: ReplayBuffer, total_iterations: int = sys.maxsize):
-        """Train the policy using only the replay buffer.
+    def train_offline(self, replay_buffer: gym.Env, total_iterations: int):
+        '''
+        Train the policy using only the replay buffer.
 
         Args:
             replay_buffer (ReplayBuffer): Replay buffer to sample experiences to train the policy.
             total_iterations (int): Total number of iterations to train the policy.
 
         Raises:
             UnsupportedTrainingException:
                 Raises if the algorithm does not support offline training
-        """
-        if self._has_rnn_models():
-            self._assert_rnn_is_supported()
+        '''
+        self._max_iterations = self._iteration_num + total_iterations
         self._before_training_start(replay_buffer)
-        self._setup_hooks(total_iterations)
-        for _ in range(total_iterations):
+        while self._iteration_num < self.max_iterations:
             self._iteration_num += 1
             self._run_offline_training_iteration(replay_buffer)
             self._invoke_hooks()
-        self._teardown_hooks(total_iterations)
         self._after_training_finish(replay_buffer)
 
     def set_hooks(self, hooks: Sequence[Hook]):
-        """Set hooks for running additional operation during training.
+        '''
+        Set hooks for running additional operation during training.
         Previously set hooks will be removed and replaced with new hooks.
 
         Args:
             hooks (list of nnabla_rl.hook.Hook): Hooks to invoke during training
-        """
+        '''
         self._hooks = hooks
 
     def _invoke_hooks(self):
         for hook in self._hooks:
             hook(self)
 
-    def _setup_hooks(self, total_iterations: int):
-        for hook in self._hooks:
-            hook.setup(self, total_iterations)
-
-    def _teardown_hooks(self, total_iterations: int):
-        for hook in self._hooks:
-            hook.teardown(self, total_iterations)
-
     @abstractmethod
-    def compute_eval_action(self, state, *, begin_of_episode=False, extra_info={}) -> np.ndarray:
-        """Compute action for given state using current best policy. This is
-        usually used for evaluation.
+    def compute_eval_action(self, state) -> np.array:
+        '''
+        Compute action for given state using current best policy.
+        This is usually used for evaluation.
 
         Args:
             state (np.ndarray): state to compute the action.
-            begin_of_episode (bool): Used for rnn state resetting. This flag informs the beginning of episode.
-            extra_info (Dict[str, Any]): Dictionary to provide extra information to compute the action.
-                Most of the algorithm does not use this field.
 
         Returns:
             np.ndarray: Action for given state using current trained policy.
-        """
-        raise NotImplementedError
-
-    def compute_trajectory(self,
-                           initial_trajectory: Sequence[Tuple[np.ndarray, Optional[np.ndarray]]]) \
-            -> Tuple[Sequence[Tuple[np.ndarray, Optional[np.ndarray]]], Sequence[Dict[str, Any]]]:
-        """Compute trajectory (sequence of state and action tuples) from given
-        initial trajectory using current policy. Most of the reinforcement
-        learning algorithms does not implement this method. Only the optimal
-        control algorithms implements this method.
-
-        Args:
-            initial_trajectory (Sequence[Tuple[np.ndarray, Optional[np.ndarray]]]): initial trajectory.
-
-        Returns:
-            Tuple[Sequence[Tuple[np.ndarray, Optional[np.ndarray]]], Sequence[Dict[str, Any]]]:
-                Sequence of state and action tuples and extra information (if exist) at each timestep,
-                computed with current best policy. Extra information depends on the algorithm.
-                The sequence length is same as the length of initial trajectory.
-        """
+        '''
         raise NotImplementedError
 
     def _before_training_start(self, env_or_buffer):
         pass
 
     @abstractmethod
     def _run_online_training_iteration(self, env):
@@ -254,68 +227,30 @@
         raise NotImplementedError
 
     def _after_training_finish(self, env_or_buffer):
         pass
 
     @abstractmethod
     def _models(self):
-        """Model objects which are trained by the algorithm.
+        '''
+        Model objects which are trained by the algorithm.
 
         Returns:
             Dict[str, nnabla_rl.model.Model]: Dictionary with items of model name as key and object as value.
-        """
+        '''
         raise NotImplementedError
 
     @abstractmethod
     def _solvers(self) -> Dict[str, nn.solver.Solver]:
-        """Solver objects which are used for training the models by the
-        algorithm.
+        '''
+        Solver objects which are used for training the models by the algorithm.
 
         Returns:
             Dict[str, nn.solver.Solver]: Dictionary with items of solver name as key and object as value.
-        """
+        '''
         raise NotImplementedError
 
     def _is_env(self, env):
         return isinstance(env, gym.Env)
 
     def _is_buffer(self, env):
         return isinstance(env, ReplayBuffer)
-
-    def _has_rnn_models(self):
-        for model in self._models().values():
-            if model.is_recurrent():
-                return True
-        return False
-
-    def _assert_rnn_is_supported(self):
-        if not self.is_rnn_supported():
-            raise RuntimeError(f'{self.__name__} does not support rnn models but rnn models where given!')
-
-    @classmethod
-    @abstractmethod
-    def is_supported_env(cls, env_or_env_info: Union[gym.Env, EnvironmentInfo]) -> bool:
-        """Check whether the algorithm supports the enviroment or not.
-
-        Args:
-            env_or_env_info \
-        (gym.Env or :py:class:`EnvironmentInfo <nnabla_rl.environments.environment_info.EnvironmentInfo>`) \
-            : environment or environment info
-
-        Returns:
-            bool: True if the algorithm supports the environment. Otherwise False.
-        """
-        raise NotImplementedError
-
-    @classmethod
-    def is_rnn_supported(cls) -> bool:
-        """Check whether the algorithm supports rnn models or not.
-
-        Returns:
-            bool: True if the algorithm supports rnn models. Otherwise False.
-        """
-        return False
-
-    @property
-    @abstractmethod
-    def trainers(self) -> Dict[str, ModelTrainer]:
-        raise NotImplementedError
```

## nnabla_rl/configuration.py

```diff
@@ -59,14 +59,10 @@
         if not descending:
             raise ValueError(f'{config_name} is not in descending order!: {config}')
 
     def _assert_smaller_than(self, config, ref_value, config_name):
         if config > ref_value:
             raise ValueError(f'{config_name} is not in smaller than reference value!: {config} > {ref_value}')
 
-    def _assert_greater_than(self, config, ref_value, config_name):
-        if config < ref_value:
-            raise ValueError(f'{config_name} is not greater than reference value!: {config} < {ref_value}')
-
     def _assert_length(self, config, expected_length, config_name):
         if len(config) != expected_length:
             raise ValueError(f'{config_name} length is not {expected_length}')
```

## nnabla_rl/environment_explorer.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -11,170 +11,154 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from abc import ABCMeta, abstractmethod
 from dataclasses import dataclass
-from typing import Any, Dict, List, Tuple, Union, cast
+from typing import Any, Dict, List, Optional, Tuple, cast
 
 import gym
 import numpy as np
 
 from nnabla_rl.configuration import Configuration
 from nnabla_rl.environments.environment_info import EnvironmentInfo
-from nnabla_rl.typing import Action, Experience, State
+from nnabla_rl.typing import Experience
 
 
 @dataclass
 class EnvironmentExplorerConfig(Configuration):
     warmup_random_steps: int = 0
     reward_scalar: float = 1.0
     timelimit_as_terminal: bool = True
     initial_step_num: int = 0
 
 
 class EnvironmentExplorer(metaclass=ABCMeta):
-    """Base class for environment exploration methods."""
+    '''
+    Base class for environment exploration methods.
+    '''
 
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _env_info: EnvironmentInfo
     _config: EnvironmentExplorerConfig
-    _state: Union[State, None]
-    _action: Union[Action, None]
-    _next_state: Union[State, None]
+    _state: Optional[np.array]
+    _action: Optional[np.array]
+    _next_state: Optional[np.array]
     _steps: int
 
     def __init__(self,
                  env_info: EnvironmentInfo,
                  config: EnvironmentExplorerConfig = EnvironmentExplorerConfig()):
         self._env_info = env_info
         self._config = config
 
         self._state = None
         self._action = None
         self._next_state = None
-        self._begin_of_episode = True
 
         self._steps = self._config.initial_step_num
 
     @abstractmethod
-    def action(self, steps: int, state: np.ndarray, *, begin_of_episode: bool = False) -> Tuple[np.ndarray, Dict]:
-        """Compute the action for given state at given timestep.
+    def action(self, steps: int, state: np.array) -> np.array:
+        '''
+        Compute the action for given state at given timestep
 
         Args:
             steps(int): timesteps since the beginning of exploration
-            state(np.ndarray): current state to compute the action
-            begin_of_episode(bool): Informs the beginning of episode. Used for rnn state reset.
+            state(np.array): current state to compute the action
 
         Returns:
-            np.ndarray: action for current state at given timestep
-        """
+            np.array: action for current state at given timestep
+        '''
         raise NotImplementedError
 
     def step(self, env: gym.Env, n: int = 1, break_if_done: bool = False) -> List[Experience]:
-        """Step n timesteps in given env.
+        '''
+        Step n timesteps in given env
 
         Args:
             env(gym.Env): Environment
             n(int): Number of timesteps to act in the environment
 
         Returns:
             List[Experience]: List of experience.
                 Experience consists of (state, action, reward, terminal flag, next state and extra info).
-        """
+        '''
         assert 0 < n
         experiences = []
         if self._state is None:
-            self._state = cast(State, env.reset())
+            self._state = env.reset()
 
         for _ in range(n):
-            experience, done = self._step_once(env, begin_of_episode=self._begin_of_episode)
+            experience, done = self._step_once(env)
             experiences.append(experience)
 
-            self._begin_of_episode = done
             if done and break_if_done:
                 break
         return experiences
 
     def rollout(self, env: gym.Env) -> List[Experience]:
-        """Rollout the episode in current env.
+        '''
+        Rollout the episode in current env
 
         Args:
             env(gym.Env): Environment
 
         Returns:
             List[Experience]: List of experience.
                 Experience consists of (state, action, reward, terminal flag, next state and extra info).
-        """
-        self._state = cast(State, env.reset())
+        '''
+        self._state = env.reset()
 
         done = False
 
         experiences = []
         while not done:
-            experience, done = self._step_once(env, begin_of_episode=self._begin_of_episode)
+            experience, done = self._step_once(env)
             experiences.append(experience)
-            self._begin_of_episode = done
         return experiences
 
-    def _step_once(self, env, *, begin_of_episode=False) -> Tuple[Experience, bool]:
+    def _step_once(self, env) -> Tuple[Experience, bool]:
         self._steps += 1
         if self._steps < self._config.warmup_random_steps:
-            self._action, action_info = self._warmup_action(env, begin_of_episode=begin_of_episode)
+            action_info = {}
+            if self._env_info.is_discrete_action_env():
+                action = env.action_space.sample()
+                self._action = np.asarray(action).reshape((1, ))
+            else:
+                self._action = env.action_space.sample()
         else:
-            self._action, action_info = self.action(self._steps,
-                                                    cast(np.ndarray, self._state),
-                                                    begin_of_episode=begin_of_episode)
+            self._action, action_info = self.action(self._steps, self._state)
 
         self._next_state, r, done, step_info = env.step(self._action)
         timelimit = step_info.get('TimeLimit.truncated', False)
         if _is_end_of_episode(done, timelimit, self._config.timelimit_as_terminal):
             non_terminal = 0.0
         else:
             non_terminal = 1.0
 
         extra_info: Dict[str, Any] = {}
         extra_info.update(action_info)
         extra_info.update(step_info)
-        experience = (cast(np.ndarray, self._state),
-                      cast(np.ndarray, self._action),
+        experience = (cast(np.array, self._state),
+                      cast(np.array, self._action),
                       r * self._config.reward_scalar,
                       non_terminal,
-                      cast(np.ndarray, self._next_state),
+                      cast(np.array, self._next_state),
                       extra_info)
 
         if done:
             self._state = env.reset()
         else:
             self._state = self._next_state
-        return experience, done
 
-    def _warmup_action(self, env, *, begin_of_episode=False):
-        return _sample_action(env, self._env_info)
+        return experience, done
 
 
 def _is_end_of_episode(done, timelimit, timelimit_as_terminal):
     if not done:
         return False
     else:
         return (not timelimit) or (timelimit and timelimit_as_terminal)
-
-
-def _sample_action(env, env_info):
-    action_info: Dict[str, Any] = {}
-    if env_info.is_tuple_action_env():
-        action = []
-        for a, action_space in zip(env.action_space.sample(), env_info.action_space):
-            if isinstance(action_space, gym.spaces.Discrete):
-                a = np.asarray(a).reshape((1, ))
-            action.append(a)
-        action = tuple(action)
-    else:
-        if env_info.is_discrete_action_env():
-            action = env.action_space.sample()
-            action = np.asarray(action).reshape((1, ))
-        else:
-            action = env.action_space.sample()
-    return action, action_info
```

## nnabla_rl/exceptions.py

```diff
@@ -1,30 +1,34 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 class NNablaRLError(Exception):
-    """Base class of all specific exceptions defined for nnabla_rl."""
+    '''
+    Base class of all specific exceptions defined for nnabla_rl.
+    '''
     pass
 
 
 class UnsupportedTrainingException(NNablaRLError):
-    """Raised when the algorithm does not support requested training
-    procedure."""
+    '''
+    Raised when the algorithm does not support requested training procedure.
+    '''
     pass
 
 
 class UnsupportedEnvironmentException(NNablaRLError):
-    """Raised when the algorithm does not support given environment to train
-    the policy."""
+    '''
+    Raised when the algorithm does not support given environment to train the policy.
+    '''
     pass
```

## nnabla_rl/functions.py

```diff
@@ -1,43 +1,44 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023,2024 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from typing import Callable, Optional, Sequence, Tuple, Union
+from typing import Callable, Optional, Sequence, Tuple
 
 import numpy as np
 
 import nnabla as nn
 import nnabla.functions as NF
 
 
 def sample_gaussian(mean: nn.Variable,
                     ln_var: nn.Variable,
                     noise_clip: Optional[Tuple[float, float]] = None) -> nn.Variable:
-    """Sample value from a gaussian distribution of given mean and variance.
+    '''
+    Sample value from a gaussian distribution of given mean and variance.
 
     Args:
         mean (nn.Variable): Mean of the gaussian distribution
         ln_var (nn.Variable): Logarithm of the variance of the gaussian distribution
         noise_clip (Optional[Tuple(float, float)]): Clipping value of the sampled noise.
 
     Returns:
         nn.Variable: Sampled value from gaussian distribution of given mean and variance
-    """
+    '''
     if not (mean.shape == ln_var.shape):
         raise ValueError('mean and ln_var has different shape')
 
     noise = NF.randn(shape=mean.shape)
     stddev = NF.exp(ln_var * 0.5)
     if noise_clip is not None:
         noise = NF.clip_by_value(noise, min=noise_clip[0], max=noise_clip[1])
@@ -45,27 +46,28 @@
     return mean + stddev * noise
 
 
 def sample_gaussian_multiple(mean: nn.Variable,
                              ln_var: nn.Variable,
                              num_samples: int,
                              noise_clip: Optional[Tuple[float, float]] = None) -> nn.Variable:
-    """Sample multiple values from a gaussian distribution of given mean and
-    variance. The returned variable will have an additional axis in the middle
-    as follows (batch_size, num_samples, dimension)
+    '''
+    Sample multiple values from a gaussian distribution of given mean and variance.
+    The returned variable will have an additional axis in the middle as follows
+    (batch_size, num_samples, dimension)
 
     Args:
         mean (nn.Variable): Mean of the gaussian distribution
         ln_var (nn.Variable): Logarithm of the variance of the gaussian distribution
         num_samples (int): Number of samples to sample
         noise_clip (Optional[Tuple(float, float)]): Clipping value of the sampled noise.
 
     Returns:
         nn.Variable: Sampled values from gaussian distribution of given mean and variance
-    """
+    '''
     if not (mean.shape == ln_var.shape):
         raise ValueError('mean and ln_var has different shape')
 
     batch_size = mean.shape[0]
     data_shape = mean.shape[1:]
     mean = NF.reshape(mean, shape=(batch_size, 1, *data_shape))
     stddev = NF.reshape(NF.exp(ln_var * 0.5),
@@ -78,123 +80,115 @@
         noise = NF.clip_by_value(noise, min=noise_clip[0], max=noise_clip[1])
     sample = mean + stddev * noise
     assert sample.shape == output_shape
     return sample
 
 
 def expand_dims(x: nn.Variable, axis: int) -> nn.Variable:
-    """Add dimension to target axis of given variable.
+    '''
+    Add dimension to target axis for the given variable
 
     Args:
         x (nn.Variable): Variable to expand the dimension
         axis (int): The axis to expand the dimension. Non negative.
 
     Returns:
         nn.Variable: Variable with additional dimension in the target axis
-    """
+    '''
     target_shape = (*x.shape[0:axis], 1, *x.shape[axis:])
     return NF.reshape(x, shape=target_shape, inplace=False)
 
 
 def repeat(x: nn.Variable, repeats: int, axis: int) -> nn.Variable:
-    """Repeats the value along given axis for repeats times.
+    '''
+    repeats the value along given axis for repeats times.
 
     Args:
         x (nn.Variable): Variable to repeat the values along given axis
         repeats (int): Number of times to repeat
         axis (int): The axis to expand the dimension. Non negative.
 
     Returns:
         nn.Variable: Variable with values repeated along given axis
-    """
+    '''
     # TODO: Find more efficient way
     assert isinstance(repeats, int)
     assert axis is not None
     assert axis < len(x.shape)
     reshape_size = (*x.shape[0:axis+1], 1, *x.shape[axis+1:])
     repeater_size = (*x.shape[0:axis+1], repeats, *x.shape[axis+1:])
     final_size = (*x.shape[0:axis], x.shape[axis] * repeats, *x.shape[axis+1:])
     x = NF.reshape(x=x, shape=reshape_size)
     x = NF.broadcast(x, repeater_size)
     return NF.reshape(x, final_size)
 
 
 def sqrt(x: nn.Variable):
-    """Compute the squared root of given variable.
+    '''
+    Compute the squared root of given variable
 
     Args:
         x (nn.Variable): Variable to compute the squared root
 
     Returns:
         nn.Variable: Squared root of given variable
-    """
+    '''
     return NF.pow_scalar(x, 0.5)
 
 
 def std(x: nn.Variable, axis: Optional[int] = None, keepdims: bool = False) -> nn.Variable:
-    """Compute the standard deviation of given variable along axis.
+    '''
+    Compute the standard deviation of given variable along axis.
 
     Args:
         x (nn.Variable): Variable to compute the squared root
         axis (Optional[int]): Axis to compute the standard deviation. Defaults to None. None will reduce all dimensions.
         keepdims (bool): Flag whether the reduced axis are kept as a dimension with 1 element.
 
     Returns:
         nn.Variable: Standard deviation of given variable along axis.
-    """
+    '''
     # sigma = sqrt(E[(X - E[X])^2])
     mean = NF.mean(x, axis=axis, keepdims=True)
     diff = x - mean
     variance = NF.mean(diff**2, axis=axis, keepdims=keepdims)
     return sqrt(variance)
 
 
 def argmax(x: nn.Variable, axis: Optional[int] = None, keepdims: bool = False) -> nn.Variable:
-    """Compute the index which given variable has maximum value along the axis.
+    '''
+    Compute the index which given variable has maximum value along the axis.
 
     Args:
         x (nn.Variable): Variable to compute the argmax
         axis (Optional[int]): Axis to compare the values. Defaults to None. None will reduce all dimensions.
         keepdims (bool): Flag whether the reduced axis are kept as a dimension with 1 element.
 
     Returns:
         nn.Variable: Index of the variable which its value is maximum along the axis
-    """
+    '''
     return NF.max(x=x, axis=axis, keepdims=keepdims, with_index=True, only_index=True)
 
 
-def argmin(x: nn.Variable, axis: Optional[int] = None, keepdims: bool = False) -> nn.Variable:
-    """Compute the index which given variable has minimum value along the axis.
-
-    Args:
-        x (nn.Variable): Variable to compute the argmin
-        axis (Optional[int]): Axis to compare the values. Defaults to None. None will reduce all dimensions.
-        keepdims (bool): Flag whether the reduced axis are kept as a dimension with 1 element.
-
-    Returns:
-        nn.Variable: Index of the variable which its value is minimum along the axis
-    """
-    return NF.min(x=x, axis=axis, keepdims=keepdims, with_index=True, only_index=True)
-
-
 def quantile_huber_loss(x0: nn.Variable, x1: nn.Variable, kappa: float, tau: nn.Variable) -> nn.Variable:
-    """Compute the quantile huber loss. See the following papers for details.
-
-      - https://arxiv.org/pdf/1710.10044.pdf
-      - https://arxiv.org/pdf/1806.06923.pdf
+    '''
+    Compute the quantile huber loss
+    See following papers for details:
+    https://arxiv.org/pdf/1710.10044.pdf
+    https://arxiv.org/pdf/1806.06923.pdf
 
     Args:
         x0 (nn.Variable): Quantile values
         x1 (nn.Variable): Quantile values
         kappa (float): Threshold value of huber loss which switches the loss value between squared loss and linear loss
         tau (nn.Variable): Quantile targets
 
     Returns:
         nn.Variable: Quantile huber loss
-    """
+    '''
     u = x0 - x1
     # delta(u < 0)
     delta = NF.less_scalar(u, val=0.0)
     delta.need_grad = False
     assert delta.shape == u.shape
 
     if kappa <= 0.0:
@@ -202,132 +196,105 @@
     else:
         Lk = NF.huber_loss(x0, x1, delta=kappa) * 0.5
         assert Lk.shape == u.shape
         return NF.abs(tau - delta) * Lk / kappa
 
 
 def mean_squared_error(x0: nn.Variable, x1: nn.Variable) -> nn.Variable:
-    """Convenient alias for mean squared error operation.
+    '''
+    Convenient alias for mean squared error operation
 
     Args:
         x0 (nn.Variable): N-D array
         x1 (nn.Variable): N-D array
 
     Returns:
         nn.Variable: Mean squared error between x0 and x1
-    """
+    '''
     return NF.mean(NF.squared_error(x0, x1))
 
 
 def minimum_n(variables: Sequence[nn.Variable]) -> nn.Variable:
-    """Compute the minimum among the list of variables.
+    '''
+    Compute the minimum among the list of variables
 
     Args:
         variables (Sequence[nn.Variable]): Sequence of variables. All the variables must have same shape.
 
     Returns:
         nn.Variable: Minimum value among the list of variables
-    """
+    '''
     if len(variables) < 1:
         raise ValueError('Variables must have at least 1 variable')
     if len(variables) == 1:
         return variables[0]
     if len(variables) == 2:
         return NF.minimum2(variables[0], variables[1])
 
     minimum = NF.minimum2(variables[0], variables[1])
     for variable in variables[2:]:
         minimum = NF.minimum2(minimum, variable)
     return minimum
 
 
 def gaussian_cross_entropy_method(objective_function: Callable[[nn.Variable], nn.Variable],
-                                  init_mean: Union[nn.Variable, np.ndarray], init_var: Union[nn.Variable, np.ndarray],
-                                  sample_size: int = 500, num_elites: int = 10,
+                                  init_mean: nn.Variable, init_var: nn.Variable,
+                                  pop_size: int = 500, num_elites: int = 10,
                                   num_iterations: int = 5, alpha: float = 0.25) -> Tuple[nn.Variable, nn.Variable]:
-    """Optimize objective function with respect to input using cross entropy
-    method using gaussian distribution. Candidates are sampled from a gaussian
-    distribution :math:`\\mathcal{N}(mean,\\,variance)`
+    ''' Cross Entropy Method using gaussian distribution.
+        This function optimized objective function J(x), where x is variable.
 
     Examples:
-        >>> import numpy as np
         >>> import nnabla as nn
         >>> import nnabla.functions as NF
+        >>> import numpy as np
         >>> import nnabla_rl.functions as RF
+
         >>> def objective_function(x): return -((x - 3.)**2)
-        # this function will be called with x which has (batch_size, sample_size, x_dim)
+
         >>> batch_size = 1
         >>> variable_size = 1
-        >>> init_mean = nn.Variable.from_numpy_array(np.zeros((batch_size, variable_size)))
-        >>> init_var = nn.Variable.from_numpy_array(np.ones((batch_size, variable_size)))
+
+        >>> init_mean = nn.Variable.from_numpy_array(np.zeros((batch_size, state_size)))
+        >>> init_var = nn.Variable.from_numpy_array(np.ones((batch_size, state_size)))
         >>> optimal_x, _ = RF.gaussian_cross_entropy_method(objective_function, init_mean, init_var, alpha=0)
+
         >>> optimal_x.forward()
         >>> optimal_x.shape
         (1, 1)  # (batch_size, variable_size)
         >>> optimal_x.d
         array([[3.]], dtype=float32)
 
     Args:
-        objective_function (Callable[[nn.Variable], nn.Variable]): objective function, this function will be called with
-            nn.Variable which has (batch_size, sample_size, dim) during the optimization process,
-            and should return nn.Variable such as costs which has (batch_size, sample_size, 1)
-        init_mean (Union[nn.Variable, np.ndarray]): initial mean for the gaussian distribution
-        init_var (Union[nn.Variable, np.ndarray]): initial variance for the gaussian distribution
-        sample_size (int): number of candidates at the sampling step.
-        num_elites (int): number of elites for computing the new gaussian distribution.
-        num_iterations (int): number of optimization iterations.
-        alpha (float): parameter for soft updating the gaussian distribution.
-
-    Returns:
-        Tuple[nn.Variable, nn.Variable]: mean of elites samples and top of elites samples, Both have (batch_size, dim)
-
-    Note:
-        If you want to optimize a time sequence action such as (time_steps, action_dim).
-        You can use this optimization function by transforming the action to (time_steps*action_dim).
-        For example,
-
-        .. code-block:: python
-
-            def objective_function(time_seq_action):
-                # time_seq_action.shape = (batch_size, sample_size, time_steps*action_dim)
-                # Implement the way to compute some value such as costs.
-
-            batch_size = 1
-            time_steps = 2
-            action_dim = 1
-            init_mean = nn.Variable.from_numpy_array(np.zeros((batch_size, time_steps*action_dim)))
-            init_var = nn.Variable.from_numpy_array(np.ones((batch_size, time_steps*action_dim)))
-            optimal_x, _ = RF.gaussian_cross_entropy_method(objective_function, init_mean, init_var, alpha=0)
-            optimal_x.forward()
-            # (1, 2) == (batch_size, time_steps*action_dim)
-            print(optimal_x.shape)
-    """
-    if isinstance(init_mean, np.ndarray):
-        mean = nn.Variable.from_numpy_array(init_mean)
-    else:
-        mean = init_mean
-
-    if isinstance(init_var, np.ndarray):
-        var = nn.Variable.from_numpy_array(init_var)
-    else:
-        var = init_var
+        objective_function (Callable[[nn.Variable], nn.Variable]): objective function
+        init_mean (nn.Variable): initial mean
+        init_var (nn.Variable): initial variance
+        pop_size (int): pop size
+        num_elites (int): number of elites
+        num_iterations (int): number of iterations
+        alpha (float): parameter of soft update
 
+    Returns:
+        Tuple[nn.Variable, nn.Variable]: mean of elites samples and top of elites samples
+    '''
+    mean = init_mean
+    var = init_var
     batch_size, gaussian_dimension = mean.shape
 
     elite_arange_index = np.tile(np.arange(batch_size)[:, np.newaxis], (1, num_elites))[np.newaxis, :, :]
     elite_arange_index = nn.Variable.from_numpy_array(elite_arange_index)
     top_arange_index = np.tile(np.arange(batch_size)[:, np.newaxis], (1, 1))[np.newaxis, :, :]
     top_arange_index = nn.Variable.from_numpy_array(top_arange_index)
 
     for _ in range(num_iterations):
         # samples.shape = (batch_size, pop_size, gaussian_dimension)
-        samples = sample_gaussian_multiple(mean, NF.log(var), sample_size)
+        samples = sample_gaussian_multiple(mean, NF.log(var), pop_size)
         # values.shape = (batch_size*pop_size, 1)
-        values = objective_function(samples.reshape((batch_size, sample_size, gaussian_dimension)))
-        values = values.reshape((batch_size, sample_size, 1))
+        values = objective_function(samples.reshape((-1, gaussian_dimension)))
+        values = values.reshape((batch_size, pop_size, 1))
 
         elites_index = NF.sort(values, axis=1, reverse=True, with_index=True, only_index=True)[:, :num_elites, :]
         elites_index = elites_index.reshape((1, batch_size, num_elites))
         elites_index = NF.concatenate(elite_arange_index, elites_index, axis=0)
 
         top_index = NF.max(values, axis=1, with_index=True, only_index=True, keepdims=True)
         top_index = top_index.reshape((1, batch_size, 1))
@@ -343,347 +310,7 @@
         # new_var.shape = (batch_size, 1, gaussian_dimension)
         new_var = NF.mean((elites - new_mean)**2, axis=1, keepdims=True)
 
         mean = alpha * mean + (1 - alpha) * new_mean.reshape((batch_size, gaussian_dimension))
         var = alpha * var + (1 - alpha) * new_var.reshape((batch_size, gaussian_dimension))
 
     return mean, top
-
-
-def random_shooting_method(objective_function: Callable[[nn.Variable], nn.Variable],
-                           upper_bound: np.ndarray,
-                           lower_bound: np.ndarray,
-                           sample_size: int = 500) -> nn.Variable:
-    """Optimize objective function with respect to the variable using random
-    shooting method. Candidates are sampled from a uniform distribution
-    :math:`x \\sim U(lower\\:bound, upper\\:bound)`.
-
-    Examples:
-        >>> import numpy as np
-        >>> import nnabla as nn
-        >>> import nnabla.functions as NF
-        >>> import nnabla_rl.functions as RF
-        >>> def objective_function(x): return -((x - 3.)**2)
-        # this function will be called with x which has (batch_size, sample_size, x_dim)
-        >>> batch_size = 1
-        >>> variable_size = 1
-        >>> upper_bound = np.ones((batch_size, variable_size)) * 3.5
-        >>> lower_bound = np.ones((batch_size, variable_size)) * 2.5
-        >>> optimal_x = RF.random_shooting_method(objective_function, upper_bound, lower_bound)
-        >>> optimal_x.forward()
-        >>> optimal_x.shape
-        (1, 1)  # (batch_size, variable_size)
-        >>> np.allclose(optimal_x.d, np.array([[3.]]), atol=1e-1)
-        True
-
-    Args:
-        objective_function (Callable[[nn.Variable], nn.Variable]): objective function, this function will be called with
-            nn.Variable which has (batch_size, sample_size, dim) during the optimization process,
-            and should return nn.Variable such as costs which has (batch_size, sample_size, 1)
-        upper_bound (np.ndarray): upper bound of an uniform distribution
-            for sampling candidates of the variables.
-        lower_bound (np.ndarray): lower bound of an uniform distribution
-            for sampling candidates of the variables.
-        sample_size (int): number of candidates at the sampling step.
-
-    Returns:
-        nn.Variable: argmax sample, shape is (batch_size, dim)
-
-    Note:
-        If you want to optimize a time sequence action such as (time_steps, action_dim).
-        You can use this optimization function by transforming the action to (time_steps*action_dim).
-        For example,
-
-        .. code-block:: python
-
-            def objective_function(time_seq_action):
-                # time_seq_action.shape = (batch_size, sample_size, time_steps*action_dim)
-                # Implement the way to compute some value such as costs.
-
-            batch_size = 1
-            time_steps = 2
-            action_dim = 1
-            upper_bound = np.ones((batch_size, time_steps*action_dim)) * 3.5)
-            lower_bound = np.ones((batch_size, time_steps*action_dim)) * 2.5)
-            optimal_x = RF.random_shooting_method(objective_function, upper_bound, lower_bound)
-            optimal_x.forward()
-            # (1, 2) == (batch_size, time_steps*action_dim)
-            print(optimal_x.shape)
-    """
-    batch_size, dim = upper_bound.shape
-    assert lower_bound.shape[0] == batch_size
-    assert lower_bound.shape[1] == dim
-
-    if not np.all(upper_bound >= lower_bound):
-        raise ValueError("Invalid upper_bound and lower_bound.")
-
-    upper_bound = nn.Variable().from_numpy_array(upper_bound)
-    lower_bound = nn.Variable().from_numpy_array(lower_bound)
-
-    upper_bound = expand_dims(upper_bound, 0)
-    lower_bound = expand_dims(lower_bound, 0)
-    samples = NF.rand(shape=(sample_size, batch_size, dim)) * (upper_bound - lower_bound) + lower_bound
-    samples = NF.transpose(samples, (1, 0, 2))
-
-    # values.shape = (batch_size, sample_size, 1)
-    values = objective_function(samples.reshape((batch_size, sample_size, dim)))
-    values = values.reshape((batch_size, sample_size, 1))
-
-    arange_index = np.tile(np.arange(batch_size)[:, np.newaxis], (1, 1))[np.newaxis, :, :]
-    arange_index = nn.Variable.from_numpy_array(arange_index)
-    # argmax_index.shape = (pop_size, 1)
-    argmax_index = argmax(values, axis=1, keepdims=True)
-    argmax_index = argmax_index.reshape((1, batch_size, 1))
-    argmax_index = NF.concatenate(arange_index, argmax_index, axis=0)
-
-    # top.shape = (batch_size, dim)
-    top = NF.gather_nd(samples, argmax_index).reshape((batch_size, dim))
-    return top
-
-
-def triangular_matrix(diagonal: nn.Variable, non_diagonal: Optional[nn.Variable] = None, upper=False) -> nn.Variable:
-    """Compute triangular_matrix from given diagonal and non_diagonal elements.
-    If non_diagonal is None, will create a diagonal matrix.
-
-    Example:
-        >>> import numpy as np
-        >>> import nnabla as nn
-        >>> import nnabla.functions as NF
-        >>> import nnabla_rl.functions as RF
-        >>> diag_size = 3
-        >>> batch_size = 2
-        >>> non_diag_size = diag_size * (diag_size - 1) // 2
-        >>> diagonal = nn.Variable.from_numpy_array(np.ones(6).astype(np.float32).reshape((batch_size, diag_size)))
-        >>> non_diagonal = nn.Variable.from_numpy_array(np.arange(batch_size*non_diag_size).astype(np.float32)\
-.reshape((batch_size, non_diag_size)))
-        >>> diagonal.d
-        array([[1., 1., 1.],
-               [1., 1., 1.]], dtype=float32)
-        >>> non_diagonal.d
-        array([[0., 1., 2.],
-               [3., 4., 5.]], dtype=float32)
-        >>> lower_triangular_matrix = RF.triangular_matrix(diagonal, non_diagonal)
-        >>> lower_triangular_matrix.forward()
-        >>> lower_triangular_matrix.d
-        array([[[1., 0., 0.],
-                [0., 1., 0.],
-                [1., 2., 1.]],
-               [[1., 0., 0.],
-                [3., 1., 0.],
-                [4., 5., 1.]]], dtype=float32)
-
-    Args:
-        diagonal (nn.Variable): diagonal elements of lower triangular matrix.
-            It's shape must be (batch_size, diagonal_size).
-        non_diagonal (nn.Variable or None): non-diagonal part of lower triangular elements.
-            It's shape must be (batch_size, diagonal_size * (diagonal_size - 1) // 2).
-        upper (bool): If true will create an upper triangular matrix. Otherwise will create a lower triangular matrix.
-
-    Returns:
-        nn.Variable: lower triangular matrix constructed from given variables.
-    """
-    def _flat_tri_indices(batch_size, matrix_dim, upper):
-        matrix_size = matrix_dim * matrix_dim
-
-        tri_indices = np.triu_indices(n=matrix_dim, k=1) if upper else np.tril_indices(n=matrix_dim, k=-1)
-        ravel_tril_indices = np.ravel_multi_index(tri_indices, dims=(matrix_dim, matrix_dim)).reshape((1, -1))
-        scatter_indices = np.concatenate([ravel_tril_indices + b * matrix_size for b in range(batch_size)], axis=1)
-
-        return nn.Variable.from_numpy_array(scatter_indices)
-
-    (batch_size, diagonal_size) = diagonal.shape
-    diagonal_part = NF.matrix_diag(diagonal)
-
-    if non_diagonal is None:
-        return diagonal_part
-    else:
-        non_diagonal_size = diagonal_size * (diagonal_size - 1) // 2
-        assert non_diagonal.shape == (batch_size, non_diagonal_size)
-
-        scatter_indices = _flat_tri_indices(batch_size, matrix_dim=diagonal_size, upper=upper)
-
-        matrix_size = diagonal_size * diagonal_size
-        non_diagonal_part = NF.reshape(non_diagonal, shape=(batch_size * non_diagonal_size, ))
-        non_diagonal_part = NF.scatter_nd(non_diagonal_part, scatter_indices, shape=(batch_size * matrix_size, ))
-        non_diagonal_part = NF.reshape(non_diagonal_part, shape=(batch_size, diagonal_size, diagonal_size))
-
-        return diagonal_part + non_diagonal_part
-
-
-def batch_flatten(x: nn.Variable) -> nn.Variable:
-    """Collapse the variable shape into (batch_size, rest).
-
-    Example:
-        >>> import numpy as np
-        >>> import nnabla as nn
-        >>> import nnabla_rl.functions as RF
-        >>> variable_shape = (3, 4, 5, 6)
-        >>> x = nn.Variable.from_numpy_array(np.random.normal(size=variable_shape))
-        >>> x.shape
-        (3, 4, 5, 6)
-        >>> flattened_x = RF.batch_flatten(x)
-        >>> flattened_x.shape
-        (3, 120)
-
-    Args:
-        x (nn.Variable): N-D array
-
-    Returns:
-        nn.Variable: Flattened variable.
-    """
-    original_shape = x.shape
-    flatten_shape = (original_shape[0], np.prod(original_shape[1:]))
-    return NF.reshape(x, shape=flatten_shape)
-
-
-def stop_gradient(variable: nn.Variable) -> nn.Variable:
-    return variable.get_unlinked_variable(need_grad=False)
-
-
-def pytorch_equivalent_gather(x: nn.Variable, indices: nn.Variable, axis: int) -> nn.Variable:
-    """Pytorch equivalent gather function. Gather according to given indices
-    from x.
-
-    See https://pytorch.org/docs/stable/generated/torch.gather.html for details.
-
-    The shape of x and indices should be the same except for the given axis' dimension.
-
-    Args:
-        x (nn.Variable): N-D array. The data to gather from.
-        indices (nn.Variable): N-D array. The index of elements to gather.
-        axis (int): indexing axis.
-
-    Returns:
-        nn.Variable: gathered (in pytorch's style) variable.
-    """
-    assert x.shape[:axis] == indices.shape[:axis]
-    assert x.shape[axis+1:] == indices.shape[axis+1:]
-    if axis != len(x.shape) - 1:
-        x = swapaxes(x, axis, len(x.shape) - 1)
-        indices = swapaxes(indices, axis, len(indices.shape) - 1)
-    result = NF.gather(x, indices, axis=len(x.shape) - 1, batch_dims=len(indices.shape) - 1)
-    if axis != len(x.shape) - 1:
-        result = swapaxes(result, axis, len(result.shape) - 1)
-    return result
-
-
-def concat_interleave(variables: Sequence[nn.Variable], axis: int) -> nn.Variable:
-    """Concat given variables along given axis. For example if we have a
-    sequence which consists of 3 variables A, B, C with same size. This
-    function will concat A, B, and C along given axis but interleaving its
-    elements. For example, if you concat 3 variables along axis = 0 this
-    function should return:
-
-        >>> interleaved[0::3, ...] == A
-        >>> interleaved[1::3, ...] == B
-        >>> interleaved[2::3, ...] == C
-
-    Args:
-        x (Sequence[nn.Variable]): sequence of N-D array. The data to concatenate.
-        axis (int): concatenating axis.
-
-    Returns:
-        nn.Variable: concatenated variable which elements are interleaved in given axis.
-    """
-    assert all([variable.shape == variables[0].shape for variable in variables])
-    variable_num = len(variables)
-    if variable_num == 1:
-        return variables[0]
-    concatenated = NF.concatenate(*variables, axis=axis)
-    indices_shape = concatenated.shape
-    indices = np.zeros(indices_shape, dtype=int)
-    # Move target axis dimenstion to the end
-    axis_size = indices.shape[axis]
-    indices = np.swapaxes(indices, axis, len(indices.shape) - 1)
-    original_size = axis_size // variable_num
-    for i in range(axis_size):
-        item_index = (i // variable_num)
-        var_index = i % variable_num
-        data_index = var_index * original_size + item_index
-        indices[..., i] = data_index
-    # Transpose again to original dimension
-    indices = nn.Variable.from_numpy_array(np.swapaxes(indices, axis, len(indices.shape) - 1))
-    return pytorch_equivalent_gather(concatenated, indices, axis=axis)
-
-
-def swapaxes(x: nn.Variable, axis1: int, axis2: int) -> nn.Variable:
-    """Interchange two axis of given variable.
-
-    Args:
-        x (Sequence[nn.Variable]): Target variable to interchange its axis.
-        axis1 (int): first axis.
-        axis2 (int): second axis.
-
-    Returns:
-        nn.Variable: Interchanged variable.
-    """
-    axes = [i for i in range(len(x.shape))]
-    axes[axis1] = axis2
-    axes[axis2] = axis1
-    return NF.transpose(x, axes=axes)
-
-
-def normalize(x: nn.Variable,
-              mean: nn.Variable,
-              std: nn.Variable,
-              value_clip: Optional[Tuple[float, float]] = None) -> nn.Variable:
-    """Normalize the given variable.
-
-    Args:
-        x (nn.Varible): variable to be normalized.
-        mean (nn.Variable): mean.
-        std (nn.Variable): standard deviation.
-        value_clip (Optional[Tuple[float, float]]): clipping value. defaults to None.
-
-    Returns:
-        nn.Variable: normalized value.
-    """
-    normalized = (x - mean) / std
-    if value_clip is not None:
-        normalized = NF.clip_by_value(normalized, min=value_clip[0], max=value_clip[1])
-    return normalized
-
-
-def unnormalize(x: nn.Variable,
-                mean: nn.Variable,
-                std: nn.Variable,
-                value_clip: Optional[Tuple[float, float]] = None) -> nn.Variable:
-    """Unnormalize the given variable.
-
-    Args:
-        x (nn.Varible): variable to be normalized.
-        mean (nn.Variable): mean.
-        std (nn.Variable): standard deviation.
-        value_clip (Optional[Tuple[float, float]]): clipping value. defaults to None.
-
-    Returns:
-        nn.Variable: unnormalized value.
-    """
-    unnormalized = x * std + mean
-    if value_clip is not None:
-        unnormalized = NF.clip_by_value(unnormalized, min=value_clip[0], max=value_clip[1])
-    return unnormalized
-
-
-def compute_std(var: nn.Variable, epsilon: float, mode_for_floating_point_error: str) -> nn.Variable:
-    """Compute standard deviation.
-
-    Args:
-        variance (nn.Variable)
-        epsilon (float): value to improve numerical stability for computing the standard deviation.
-        mode_for_floating_point_error (str): mode for avoiding a floating point error
-            when computing the standard deviation. Must be one of:
-
-            - `add`: It returns the square root of the sum of var and epsilon.
-            - `max`: It returns epsilon if the square root of var is smaller than epsilon, \
-                otherwise it returns the square root of var.
-
-    Returns:
-        nn.Variable: standard deviation
-    """
-    if mode_for_floating_point_error == "add":
-        std = (var + epsilon) ** 0.5
-    elif mode_for_floating_point_error == "max":
-        std = NF.maximum_scalar(var**0.5, epsilon)
-    else:
-        raise ValueError
-    return std
```

## nnabla_rl/hook.py

```diff
@@ -1,70 +1,46 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from abc import ABCMeta, abstractmethod
-from typing import TYPE_CHECKING
-
-if TYPE_CHECKING:
-    from nnabla_rl.algorithm import Algorithm
 
 
 class Hook(metaclass=ABCMeta):
-    """Base class of hooks for Algorithm classes.
+    '''
+    Base class of hooks for Algorithm classes.
 
     Hook is called at every 'timing' iterations during the training.
     'timing' is specified at the beginning of the class instantiation.
-    """
+    '''
 
-    _timing: int = 1
+    timing = 1
 
-    def __init__(self, timing: int = 1000):
+    def __init__(self, timing=1000):
         self._timing = timing
 
-    def __call__(self, algorithm: 'Algorithm'):
+    def __call__(self, algorithm):
         if algorithm.iteration_num % self._timing != 0:
             return
         self.on_hook_called(algorithm)
 
     @abstractmethod
-    def on_hook_called(self, algorithm: 'Algorithm'):
-        """Called every "timing" iteration which is set on Hook's instance
-        creation. Will run additional periodical operation (see each class'
-        documentation) during the training.
+    def on_hook_called(self, algorithm):
+        '''
+        Called every "timing" iteration which is set on Hook's instance creation.
+        Will run additional periodical operation (see each class' documentation) during the training.
 
         Args:
-            algorithm (:py:class:`Algorithm <nnabla_rl.algorithm.Algorithm>`):
-                Algorithm instance to perform additional operation.
-        """
+            algorithm (nnabla_rl.algorithm.Algorithm): Algorithm instance to perform additional operation.
+        '''
         raise NotImplementedError
-
-    def setup(self, algorithm: 'Algorithm', total_iterations: int):
-        """Called before the training starts.
-
-        Args:
-            algorithm (:py:class:`Algorithm <nnabla_rl.algorithm.Algorithm>`):
-                Algorithm instance to perform additional operation.
-            total_iteration (int): the number of total iterations.
-        """
-        pass
-
-    def teardown(self, algorithm: 'Algorithm', total_iterations: int):
-        """Called after the training ends.
-
-        Args:
-            algorithm (:py:class:`Algorithm <nnabla_rl.algorithm.Algorithm>`):
-                Algorithm instance to perform additional operation.
-            total_iteration (int): the number of total iterations.
-        """
-        pass
```

## nnabla_rl/initializers.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -15,107 +15,104 @@
 
 import numpy as np
 
 import nnabla.initializer as NI
 
 
 def HeNormal(inmaps, outmaps, kernel=(1, 1), factor=2.0, mode='fan_in'):
-    """Create Weight initializer proposed by He et al. (Normal distribution
-    version)
+    ''' Create Weight initializer proposed by He et al. (Normal distribution version)
 
     Args:
         inmaps (int): Map size of an input Variable,
         outmaps (int): Map size of an output Variable,
         kernel (tuple(int) or None): Convolution kernel spatial shape.
             In Affine, use the default setting
         factor (float): Coefficient applied to the standard deviation computation. default is 2.0
         mode (str): 'fan_in' or 'fan_out' is supported.
     Returns:
         HeNormal : weight initialzier
     Raises:
         NotImplementedError: mode other than 'fan_in' or 'fan_out' is given
-    """
+    '''
     if mode == 'fan_in':
         s = calc_normal_std_he_forward(
             inmaps, outmaps, kernel, factor)
     elif mode == 'fan_out':
         s = calc_normal_std_he_backward(
             inmaps, outmaps, kernel, factor)
     else:
         raise NotImplementedError('Unknown init mode: {}'.format(mode))
 
     return NI.NormalInitializer(s)
 
 
 def LeCunNormal(inmaps, outmaps, kernel=(1, 1), factor=1.0, mode='fan_in'):
-    """Create Weight initializer proposed in LeCun 98, Efficient Backprop
-    (Normal distribution version)
+    ''' Create Weight initializer proposed in LeCun 98, Efficient Backprop (Normal distribution version)
 
     Args:
         inmaps (int): Map size of an input Variable,
         outmaps (int): Map size of an output Variable,
         kernel (tuple(int) or None): Convolution kernel spatial shape.
             In Affine, use the default setting
         factor (float): Coefficient applied to the standard deviation computation. default is 1.0
         mode (str): 'fan_in' is the only mode supported for this initializer.
     Returns:
         LeCunNormal : weight initialzier
     Raises:
         NotImplementedError: mode other than 'fan_in' is given
-    """
+    '''
     if mode == 'fan_in':
         s = calc_normal_std_he_forward(inmaps, outmaps, kernel, factor)
     else:
         raise NotImplementedError('Unknown init mode: {}'.format(mode))
 
     return NI.NormalInitializer(s)
 
 
-def HeUniform(inmaps, outmaps, kernel=(1, 1), factor=2.0, mode='fan_in', rng=None):
-    """Create Weight initializer proposed by He et al. (Uniform distribution
-    version)
+def HeUniform(inmaps, outmaps, kernel=(1, 1), factor=2.0, mode='fan_in'):
+    ''' Create Weight initializer proposed by He et al. (Uniform distribution version)
 
     Args:
         inmaps (int): Map size of an input Variable,
         outmaps (int): Map size of an output Variable,
         kernel (tuple(int) or None): Convolution kernel spatial shape.
             In Affine, use the default setting
         factor (float): Coefficient applied to the uniform distribution limit computation. default is 2.0
         mode (str): 'fan_in' or 'fan_out' is supported.
     Returns:
         HeUniform : weight initialzier
     Raises:
         NotImplementedError: mode other than 'fan_in' or 'fan_out' is given
-    """
+    '''
     if mode == 'fan_in':
         lim = calc_uniform_lim_he_forward(inmaps, outmaps, kernel, factor)
     elif mode == 'fan_out':
         lim = calc_uniform_lim_he_backward(inmaps, outmaps, kernel, factor)
     else:
         raise NotImplementedError('Unknown init mode: {}'.format(mode))
 
-    return NI.UniformInitializer(lim=(-lim, lim), rng=rng)
+    return NI.UniformInitializer(lim=(-lim, lim))
 
 
-def GlorotUniform(inmaps, outmaps, kernel=(1, 1), rng=None):
+def GlorotUniform(inmaps, outmaps, kernel=(1, 1)):
     lb, ub = NI.calc_uniform_lim_glorot(inmaps, outmaps, kernel)
-    return NI.UniformInitializer(lim=(lb, ub), rng=rng)
+    return NI.UniformInitializer(lim=(lb, ub))
 
 
 class NormcInitializer(NI.BaseInitializer):
     ''' Create Normc initializer
     See: https://github.com/openai/baselines/blob/master/baselines/common/tf_util.py
     Initializes the parameter which normalized along 'axis' dimension.
 
     Args:
         std (float): normalization scaling value. Defaults to 1.
         axis (int): dimension to normalize. Defaults to 0.
         rng (np.random.RandomState):
             Random number generator to sample numbers from. Defaults to None.
-            When None, nnabla's default random nunmber generator will be used.
+            When None, NNabla's default random nunmber generator will be used.
     Returns:
         NormcInitializer : weight initialzier
     '''
 
     def __init__(self, std=1.0, axis=0, rng=None):
         if rng is None:
             rng = NI.random.prng
```

## nnabla_rl/logger.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -12,36 +12,15 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import logging
 from contextlib import contextmanager
 
-from tqdm.contrib.logging import logging_redirect_tqdm
-
-
-class TqdmAdapter(logging.LoggerAdapter):
-    def log(self, level, msg, *args, **kwargs):
-        with logging_redirect_tqdm():
-            super().log(level, msg, *args, **kwargs)
-
-    @property
-    def disabled(self):
-        return self.logger.disabled
-
-    @disabled.setter
-    def disabled(self, flag):
-        self.logger.disabled = flag
-
-    @property
-    def level(self):
-        return self.logger.level
-
-
-logger = TqdmAdapter(logging.getLogger('nnabla_rl'), {})
+logger = logging.getLogger('nnabla_rl')
 logger.disabled = False
 
 
 @contextmanager
 def enable_logging(level=logging.INFO):
     return _switch_logability(disabled=False, level=level)
```

## nnabla_rl/replay_buffer.py

```diff
@@ -1,144 +1,129 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import random
 from collections import deque
-from typing import Any, Dict, MutableSequence, Optional, Sequence, Tuple, Union, cast
+from typing import Any, Dict, MutableSequence, Optional, Sequence, Tuple, Union
 
 import numpy as np
 
-import nnabla_rl as rl
 from nnabla_rl.typing import Experience
-from nnabla_rl.utils.data import DataHolder, RingBuffer
+from nnabla_rl.utils.data import RingBuffer
 
 
 class ReplayBuffer(object):
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
-    _buffer: Union[MutableSequence[Any], DataHolder]
+    _buffer: Union[MutableSequence[Experience], RingBuffer]
 
     def __init__(self, capacity: Optional[int] = None):
         assert capacity is None or capacity >= 0
         self._capacity = capacity
 
         if capacity is None:
             self._buffer = deque(maxlen=capacity)
         else:
             self._buffer = RingBuffer(maxlen=capacity)
 
-    def __getitem__(self, item: int) -> Experience:
-        return cast(Experience, self._buffer[item])
+    def __getitem__(self, item):
+        return self._buffer[item]
 
     @property
     def capacity(self) -> Union[int, None]:
-        """Capacity (max length) of this replay buffer otherwise None."""
+        '''
+        Capacity (max length) of this replay buffer otherwise None
+        '''
         return self._capacity
 
     def append(self, experience: Experience):
-        """Add new experience to the replay buffer.
+        '''
+        Add new experience to the replay buffer.
 
         Args:
             experience (array-like): Experience includes trainsitions,
                 such as state, action, reward, the iteration of environment has done or not.
                 Please see to get more information in [Replay buffer documents](replay_buffer.md)
 
         Notes:
             If the replay buffer size is full, the oldest (head of the buffer) experience will be dropped off
             and the given experince will be added to the tail of the buffer.
-        """
+        '''
         self._buffer.append(experience)
 
     def append_all(self, experiences: Sequence[Experience]):
-        """Add list of experiences to the replay buffer.
+        '''
+        Add list of experiences to the replay buffer.
 
         Args:
             experiences (Sequence[Experience]): Sequence of experiences to insert to the buffer
 
         Notes:
             If the replay buffer size is full, the oldest (head of the buffer) experience will be dropped off
             and the given experince will be added to the tail of the buffer.
-        """
+        '''
         for experience in experiences:
-            self.append(experience)
+            self._buffer.append(experience)
 
-    def sample(self, num_samples: int = 1, num_steps: int = 1) \
-            -> Tuple[Union[Sequence[Experience], Tuple[Sequence[Experience], ...]], Dict[str, Any]]:
-        """Randomly sample num_samples experiences from the replay buffer.
+    def sample(self, num_samples: int = 1) -> Tuple[Sequence[Experience], Dict[str, Any]]:
+        '''
+        Randomly sample num_samples experiences from the replay buffer.
 
         Args:
-            num_samples (int): Number of samples to sample from the replay buffer. Defaults to 1.
-            num_steps (int): Number of timesteps to sample. Should be greater than 0. Defaults to 1.
+            num_samples (int): Number of samples to sample from the replay buffer.
 
         Returns:
-            experiences (Sequence[Experience] or Tuple[Sequence[Experience], ...]):
-                Random num_samples of experiences. If num_steps is greater than 1, will return a tuple of size num_steps
-                which contains num_samples of experiences for each entry.
+            experiences (Sequence[Experience]): Random num_samples of experiences.
             info (Dict[str, Any]): dictionary of information about experiences.
 
-        Raises:
-            ValueError: num_samples exceeds the maximum possible index or num_steps is 0 or negative.
-
         Notes
         ----
-        Sampling strategy depends on undelying implementation.
-        """
-        max_index = len(self) - num_steps + 1
-        if num_samples > max_index:
-            raise ValueError(f'num_samples: {num_samples} is greater than the size of buffer: {max_index}')
-        indices = self._random_indices(num_samples=num_samples, max_index=max_index)
-        return self.sample_indices(indices, num_steps=num_steps)
-
-    def sample_indices(self, indices: Sequence[int], num_steps: int = 1) \
-            -> Tuple[Union[Sequence[Experience], Tuple[Sequence[Experience], ...]], Dict[str, Any]]:
-        """Sample experiences for given indices from the replay buffer.
+        Sampling strategy depends on the undelying implementation.
+        '''
+        buffer_length = len(self)
+        if num_samples > buffer_length:
+            raise ValueError(
+                'num_samples: {} is greater than the size of buffer: {}'.format(num_samples, buffer_length))
+        indices = self._random_indices(num_samples=num_samples)
+        return self.sample_indices(indices)
+
+    def sample_indices(self, indices: Sequence[int]) -> Tuple[Sequence[Experience], Dict[str, Any]]:
+        '''
+        Sample experiences for given indices from the replay buffer.
 
         Args:
             indices (array-like): list of array index to sample the data
-            num_steps (int): Number of timesteps to sample. Should not be negative. Defaults to 1.
 
         Returns:
-            experiences (Sequence[Experience] or Tuple[Sequence[Experience], ...]):
-                Random num_samples of experiences. If num_steps is greater than 1, will return a tuple of size num_steps
-                which contains num_samples of experiences for each entry.
-            info (Dict[str, Any]): dictionary of information about experiences.
+            experiences (array-like): Sample of experiences for given indices.
 
         Raises:
-            ValueError: If indices are empty or num_steps is 0 or negative.
-        """
+            ValueError: If indices are empty
+
+        '''
         if len(indices) == 0:
             raise ValueError('Indices are empty')
-        if num_steps < 1:
-            raise ValueError(f'num_steps: {num_steps} should be greater than 0!')
-        experiences: Union[Sequence[Experience], Tuple[Sequence[Experience], ...]]
-        if num_steps == 1:
-            experiences = [self.__getitem__(index) for index in indices]
-        else:
-            experiences = tuple([self.__getitem__(index+i) for index in indices] for i in range(num_steps))
         weights = np.ones([len(indices), 1])
-        return experiences, dict(weights=weights)
+        return [self.__getitem__(index) for index in indices], dict(weights=weights)
 
-    def update_priorities(self, errors: np.ndarray):
+    def update_priorities(self, errors: np.array):
         pass
 
     def __len__(self):
         return len(self._buffer)
 
-    def _random_indices(self, num_samples: int, max_index: Optional[int] = None) -> Sequence[int]:
-        if max_index is None:
-            max_index = len(self)
-        # NOTE: Do NOT replace with np.random.choice(max_index, size=num_samples, replace=False)
-        # np.random.choice is terribly slow when sampling without replacement
-        indices = rl.random.drng.choice(max_index, size=num_samples, replace=False)
-        return cast(Sequence[int], indices)
+    def _random_indices(self, num_samples: int) -> Sequence[int]:
+        buffer_length = len(self)
+        return random.sample(range(buffer_length), k=num_samples)
```

## nnabla_rl/typing.py

```diff
@@ -1,101 +1,26 @@
 # Copyright 2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from inspect import signature
-from typing import Any, Callable, Dict, Optional, Sequence, Tuple, TypeVar, Union, cast
+from typing import Any, Dict, Tuple, Type
 
 import numpy as np
 
-F = TypeVar('F', bound=Callable[..., Any])
-
-
-State = Union[np.ndarray, Tuple[np.ndarray, ...]]
-# https://github.com/python/mypy/issues/7866
-# FIXME: This is a workaround for avoiding mypy error about creating a type alias.
-Action = Union[np.ndarray]
-Reward = Union[float, np.ndarray]
+State = Type[np.array]
+Action = Type[np.array]
+Reward = float
 NonTerminal = float
-NextState = Union[np.ndarray, Tuple[np.ndarray, ...]]
+NextState = Type[np.array]
 Info = Dict[str, Any]
 Experience = Tuple[State, Action, Reward, NonTerminal, NextState, Info]
-Shape = Union[int, Tuple[int, ...], Tuple[Tuple[int, ...], ...]]
-TupledData = Tuple
-Trajectory = Sequence[Experience]
-
-
-try:
-    from typing_extensions import Protocol
-except ModuleNotFoundError:
-    # User have not installed typing_extensions.
-    # However, typing_extension is not necessary for running the library
-    Protocol = object  # type: ignore
-
-
-class ActionSelector(Protocol):
-    def __call__(self, state: np.ndarray, *, begin_of_episode=False) -> Tuple[np.ndarray, Dict]: ...
-
-
-def accepted_shapes(**shape_kwargs: Dict[str, Optional[Tuple[int]]]) -> Callable[[F], F]:
-    """Accepted shape decorator. This decorator checks the argument shapes are
-    the same as the expected shapes. If their sizes are different, Assertation
-    error will be raised.
-
-    Args:
-        shape_kwargs(Dict[str, Optional[Tuple[int]]]): Expected shape.
-            Not define the shape where the shape check is not needed.
-            Also if the shape check for a part of axis is not needed, you can use None such as `x=(None, 1)`.
-
-    Examples:
-
-        .. code-block:: python
-
-            @accepted_shapes(x=(3, 2), y=(1, 5), z=(None, 3))
-            def dummy_function(x, y, z, non_shape_args=False):
-                pass
-
-            # Assertation error will be raised, x size is different
-            dummy_function(x=np.zeros((3, 3)), y=np.zeros((1, 5)), z=np.zeros((3, 3)), non_shape_args=False)
-
-            # Pass the decorator
-            dummy_function(x=np.zeros((3, 2)), y=np.zeros((1, 5)), z=np.zeros((3, 3)), non_shape_args=False)
-
-            # You can define the decorator in this way not to check the shape of z
-            @accepted_shapes(x=(3, 2), y=(1, 5))
-            def dummy_function(x, y, z, non_shape_args=False):
-                pass
-    """
-    def accepted_shapes_wrapper(f: F) -> F:
-        signature_f = signature(f)
-
-        def wrapped_with_accepted_shapes(*args, **kwargs):
-            binded_args = signature_f.bind(*args, **kwargs)
-            _check_kwargs_shape(binded_args.arguments, shape_kwargs)
-            return f(*args, **kwargs)
-
-        return cast(F, wrapped_with_accepted_shapes)
-    return accepted_shapes_wrapper
-
-
-def _is_same_shape(actual_shape: Tuple[int], expected_shape: Tuple[int]) -> bool:
-    if len(actual_shape) != len(expected_shape):
-        return False
-    return all([actual == expected or expected is None
-                for actual, expected in zip(actual_shape, expected_shape)])
-
-
-def _check_kwargs_shape(kwargs, expected_kwargs_shapes):
-    for kw, expected_shape in expected_kwargs_shapes.items():
-        assert kw in kwargs
-        assert _is_same_shape(kwargs[kw].shape, expected_shape)
```

## nnabla_rl/writer.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -14,32 +14,32 @@
 # limitations under the License.
 
 class Writer(object):
     def __init__(self):
         pass
 
     def write_scalar(self, iteration_num, scalar):
-        """Write scalar with your favorite tools.
+        ''' Write scalar with your favorite tools
 
         Args:
             iteration_num (int): iteration number
             scalar (dict): scalar of the latest iteration state
-        """
+        '''
         pass
 
     def write_histogram(self, iteration_num, histogram):
-        """Write histogram with your favorite tools.
+        ''' Write histogram with your favorite tools
 
         Args:
             iteration_num (int): iteration number
             histogram: histogram of the latest iteration state
-        """
+        '''
         pass
 
     def write_image(self, iteration_num, image):
-        """Write image with your favorite tools.
+        ''' Write image with your favorite tools
 
         Args:
             iteration_num (int): iteration number
             image: image of the latest iteration state
-        """
+        '''
         pass
```

## nnabla_rl/algorithms/__init__.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023,2024 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -11,53 +11,32 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from nnabla_rl.algorithm import Algorithm, AlgorithmConfig
 from nnabla_rl.algorithms.a2c import A2C, A2CConfig
-from nnabla_rl.algorithms.amp import AMP, AMPConfig
-from nnabla_rl.algorithms.atrpo import ATRPO, ATRPOConfig
 from nnabla_rl.algorithms.bcq import BCQ, BCQConfig
 from nnabla_rl.algorithms.bear import BEAR, BEARConfig
-from nnabla_rl.algorithms.categorical_ddqn import CategoricalDDQN, CategoricalDDQNConfig
 from nnabla_rl.algorithms.categorical_dqn import CategoricalDQN, CategoricalDQNConfig
-from nnabla_rl.algorithms.ddp import DDP, DDPConfig
 from nnabla_rl.algorithms.ddpg import DDPG, DDPGConfig
-from nnabla_rl.algorithms.ddqn import DDQN, DDQNConfig
-from nnabla_rl.algorithms.decision_transformer import DecisionTransformer, DecisionTransformerConfig
-from nnabla_rl.algorithms.demme_sac import DEMMESAC, DEMMESACConfig
 from nnabla_rl.algorithms.dqn import DQN, DQNConfig
-from nnabla_rl.algorithms.drqn import DRQN, DRQNConfig
 from nnabla_rl.algorithms.dummy import Dummy, DummyConfig
 from nnabla_rl.algorithms.gail import GAIL, GAILConfig
-from nnabla_rl.algorithms.her import HER, HERConfig
-from nnabla_rl.algorithms.hyar import HyAR, HyARConfig
 from nnabla_rl.algorithms.icml2015_trpo import ICML2015TRPO, ICML2015TRPOConfig
 from nnabla_rl.algorithms.icml2018_sac import ICML2018SAC, ICML2018SACConfig
-from nnabla_rl.algorithms.icra2018_qtopt import ICRA2018QtOpt, ICRA2018QtOptConfig
-from nnabla_rl.algorithms.ilqr import iLQR, iLQRConfig
 from nnabla_rl.algorithms.iqn import IQN, IQNConfig
-from nnabla_rl.algorithms.lqr import LQR, LQRConfig
-from nnabla_rl.algorithms.mme_sac import MMESAC, MMESACConfig
-from nnabla_rl.algorithms.mppi import MPPI, MPPIConfig
 from nnabla_rl.algorithms.munchausen_dqn import MunchausenDQN, MunchausenDQNConfig
 from nnabla_rl.algorithms.munchausen_iqn import MunchausenIQN, MunchausenIQNConfig
 from nnabla_rl.algorithms.ppo import PPO, PPOConfig
 from nnabla_rl.algorithms.qrdqn import QRDQN, QRDQNConfig
-from nnabla_rl.algorithms.qrsac import QRSAC, QRSACConfig
-from nnabla_rl.algorithms.rainbow import Rainbow, RainbowConfig
-from nnabla_rl.algorithms.redq import REDQ, REDQConfig
 from nnabla_rl.algorithms.reinforce import REINFORCE, REINFORCEConfig
 from nnabla_rl.algorithms.sac import SAC, SACConfig
-from nnabla_rl.algorithms.sacd import SACD, SACDConfig
-from nnabla_rl.algorithms.srsac import SRSAC, EfficientSRSAC, EfficientSRSACConfig, SRSACConfig
 from nnabla_rl.algorithms.td3 import TD3, TD3Config
 from nnabla_rl.algorithms.trpo import TRPO, TRPOConfig
-from nnabla_rl.algorithms.xql import XQL, XQLConfig
 
 # Do NOT manipulate this dictionary directly.
 # Use register_algorithm() instead.
 _ALGORITHMS = {}
 
 
 def register_algorithm(algorithm_class, config_class):
@@ -79,47 +58,25 @@
 
 
 def get_class_of(name):
     return _ALGORITHMS[name]
 
 
 register_algorithm(A2C, A2CConfig)
-register_algorithm(ATRPO, ATRPOConfig)
-register_algorithm(AMP, AMPConfig)
 register_algorithm(BCQ, BCQConfig)
 register_algorithm(BEAR, BEARConfig)
-register_algorithm(CategoricalDDQN, CategoricalDDQNConfig)
 register_algorithm(CategoricalDQN, CategoricalDQNConfig)
-register_algorithm(DDP, DDPConfig)
 register_algorithm(DDPG, DDPGConfig)
-register_algorithm(DDQN, DDQNConfig)
-register_algorithm(DecisionTransformer, DecisionTransformerConfig)
-register_algorithm(DEMMESAC, DEMMESACConfig)
 register_algorithm(DQN, DQNConfig)
-register_algorithm(DRQN, DRQNConfig)
 register_algorithm(Dummy, DummyConfig)
-register_algorithm(HER, HERConfig)
-register_algorithm(HyAR, HyARConfig)
 register_algorithm(ICML2018SAC, ICML2018SACConfig)
-register_algorithm(iLQR, iLQRConfig)
 register_algorithm(IQN, IQNConfig)
-register_algorithm(LQR, LQRConfig)
-register_algorithm(MMESAC, MMESACConfig)
-register_algorithm(MPPI, MPPIConfig)
 register_algorithm(MunchausenDQN, MunchausenDQNConfig)
 register_algorithm(MunchausenIQN, MunchausenIQNConfig)
 register_algorithm(PPO, PPOConfig)
-register_algorithm(QRSAC, QRSACConfig)
 register_algorithm(QRDQN, QRDQNConfig)
-register_algorithm(Rainbow, RainbowConfig)
-register_algorithm(REDQ, REDQConfig)
 register_algorithm(REINFORCE, REINFORCEConfig)
 register_algorithm(SAC, SACConfig)
-register_algorithm(SACD, SACDConfig)
-register_algorithm(SRSAC, SRSACConfig)
-register_algorithm(EfficientSRSAC, EfficientSRSACConfig)
 register_algorithm(TD3, TD3Config)
 register_algorithm(ICML2015TRPO, ICML2015TRPOConfig)
 register_algorithm(TRPO, TRPOConfig)
 register_algorithm(GAIL, GAILConfig)
-register_algorithm(ICRA2018QtOpt, ICRA2018QtOptConfig)
-register_algorithm(XQL, XQLConfig)
```

## nnabla_rl/algorithms/a2c.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -15,38 +15,36 @@
 
 import multiprocessing as mp
 import os
 from collections import namedtuple
 from dataclasses import dataclass
 from typing import Any, Dict, List, Optional
 
-import gym
 import numpy as np
 
 import nnabla as nn
 import nnabla_rl.model_trainers as MT
 import nnabla_rl.utils.context as context
 from nnabla import solvers as NS
 from nnabla_rl import environment_explorers as EE
 from nnabla_rl.algorithm import Algorithm, AlgorithmConfig, eval_api
-from nnabla_rl.algorithms.common_utils import _StochasticPolicyActionSelector
 from nnabla_rl.builders import ModelBuilder, SolverBuilder
 from nnabla_rl.environments.environment_info import EnvironmentInfo
 from nnabla_rl.model_trainers.model_trainer import ModelTrainer, TrainingBatch
 from nnabla_rl.models import A3CPolicy, A3CSharedFunctionHead, A3CVFunction, StochasticPolicy, VFunction
 from nnabla_rl.utils.data import marshal_experiences, unzip
 from nnabla_rl.utils.multiprocess import (copy_mp_arrays_to_params, copy_params_to_mp_arrays, mp_array_from_np_array,
                                           mp_to_np_array, new_mp_arrays_from_params, np_to_mp_array)
 from nnabla_rl.utils.reproductions import set_global_seed
-from nnabla_rl.utils.solver_wrappers import AutoClipGradByGlobalNorm
 
 
 @dataclass
 class A2CConfig(AlgorithmConfig):
-    """List of configurations for A2C algorithm.
+    """
+    List of configurations for A2C algorithm
 
     Args:
         gamma (float): discount factor of rewards. Defaults to 0.99.
         n_steps (int): number of rollout steps. Defaults to 5.
         learning_rate (float): learning rate which is set to all solvers. \
             You can customize/override the learning rate for each solver by implementing the \
             (:py:class:`SolverBuilder <nnabla_rl.builders.SolverBuilder>`) by yourself. \
@@ -58,38 +56,36 @@
         start_timesteps (int): the timestep when training starts.\
             The algorithm will collect experiences from the environment by acting randomly until this timestep.
             Defaults to 1.
         actor_num (int): number of parallel actors. Defaults to 8.
         timelimit_as_terminal (bool): Treat as done if the environment reaches the \
             `timelimit <https://github.com/openai/gym/blob/master/gym/wrappers/time_limit.py>`_.\
             Defaults to False.
-        max_grad_norm (Optional[float]): threshold value for clipping gradient. Defaults to 0.5.
+        max_grad_norm (float): threshold value for clipping gradient. Defaults to 0.5.
         seed (int): base seed of random number generator used by the actors. Defaults to 1.
-        learning_rate_decay_iterations (int): learning rate will be decreased lineary to 0 till this iteration number.
-            If 0 or negative, learning rate will be kept fixed. Defaults to 50000000.
     """
     gamma: float = 0.99
     n_steps: int = 5
     learning_rate: float = 7e-4
     entropy_coefficient: float = 0.01
     value_coefficient: float = 0.5
     decay: float = 0.99
     epsilon: float = 1e-5
     start_timesteps: int = 1
     actor_num: int = 8
     timelimit_as_terminal: bool = False
     max_grad_norm: Optional[float] = 0.5
     seed: int = -1
-    learning_rate_decay_iterations: int = 50000000
 
     def __post_init__(self):
-        """__post_init__
+        '''__post_init__
 
         Check the set values are in valid range.
-        """
+
+        '''
         self._assert_between(self.gamma, 0.0, 1.0, 'gamma')
         self._assert_between(self.decay, 0.0, 1.0, 'decay')
         self._assert_positive(self.n_steps, 'n_steps')
         self._assert_positive(self.actor_num, 'actor num')
         self._assert_positive(self.learning_rate, 'learning_rate')
 
 
@@ -121,25 +117,21 @@
 
 
 class DefaultSolverBuilder(SolverBuilder):
     def build_solver(self,  # type: ignore[override]
                      env_info: EnvironmentInfo,
                      algorithm_config: A2CConfig,
                      **kwargs) -> nn.solver.Solver:
-        solver = NS.RMSprop(lr=algorithm_config.learning_rate,
-                            decay=algorithm_config.decay,
-                            eps=algorithm_config.epsilon)
-        if algorithm_config.max_grad_norm is None:
-            return solver
-        else:
-            return AutoClipGradByGlobalNorm(solver, algorithm_config.max_grad_norm)
+        return NS.RMSprop(lr=algorithm_config.learning_rate,
+                          decay=algorithm_config.decay,
+                          eps=algorithm_config.epsilon)
 
 
 class A2C(Algorithm):
-    """Advantage Actor-Critic (A2C) algorithm implementation.
+    '''Advantage Actor-Critic (A2C) algorithm implementation.
 
     This class implements the Advantage Actor-Critic (A2C) algorithm.
     A2C is the synchronous version of A3C, Asynchronous Advantage Actor-Critic.
     A3C was proposed by V. Mnih, et al. in the paper: "Asynchronous Methods for Deep Reinforcement Learning"
     For detail see: https://arxiv.org/abs/1602.01783
 
     This algorithm only supports online training.
@@ -151,67 +143,77 @@
         v_function_builder (:py:class:`ModelBuilder[VFunction] <nnabla_rl.builders.ModelBuilder>`):
             builder of v function models
         v_solver_builder (:py:class:`SolverBuilder <nnabla_rl.builders.SolverBuilder>`): builder for v function solvers
         policy_builder (:py:class:`ModelBuilder[StochasicPolicy] <nnabla_rl.builders.ModelBuilder>`):
             builder of policy models
         policy_solver_builder (:py:class:`SolverBuilder <nnabla_rl.builders.SolverBuilder>`): builder for policy solvers
         config (:py:class:`A2CConfig <nnabla_rl.algorithms.a2c.A2CConfig>`): configuration of A2C algorithm
-    """
+    '''
 
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _config: A2CConfig
     _v_function: VFunction
     _v_function_solver: nn.solver.Solver
     _policy: StochasticPolicy
     _policy_solver: nn.solver.Solver
     _actors: List['_A2CActor']
     _actor_processes: List[mp.Process]
+    _eval_state_var: nn.Variable
+    _eval_action: nn.Variable
     _s_current_var: nn.Variable
     _a_current_var: nn.Variable
     _returns_var: nn.Variable
 
     _policy_trainer: ModelTrainer
     _v_function_trainer: ModelTrainer
 
     _policy_solver_builder: SolverBuilder
     _v_solver_builder: SolverBuilder
 
     _policy_trainer_state: Dict[str, Any]
     _v_function_trainer_state: Dict[str, Any]
 
-    _evaluation_actor: _StochasticPolicyActionSelector
-
     def __init__(self, env_or_env_info,
                  v_function_builder: ModelBuilder[VFunction] = DefaultVFunctionBuilder(),
                  v_solver_builder: SolverBuilder = DefaultSolverBuilder(),
                  policy_builder: ModelBuilder[StochasticPolicy] = DefaultPolicyBuilder(),
                  policy_solver_builder: SolverBuilder = DefaultSolverBuilder(),
                  config=A2CConfig()):
         super(A2C, self).__init__(env_or_env_info, config=config)
+        if self._env_info.is_continuous_action_env():
+            raise NotImplementedError
 
         # Initialize on cpu and change the context later
         with nn.context_scope(context.get_nnabla_context(-1)):
             self._policy = policy_builder('pi', self._env_info, self._config)
             self._v_function = v_function_builder('v', self._env_info, self._config)
 
             self._policy_solver = policy_solver_builder(self._env_info, self._config)
             self._policy_solver_builder = policy_solver_builder  # keep for later use
             self._v_function_solver = v_solver_builder(self._env_info, self._config)
             self._v_solver_builder = v_solver_builder  # keep for later use
 
-        self._evaluation_actor = _StochasticPolicyActionSelector(
-            self._env_info, self._policy.shallowcopy(), deterministic=False)
-
     @eval_api
-    def compute_eval_action(self, state, *, begin_of_episode=False, extra_info={}):
+    def compute_eval_action(self, state):
         with nn.context_scope(context.get_nnabla_context(self._config.gpu_id)):
-            action, *_ = self._evaluation_actor(state, begin_of_episode=begin_of_episode)
-            return action
+            s = np.expand_dims(state, axis=0)
+            if not hasattr(self, '_eval_state_var'):
+                self._eval_state_var = nn.Variable(s.shape)
+                distribution = self._policy.pi(self._eval_state_var)
+                self._eval_action = distribution.sample()
+                self._eval_action.need_grad = False
+            self._eval_state_var.d = s
+            self._eval_action.forward(clear_no_need_grad=True)
+            action = np.squeeze(self._eval_action.d, axis=0)
+            if self._env_info.is_discrete_action_env():
+                return np.int(action)
+            else:
+                return action
 
     def _before_training_start(self, env_or_buffer):
         if not self._is_env(env_or_buffer):
             raise ValueError('A2C only supports online training')
         env = env_or_buffer
 
         # FIXME: This setup is a workaround for creating underlying model parameters
@@ -236,28 +238,30 @@
         old_v_function_solver = self._v_function_solver
         self._v_function_solver = self._v_solver_builder(self._env_info, self._config)
         self._v_function_trainer = self._setup_v_function_training(env)
         self._v_function_solver.set_states(old_v_function_solver.get_states())
 
     def _setup_policy_training(self, env_or_buffer):
         policy_trainer_config = MT.policy_trainers.A2CPolicyTrainerConfig(
-            entropy_coefficient=self._config.entropy_coefficient
+            entropy_coefficient=self._config.entropy_coefficient,
+            max_grad_norm=self._config.max_grad_norm
         )
         policy_trainer = MT.policy_trainers.A2CPolicyTrainer(
             models=self._policy,
             solvers={self._policy.scope_name: self._policy_solver},
             env_info=self._env_info,
             config=policy_trainer_config)
         return policy_trainer
 
     def _setup_v_function_training(self, env_or_buffer):
         # training input/loss variables
         v_function_trainer_config = MT.v_value.MonteCarloVTrainerConfig(
             reduction_method='mean',
-            v_loss_scalar=self._config.value_coefficient
+            v_loss_scalar=self._config.value_coefficient,
+            max_grad_norm=self._config.max_grad_norm
         )
         v_function_trainer = MT.v_value.MonteCarloVTrainer(
             train_functions=self._v_function,
             solvers={self._v_function.scope_name: self._v_function_solver},
             env_info=self._env_info,
             config=v_function_trainer_config
         )
@@ -322,18 +326,15 @@
         extra['v_target'] = returns
         batch = TrainingBatch(batch_size=len(a),
                               s_current=s,
                               a_current=a,
                               extra=extra)
 
         # lr decay
-        alpha = self._config.learning_rate
-        if 0 < self._config.learning_rate_decay_iterations:
-            learning_rate_decay = max(1.0 - self._iteration_num / self._config.learning_rate_decay_iterations, 0.0)
-            alpha = alpha * learning_rate_decay
+        alpha = self._config.learning_rate * (1.0 - self._iteration_num / self.max_iterations)
         self._policy_trainer.set_learning_rate(alpha)
         self._v_function_trainer.set_learning_rate(alpha)
 
         # model update
         self._policy_trainer_state = self._policy_trainer.train(batch)
         self._v_function_trainer_state = self._v_function_trainer.train(batch)
 
@@ -358,33 +359,23 @@
 
     def _solvers(self):
         solvers = {}
         solvers[self._policy.scope_name] = self._policy_solver
         solvers[self._v_function.scope_name] = self._v_function_solver
         return solvers
 
-    @classmethod
-    def is_supported_env(cls, env_or_env_info):
-        env_info = EnvironmentInfo.from_env(env_or_env_info) if isinstance(env_or_env_info, gym.Env) \
-            else env_or_env_info
-        return not env_info.is_continuous_action_env() and not env_info.is_tuple_action_env()
-
     @property
     def latest_iteration_state(self):
         latest_iteration_state = super(A2C, self).latest_iteration_state
         if hasattr(self, '_policy_trainer_state'):
-            latest_iteration_state['scalar'].update({'pi_loss': float(self._policy_trainer_state['pi_loss'])})
+            latest_iteration_state['scalar'].update({'pi_loss': self._policy_trainer_state['pi_loss']})
         if hasattr(self, '_v_function_trainer_state'):
-            latest_iteration_state['scalar'].update({'v_loss': float(self._v_function_trainer_state['v_loss'])})
+            latest_iteration_state['scalar'].update({'v_loss': self._v_function_trainer_state['v_loss']})
         return latest_iteration_state
 
-    @property
-    def trainers(self):
-        return {"v_function": self._v_function_trainer, "policy": self._policy_trainer}
-
 
 class _A2CActor(object):
     def __init__(self, actor_num, env, env_info, policy, v_function, config):
         self._actor_num = actor_num
         self._env = env
         self._env_info = env_info
         self._policy = policy
@@ -430,16 +421,14 @@
 
         self._mp_arrays = MultiProcessingArrays(
             (state_mp_array, state_mp_array_shape, obs_space.dtype),
             (action_mp_array, action_mp_array_shape, action_space.dtype),
             (returns_mp_array, scalar_mp_array_shape, np.float32)
         )
 
-        self._exploration_actor = _StochasticPolicyActionSelector(env_info, policy, deterministic=False)
-
     def __call__(self):
         self._run_actor_loop()
 
     def dispose(self):
         self._disposed = True
         self._task_start_event.set()
 
@@ -516,20 +505,29 @@
             return mp_arrays_item[0], mp_arrays_item[2]
         (s, a, returns) = experiences
         np_to_mp_array(s, *array_and_dtype(self._mp_arrays.state))
         np_to_mp_array(a, *array_and_dtype(self._mp_arrays.action))
         np_to_mp_array(returns, *array_and_dtype(self._mp_arrays.returns))
 
     @eval_api
-    def _compute_action(self, s, *, begin_of_episode=False):
-        action, info = self._exploration_actor(s, begin_of_episode=begin_of_episode)
+    def _compute_action(self, s):
+        s = np.expand_dims(s, axis=0)
+        if not hasattr(self, '_eval_state_var'):
+            self._eval_state_var = nn.Variable(s.shape)
+            distribution = self._policy.pi(self._eval_state_var)
+            self._eval_action = distribution.sample()
+            self._eval_state_var.need_grad = False
+            self._eval_action.need_grad = False
+        self._eval_state_var.d = s
+        self._eval_action.forward(clear_no_need_grad=True)
+        action = np.squeeze(self._eval_action.d, axis=0)
         if self._env_info.is_discrete_action_env():
-            return np.int32(action), info
+            return np.int(action), {}
         else:
-            return action, info
+            return action, {}
 
     def _update_params(self, src, dest):
         copy_params_to_mp_arrays(src, dest)
 
     def _synchronize_policy_params(self, params):
         self._synchronize_params(src=self._policy_mp_arrays, dest=params)
```

## nnabla_rl/algorithms/bcq.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -13,34 +13,36 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from dataclasses import dataclass
 from typing import Any, Dict, List, Union
 
 import gym
+import numpy as np
 
 import nnabla as nn
 import nnabla.solvers as NS
 import nnabla_rl.functions as RF
 import nnabla_rl.model_trainers as MT
 from nnabla_rl.algorithm import Algorithm, AlgorithmConfig, eval_api
-from nnabla_rl.algorithms.common_utils import has_batch_dimension
 from nnabla_rl.builders import ModelBuilder, SolverBuilder
 from nnabla_rl.environments.environment_info import EnvironmentInfo
+from nnabla_rl.exceptions import UnsupportedEnvironmentException
 from nnabla_rl.model_trainers.model_trainer import ModelTrainer, TrainingBatch
 from nnabla_rl.models import (BCQPerturbator, BCQVariationalAutoEncoder, DeterministicPolicy, Perturbator, QFunction,
                               TD3QFunction, VariationalAutoEncoder)
 from nnabla_rl.utils import context
-from nnabla_rl.utils.data import add_batch_dimension, marshal_experiences, set_data_to_variable
-from nnabla_rl.utils.misc import create_variable, sync_model
+from nnabla_rl.utils.data import marshal_experiences
+from nnabla_rl.utils.misc import sync_model
 
 
 @dataclass
 class BCQConfig(AlgorithmConfig):
-    """BCQConfig List of configurations for BCQ algorithm.
+    '''BCQConfig
+    List of configurations for BCQ algorithm
 
     Args:
         gamma (float): discount factor of reward. Defaults to 0.99.
         learning_rate (float): learning rate which is set to all solvers. \
             You can customize/override the learning rate for each solver by implementing the \
             (:py:class:`SolverBuilder <nnabla_rl.builders.SolverBuilder>`) by yourself. \
             Defaults to 0.001.
@@ -48,29 +50,30 @@
         tau (float): target network's parameter update coefficient. Defaults to 0.005.
         lmb (float): weight :math:`\\lambda` used for balancing the ratio between :math:`\\min{Q}` and :math:`\\max{Q}`\
             on target q value generation (i.e. :math:`\\lambda\\min{Q} + (1 - \\lambda)\\max{Q}`).\
             Defaults to 0.75.
         phi (float): action perturbator noise coefficient. Defaults to 0.05.
         num_q_ensembles (int): number of q function ensembles . Defaults to 2.
         num_action_samples (int): number of actions to sample for computing target q values. Defaults to 10.
-    """
+    '''
     gamma: float = 0.99
     learning_rate: float = 1.0*1e-3
     batch_size: int = 100
     tau: float = 0.005
     lmb: float = 0.75
     phi: float = 0.05
     num_q_ensembles: int = 2
     num_action_samples: int = 10
 
     def __post_init__(self):
-        """__post_init__
+        '''__post_init__
 
         Check set values are in valid range.
-        """
+
+        '''
         self._assert_between(self.tau, 0.0, 1.0, 'tau')
         self._assert_between(self.gamma, 0.0, 1.0, 'gamma')
         self._assert_positive(self.lmb, 'lmb')
         self._assert_positive(self.phi, 'phi')
         self._assert_positive(self.num_q_ensembles, 'num_q_ensembles')
         self._assert_positive(self.num_action_samples, 'num_action_samples')
         self._assert_positive(self.batch_size, 'batch_size')
@@ -87,29 +90,29 @@
 
 class DefaultVAEBuilder(ModelBuilder[VariationalAutoEncoder]):
     def build_model(self,  # type: ignore[override]
                     scope_name: str,
                     env_info: EnvironmentInfo,
                     algorithm_config: BCQConfig,
                     **kwargs) -> VariationalAutoEncoder:
-        max_action_value = float(env_info.action_high[0])
+        max_action_value = float(env_info.action_space.high[0])
         return BCQVariationalAutoEncoder(scope_name,
                                          env_info.state_dim,
                                          env_info.action_dim,
                                          env_info.action_dim*2,
                                          max_action_value)
 
 
 class DefaultPerturbatorBuilder(ModelBuilder[Perturbator]):
     def build_model(self,  # type: ignore[override]
                     scope_name: str,
                     env_info: EnvironmentInfo,
                     algorithm_config: BCQConfig,
                     **kwargs) -> Perturbator:
-        max_action_value = float(env_info.action_high[0])
+        max_action_value = float(env_info.action_space.high[0])
         return BCQPerturbator(scope_name,
                               env_info.state_dim,
                               env_info.action_dim,
                               max_action_value)
 
 
 class DefaultSolverBuilder(SolverBuilder):
@@ -117,15 +120,15 @@
                      env_info: EnvironmentInfo,
                      algorithm_config: BCQConfig,
                      **kwargs):
         return NS.Adam(alpha=algorithm_config.learning_rate)
 
 
 class BCQ(Algorithm):
-    """Batch-Constrained Q-learning (BCQ) algorithm.
+    '''Batch-Constrained Q-learning (BCQ) algorithm
 
     This class implements the Batch-Constrained Q-learning (BCQ) algorithm
     proposed by S. Fujimoto, et al. in the paper: "Off-Policy Deep Reinforcement Learning without Exploration"
     For details see: https://arxiv.org/abs/1812.02900
 
     This algorithm only supports offline training.
 
@@ -143,15 +146,15 @@
             builder of variational auto encoder models
         vae_solver_builder (:py:class:`SolverBuilder <nnabla_rl.builders.SolverBuilder>`):
             builder for variational auto encoder solvers
         perturbator_builder (:py:class:`PerturbatorBuilder <nnabla_rl.builders.PerturbatorBuilder>`):
             builder of perturbator models
         perturbator_solver_builder (:py:class:`SolverBuilder <nnabla_rl.builders.SolverBuilder>`):
             builder for perturbator solvers
-    """
+    '''
 
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _config: BCQConfig
     _q_ensembles: List[QFunction]
     _q_solvers: Dict[str, nn.solver.Solver]
@@ -179,14 +182,16 @@
                  q_function_builder: ModelBuilder[QFunction] = DefaultQFunctionBuilder(),
                  q_solver_builder: SolverBuilder = DefaultSolverBuilder(),
                  vae_builder: ModelBuilder[VariationalAutoEncoder] = DefaultVAEBuilder(),
                  vae_solver_builder: SolverBuilder = DefaultSolverBuilder(),
                  perturbator_builder: ModelBuilder[Perturbator] = DefaultPerturbatorBuilder(),
                  perturbator_solver_builder: SolverBuilder = DefaultSolverBuilder()):
         super(BCQ, self).__init__(env_or_env_info, config=config)
+        if self._env_info.is_discrete_action_env():
+            raise UnsupportedEnvironmentException
 
         with nn.context_scope(context.get_nnabla_context(self._config.gpu_id)):
             self._q_ensembles = []
             self._q_solvers = {}
             self._target_q_ensembles = []
 
             for i in range(self._config.num_q_ensembles):
@@ -204,36 +209,28 @@
 
             self._xi = perturbator_builder(scope_name="xi", env_info=self._env_info, algorithm_config=self._config)
             self._xi_solver = perturbator_solver_builder(env_info=self._env_info, algorithm_config=self._config)
             self._target_xi = perturbator_builder(
                 scope_name="target_xi", env_info=self._env_info, algorithm_config=self._config)
 
     @eval_api
-    def compute_eval_action(self, state, *, begin_of_episode=False, extra_info={}):
-        if has_batch_dimension(state, self._env_info):
-            raise RuntimeError(f'{self.__name__} does not support batched state!')
+    def compute_eval_action(self, s):
         with nn.context_scope(context.get_nnabla_context(self._config.gpu_id)):
-            state = add_batch_dimension(state)
+            s = np.expand_dims(s, axis=0)
             if not hasattr(self, '_eval_state_var'):
+                self._eval_state_var = nn.Variable(s.shape)
                 repeat_num = 100
-                self._eval_state_var = create_variable(1, self._env_info.state_shape)
-                if isinstance(self._eval_state_var, tuple):
-                    state_var = tuple(RF.repeat(x=s_var, repeats=repeat_num, axis=0)
-                                      for s_var in self._eval_state_var)
-                else:
-                    state_var = RF.repeat(x=self._eval_state_var, repeats=repeat_num, axis=0)
-                    assert state_var.shape == (repeat_num, self._eval_state_var.shape[1])
-
-                actions = self._vae.decode(z=None, state=state_var)
-                noise = self._xi.generate_noise(state_var, actions, self._config.phi)
+                state = RF.repeat(x=self._eval_state_var, repeats=repeat_num, axis=0)
+                assert state.shape == (repeat_num, self._eval_state_var.shape[1])
+                actions = self._vae.decode(z=None, state=state)
+                noise = self._xi.generate_noise(state, actions, self._config.phi)
                 self._eval_action = actions + noise
-                q_values = self._q_ensembles[0].q(state_var, self._eval_action)
+                q_values = self._q_ensembles[0].q(state, self._eval_action)
                 self._eval_max_index = RF.argmax(q_values, axis=0)
-
-            set_data_to_variable(self._eval_state_var, state)
+            self._eval_state_var.d = s
             nn.forward_all([self._eval_action, self._eval_max_index])
             return self._eval_action.d[self._eval_max_index.d[0]]
 
     def _before_training_start(self, env_or_buffer):
         # set context globally to ensure that the training runs on configured gpu
         context.set_nnabla_context(self._config.gpu_id)
         self._encoder_trainer = self._setup_encoder_training(env_or_buffer)
@@ -313,15 +310,15 @@
 
         # Train vae
         self._encoder_trainer_state = self._encoder_trainer.train(batch)
 
         self._q_function_trainer_state = self._q_function_trainer.train(batch)
         for q, target_q in zip(self._q_ensembles, self._target_q_ensembles):
             sync_model(q, target_q, tau=self._config.tau)
-        td_errors = self._q_function_trainer_state['td_errors']
+        td_errors = np.abs(self._q_function_trainer_state['td_errors'])
         replay_buffer.update_priorities(td_errors)
 
         self._perturbator_trainer.train(batch)
         sync_model(self._xi, self._target_xi, tau=self._config.tau)
 
         self._perturbator_trainer_state = self._perturbator_trainer.train(batch)
 
@@ -333,41 +330,26 @@
     def _solvers(self):
         solvers = {}
         solvers.update(self._q_solvers)
         solvers[self._vae.scope_name] = self._vae_solver
         solvers[self._xi.scope_name] = self._xi_solver
         return solvers
 
-    @classmethod
-    def is_supported_env(cls, env_or_env_info):
-        env_info = EnvironmentInfo.from_env(env_or_env_info) if isinstance(env_or_env_info, gym.Env) \
-            else env_or_env_info
-        return not env_info.is_discrete_action_env() and not env_info.is_tuple_action_env()
-
     @property
     def latest_iteration_state(self):
         latest_iteration_state = super(BCQ, self).latest_iteration_state
         if hasattr(self, '_encoder_trainer_state'):
-            latest_iteration_state['scalar'].update(
-                {'encoder_loss': float(self._encoder_trainer_state['encoder_loss'])})
+            latest_iteration_state['scalar'].update({'encoder_loss': self._encoder_trainer_state['encoder_loss']})
         if hasattr(self, '_perturbator_trainer_state'):
             latest_iteration_state['scalar'].update(
-                {'perturbator_loss': float(self._perturbator_trainer_state['perturbator_loss'])})
+                {'perturbator_loss': self._perturbator_trainer_state['perturbator_loss']})
         if hasattr(self, '_q_function_trainer_state'):
-            latest_iteration_state['scalar'].update({'q_loss': float(self._q_function_trainer_state['q_loss'])})
+            latest_iteration_state['scalar'].update({'q_loss': self._q_function_trainer_state['q_loss']})
             latest_iteration_state['histogram'].update(
                 {'td_errors': self._q_function_trainer_state['td_errors'].flatten()})
         return latest_iteration_state
 
-    @property
-    def trainers(self):
-        return {
-            "encoder": self._encoder_trainer,
-            "q_function": self._q_function_trainer,
-            "perturbator": self._perturbator_trainer,
-        }
-
 
 if __name__ == "__main__":
     import nnabla_rl.environments as E
     env = E.DummyContinuous()
     bcq = BCQ(env)
```

## nnabla_rl/algorithms/bear.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -21,28 +21,29 @@
 
 import nnabla as nn
 import nnabla.functions as NF
 import nnabla.solvers as NS
 import nnabla_rl.functions as RF
 import nnabla_rl.model_trainers as MT
 from nnabla_rl.algorithm import Algorithm, AlgorithmConfig, eval_api
-from nnabla_rl.algorithms.common_utils import has_batch_dimension
 from nnabla_rl.builders import ModelBuilder, SolverBuilder
 from nnabla_rl.environments.environment_info import EnvironmentInfo
+from nnabla_rl.exceptions import UnsupportedEnvironmentException
 from nnabla_rl.model_trainers.model_trainer import ModelTrainer, TrainingBatch
 from nnabla_rl.models import (BEARPolicy, DeterministicPolicy, QFunction, StochasticPolicy, TD3QFunction,
                               UnsquashedVariationalAutoEncoder, VariationalAutoEncoder)
 from nnabla_rl.utils import context
-from nnabla_rl.utils.data import add_batch_dimension, marshal_experiences, set_data_to_variable
-from nnabla_rl.utils.misc import create_variable, sync_model
+from nnabla_rl.utils.data import marshal_experiences
+from nnabla_rl.utils.misc import sync_model
 
 
 @dataclass
 class BEARConfig(AlgorithmConfig):
-    """BEARConfig List of configurations for BEAR algorithm.
+    '''BEARConfig
+    List of configurations for BEAR algorithm.
 
     Args:
         gamma (float): discount factor of rewards. Defaults to 0.99.
         learning_rate (float): learning rate which is set to all solvers. \
             You can customize/override the learning rate for each solver by implementing the \
             (:py:class:`SolverBuilder <nnabla_rl.builders.SolverBuilder>`) by yourself. \
             Defaults to 0.001.
@@ -59,15 +60,15 @@
         mmd_sigma (float): parameter used for adjusting the  MMD. Defaults to 20.0.
         initial_lagrange_multiplier (float, optional): Initial value of lagrange multiplier. \
             If not specified, random value sampled from normal distribution will be used instead.
         fix_lagrange_multiplier (bool): Either to fix the lagrange multiplier or not. Defaults to False.
         warmup_iterations (int): Number of iterations until start updating the policy. Defaults to 20000
         use_mean_for_eval (bool): Use mean value instead of best action among the samples for evaluation.\
             Defaults to False.
-    """
+    '''
     gamma: float = 0.99
     learning_rate: float = 1e-3
     batch_size: int = 100
     tau: float = 0.005
     lmb: float = 0.75
     epsilon: float = 0.05
     num_q_ensembles: int = 2
@@ -77,18 +78,19 @@
     mmd_sigma: float = 20.0
     initial_lagrange_multiplier: Optional[float] = None
     fix_lagrange_multiplier: bool = False
     warmup_iterations: int = 20000
     use_mean_for_eval: bool = False
 
     def __post_init__(self):
-        """__post_init__
+        '''__post_init__
 
         Check set values are in valid range.
-        """
+
+        '''
         if not ((0.0 <= self.tau) & (self.tau <= 1.0)):
             raise ValueError('tau must lie between [0.0, 1.0]')
         if not ((0.0 <= self.gamma) & (self.gamma <= 1.0)):
             raise ValueError('gamma must lie between [0.0, 1.0]')
         if not (0 <= self.num_q_ensembles):
             raise ValueError('num q ensembles must not be negative')
         if not (0 <= self.num_mmd_actions):
@@ -136,15 +138,15 @@
                      env_info: EnvironmentInfo,
                      algorithm_config: BEARConfig,
                      **kwargs) -> nn.solver.Solver:
         return NS.Adam(alpha=algorithm_config.learning_rate)
 
 
 class BEAR(Algorithm):
-    """Bootstrapping Error Accumulation Reduction (BEAR) algorithm.
+    '''Bootstrapping Error Accumulation Reduction (BEAR) algorithm.
 
     This class implements the Bootstrapping Error Accumulation Reduction (BEAR) algorithm
     proposed by A. Kumar, et al. in the paper: "Stabilizing Off-Policy Q-learning via Bootstrapping Error Reduction"
     For details see: https://arxiv.org/abs/1906.00949
 
     This algorithm only supports offline training.
 
@@ -164,15 +166,15 @@
             builder for policy solvers
         vae_builder (:py:class:`ModelBuilder[VariationalAutoEncoder] <nnabla_rl.builders.ModelBuilder>`):
             builder of variational auto encoder models
         vae_solver_builder (:py:class:`SolverBuilder <nnabla_rl.builders.SolverBuilder>`):
             builder for variational auto encoder solvers
         lagrange_solver_builder (:py:class:`SolverBuilder <nnabla_rl.builders.SolverBuilder>`):
             builder for lagrange multiplier solver
-    """
+    '''
 
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _config: BEARConfig
     _q_ensembles: List[QFunction]
     _q_solvers: Dict[str, nn.solver.Solver]
@@ -201,14 +203,16 @@
                  q_solver_builder: SolverBuilder = DefaultSolverBuilder(),
                  pi_builder: ModelBuilder[StochasticPolicy] = DefaultPolicyBuilder(),
                  pi_solver_builder: SolverBuilder = DefaultSolverBuilder(),
                  vae_builder: ModelBuilder[VariationalAutoEncoder] = DefaultVAEBuilder(),
                  vae_solver_builder: SolverBuilder = DefaultSolverBuilder(),
                  lagrange_solver_builder: SolverBuilder = DefaultSolverBuilder()):
         super(BEAR, self).__init__(env_or_env_info, config=config)
+        if self._env_info.is_discrete_action_env():
+            raise UnsupportedEnvironmentException
 
         with nn.context_scope(context.get_nnabla_context(self._config.gpu_id)):
             self._q_ensembles = []
             self._q_solvers = {}
             self._target_q_ensembles = []
             for i in range(self._config.num_q_ensembles):
                 q = q_function_builder(scope_name="q{}".format(
@@ -228,39 +232,31 @@
 
             self._lagrange = MT.policy_trainers.bear_policy_trainer.AdjustableLagrangeMultiplier(
                 scope_name="alpha",
                 initial_value=self._config.initial_lagrange_multiplier)
             self._lagrange_solver = lagrange_solver_builder(env_info=self._env_info, algorithm_config=self._config)
 
     @eval_api
-    def compute_eval_action(self, state, *, begin_of_episode=False, extra_info={}):
-        if has_batch_dimension(state, self._env_info):
-            raise RuntimeError(f'{self.__name__} does not support batched state!')
+    def compute_eval_action(self, s):
         with nn.context_scope(context.get_nnabla_context(self._config.gpu_id)):
-            state = add_batch_dimension(state)
+            s = np.expand_dims(s, axis=0)
             if not hasattr(self, '_eval_state_var'):
-                self._eval_state_var = create_variable(1, self._env_info.state_shape)
+                self._eval_state_var = nn.Variable(s.shape)
                 if self._config.use_mean_for_eval:
                     eval_distribution = self._pi.pi(self._eval_state_var)
                     self._eval_action = NF.tanh(eval_distribution.mean())
                 else:
                     repeat_num = 100
-                    if isinstance(self._eval_state_var, tuple):
-                        state_var = tuple(RF.repeat(x=s_var, repeats=repeat_num, axis=0)
-                                          for s_var in self._eval_state_var)
-                    else:
-                        state_var = RF.repeat(x=self._eval_state_var, repeats=repeat_num, axis=0)
-                        assert state_var.shape == (repeat_num, self._eval_state_var.shape[1])
-
-                    eval_distribution = self._pi.pi(state_var)
+                    state = RF.repeat(x=self._eval_state_var, repeats=repeat_num, axis=0)
+                    assert state.shape == (repeat_num, self._eval_state_var.shape[1])
+                    eval_distribution = self._pi.pi(state)
                     self._eval_action = NF.tanh(eval_distribution.sample())
-                    q_values = self._q_ensembles[0].q(state_var, self._eval_action)
+                    q_values = self._q_ensembles[0].q(state, self._eval_action)
                     self._eval_max_index = RF.argmax(q_values, axis=0)
-
-            set_data_to_variable(self._eval_state_var, state)
+            self._eval_state_var.d = s
             if self._config.use_mean_for_eval:
                 self._eval_action.forward()
                 return np.squeeze(self._eval_action.d, axis=0)
             else:
                 nn.forward_all([self._eval_action, self._eval_max_index])
                 return self._eval_action.d[self._eval_max_index.d[0]]
 
@@ -371,15 +367,15 @@
                               non_terminal=non_terminal,
                               s_next=s_next,
                               weight=info['weights'])
 
         self._q_function_trainer_state = self._q_function_trainer.train(batch)
         for q, target_q in zip(self._q_ensembles, self._target_q_ensembles):
             sync_model(q, target_q, tau=self._config.tau)
-        td_errors = self._q_function_trainer_state['td_errors']
+        td_errors = np.abs(self._q_function_trainer_state['td_errors'])
         replay_buffer.update_priorities(td_errors)
 
         self._encoder_trainer_state = self._encoder_trainer.train(batch)
         self._policy_trainer_state = self._policy_trainer.train(batch)
         sync_model(self._pi, self._target_pi, tau=self._config.tau)
 
     def _models(self):
@@ -393,33 +389,18 @@
         solvers.update(self._q_solvers)
         solvers[self._pi.scope_name] = self._pi_solver
         solvers[self._vae.scope_name] = self._vae_solver
         if not self._config.fix_lagrange_multiplier:
             solvers[self._lagrange.scope_name] = self._lagrange_solver
         return solvers
 
-    @classmethod
-    def is_supported_env(cls, env_or_env_info):
-        env_info = EnvironmentInfo.from_env(env_or_env_info) if isinstance(env_or_env_info, gym.Env) \
-            else env_or_env_info
-        return not env_info.is_discrete_action_env() and not env_info.is_tuple_action_env()
-
     @property
     def latest_iteration_state(self):
         latest_iteration_state = super(BEAR, self).latest_iteration_state
         if hasattr(self, '_encoder_trainer_state'):
-            latest_iteration_state['scalar'].update(
-                {'encoder_loss': float(self._encoder_trainer_state['encoder_loss'])})
+            latest_iteration_state['scalar'].update({'encoder_loss': self._encoder_trainer_state['encoder_loss']})
         if hasattr(self, '_policy_trainer_state'):
-            latest_iteration_state['scalar'].update({'pi_loss': float(self._policy_trainer_state['pi_loss'])})
+            latest_iteration_state['scalar'].update({'pi_loss': self._policy_trainer_state['pi_loss']})
         if hasattr(self, '_q_function_trainer_state'):
-            latest_iteration_state['scalar'].update({'q_loss': float(self._q_function_trainer_state['q_loss'])})
+            latest_iteration_state['scalar'].update({'q_loss': self._q_function_trainer_state['q_loss']})
             latest_iteration_state['histogram'].update({'td_errors': self._q_function_trainer_state['td_errors']})
         return latest_iteration_state
-
-    @property
-    def trainers(self):
-        return {
-            "encoder": self._encoder_trainer,
-            "q_function": self._q_function_trainer,
-            "policy": self._policy_trainer,
-        }
```

## nnabla_rl/algorithms/categorical_dqn.py

```diff
@@ -1,59 +1,58 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from dataclasses import dataclass
-from typing import Any, Dict, Union
+from typing import Any, Dict, Union, cast
 
 import gym
 import numpy as np
 
 import nnabla as nn
 import nnabla.solvers as NS
 import nnabla_rl.environment_explorers as EE
 import nnabla_rl.model_trainers as MT
 from nnabla_rl.algorithm import Algorithm, AlgorithmConfig, eval_api
-from nnabla_rl.algorithms.common_utils import _GreedyActionSelector
-from nnabla_rl.builders import ExplorerBuilder, ModelBuilder, ReplayBufferBuilder, SolverBuilder
+from nnabla_rl.builders import ModelBuilder, ReplayBufferBuilder, SolverBuilder
 from nnabla_rl.environment_explorer import EnvironmentExplorer
 from nnabla_rl.environment_explorers.epsilon_greedy_explorer import epsilon_greedy_action_selection
 from nnabla_rl.environments.environment_info import EnvironmentInfo
+from nnabla_rl.exceptions import UnsupportedEnvironmentException
 from nnabla_rl.model_trainers.model_trainer import ModelTrainer, TrainingBatch
 from nnabla_rl.models import C51ValueDistributionFunction, ValueDistributionFunction
 from nnabla_rl.replay_buffer import ReplayBuffer
 from nnabla_rl.utils import context
 from nnabla_rl.utils.data import marshal_experiences
 from nnabla_rl.utils.misc import sync_model
 
 
 @dataclass
 class CategoricalDQNConfig(AlgorithmConfig):
-    """CategoricalDQNConfig List of configurations for CategoricalDQN
-    algorithm.
+    '''CategoricalDQNConfig
+    List of configurations for CategoricalDQN algorithm.
 
     Args:
         gamma (float): discount factor of rewards. Defaults to 0.99.
         learning_rate (float): learning rate which is set to all solvers. \
             You can customize/override the learning rate for each solver by implementing the \
             (:py:class:`SolverBuilder <nnabla_rl.builders.SolverBuilder>`) by yourself. \
             Defaults to 0.001.
-        batch_size (int): training batch size. Defaults to 32.
-        num_steps (int): number of steps for N-step Q targets. Defaults to 1.
+        batch_size (int): training atch size. Defaults to 32.
         start_timesteps (int): the timestep when training starts.\
             The algorithm will collect experiences from the environment by acting randomly until this timestep.
             Defaults to 50000.
         replay_buffer_size (int): the capacity of replay buffer. Defaults to 1000000.
         learner_update_frequency (float): the interval of learner update. Defaults to 4
         target_update_frequency (float): the interval of target q-function update. Defaults to 10000.
         max_explore_steps (int): the number of steps decaying the epsilon value.\
@@ -63,69 +62,30 @@
             Defaults to 1000000.
         initial_epsilon (float): the initial epsilon value for -greedy explorer. Defaults to 1.0.
         final_epsilon (float): the last epsilon value for -greedy explorer. Defaults to 0.01.
         test_epsilon (float): the epsilon value on testing. Defaults to 0.001.
         v_min (float): lower limit of the value used in value distribution function. Defaults to -10.0.
         v_max (float): upper limit of the value used in value distribution function. Defaults to 10.0.
         num_atoms (int): the number of bins used in value distribution function. Defaults to 51.
-        loss_reduction_method (str): KL loss reduction method. "sum" or "mean" is supported. Defaults to mean.
-        unroll_steps (int): Number of steps to unroll tranining network.
-            The network will be unrolled even though the provided model doesn't have RNN layers.
-            Defaults to 1.
-        burn_in_steps (int): Number of burn-in steps to initiaze recurrent layer states during training.
-            This flag does not take effect if given model is not an RNN model.
-            Defaults to 0.
-        reset_rnn_on_terminal (bool): Reset recurrent internal states to zero during training if episode ends.
-            This flag does not take effect if given model is not an RNN model.
-            Defaults to True.
-    """
+    '''
 
     gamma: float = 0.99
     learning_rate: float = 0.00025
     batch_size: int = 32
-    num_steps: int = 1
     start_timesteps: int = 50000
     replay_buffer_size: int = 1000000
     learner_update_frequency: int = 4
     target_update_frequency: int = 10000
     max_explore_steps: int = 1000000
     initial_epsilon: float = 1.0
     final_epsilon: float = 0.01
     test_epsilon: float = 0.001
     v_min: float = -10.0
     v_max: float = 10.0
     num_atoms: int = 51
-    loss_reduction_method: str = "mean"
-
-    # rnn model support
-    unroll_steps: int = 1
-    burn_in_steps: int = 0
-    reset_rnn_on_terminal: bool = True
-
-    def __post_init__(self):
-        """__post_init__
-
-        Check set values are in valid range.
-        """
-        self._assert_between(self.gamma, 0.0, 1.0, 'gamma')
-        self._assert_positive(self.learning_rate, 'learning_rate')
-        self._assert_positive(self.batch_size, 'batch_size')
-        self._assert_positive(self.num_steps, 'num_steps')
-        self._assert_positive(self.learner_update_frequency, 'learner_update_frequency')
-        self._assert_positive(self.target_update_frequency, 'target_update_frequency')
-        self._assert_positive(self.start_timesteps, 'start_timesteps')
-        self._assert_positive(self.replay_buffer_size, 'replay_buffer_size')
-        self._assert_smaller_than(self.start_timesteps, self.replay_buffer_size, 'start_timesteps')
-        self._assert_positive(self.max_explore_steps, 'max_explore_steps')
-        self._assert_between(self.initial_epsilon, 0.0, 1.0, 'initial_epsilon')
-        self._assert_between(self.final_epsilon, 0.0, 1.0, 'final_epsilon')
-        self._assert_between(self.test_epsilon, 0.0, 1.0, 'test_epsilon')
-        self._assert_positive(self.num_atoms, 'num_atoms')
-        self._assert_positive(self.unroll_steps, 'unroll_steps')
-        self._assert_positive_or_zero(self.burn_in_steps, 'burn_in_steps')
 
 
 class DefaultValueDistFunctionBuilder(ModelBuilder[ValueDistributionFunction]):
     def build_model(self,  # type: ignore[override]
                     scope_name: str,
                     env_info: EnvironmentInfo,
                     algorithm_config: CategoricalDQNConfig,
@@ -149,37 +109,16 @@
     def build_solver(self,  # type: ignore[override]
                      env_info: EnvironmentInfo,
                      algorithm_config: CategoricalDQNConfig,
                      **kwargs) -> nn.solver.Solver:
         return NS.Adam(alpha=algorithm_config.learning_rate, eps=1e-2 / algorithm_config.batch_size)
 
 
-class DefaultExplorerBuilder(ExplorerBuilder):
-    def build_explorer(self,  # type: ignore[override]
-                       env_info: EnvironmentInfo,
-                       algorithm_config: CategoricalDQNConfig,
-                       algorithm: "CategoricalDQN",
-                       **kwargs) -> EnvironmentExplorer:
-        explorer_config = EE.LinearDecayEpsilonGreedyExplorerConfig(
-            warmup_random_steps=algorithm_config.start_timesteps,
-            initial_step_num=algorithm.iteration_num,
-            initial_epsilon=algorithm_config.initial_epsilon,
-            final_epsilon=algorithm_config.final_epsilon,
-            max_explore_steps=algorithm_config.max_explore_steps
-        )
-        explorer = EE.LinearDecayEpsilonGreedyExplorer(
-            greedy_action_selector=algorithm._exploration_action_selector,
-            random_action_selector=algorithm._random_action_selector,
-            env_info=env_info,
-            config=explorer_config)
-        return explorer
-
-
 class CategoricalDQN(Algorithm):
-    """Categorical DQN algorithm.
+    '''Categorical DQN algorithm.
 
     This class implements the Categorical DQN algorithm
     proposed by M. Bellemare, et al. in the paper: "A Distributional Perspective on Reinfocement Learning"
     For details see: https://arxiv.org/abs/1707.06887
 
     Args:
         env_or_env_info \
@@ -189,85 +128,86 @@
             configuration of the CategoricalDQN algorithm
         value_distribution_builder (:py:class:`ModelBuilder[ValueDistributionFunctionFunction] \
             <nnabla_rl.builders.ModelBuilder>`): builder of value distribution function models
         value_distribution_solver_builder (:py:class:`SolverBuilder <nnabla_rl.builders.SolverBuilder>`):
             builder of value distribution function solvers
         replay_buffer_builder (:py:class:`ReplayBufferBuilder <nnabla_rl.builders.ReplayBufferBuilder>`):
             builder of replay_buffer
-        explorer_builder (:py:class:`ExplorerBuilder <nnabla_rl.builders.ExplorerBuilder>`):
-            builder of environment explorer
-    """
+     '''
 
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _config: CategoricalDQNConfig
     _atom_p: ValueDistributionFunction
     _atom_p_solver: nn.solver.Solver
     _target_atom_p: ValueDistributionFunction
     _replay_buffer: ReplayBuffer
-    _explorer_builder: ExplorerBuilder
     _environment_explorer: EnvironmentExplorer
     _model_trainer: ModelTrainer
 
-    _evaluation_actor: _GreedyActionSelector
-    _exploration_actor: _GreedyActionSelector
+    _eval_state_var: nn.Variable
+    _a_greedy: nn.Variable
 
     _model_trainer_state: Dict[str, Any]
 
     def __init__(self, env_or_env_info: Union[gym.Env, EnvironmentInfo],
                  config: CategoricalDQNConfig = CategoricalDQNConfig(),
                  value_distribution_builder: ModelBuilder[ValueDistributionFunction]
                  = DefaultValueDistFunctionBuilder(),
                  value_distribution_solver_builder: SolverBuilder = DefaultSolverBuilder(),
-                 replay_buffer_builder: ReplayBufferBuilder = DefaultReplayBufferBuilder(),
-                 explorer_builder: ExplorerBuilder = DefaultExplorerBuilder()):
+                 replay_buffer_builder: ReplayBufferBuilder = DefaultReplayBufferBuilder()):
         super(CategoricalDQN, self).__init__(env_or_env_info, config=config)
-
-        self._explorer_builder = explorer_builder
+        if not self._env_info.is_discrete_action_env():
+            raise UnsupportedEnvironmentException('{} only supports discrete action environment'.format(self.__name__))
 
         with nn.context_scope(context.get_nnabla_context(self._config.gpu_id)):
             self._atom_p = value_distribution_builder('atom_p_train', self._env_info, self._config)
             self._atom_p_solver = value_distribution_solver_builder(self._env_info, self._config)
-            self._target_atom_p = self._atom_p.deepcopy('target_atom_p_train')
+            self._target_atom_p = cast(ValueDistributionFunction, self._atom_p.deepcopy('target_atom_p_train'))
 
             self._replay_buffer = replay_buffer_builder(self._env_info, self._config)
 
-        self._evaluation_actor = _GreedyActionSelector(self._env_info, self._atom_p.shallowcopy().as_q_function())
-        self._exploration_actor = _GreedyActionSelector(self._env_info, self._atom_p.shallowcopy().as_q_function())
-
     @eval_api
-    def compute_eval_action(self, state, *, begin_of_episode=False, extra_info={}):
+    def compute_eval_action(self, state):
         with nn.context_scope(context.get_nnabla_context(self._config.gpu_id)):
             (action, _), _ = epsilon_greedy_action_selection(state,
-                                                             self._evaluation_action_selector,
+                                                             self._greedy_action_selector,
                                                              self._random_action_selector,
-                                                             epsilon=self._config.test_epsilon,
-                                                             begin_of_episode=begin_of_episode)
+                                                             epsilon=self._config.test_epsilon)
             return action
 
     def _before_training_start(self, env_or_buffer):
         # set context globally to ensure that the training runs on configured gpu
         context.set_nnabla_context(self._config.gpu_id)
         self._environment_explorer = self._setup_environment_explorer(env_or_buffer)
         self._model_trainer = self._setup_value_distribution_function_training(env_or_buffer)
 
     def _setup_environment_explorer(self, env_or_buffer):
-        return None if self._is_buffer(env_or_buffer) else self._explorer_builder(self._env_info, self._config, self)
+        if self._is_buffer(env_or_buffer):
+            return None
+        explorer_config = EE.LinearDecayEpsilonGreedyExplorerConfig(
+            warmup_random_steps=self._config.start_timesteps,
+            initial_step_num=self.iteration_num,
+            initial_epsilon=self._config.initial_epsilon,
+            final_epsilon=self._config.final_epsilon,
+            max_explore_steps=self._config.max_explore_steps
+        )
+        explorer = EE.LinearDecayEpsilonGreedyExplorer(
+            greedy_action_selector=self._greedy_action_selector,
+            random_action_selector=self._random_action_selector,
+            env_info=self._env_info,
+            config=explorer_config)
+        return explorer
 
     def _setup_value_distribution_function_training(self, env_or_buffer):
         trainer_config = MT.q_value_trainers.CategoricalDQNQTrainerConfig(
-            num_steps=self._config.num_steps,
             v_min=self._config.v_min,
             v_max=self._config.v_max,
-            num_atoms=self._config.num_atoms,
-            reduction_method=self._config.loss_reduction_method,
-            unroll_steps=self._config.unroll_steps,
-            burn_in_steps=self._config.burn_in_steps,
-            reset_on_terminal=self._config.reset_rnn_on_terminal)
+            num_atoms=self._config.num_atoms)
 
         model_trainer = MT.q_value_trainers.CategoricalDQNQTrainer(
             train_functions=self._atom_p,
             solvers={self._atom_p.scope_name: self._atom_p_solver},
             target_function=self._target_atom_p,
             env_info=self._env_info,
             config=trainer_config)
@@ -284,76 +224,57 @@
             if self.iteration_num % self._config.learner_update_frequency == 0:
                 self._categorical_dqn_training(self._replay_buffer)
 
     def _run_offline_training_iteration(self, buffer):
         self._categorical_dqn_training(buffer)
 
     def _categorical_dqn_training(self, replay_buffer):
-        num_steps = self._config.num_steps + self._config.burn_in_steps + self._config.unroll_steps - 1
-        experiences_tuple, info = replay_buffer.sample(self._config.batch_size, num_steps=num_steps)
-        if num_steps == 1:
-            experiences_tuple = (experiences_tuple, )
-        assert len(experiences_tuple) == num_steps
-
-        batch = None
-        for experiences in reversed(experiences_tuple):
-            (s, a, r, non_terminal, s_next, rnn_states_dict, *_) = marshal_experiences(experiences)
-            rnn_states = rnn_states_dict['rnn_states'] if 'rnn_states' in rnn_states_dict else {}
-            batch = TrainingBatch(batch_size=self._config.batch_size,
-                                  s_current=s,
-                                  a_current=a,
-                                  gamma=self._config.gamma,
-                                  reward=r,
-                                  non_terminal=non_terminal,
-                                  s_next=s_next,
-                                  weight=info['weights'],
-                                  next_step_batch=batch,
-                                  rnn_states=rnn_states)
+        experiences, info = replay_buffer.sample(self._config.batch_size)
+        (s, a, r, non_terminal, s_next, *_) = marshal_experiences(experiences)
+        batch = TrainingBatch(batch_size=self._config.batch_size,
+                              s_current=s,
+                              a_current=a,
+                              gamma=self._config.gamma,
+                              reward=r,
+                              non_terminal=non_terminal,
+                              s_next=s_next,
+                              weight=info['weights'])
 
         self._model_trainer_state = self._model_trainer.train(batch)
         if self.iteration_num % self._config.target_update_frequency == 0:
             sync_model(self._atom_p, self._target_atom_p)
-        td_errors = self._model_trainer_state['td_errors']
+        td_errors = np.abs(self._model_trainer_state['td_errors'])
         replay_buffer.update_priorities(td_errors)
 
-    def _evaluation_action_selector(self, s, *, begin_of_episode=False):
-        return self._evaluation_actor(s, begin_of_episode=begin_of_episode)
-
-    def _exploration_action_selector(self, s, *, begin_of_episode=False):
-        return self._exploration_actor(s, begin_of_episode=begin_of_episode)
+    @eval_api
+    def _greedy_action_selector(self, s):
+        s = np.expand_dims(s, axis=0)
+        if not hasattr(self, '_eval_state_var'):
+            self._eval_state_var = nn.Variable(s.shape)
+            q_function = self._atom_p.as_q_function()
+            self._a_greedy = q_function.argmax_q(self._eval_state_var)
+        self._eval_state_var.d = s
+        self._a_greedy.forward()
+        return np.squeeze(self._a_greedy.d, axis=0), {}
 
-    def _random_action_selector(self, s, *, begin_of_episode=False):
+    def _random_action_selector(self, s):
         action = self._env_info.action_space.sample()
         return np.asarray(action).reshape((1, )), {}
 
     def _models(self):
         models = {}
         models[self._atom_p.scope_name] = self._atom_p
         return models
 
     def _solvers(self):
         solvers = {}
         solvers[self._atom_p.scope_name] = self._atom_p_solver
         return solvers
 
-    @classmethod
-    def is_supported_env(cls, env_or_env_info):
-        env_info = EnvironmentInfo.from_env(env_or_env_info) if isinstance(env_or_env_info, gym.Env) \
-            else env_or_env_info
-        return not env_info.is_continuous_action_env() and not env_info.is_tuple_action_env()
-
-    @classmethod
-    def is_rnn_supported(self):
-        return True
-
     @property
     def latest_iteration_state(self):
         latest_iteration_state = super(CategoricalDQN, self).latest_iteration_state
         if hasattr(self, '_model_trainer_state'):
             latest_iteration_state['scalar'].update(
-                {'cross_entropy_loss': float(self._model_trainer_state['cross_entropy_loss'])})
+                {'cross_entropy_loss': self._model_trainer_state['cross_entropy_loss']})
             latest_iteration_state['histogram'].update({'td_errors': self._model_trainer_state['td_errors'].flatten()})
         return latest_iteration_state
-
-    @property
-    def trainers(self):
-        return {"q_function": self._model_trainer}
```

## nnabla_rl/algorithms/common_utils.py

```diff
@@ -1,165 +1,78 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from abc import ABCMeta, abstractmethod
-from typing import Dict, Generic, Optional, Sequence, Tuple, TypeVar, Union, cast
-
 import numpy as np
 
 import nnabla as nn
-import nnabla.functions as NF
-import nnabla_rl.functions as RF
-from nnabla_rl.algorithm import eval_api
-from nnabla_rl.distributions.distribution import Distribution
-from nnabla_rl.environments.environment_info import EnvironmentInfo
-from nnabla_rl.models import (DeterministicDecisionTransformer, DeterministicDynamics, DeterministicPolicy,
-                              FactoredContinuousQFunction, Model, QFunction, RewardFunction,
-                              StochasticDecisionTransformer, StochasticPolicy, VFunction)
+from nnabla_rl.models import Model, RewardFunction, StochasticPolicy, VFunction
 from nnabla_rl.preprocessors import Preprocessor
-from nnabla_rl.typing import Experience, State
-from nnabla_rl.utils.data import add_batch_dimension, marshal_experiences, set_data_to_variable
-from nnabla_rl.utils.misc import create_variable, create_variables
-
-DecisionTransformerModel = Union[StochasticDecisionTransformer, DeterministicDecisionTransformer]
-
-
-def _get_shape(state: State) -> Union[Tuple[int, ...], Tuple[Tuple[int, ...], ...]]:
-    shape: Union[Tuple[int, ...], Tuple[Tuple[int, ...], ...]]
-    if isinstance(state, tuple):
-        shape = tuple(np.atleast_1d(s).shape for s in state)
-    else:
-        shape = np.atleast_1d(state).shape
-    return shape
-
-
-def has_batch_dimension(state: State, env_info: EnvironmentInfo):
-    fed_state_shape = _get_shape(state)
-    env_state_shape = env_info.state_shape
-
-    return not (fed_state_shape == env_state_shape)
 
 
-def compute_v_target_and_advantage(v_function: VFunction,
-                                   experiences: Sequence[Experience],
-                                   gamma: float = 0.99,
-                                   lmb: float = 0.97) -> Tuple[np.ndarray, np.ndarray]:
-    """Compute value target and advantage by using Generalized Advantage
-    Estimation (GAE)
+def compute_v_target_and_advantage(v_function, experiences, gamma=0.99, lmb=0.97):
+    ''' Compute value target and advantage by using Generalized Advantage Estimation (GAE)
 
     Args:
         v_function (M.VFunction): value function
-        experiences (Sequence[Experience]): list of experience.
+        experiences (list): list of experience.
             experience should have [state_current, action, reward, non_terminal, state_next]
         gamma (float): discount rate
         lmb (float): lambda
         preprocess_func (callable): preprocess function of states
     Returns:
-        Tuple[np.ndarray, np.ndarray]: target of value and advantage
+        v_target (numpy.ndarray): target of value
+        advantage (numpy.ndarray): advantage
     Ref:
         https://arxiv.org/pdf/1506.02438.pdf
-    """
-    assert isinstance(v_function, VFunction), "Invalid v_function"
-
-    T = len(experiences)
-    v_targets: np.ndarray = np.empty(shape=(T, 1), dtype=np.float32)
-    advantages: np.ndarray = np.empty(shape=(T, 1), dtype=np.float32)
-    advantage: np.float32 = np.float32(0.)
-
-    v_current = None
-    v_next = None
-    s_var = create_variable(1, _get_shape(experiences[0][0]))
-    v = v_function.v(s_var)  # build graph
-
-    for t in reversed(range(T)):
-        s_current, _, r, non_terminal, s_next, *_ = experiences[t]
-
-        # predict current v
-        set_data_to_variable(s_var, s_current)
-        v.forward()
-        v_current = np.squeeze(v.d)
-
-        if v_next is None:
-            set_data_to_variable(s_var, s_next)
-            v.forward()
-            v_next = np.squeeze(v.d)
-
-        delta = r + gamma * non_terminal * v_next - v_current
-        advantage = np.float32(delta + gamma * lmb * non_terminal * advantage)
-        # A = Q - V, V = E[Q] -> v_target = A + V
-        v_target = advantage + v_current
-
-        v_targets[t] = v_target
-        advantages[t] = advantage
-
-        v_next = v_current
-
-    return np.array(v_targets, dtype=np.float32), np.array(advantages, dtype=np.float32)
-
-
-def compute_average_v_target_and_advantage(v_function: VFunction,
-                                           experiences: Sequence[Experience],
-                                           lmb=0.95):
-    ''' Compute value target and advantage by using Average Reward Criterion
-    See: https://arxiv.org/pdf/2106.07329.pdf
-
-    Args:
-        v_function (VFunction): value function
-        experiences (Sequence[Experience]): list of experience.
-            experience should have [state_current, action, reward, non_terminal, state_next]
-        lmb (float): lambda
-    Returns:
-        Tuple[np.ndarray, np.ndarray]: target of value and advantage
     '''
     assert isinstance(v_function, VFunction), "Invalid v_function"
     T = len(experiences)
-    v_targets: np.ndarray = np.empty(shape=(T, 1), dtype=np.float32)
-    advantages: np.ndarray = np.empty(shape=(T, 1), dtype=np.float32)
-    advantage: np.float32 = np.float32(0.)
+    v_targets = []
+    advantages = []
+    advantage = 0.
 
     v_current = None
     v_next = None
-    s_var = create_variable(1, _get_shape(experiences[0][0]))
+    state_shape = experiences[0][0].shape  # get state shape
+    s_var = nn.Variable((1, *state_shape))
     v = v_function.v(s_var)  # build graph
 
-    _, _, batch_r, *_ = marshal_experiences(experiences)
-    average_r = np.mean(batch_r)
-
     for t in reversed(range(T)):
         s_current, _, r, non_terminal, s_next, *_ = experiences[t]
 
         # predict current v
-        set_data_to_variable(s_var, s_current)
+        s_var.d = s_current
         v.forward()
         v_current = np.squeeze(v.d)
 
         if v_next is None:
-            set_data_to_variable(s_var, s_next)
+            s_var.d = s_next
             v.forward()
             v_next = np.squeeze(v.d)
 
-        delta = (r - average_r) + non_terminal * v_next - v_current
-        advantage = np.float32(delta + lmb * non_terminal * advantage)
+        delta = r + gamma * non_terminal * v_next - v_current
+        advantage = np.float32(
+            delta + gamma * lmb * non_terminal * advantage)
         # A = Q - V, V = E[Q] -> v_target = A + V
         v_target = advantage + v_current
 
-        v_targets[t] = v_target
-        advantages[t] = advantage
+        v_targets.insert(0, v_target)
+        advantages.insert(0, advantage)
 
         v_next = v_current
 
     return np.array(v_targets, dtype=np.float32), np.array(advantages, dtype=np.float32)
 
 
 class _StatePreprocessedVFunction(VFunction):
@@ -171,96 +84,40 @@
         self._v_function = v_function
         self._preprocessor = preprocessor
 
     def v(self, s: nn.Variable):
         preprocessed_state = self._preprocessor.process(s)
         return self._v_function.v(preprocessed_state)
 
-    def deepcopy(self, new_scope_name: str) -> '_StatePreprocessedVFunction':
+    def deepcopy(self, new_scope_name: str) -> Model:
         copied = super().deepcopy(new_scope_name=new_scope_name)
         assert isinstance(copied,  _StatePreprocessedVFunction)
         copied._v_function._scope_name = new_scope_name
         return copied
 
-    def is_recurrent(self) -> bool:
-        return self._v_function.is_recurrent()
-
-    def internal_state_shapes(self) -> Dict[str, Tuple[int, ...]]:
-        return self._v_function.internal_state_shapes()
 
-    def set_internal_states(self, states: Optional[Dict[str, nn.Variable]] = None):
-        return self._v_function.set_internal_states(states)
-
-    def get_internal_states(self) -> Dict[str, nn.Variable]:
-        return self._v_function.get_internal_states()
-
-
-class _StatePreprocessedDeterministicPolicy(DeterministicPolicy):
-    _policy: DeterministicPolicy
-    _preprocessor: Preprocessor
-
-    def __init__(self, policy: DeterministicPolicy, preprocessor: Preprocessor):
-        super(_StatePreprocessedDeterministicPolicy, self).__init__(policy.scope_name)
-        self._policy = policy
-        self._preprocessor = preprocessor
-
-    def pi(self, s: nn.Variable) -> nn.Variable:
-        preprocessed_state = self._preprocessor.process(s)
-        return self._policy.pi(preprocessed_state)
-
-    def deepcopy(self, new_scope_name: str) -> '_StatePreprocessedDeterministicPolicy':
-        copied = super().deepcopy(new_scope_name=new_scope_name)
-        assert isinstance(copied,  _StatePreprocessedDeterministicPolicy)
-        copied._policy._scope_name = new_scope_name
-        return copied
-
-    def is_recurrent(self) -> bool:
-        return self._policy.is_recurrent()
-
-    def internal_state_shapes(self) -> Dict[str, Tuple[int, ...]]:
-        return self._policy.internal_state_shapes()
-
-    def set_internal_states(self, states: Optional[Dict[str, nn.Variable]] = None):
-        return self._policy.set_internal_states(states)
-
-    def get_internal_states(self) -> Dict[str, nn.Variable]:
-        return self._policy.get_internal_states()
-
-
-class _StatePreprocessedStochasticPolicy(StochasticPolicy):
+class _StatePreprocessedPolicy(StochasticPolicy):
     _policy: StochasticPolicy
     _preprocessor: Preprocessor
 
     def __init__(self, policy: StochasticPolicy, preprocessor: Preprocessor):
-        super(_StatePreprocessedStochasticPolicy, self).__init__(policy.scope_name)
+        super(_StatePreprocessedPolicy, self).__init__(policy.scope_name)
         self._policy = policy
         self._preprocessor = preprocessor
 
-    def pi(self, s: nn.Variable) -> Distribution:
+    def pi(self, s: nn.Variable):
         preprocessed_state = self._preprocessor.process(s)
         return self._policy.pi(preprocessed_state)
 
-    def deepcopy(self, new_scope_name: str) -> '_StatePreprocessedStochasticPolicy':
+    def deepcopy(self, new_scope_name: str) -> Model:
         copied = super().deepcopy(new_scope_name=new_scope_name)
-        assert isinstance(copied,  _StatePreprocessedStochasticPolicy)
+        assert isinstance(copied,  _StatePreprocessedPolicy)
         copied._policy._scope_name = new_scope_name
         return copied
 
-    def is_recurrent(self) -> bool:
-        return self._policy.is_recurrent()
-
-    def internal_state_shapes(self) -> Dict[str, Tuple[int, ...]]:
-        return self._policy.internal_state_shapes()
-
-    def set_internal_states(self, states: Optional[Dict[str, nn.Variable]] = None):
-        return self._policy.set_internal_states(states)
-
-    def get_internal_states(self) -> Dict[str, nn.Variable]:
-        return self._policy.get_internal_states()
-
 
 class _StatePreprocessedRewardFunction(RewardFunction):
     _reward_function: RewardFunction
     _preprocessor: Preprocessor
 
     def __init__(self, reward_function: RewardFunction, preprocessor: Preprocessor):
         super(_StatePreprocessedRewardFunction, self).__init__(reward_function.scope_name)
@@ -268,389 +125,12 @@
         self._preprocessor = preprocessor
 
     def r(self, s_current: nn.Variable, a_current: nn.Variable, s_next: nn.Variable) -> nn.Variable:
         preprocessed_state_current = self._preprocessor.process(s_current)
         preprocessed_state_next = self._preprocessor.process(s_next)
         return self._reward_function.r(preprocessed_state_current, a_current, preprocessed_state_next)
 
-    def deepcopy(self, new_scope_name: str) -> '_StatePreprocessedRewardFunction':
+    def deepcopy(self, new_scope_name: str) -> Model:
         copied = super().deepcopy(new_scope_name=new_scope_name)
         assert isinstance(copied,  _StatePreprocessedRewardFunction)
         copied._reward_function._scope_name = new_scope_name
         return copied
-
-    def is_recurrent(self) -> bool:
-        return self._reward_function.is_recurrent()
-
-    def internal_state_shapes(self) -> Dict[str, Tuple[int, ...]]:
-        return self._reward_function.internal_state_shapes()
-
-    def set_internal_states(self, states: Optional[Dict[str, nn.Variable]] = None):
-        return self._reward_function.set_internal_states(states)
-
-    def get_internal_states(self) -> Dict[str, nn.Variable]:
-        return self._reward_function.get_internal_states()
-
-
-class _StatePreprocessedQFunction(QFunction):
-    _q_function: QFunction
-    _preprocessor: Preprocessor
-
-    def __init__(self, q_function: QFunction, preprocessor: Preprocessor):
-        super(_StatePreprocessedQFunction, self).__init__(q_function.scope_name)
-        self._q_function = q_function
-        self._preprocessor = preprocessor
-
-    def q(self, s: nn.Variable, a: nn.Variable):
-        preprocessed_state = self._preprocessor.process(s)
-        return self._q_function.q(preprocessed_state, a)
-
-    def all_q(self, s: nn.Variable) -> nn.Variable:
-        preprocessed_state = self._preprocessor.process(s)
-        return self._q_function.all_q(preprocessed_state)
-
-    def max_q(self, s: nn.Variable) -> nn.Variable:
-        preprocessed_state = self._preprocessor.process(s)
-        return self._q_function.max_q(preprocessed_state)
-
-    def argmax_q(self, s: nn.Variable) -> nn.Variable:
-        preprocessed_state = self._preprocessor.process(s)
-        return self._q_function.argmax_q(preprocessed_state)
-
-    def deepcopy(self, new_scope_name: str) -> '_StatePreprocessedQFunction':
-        copied = super().deepcopy(new_scope_name=new_scope_name)
-        assert isinstance(copied,  _StatePreprocessedQFunction)
-        copied._q_function._scope_name = new_scope_name
-        return copied
-
-    def is_recurrent(self) -> bool:
-        return self._q_function.is_recurrent()
-
-    def internal_state_shapes(self) -> Dict[str, Tuple[int, ...]]:
-        return self._q_function.internal_state_shapes()
-
-    def set_internal_states(self, states: Optional[Dict[str, nn.Variable]] = None):
-        return self._q_function.set_internal_states(states)
-
-    def get_internal_states(self) -> Dict[str, nn.Variable]:
-        return self._q_function.get_internal_states()
-
-
-M = TypeVar('M', bound=Model)
-
-
-class _ActionSelector(Generic[M], metaclass=ABCMeta):
-    # type declarations to type check with mypy
-    # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
-    # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
-    _env_info: EnvironmentInfo
-    _model: M
-
-    def __init__(self, env_info: EnvironmentInfo, model: M):
-        self._env_info = env_info
-        self._model = model
-        self._batch_size = 1
-
-    @eval_api
-    def __call__(self, s: Union[np.ndarray, Tuple[np.ndarray, ...]], *, begin_of_episode: bool = False, extra_info={}):
-        if not has_batch_dimension(s, self._env_info):
-            s = add_batch_dimension(s)
-        batch_size = len(s[0]) if self._env_info.is_tuple_state_env() else len(s)
-        if not hasattr(self, '_eval_state_var') or batch_size != self._batch_size:
-            # Variable creation
-            self._eval_state_var = create_variable(batch_size, self._env_info.state_shape)
-            if self._model.is_recurrent():
-                self._rnn_internal_states = create_variables(batch_size, self._model.internal_state_shapes())
-                self._model.set_internal_states(self._rnn_internal_states)
-            self._action = self._compute_action(self._eval_state_var)
-            if self._model.is_recurrent():
-                self._model.reset_internal_states()
-            self._batch_size = batch_size
-        # Forward network
-        if self._model.is_recurrent() and begin_of_episode:
-            self._model.reset_internal_states()
-        set_data_to_variable(self._eval_state_var, s)
-        if self._model.is_recurrent():
-            prev_rnn_states = self._model.get_internal_states()
-            for key in self._rnn_internal_states.keys():
-                # copy internal states of previous iteration
-                self._rnn_internal_states[key].d = prev_rnn_states[key].d
-        if self._env_info.is_tuple_action_env():
-            nn.forward_all(self._action, clear_no_need_grad=True)
-            action = tuple(np.squeeze(a.d, axis=0) if batch_size == 1 else a.d for a in self._action)
-        else:
-            self._action.forward(clear_no_need_grad=True)
-            # No need to save internal states
-            action = np.squeeze(self._action.d, axis=0) if batch_size == 1 else self._action.d
-        return action, {}
-
-    @abstractmethod
-    def _compute_action(self, state_var: nn.Variable) -> nn.Variable:
-        raise NotImplementedError
-
-
-class _DecisionTransformerActionSelector(_ActionSelector[DecisionTransformerModel]):
-    def __init__(self, env_info: EnvironmentInfo, decision_transformer: DecisionTransformerModel,
-                 max_timesteps: int, context_length: int, target_return: float, reward_scale: float):
-        super().__init__(env_info, decision_transformer)
-        self._max_timesteps = max_timesteps
-        self._context_length = context_length
-        self._target_return = target_return
-        self._reward_scale = reward_scale
-
-    def __call__(self, s: Union[np.ndarray, Tuple[np.ndarray, ...]], *, begin_of_episode: bool = False, extra_info={}):
-        if self._env_info.is_tuple_state_env():
-            raise NotImplementedError('Tuple env not supported')
-
-        if not has_batch_dimension(s, self._env_info):
-            s = add_batch_dimension(s)
-        batch_size = len(s)
-
-        if not hasattr(self, '_eval_states') or batch_size != self._batch_size:
-            self._eval_states = np.empty(shape=(batch_size, self._context_length, *self._env_info.state_shape))
-            self._eval_actions = np.empty(shape=(batch_size, self._context_length, *self._env_info.action_shape))
-            self._eval_rtgs = np.empty(shape=(batch_size, self._context_length, 1))
-            self._eval_timesteps = np.zeros(shape=(batch_size, 1, 1))
-
-        if begin_of_episode:
-            self._eval_timesteps[...] = 0
-
-        t = int(self._eval_timesteps)
-        T = min(t, self._context_length - 1)
-        self._eval_states[:, T, ...] = s
-        if t == 0:
-            self._eval_rtgs[:, T, ...] = self._target_return * self._reward_scale
-        else:
-            reward = extra_info['reward'] * self._reward_scale
-            self._eval_rtgs[:, T, ...] = self._eval_rtgs[:, T-1, ...] - reward
-
-        with nn.auto_forward():
-            states_var = nn.Variable.from_numpy_array(self._eval_states[:, 0:T+1, ...])
-            if begin_of_episode:
-                actions_var = None
-            else:
-                if self._context_length <= t:
-                    actions_var = nn.Variable.from_numpy_array(self._eval_actions[:, 0:T+1, ...])
-                else:
-                    actions_var = nn.Variable.from_numpy_array(self._eval_actions[:, 0:T, ...])
-            rtgs_var = nn.Variable.from_numpy_array(self._eval_rtgs[:, 0:T+1, ...])
-            timesteps_var = nn.Variable.from_numpy_array(self._eval_timesteps)
-
-            if isinstance(self._model, DeterministicDecisionTransformer):
-                actions = self._model.pi(states_var, actions_var, rtgs_var, timesteps_var)
-                action = np.squeeze(actions.d[:, T, :], axis=0)
-            else:
-                pi = self._model.pi(states_var, actions_var, rtgs_var, timesteps_var)
-                actions = cast(nn.Variable, pi.sample())
-                action = np.squeeze(actions.d[:, T, :], axis=0)
-
-        if T == self._context_length - 1:
-            self._eval_states = np.roll(self._eval_states, shift=-1, axis=1)
-            self._eval_rtgs = np.roll(self._eval_rtgs, shift=-1, axis=1)
-        if self._context_length <= t:
-            self._eval_actions = np.roll(self._eval_actions, shift=-1, axis=1)
-
-        self._eval_actions[:, T, ...] = action
-        self._eval_timesteps[...] = min(cast(int, self._eval_timesteps + 1), self._max_timesteps)
-
-        return action
-
-    def _compute_action(self, state_var: nn.Variable) -> nn.Variable:
-        raise NotImplementedError
-
-
-class _GreedyActionSelector(_ActionSelector[QFunction]):
-    def __init__(self, env_info: EnvironmentInfo, q_function: QFunction):
-        super().__init__(env_info, q_function)
-        self._env_info = env_info
-        self._q = q_function
-
-    def _compute_action(self, state_var: nn.Variable) -> nn.Variable:
-        return self._q.argmax_q(self._eval_state_var)
-
-
-class _StochasticPolicyActionSelector(_ActionSelector[StochasticPolicy]):
-    def __init__(self, env_info, policy: StochasticPolicy, deterministic: bool = True):
-        super().__init__(env_info, policy)
-        self._deterministic = deterministic
-
-    def _compute_action(self, state_var: nn.Variable) -> nn.Variable:
-        distribution = self._model.pi(self._eval_state_var)
-
-        if self._deterministic:
-            return distribution.choose_probable()
-        else:
-            return distribution.sample()
-
-
-class _DeterministicPolicyActionSelector(_ActionSelector[DeterministicPolicy]):
-    def __init__(self, env_info, policy: DeterministicPolicy):
-        super().__init__(env_info, policy)
-
-    def _compute_action(self, state_var: nn.Variable) -> nn.Variable:
-        return self._model.pi(self._eval_state_var)
-
-
-class _StatePredictor(Generic[M], metaclass=ABCMeta):
-    # type declarations to type check with mypy
-    # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
-    # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
-    _env_info: EnvironmentInfo
-    _model: M
-
-    def __init__(self, env_info: EnvironmentInfo, model: M):
-        self._env_info = env_info
-        self._model = model
-        self._batch_size = 1
-
-    @eval_api
-    def __call__(self,
-                 s: Union[np.ndarray, Tuple[np.ndarray, ...]],
-                 a: np.ndarray,
-                 *,
-                 begin_of_episode: bool = False):
-        if not has_batch_dimension(s, self._env_info):
-            s = add_batch_dimension(s)
-        if not has_batch_dimension(a, self._env_info):
-            a = cast(np.ndarray, add_batch_dimension(a))
-        batch_size = len(s[0]) if self._env_info.is_tuple_state_env() else len(s)
-        if not hasattr(self, '_eval_state_var') or batch_size != self._batch_size:
-            # Variable creation
-            self._eval_state_var = create_variable(batch_size, self._env_info.state_shape)
-            self._eval_action_var = create_variable(batch_size, self._env_info.action_shape)
-            if self._model.is_recurrent():
-                self._rnn_internal_states = create_variables(batch_size, self._model.internal_state_shapes())
-                self._model.set_internal_states(self._rnn_internal_states)
-            self._next_state = self._compute_next_state(self._eval_state_var, self._eval_action_var)
-            if self._model.is_recurrent():
-                self._model.reset_internal_states()
-            self._batch_size = batch_size
-        # Forward network
-        if self._model.is_recurrent() and begin_of_episode:
-            self._model.reset_internal_states()
-        set_data_to_variable(self._eval_state_var, s)
-        set_data_to_variable(self._eval_action_var, a)
-        if self._model.is_recurrent():
-            prev_rnn_states = self._model.get_internal_states()
-            for key in self._rnn_internal_states.keys():
-                # copy internal states of previous iteration
-                self._rnn_internal_states[key].d = prev_rnn_states[key].d
-        self._next_state.forward(clear_no_need_grad=True)
-        # No need to save internal states
-        next_state = np.squeeze(self._next_state.d, axis=0) if batch_size == 1 else self._next_state.d
-        return next_state, {}
-
-    @abstractmethod
-    def _compute_next_state(self, state_var: nn.Variable, action_var: nn.Variable) -> nn.Variable:
-        raise NotImplementedError
-
-
-class _DeterministicStatePredictor(_StatePredictor[DeterministicDynamics]):
-    def __init__(self, env_info: EnvironmentInfo, dynamics: DeterministicDynamics):
-        super().__init__(env_info, dynamics)
-        self._dynamics = dynamics
-
-    def _compute_next_state(self, state_var: nn.Variable, action_var: nn.Variable) -> nn.Variable:
-        return self._dynamics.next_state(state_var, action_var)
-
-
-class _InfluenceMetricsEvaluator:
-    """Influence metrics evaluator.
-
-    See details at https://arxiv.org/abs/2206.13901
-
-    Args:
-        env_info (EnvironmentInfo): Environment infomation.
-        q_function (FactoredContinuousQFunction): Factored Q-function for continuous action.
-    """
-    # type declarations to type check with mypy
-    # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
-    # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
-    _env_info: EnvironmentInfo
-    _q_function: FactoredContinuousQFunction
-
-    def __init__(self, env_info: EnvironmentInfo, q_function: FactoredContinuousQFunction):
-        self._env_info = env_info
-        self._q_function = q_function
-        self._batch_size = 1
-
-    @eval_api
-    def __call__(self, s: Union[np.ndarray, Tuple[np.ndarray, ...]], a: np.ndarray, *, begin_of_episode: bool = False) \
-            -> Tuple[np.ndarray, Dict]:
-        if not has_batch_dimension(s, self._env_info):
-            s = add_batch_dimension(s)
-            a = cast(np.ndarray, add_batch_dimension(a))
-        batch_size = len(s[0]) if self._env_info.is_tuple_state_env() else len(s)
-        if not hasattr(self, '_eval_state_var') or batch_size != self._batch_size:
-            # Variable creation
-            self._batch_size = batch_size
-            self._eval_state_var = create_variable(batch_size, self._env_info.state_shape)
-            self._eval_action_var = create_variable(batch_size, self._env_info.action_shape)
-            if self._q_function.is_recurrent():
-                self._rnn_internal_states = create_variables(batch_size, self._q_function.internal_state_shapes())
-                self._q_function.set_internal_states(self._rnn_internal_states)
-            self._metrics = self._compute_influence_metrics(self._eval_state_var, self._eval_action_var)
-            if self._q_function.is_recurrent():
-                self._q_function.reset_internal_states()
-        # Forward network
-        if self._q_function.is_recurrent() and begin_of_episode:
-            self._q_function.reset_internal_states()
-        set_data_to_variable(self._eval_state_var, s)
-        if self._q_function.is_recurrent():
-            prev_rnn_states = self._q_function.get_internal_states()
-            for key in self._rnn_internal_states.keys():
-                # copy internal states of previous iteration
-                self._rnn_internal_states[key].d = prev_rnn_states[key].d
-        self._metrics.forward(clear_no_need_grad=True)
-        # No need to save internal states
-        metrics = np.squeeze(self._metrics.d, axis=0) if batch_size == 1 else self._metrics.d
-        return metrics, {}
-
-    def _compute_influence_metrics(self, state_var: nn.Variable, action_var: nn.Variable) -> nn.Variable:
-        # TODO: support tuple state
-        assert isinstance(state_var, nn.Variable), "Tuple states are not supported yet."
-
-        num_factors = self._q_function.num_factors
-
-        # compute base gradient
-        # (B, A)
-        action_var.need_grad = True
-        base_factored_q = self._q_function.factored_q(state_var, action_var)
-        base_grads = RF.expand_dims(nn.grad([-NF.sum(base_factored_q)], [action_var])[0], axis=1)
-
-        # expand batch to factors
-        # (B, S) -> (B, N, S)
-        expand_state = RF.repeat(RF.expand_dims(state_var, axis=1), num_factors, axis=1)
-        # (B, A) -> (B, N, A)
-        expand_action = RF.repeat(RF.expand_dims(action_var, axis=1), num_factors, axis=1)
-
-        # flatten shapes
-        # (B, N, S) -> (B * N, S)
-        flat_state = NF.reshape(expand_state, [-1, *state_var.shape[1:]])
-        # (B, N, A) -> (B * N, A)
-        flat_action = NF.reshape(expand_action, [-1, *action_var.shape[1:]])
-        flat_action.need_grad = True
-
-        # create mask
-        # (N, N)
-        mask = 1.0 - NF.one_hot(RF.expand_dims(NF.arange(0, num_factors), axis=1), shape=(num_factors,))
-        # (N, N) -> (B, N, N)
-        expand_mask = RF.repeat(RF.expand_dims(mask, axis=0), self._batch_size, axis=0)
-        # (B * N, N)
-        flat_mask = NF.reshape(expand_mask, [-1, num_factors])
-
-        # compute Q-values
-        # (B * N, N)
-        factored_q = self._q_function.factored_q(flat_state, flat_action)
-
-        # compute action gradients
-        # (B * N, A)
-        grads = nn.grad([-NF.sum(flat_mask * factored_q)], [flat_action])[0]
-
-        # compute relative influence
-        # (B * N, A) -> (B, N, A)
-        squared_diff = (base_grads - NF.reshape(grads, [self._batch_size, num_factors, -1])) ** 2
-        # (B, N, A) -> (B, N)
-        influence = NF.sum(squared_diff, axis=2) ** 0.5
-
-        # normalize
-        return influence / (NF.sum(influence, axis=1, keepdims=True) + 1e-5)
```

## nnabla_rl/algorithms/ddpg.py

```diff
@@ -1,116 +1,72 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from dataclasses import dataclass
-from typing import Any, Dict, Union
+from typing import Any, Dict, Union, cast
 
 import gym
+import numpy as np
 
 import nnabla as nn
 import nnabla.solvers as NS
 import nnabla_rl.environment_explorers as EE
 import nnabla_rl.model_trainers as MT
 from nnabla_rl.algorithm import Algorithm, AlgorithmConfig, eval_api
-from nnabla_rl.algorithms.common_utils import _DeterministicPolicyActionSelector
-from nnabla_rl.builders import ExplorerBuilder, ModelBuilder, ReplayBufferBuilder, SolverBuilder
+from nnabla_rl.builders import ModelBuilder, ReplayBufferBuilder, SolverBuilder
 from nnabla_rl.environment_explorer import EnvironmentExplorer
 from nnabla_rl.environments.environment_info import EnvironmentInfo
+from nnabla_rl.exceptions import UnsupportedEnvironmentException
 from nnabla_rl.model_trainers.model_trainer import ModelTrainer, TrainingBatch
 from nnabla_rl.models import DeterministicPolicy, QFunction, TD3Policy, TD3QFunction
 from nnabla_rl.replay_buffer import ReplayBuffer
 from nnabla_rl.utils import context
 from nnabla_rl.utils.data import marshal_experiences
 from nnabla_rl.utils.misc import sync_model
 
 
 @dataclass
 class DDPGConfig(AlgorithmConfig):
-    """DDPGConfig List of configurations for DDPG algorithm.
+    '''DDPGConfig
+    List of configurations for DDPG algorithm
 
     Args:
         gamma (float): discount factor of rewards. Defaults to 0.99.
         learning_rate (float): learning rate which is set to all solvers. \
             You can customize/override the learning rate for each solver by implementing the \
             (:py:class:`SolverBuilder <nnabla_rl.builders.SolverBuilder>`) by yourself. \
             Defaults to 0.001.
         batch_size(int): training batch size. Defaults to 100.
         tau (float): target network's parameter update coefficient. Defaults to 0.005.
         start_timesteps (int): the timestep when training starts.\
             The algorithm will collect experiences from the environment by acting randomly until this timestep.\
             Defaults to 10000.
         replay_buffer_size (int): capacity of the replay buffer. Defaults to 1000000.
         exploration_noise_sigma (float): standard deviation of gaussian exploration noise. Defaults to 0.1.
-        num_steps (int): number of steps for N-step Q targets. Defaults to 1.
-        actor_unroll_steps (int): Number of steps to unroll actor's tranining network.\
-            The network will be unrolled even though the provided model doesn't have RNN layers.\
-            Defaults to 1.
-        actor_burn_in_steps (int): Number of burn-in steps to initiaze actor's recurrent layer states during training.\
-            This flag does not take effect if given model is not an RNN model.\
-            Defaults to 0.
-        actor_reset_rnn_on_terminal (bool): Reset actor's recurrent internal states to zero during training\
-            if episode ends. This flag does not take effect if given model is not an RNN model.\
-            Defaults to False.
-        critic_unroll_steps (int): Number of steps to unroll critic's tranining network.\
-            The network will be unrolled even though the provided model doesn't have RNN layers.\
-            Defaults to 1.
-        critic_burn_in_steps (int): Number of burn-in steps to initiaze critic's recurrent layer states\
-            during training. This flag does not take effect if given model is not an RNN model.\
-            Defaults to 0.
-        critic_reset_rnn_on_terminal (bool): Reset critic's recurrent internal states to zero during training\
-            if episode ends. This flag does not take effect if given model is not an RNN model.\
-            Defaults to False.
-    """
+    '''
 
     gamma: float = 0.99
     learning_rate: float = 1.0*1e-3
     batch_size: int = 100
     tau: float = 0.005
     start_timesteps: int = 10000
     replay_buffer_size: int = 1000000
     exploration_noise_sigma: float = 0.1
-    num_steps: int = 1
-
-    # rnn model support
-    actor_unroll_steps: int = 1
-    actor_burn_in_steps: int = 0
-    actor_reset_rnn_on_terminal: bool = True
-
-    critic_unroll_steps: int = 1
-    critic_burn_in_steps: int = 0
-    critic_reset_rnn_on_terminal: bool = True
-
-    def __post_init__(self):
-        """__post_init__
-
-        Check set values are in valid range.
-        """
-        self._assert_between(self.gamma, 0.0, 1.0, 'gamma')
-        self._assert_positive(self.learning_rate, 'learning_rate')
-        self._assert_positive(self.batch_size, 'batch_size')
-        self._assert_positive(self.start_timesteps, 'start_timesteps')
-        self._assert_positive(self.replay_buffer_size, 'replay_buffer_size')
-        self._assert_positive(self.exploration_noise_sigma, 'exploration_noise_sigma')
-
-        self._assert_positive(self.critic_unroll_steps, 'critic_unroll_steps')
-        self._assert_positive_or_zero(self.critic_burn_in_steps, 'critic_burn_in_steps')
-        self._assert_positive(self.actor_unroll_steps, 'actor_unroll_steps')
-        self._assert_positive_or_zero(self.actor_burn_in_steps, 'actor_burn_in_steps')
 
 
 class DefaultCriticBuilder(ModelBuilder[QFunction]):
     def build_model(self,  # type: ignore[override]
                     scope_name: str,
                     env_info: EnvironmentInfo,
                     algorithm_config: DDPGConfig,
@@ -121,15 +77,15 @@
 
 class DefaultActorBuilder(ModelBuilder[DeterministicPolicy]):
     def build_model(self,  # type: ignore[override]
                     scope_name: str,
                     env_info: EnvironmentInfo,
                     algorithm_config: DDPGConfig,
                     **kwargs) -> DeterministicPolicy:
-        max_action_value = float(env_info.action_high[0])
+        max_action_value = float(env_info.action_space.high[0])
         return TD3Policy(scope_name, env_info.action_dim, max_action_value=max_action_value)
 
 
 class DefaultSolverBuilder(SolverBuilder):
     def build_solver(self,  # type: ignore[override]
                      env_info: EnvironmentInfo,
                      algorithm_config: DDPGConfig,
@@ -141,36 +97,16 @@
     def build_replay_buffer(self,  # type: ignore[override]
                             env_info: EnvironmentInfo,
                             algorithm_config: DDPGConfig,
                             **kwargs) -> ReplayBuffer:
         return ReplayBuffer(capacity=algorithm_config.replay_buffer_size)
 
 
-class DefaultExplorerBuilder(ExplorerBuilder):
-    def build_explorer(self,  # type: ignore[override]
-                       env_info: EnvironmentInfo,
-                       algorithm_config: DDPGConfig,
-                       algorithm: "DDPG",
-                       **kwargs) -> EnvironmentExplorer:
-        explorer_config = EE.GaussianExplorerConfig(
-            warmup_random_steps=algorithm_config.start_timesteps,
-            initial_step_num=algorithm.iteration_num,
-            timelimit_as_terminal=False,
-            action_clip_low=env_info.action_low,
-            action_clip_high=env_info.action_high,
-            sigma=algorithm_config.exploration_noise_sigma
-        )
-        explorer = EE.GaussianExplorer(policy_action_selector=algorithm._exploration_action_selector,
-                                       env_info=env_info,
-                                       config=explorer_config)
-        return explorer
-
-
 class DDPG(Algorithm):
-    """Deep Deterministic Policy Gradient (DDPG) algorithm.
+    '''Deep Deterministic Policy Gradient (DDPG) algorithm.
 
     This class implements the modified version of the Deep Deterministic Policy Gradient (DDPG) algorithm
     proposed by T. P.  Lillicrap, et al. in the paper: "Continuous control with deep reinforcement learning"
     For details see: https://arxiv.org/abs/1509.02971
     We use gaussian noise instead of Ornstein-Uhlenbeck process to explore in the environment.
     The effectiveness of using gaussian noise for DDPG is reported in the paper:
     "Addressing Funciton Approximaiton Error in Actor-Critic Methods". see https://arxiv.org/abs/1802.09477
@@ -187,191 +123,168 @@
             builder of critic solvers
         actor_builder (:py:class:`ModelBuilder[DeterministicPolicy] <nnabla_rl.builders.ModelBuilder>`):
             builder of actor models
         actor_solver_builder (:py:class:`SolverBuilder <nnabla_rl.builders.SolverBuilder>`):
             builder of actor solvers
         replay_buffer_builder (:py:class:`ReplayBufferBuilder <nnabla_rl.builders.ReplayBufferBuilder>`):
             builder of replay_buffer
-        explorer_builder (:py:class:`ExplorerBuilder <nnabla_rl.builders.ExplorerBuilder>`):
-            builder of environment explorer
-    """
+    '''
 
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _config: DDPGConfig
     _q: QFunction
     _q_solver: nn.solver.Solver
     _target_q: QFunction
     _pi: DeterministicPolicy
     _pi_solver: nn.solver.Solver
     _target_pi: DeterministicPolicy
     _replay_buffer: ReplayBuffer
-    _explorer_builder: ExplorerBuilder
     _environment_explorer: EnvironmentExplorer
     _q_function_trainer: ModelTrainer
     _policy_trainer: ModelTrainer
 
     _policy_trainer_state: Dict[str, Any]
     _q_function_trainer_state: Dict[str, Any]
 
     def __init__(self, env_or_env_info: Union[gym.Env, EnvironmentInfo],
                  config: DDPGConfig = DDPGConfig(),
                  critic_builder: ModelBuilder[QFunction] = DefaultCriticBuilder(),
                  critic_solver_builder: SolverBuilder = DefaultSolverBuilder(),
                  actor_builder: ModelBuilder[DeterministicPolicy] = DefaultActorBuilder(),
                  actor_solver_builder: SolverBuilder = DefaultSolverBuilder(),
-                 replay_buffer_builder: ReplayBufferBuilder = DefaultReplayBufferBuilder(),
-                 explorer_builder: ExplorerBuilder = DefaultExplorerBuilder()):
+                 replay_buffer_builder: ReplayBufferBuilder = DefaultReplayBufferBuilder()):
         super(DDPG, self).__init__(env_or_env_info, config=config)
-
-        self._explorer_builder = explorer_builder
+        if self._env_info.is_discrete_action_env():
+            raise UnsupportedEnvironmentException
 
         with nn.context_scope(context.get_nnabla_context(self._config.gpu_id)):
             self._q = critic_builder(scope_name="q", env_info=self._env_info, algorithm_config=self._config)
             self._q_solver = critic_solver_builder(env_info=self._env_info, algorithm_config=self._config)
-            self._target_q = self._q.deepcopy('target_' + self._q.scope_name)
+            self._target_q = cast(QFunction, self._q.deepcopy('target_' + self._q.scope_name))
 
             self._pi = actor_builder(scope_name="pi", env_info=self._env_info, algorithm_config=self._config)
             self._pi_solver = actor_solver_builder(env_info=self._env_info, algorithm_config=self._config)
-            self._target_pi = self._pi.deepcopy("target_" + self._pi.scope_name)
+            self._target_pi = cast(DeterministicPolicy, self._pi.deepcopy("target_" + self._pi.scope_name))
 
             self._replay_buffer = replay_buffer_builder(env_info=self._env_info, algorithm_config=self._config)
 
-        self._evaluation_actor = _DeterministicPolicyActionSelector(self._env_info, self._pi.shallowcopy())
-        self._exploration_actor = _DeterministicPolicyActionSelector(self._env_info, self._pi.shallowcopy())
-
-    @eval_api
-    def compute_eval_action(self, state, *, begin_of_episode=False, extra_info={}):
-        with nn.context_scope(context.get_nnabla_context(self._config.gpu_id)):
-            action, _ = self._evaluation_action_selector(state, begin_of_episode=begin_of_episode)
-            return action
-
     def _before_training_start(self, env_or_buffer):
         # set context globally to ensure that the training runs on configured gpu
         context.set_nnabla_context(self._config.gpu_id)
         self._environment_explorer = self._setup_environment_explorer(env_or_buffer)
         self._q_function_trainer = self._setup_q_function_training(env_or_buffer)
         self._policy_trainer = self._setup_policy_training(env_or_buffer)
 
     def _setup_environment_explorer(self, env_or_buffer):
-        return None if self._is_buffer(env_or_buffer) else self._explorer_builder(self._env_info, self._config, self)
+        if self._is_buffer(env_or_buffer):
+            return None
+
+        explorer_config = EE.GaussianExplorerConfig(
+            warmup_random_steps=self._config.start_timesteps,
+            initial_step_num=self.iteration_num,
+            timelimit_as_terminal=False,
+            action_clip_low=self._env_info.action_space.low,
+            action_clip_high=self._env_info.action_space.high,
+            sigma=self._config.exploration_noise_sigma
+        )
+        explorer = EE.GaussianExplorer(policy_action_selector=self._compute_greedy_action,
+                                       env_info=self._env_info,
+                                       config=explorer_config)
+        return explorer
 
     def _setup_q_function_training(self, env_or_buffer):
-        q_function_trainer_config = MT.q_value_trainers.DDPGQTrainerConfig(
+        q_function_trainer_config = MT.q_value.DDPGQTrainerConfig(
             reduction_method='mean',
-            grad_clip=None,
-            num_steps=self._config.num_steps,
-            unroll_steps=self._config.critic_unroll_steps,
-            burn_in_steps=self._config.critic_burn_in_steps,
-            reset_on_terminal=self._config.critic_reset_rnn_on_terminal)
+            grad_clip=None)
 
-        q_function_trainer = MT.q_value_trainers.DDPGQTrainer(
+        q_function_trainer = MT.q_value.DDPGQTrainer(
             train_functions=self._q,
             solvers={self._q.scope_name: self._q_solver},
             target_functions=self._target_q,
             target_policy=self._target_pi,
             env_info=self._env_info,
             config=q_function_trainer_config)
         sync_model(self._q, self._target_q)
         return q_function_trainer
 
     def _setup_policy_training(self, env_or_buffer):
-        policy_trainer_config = MT.policy_trainers.DPGPolicyTrainerConfig(
-            unroll_steps=self._config.actor_unroll_steps,
-            burn_in_steps=self._config.actor_burn_in_steps,
-            reset_on_terminal=self._config.actor_reset_rnn_on_terminal)
-
+        policy_trainer_config = MT.policy_trainers.DPGPolicyTrainerConfig()
         policy_trainer = MT.policy_trainers.DPGPolicyTrainer(
             models=self._pi,
             solvers={self._pi.scope_name: self._pi_solver},
             q_function=self._q,
             env_info=self._env_info,
             config=policy_trainer_config)
         sync_model(self._pi, self._target_pi, tau=1.0)
         return policy_trainer
 
+    def compute_eval_action(self, state):
+        with nn.context_scope(context.get_nnabla_context(self._config.gpu_id)):
+            action, _ = self._compute_greedy_action(state)
+            return action
+
     def _run_online_training_iteration(self, env):
         experiences = self._environment_explorer.step(env)
         self._replay_buffer.append_all(experiences)
         if self._config.start_timesteps < self.iteration_num:
             self._ddpg_training(self._replay_buffer)
 
     def _run_offline_training_iteration(self, buffer):
         self._ddpg_training(buffer)
 
     def _ddpg_training(self, replay_buffer):
-        actor_steps = self._config.actor_burn_in_steps + self._config.actor_unroll_steps
-        critic_steps = self._config.num_steps + self._config.critic_burn_in_steps + self._config.critic_unroll_steps - 1
-        num_steps = max(actor_steps, critic_steps)
-        experiences_tuple, info = replay_buffer.sample(self._config.batch_size, num_steps=num_steps)
-        if num_steps == 1:
-            experiences_tuple = (experiences_tuple, )
-        assert len(experiences_tuple) == num_steps
-
-        batch = None
-        for experiences in reversed(experiences_tuple):
-            (s, a, r, non_terminal, s_next, rnn_states_dict, *_) = marshal_experiences(experiences)
-            rnn_states = rnn_states_dict['rnn_states'] if 'rnn_states' in rnn_states_dict else {}
-            batch = TrainingBatch(batch_size=self._config.batch_size,
-                                  s_current=s,
-                                  a_current=a,
-                                  gamma=self._config.gamma,
-                                  reward=r,
-                                  non_terminal=non_terminal,
-                                  s_next=s_next,
-                                  weight=info['weights'],
-                                  next_step_batch=batch,
-                                  rnn_states=rnn_states)
+        experiences, info = replay_buffer.sample(self._config.batch_size)
+        (s, a, r, non_terminal, s_next, *_) = marshal_experiences(experiences)
+        batch = TrainingBatch(batch_size=self._config.batch_size,
+                              s_current=s,
+                              a_current=a,
+                              gamma=self._config.gamma,
+                              reward=r,
+                              non_terminal=non_terminal,
+                              s_next=s_next,
+                              weight=info['weights'])
 
         self._q_function_trainer_state = self._q_function_trainer.train(batch)
         sync_model(self._q, self._target_q, tau=self._config.tau)
 
         self._policy_trainer_state = self._policy_trainer.train(batch)
         sync_model(self._pi, self._target_pi, tau=self._config.tau)
 
-        td_errors = self._q_function_trainer_state['td_errors']
+        td_errors = np.abs(self._q_function_trainer_state['td_errors'])
         replay_buffer.update_priorities(td_errors)
 
-    def _evaluation_action_selector(self, s, *, begin_of_episode=False):
-        return self._evaluation_actor(s, begin_of_episode=begin_of_episode)
-
-    def _exploration_action_selector(self, s, *, begin_of_episode=False):
-        return self._exploration_actor(s, begin_of_episode=begin_of_episode)
+    @eval_api
+    def _compute_greedy_action(self, s):
+        # evaluation input/action variables
+        s = np.expand_dims(s, axis=0)
+        if not hasattr(self, '_eval_state_var'):
+            self._eval_state_var = nn.Variable(s.shape)
+            self._eval_action = self._pi.pi(self._eval_state_var)
+        self._eval_state_var.d = s
+        self._eval_action.forward()
+        return np.squeeze(self._eval_action.d, axis=0), {}
 
     def _models(self):
         models = {}
         models[self._q.scope_name] = self._q
         models[self._pi.scope_name] = self._pi
         models[self._target_pi.scope_name] = self._target_pi
         return models
 
     def _solvers(self):
         solvers = {}
         solvers[self._pi.scope_name] = self._pi_solver
         solvers[self._q.scope_name] = self._q_solver
         return solvers
 
-    @classmethod
-    def is_rnn_supported(self):
-        return True
-
-    @classmethod
-    def is_supported_env(cls, env_or_env_info):
-        env_info = EnvironmentInfo.from_env(env_or_env_info) if isinstance(env_or_env_info, gym.Env) \
-            else env_or_env_info
-        return not env_info.is_discrete_action_env() and not env_info.is_tuple_action_env()
-
     @property
     def latest_iteration_state(self):
         latest_iteration_state = super(DDPG, self).latest_iteration_state
         if hasattr(self, '_policy_trainer_state'):
-            latest_iteration_state['scalar'].update({'pi_loss': float(self._policy_trainer_state['pi_loss'])})
+            latest_iteration_state['scalar'].update({'pi_loss': self._policy_trainer_state['pi_loss']})
         if hasattr(self, '_q_function_trainer_state'):
-            latest_iteration_state['scalar'].update({'q_loss': float(self._q_function_trainer_state['q_loss'])})
+            latest_iteration_state['scalar'].update({'q_loss': self._q_function_trainer_state['q_loss']})
             latest_iteration_state['histogram'].update(
                 {'td_errors': self._q_function_trainer_state['td_errors'].flatten()})
         return latest_iteration_state
-
-    @property
-    def trainers(self):
-        return {"q_function": self._q_function_trainer, "policy": self._policy_trainer}
```

## nnabla_rl/algorithms/dqn.py

```diff
@@ -1,58 +1,58 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from dataclasses import dataclass
-from typing import Any, Dict, Optional, Tuple, Union
+from typing import Any, Dict, Optional, Tuple, Union, cast
 
 import gym
 import numpy as np
 
 import nnabla as nn
 import nnabla.solvers as NS
 import nnabla_rl.environment_explorers as EE
 import nnabla_rl.model_trainers as MT
 from nnabla_rl.algorithm import Algorithm, AlgorithmConfig, eval_api
-from nnabla_rl.algorithms.common_utils import _GreedyActionSelector
-from nnabla_rl.builders import ExplorerBuilder, ModelBuilder, ReplayBufferBuilder, SolverBuilder
+from nnabla_rl.builders import ModelBuilder, ReplayBufferBuilder, SolverBuilder
 from nnabla_rl.environment_explorer import EnvironmentExplorer
 from nnabla_rl.environment_explorers.epsilon_greedy_explorer import epsilon_greedy_action_selection
 from nnabla_rl.environments.environment_info import EnvironmentInfo
+from nnabla_rl.exceptions import UnsupportedEnvironmentException
 from nnabla_rl.model_trainers.model_trainer import ModelTrainer, TrainingBatch
 from nnabla_rl.models import DQNQFunction, QFunction
 from nnabla_rl.replay_buffer import ReplayBuffer
 from nnabla_rl.utils import context
 from nnabla_rl.utils.data import marshal_experiences
 from nnabla_rl.utils.misc import sync_model
 
 
 @dataclass
 class DQNConfig(AlgorithmConfig):
-    """List of configurations for DQN algorithm.
+    """
+    List of configurations for DQN algorithm
 
     Args:
         gamma (float): discount factor of rewards. Defaults to 0.99.
         learning_rate (float): learning rate which is set to all solvers. \
             You can customize/override the learning rate for each solver by implementing the \
             (:py:class:`SolverBuilder <nnabla_rl.builders.SolverBuilder>`) by yourself. \
             Defaults to 0.00025.
-        batch_size (int): training batch size. Defaults to 32.
-        num_steps (int): number of steps for N-step Q targets. Defaults to 1.
+        batch_size (int): training atch size. Defaults to 32.
         learner_update_frequency (int): the interval of learner update. Defaults to 4.
         target_update_frequency (int): the interval of target q-function update. Defaults to 10000.
         start_timesteps (int): the timestep when training starts.\
             The algorithm will collect experiences from the environment by acting randomly until this timestep.
             Defaults to 50000.
         replay_buffer_size (int): the capacity of replay buffer. Defaults to 1000000.
         max_explore_steps (int): the number of steps decaying the epsilon value.\
@@ -60,65 +60,49 @@
             :math:`\\epsilon=\\epsilon_{init} - step\\times\\frac{\\epsilon_{init} - \
             \\epsilon_{final}}{max\\_explore\\_steps}`.\
             Defaults to 1000000.
         initial_epsilon (float): the initial epsilon value for -greedy explorer. Defaults to 1.0.
         final_epsilon (float): the last epsilon value for -greedy explorer. Defaults to 0.1.
         test_epsilon (float): the epsilon value on testing. Defaults to 0.05.
         grad_clip (Optional[Tuple[float, float]]): Clip the gradient of final layer. Defaults to (-1.0, 1.0).
-        unroll_steps (int): Number of steps to unroll tranining network.
-            The network will be unrolled even though the provided model doesn't have RNN layers.
-            Defaults to 1.
-        burn_in_steps (int): Number of burn-in steps to initiaze recurrent layer states during training.
-            This flag does not take effect if given model is not an RNN model.
-            Defaults to 0.
-        reset_rnn_on_terminal (bool): Reset recurrent internal states to zero during training if episode ends.
-            This flag does not take effect if given model is not an RNN model.
-            Defaults to False.
     """
     gamma: float = 0.99
     learning_rate: float = 2.5e-4
     batch_size: int = 32
-    num_steps: int = 1
     # network update
     learner_update_frequency: float = 4
     target_update_frequency: float = 10000
     # buffers
     start_timesteps: int = 50000
     replay_buffer_size: int = 1000000
     # explore
     max_explore_steps: int = 1000000
     initial_epsilon: float = 1.0
     final_epsilon: float = 0.1
     test_epsilon: float = 0.05
     grad_clip: Optional[Tuple[float, float]] = (-1.0, 1.0)
-    # rnn model support
-    unroll_steps: int = 1
-    burn_in_steps: int = 0
-    reset_rnn_on_terminal: bool = True
 
     def __post_init__(self):
-        """__post_init__
+        '''__post_init__
 
         Check set values are in valid range.
-        """
+
+        '''
         self._assert_between(self.gamma, 0.0, 1.0, 'gamma')
-        self._assert_positive(self.learning_rate, 'learning_rate')
         self._assert_positive(self.batch_size, 'batch_size')
-        self._assert_positive(self.num_steps, 'num_steps')
+        self._assert_positive(self.learning_rate, 'learning_rate')
         self._assert_positive(self.learner_update_frequency, 'learner_update_frequency')
         self._assert_positive(self.target_update_frequency, 'target_update_frequency')
         self._assert_positive(self.start_timesteps, 'start_timesteps')
         self._assert_positive(self.replay_buffer_size, 'replay_buffer_size')
         self._assert_smaller_than(self.start_timesteps, self.replay_buffer_size, 'start_timesteps')
         self._assert_between(self.initial_epsilon, 0.0, 1.0, 'initial_epsilon')
         self._assert_between(self.final_epsilon, 0.0, 1.0, 'final_epsilon')
         self._assert_between(self.test_epsilon, 0.0, 1.0, 'test_epsilon')
         self._assert_positive(self.max_explore_steps, 'max_explore_steps')
-        self._assert_positive(self.unroll_steps, 'unroll_steps')
-        self._assert_positive_or_zero(self.burn_in_steps, 'burn_in_steps')
 
 
 class DefaultQFunctionBuilder(ModelBuilder[QFunction]):
     def build_model(self,  # type: ignore[override]
                     scope_name: str,
                     env_info: EnvironmentInfo,
                     algorithm_config: DQNConfig,
@@ -144,37 +128,16 @@
     def build_replay_buffer(self,  # type: ignore[override]
                             env_info: EnvironmentInfo,
                             algorithm_config: DQNConfig,
                             **kwargs) -> ReplayBuffer:
         return ReplayBuffer(capacity=algorithm_config.replay_buffer_size)
 
 
-class DefaultExplorerBuilder(ExplorerBuilder):
-    def build_explorer(self,  # type: ignore[override]
-                       env_info: EnvironmentInfo,
-                       algorithm_config: DQNConfig,
-                       algorithm: "DQN",
-                       **kwargs) -> EnvironmentExplorer:
-        explorer_config = EE.LinearDecayEpsilonGreedyExplorerConfig(
-            warmup_random_steps=algorithm_config.start_timesteps,
-            initial_step_num=algorithm.iteration_num,
-            initial_epsilon=algorithm_config.initial_epsilon,
-            final_epsilon=algorithm_config.final_epsilon,
-            max_explore_steps=algorithm_config.max_explore_steps
-        )
-        explorer = EE.LinearDecayEpsilonGreedyExplorer(
-            greedy_action_selector=algorithm._exploration_action_selector,
-            random_action_selector=algorithm._random_action_selector,
-            env_info=env_info,
-            config=explorer_config)
-        return explorer
-
-
 class DQN(Algorithm):
-    """DQN algorithm.
+    '''DQN algorithm.
 
     This class implements the Deep Q-Network (DQN) algorithm
     proposed by V. Mnih, et al. in the paper: "Human-level control through deep reinforcement learning"
     For details see: https://www.nature.com/articles/nature14236
 
     Note that default solver used in this implementation is RMSPropGraves as in the original paper.
     However, in practical applications, we recommend using Adam as the optimizer of DQN.
@@ -187,85 +150,83 @@
             the environment to train or environment info
         config (:py:class:`DQNConfig <nnabla_rl.algorithms.dqn.DQNConfig>`):
             the parameter for DQN training
         q_func_builder (:py:class:`ModelBuilder <nnabla_rl.builders.ModelBuilder>`): builder of q function model
         q_solver_builder (:py:class:`SolverBuilder <nnabla_rl.builders.SolverBuilder>`): builder of q function solver
         replay_buffer_builder (:py:class:`ReplayBufferBuilder <nnabla_rl.builders.ReplayBufferBuilder>`):
             builder of replay_buffer
-        explorer_builder (:py:class:`ExplorerBuilder <nnabla_rl.builders.ExplorerBuilder>`):
-            builder of environment explorer
-    """
+    '''
 
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _config: DQNConfig
     _q: QFunction
     _q_solver: nn.solver.Solver
     _target_q: QFunction
     _replay_buffer: ReplayBuffer
-    _explorer_builder: ExplorerBuilder
     _environment_explorer: EnvironmentExplorer
     _q_function_trainer: ModelTrainer
+    _eval_state_var: nn.Variable
+    _a_greedy: nn.Variable
 
     _q_function_trainer_state: Dict[str, Any]
 
-    _evaluation_actor: _GreedyActionSelector
-    _exploration_actor: _GreedyActionSelector
-
     def __init__(self, env_or_env_info: Union[gym.Env, EnvironmentInfo],
                  config: DQNConfig = DQNConfig(),
                  q_func_builder: ModelBuilder[QFunction] = DefaultQFunctionBuilder(),
                  q_solver_builder: SolverBuilder = DefaultSolverBuilder(),
-                 replay_buffer_builder: ReplayBufferBuilder = DefaultReplayBufferBuilder(),
-                 explorer_builder: ExplorerBuilder = DefaultExplorerBuilder()):
+                 replay_buffer_builder: ReplayBufferBuilder = DefaultReplayBufferBuilder()):
         super(DQN, self).__init__(env_or_env_info, config=config)
-
-        self._explorer_builder = explorer_builder
-
+        if not self._env_info.is_discrete_action_env():
+            raise UnsupportedEnvironmentException('{} only supports discrete action environment'.format(self.__name__))
         with nn.context_scope(context.get_nnabla_context(self._config.gpu_id)):
             self._q = q_func_builder(scope_name='q', env_info=self._env_info, algorithm_config=self._config)
             self._q_solver = q_solver_builder(env_info=self._env_info, algorithm_config=self._config)
-            self._target_q = self._q.deepcopy('target_' + self._q.scope_name)
+            self._target_q = cast(QFunction, self._q.deepcopy('target_' + self._q.scope_name))
 
             self._replay_buffer = replay_buffer_builder(env_info=self._env_info, algorithm_config=self._config)
-            self._environment_explorer = explorer_builder(env_info=self._env_info,
-                                                          algorithm_config=self._config,
-                                                          algorithm=self)
-
-        self._evaluation_actor = _GreedyActionSelector(self._env_info, self._q.shallowcopy())
-        self._exploration_actor = _GreedyActionSelector(self._env_info, self._q.shallowcopy())
 
     @eval_api
-    def compute_eval_action(self, state, *, begin_of_episode=False, extra_info={}):
+    def compute_eval_action(self, s):
         with nn.context_scope(context.get_nnabla_context(self._config.gpu_id)):
-            (action, _), _ = epsilon_greedy_action_selection(state,
-                                                             self._evaluation_action_selector,
+            (action, _), _ = epsilon_greedy_action_selection(s,
+                                                             self._greedy_action_selector,
                                                              self._random_action_selector,
-                                                             epsilon=self._config.test_epsilon,
-                                                             begin_of_episode=begin_of_episode)
+                                                             epsilon=self._config.test_epsilon)
             return action
 
     def _before_training_start(self, env_or_buffer):
         # set context globally to ensure that the training runs on configured gpu
         context.set_nnabla_context(self._config.gpu_id)
         self._environment_explorer = self._setup_environment_explorer(env_or_buffer)
         self._q_function_trainer = self._setup_q_function_training(env_or_buffer)
 
     def _setup_environment_explorer(self, env_or_buffer):
-        return None if self._is_buffer(env_or_buffer) else self._explorer_builder(self._env_info, self._config, self)
+        if self._is_buffer(env_or_buffer):
+            return None
+
+        explorer_config = EE.LinearDecayEpsilonGreedyExplorerConfig(
+            warmup_random_steps=self._config.start_timesteps,
+            initial_step_num=self.iteration_num,
+            initial_epsilon=self._config.initial_epsilon,
+            final_epsilon=self._config.final_epsilon,
+            max_explore_steps=self._config.max_explore_steps
+        )
+        explorer = EE.LinearDecayEpsilonGreedyExplorer(
+            greedy_action_selector=self._greedy_action_selector,
+            random_action_selector=self._random_action_selector,
+            env_info=self._env_info,
+            config=explorer_config)
+        return explorer
 
     def _setup_q_function_training(self, env_or_buffer):
         trainer_config = MT.q_value_trainers.DQNQTrainerConfig(
-            num_steps=self._config.num_steps,
             reduction_method='sum',
-            grad_clip=self._config.grad_clip,
-            unroll_steps=self._config.unroll_steps,
-            burn_in_steps=self._config.burn_in_steps,
-            reset_on_terminal=self._config.reset_rnn_on_terminal)
+            grad_clip=self._config.grad_clip)
 
         q_function_trainer = MT.q_value_trainers.DQNQTrainer(
             train_functions=self._q,
             solvers={self._q.scope_name: self._q_solver},
             target_function=self._target_q,
             env_info=self._env_info,
             config=trainer_config)
@@ -278,78 +239,58 @@
         if self._config.start_timesteps < self.iteration_num:
             if self.iteration_num % self._config.learner_update_frequency == 0:
                 self._dqn_training(self._replay_buffer)
 
     def _run_offline_training_iteration(self, buffer):
         self._dqn_training(buffer)
 
-    def _evaluation_action_selector(self, s, *, begin_of_episode=False):
-        return self._evaluation_actor(s, begin_of_episode=begin_of_episode)
-
-    def _exploration_action_selector(self, s, *, begin_of_episode=False):
-        return self._exploration_actor(s, begin_of_episode=begin_of_episode)
+    @eval_api
+    def _greedy_action_selector(self, s):
+        s = np.expand_dims(s, axis=0)
+        if not hasattr(self, '_eval_state_var'):
+            self._eval_state_var = nn.Variable(s.shape)
+            self._a_greedy = self._q.argmax_q(self._eval_state_var)
+        self._eval_state_var.d = s
+        self._a_greedy.forward()
+        return np.squeeze(self._a_greedy.d, axis=0), {}
 
-    def _random_action_selector(self, s, *, begin_of_episode=False):
+    def _random_action_selector(self, s):
         action = self._env_info.action_space.sample()
         return np.asarray(action).reshape((1, )), {}
 
     def _dqn_training(self, replay_buffer):
-        num_steps = self._config.num_steps + self._config.burn_in_steps + self._config.unroll_steps - 1
-        experiences_tuple, info = replay_buffer.sample(self._config.batch_size, num_steps=num_steps)
-        if num_steps == 1:
-            experiences_tuple = (experiences_tuple, )
-        assert len(experiences_tuple) == num_steps
-
-        batch = None
-        for experiences in reversed(experiences_tuple):
-            (s, a, r, non_terminal, s_next, rnn_states_dict, *_) = marshal_experiences(experiences)
-            rnn_states = rnn_states_dict['rnn_states'] if 'rnn_states' in rnn_states_dict else {}
-            batch = TrainingBatch(batch_size=self._config.batch_size,
-                                  s_current=s,
-                                  a_current=a,
-                                  gamma=self._config.gamma,
-                                  reward=r,
-                                  non_terminal=non_terminal,
-                                  s_next=s_next,
-                                  weight=info['weights'],
-                                  next_step_batch=batch,
-                                  rnn_states=rnn_states)
+        experiences, info = replay_buffer.sample(self._config.batch_size)
+        (s, a, r, non_terminal, s_next, *_) = marshal_experiences(experiences)
+        batch = TrainingBatch(batch_size=self._config.batch_size,
+                              s_current=s,
+                              a_current=a,
+                              gamma=self._config.gamma,
+                              reward=r,
+                              non_terminal=non_terminal,
+                              s_next=s_next,
+                              weight=info['weights'])
 
         self._q_function_trainer_state = self._q_function_trainer.train(batch)
         if self.iteration_num % self._config.target_update_frequency == 0:
             sync_model(self._q, self._target_q)
 
-        td_errors = self._q_function_trainer_state['td_errors']
+        td_errors = np.abs(self._q_function_trainer_state['td_errors'])
         replay_buffer.update_priorities(td_errors)
 
     def _models(self):
         models = {}
         models[self._q.scope_name] = self._q
         return models
 
     def _solvers(self):
         solvers = {}
         solvers[self._q.scope_name] = self._q_solver
         return solvers
 
-    @classmethod
-    def is_supported_env(cls, env_or_env_info):
-        env_info = EnvironmentInfo.from_env(env_or_env_info) if isinstance(env_or_env_info, gym.Env) \
-            else env_or_env_info
-        return not env_info.is_continuous_action_env() and not env_info.is_tuple_action_env()
-
-    @classmethod
-    def is_rnn_supported(self):
-        return True
-
     @property
     def latest_iteration_state(self):
         latest_iteration_state = super(DQN, self).latest_iteration_state
         if hasattr(self, '_q_function_trainer_state'):
-            latest_iteration_state['scalar'].update({'q_loss': float(self._q_function_trainer_state['q_loss'])})
+            latest_iteration_state['scalar'].update({'q_loss': self._q_function_trainer_state['q_loss']})
             latest_iteration_state['histogram'].update(
                 {'td_errors': self._q_function_trainer_state['td_errors'].flatten()})
         return latest_iteration_state
-
-    @property
-    def trainers(self):
-        return {"q_function": self._q_function_trainer}
```

## nnabla_rl/algorithms/dummy.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -19,25 +19,24 @@
 
 
 class DummyConfig(AlgorithmConfig):
     pass
 
 
 class Dummy(Algorithm):
-    """This algorithm does nothing.
-
-    Just used for understanding the concept of the framework.
-    """
+    '''
+    This algorithm does nothing. Just used for understanding the concept of the framework.
+    '''
 
     def __init__(self, env_or_env_info, config=DummyConfig()):
         super(Dummy, self).__init__(env_or_env_info, config=config)
         self._action_space = self._env_info.action_space
 
     @eval_api
-    def compute_eval_action(self, state, *, begin_of_episode=False, extra_info={}):
+    def compute_eval_action(self, state):
         return self._action_space.sample()
 
     def _build_evaluation_graph(self):
         assert rl.is_eval_scope()
 
     def _before_training_start(self, env_or_buffer):
         logger.debug("Before training start!! Write your algorithm's initializations here.")
@@ -54,15 +53,7 @@
         logger.debug("Training finished. Do your algorithm's finalizations here.")
 
     def _models(self):
         return {}
 
     def _solvers(self):
         return {}
-
-    @classmethod
-    def is_supported_env(cls, env_or_env_info):
-        return True
-
-    @property
-    def trainers(self):
-        return {}
```

## nnabla_rl/algorithms/gail.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -23,35 +23,32 @@
 import nnabla as nn
 import nnabla.functions as NF
 import nnabla.solvers as NS
 import nnabla_rl.environment_explorers as EE
 import nnabla_rl.model_trainers as MT
 import nnabla_rl.preprocessors as RP
 from nnabla_rl.algorithm import Algorithm, AlgorithmConfig, eval_api
-from nnabla_rl.algorithms.common_utils import (_StatePreprocessedRewardFunction, _StatePreprocessedStochasticPolicy,
-                                               _StatePreprocessedVFunction, _StochasticPolicyActionSelector,
-                                               compute_v_target_and_advantage)
-from nnabla_rl.builders import ExplorerBuilder, ModelBuilder, PreprocessorBuilder, SolverBuilder
+from nnabla_rl.algorithms.common_utils import (_StatePreprocessedPolicy, _StatePreprocessedRewardFunction,
+                                               _StatePreprocessedVFunction, compute_v_target_and_advantage)
+from nnabla_rl.builders import ModelBuilder, PreprocessorBuilder, SolverBuilder
 from nnabla_rl.environment_explorer import EnvironmentExplorer
 from nnabla_rl.environments.environment_info import EnvironmentInfo
 from nnabla_rl.model_trainers.model_trainer import ModelTrainer, TrainingBatch
 from nnabla_rl.models import (GAILDiscriminator, GAILPolicy, GAILVFunction, Model, RewardFunction, StochasticPolicy,
                               VFunction)
 from nnabla_rl.preprocessors import Preprocessor
 from nnabla_rl.replay_buffer import ReplayBuffer
 from nnabla_rl.replay_buffers.buffer_iterator import BufferIterator
 from nnabla_rl.utils import context
-from nnabla_rl.utils.data import add_batch_dimension, marshal_experiences, set_data_to_variable
-from nnabla_rl.utils.misc import create_variable
+from nnabla_rl.utils.data import marshal_experiences
 
 
 @dataclass
 class GAILConfig(AlgorithmConfig):
-    """List of configurations for GAIL algorithm.
-
+    '''GAIL config
     Args:
         act_deterministic_in_eval (bool): Enable act deterministically at evalution. Defaults to True.
         discriminator_batch_size (bool): Trainig batch size of discriminator.\
             Usually, discriminator_batch_size is the same as pi_batch_size. Defaults to 50000.
         discriminator_learning_rate (float): Learning rate which is set to the solvers of dicriminator function. \
             You can customize/override the learning rate for each solver by implementing the \
             (:py:class:`SolverBuilder <nnabla_rl.builders.SolverBuilder>`) by yourself. \
@@ -80,15 +77,15 @@
         vf_batch_size (int): Training batch size of value function. Defaults to 128.
         vf_learning_rate (float): Learning rate which is set to the solvers of value function. \
             You can customize/override the learning rate for each solver by implementing the \
             (:py:class:`SolverBuilder <nnabla_rl.builders.SolverBuilder>`) by yourself. \
             Defaults to 0.001.
         preprocess_state (bool): Enable preprocessing the states in the collected experiences \
             before feeding as training batch. Defaults to True.
-    """
+    '''
 
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     preprocess_state: bool = True
     act_deterministic_in_eval: bool = True
     discriminator_batch_size: int = 50000
@@ -105,18 +102,19 @@
     conjugate_gradient_damping: float = 0.1
     conjugate_gradient_iterations: int = 10
     vf_epochs: int = 5
     vf_batch_size: int = 128
     vf_learning_rate: float = 1e-3
 
     def __post_init__(self):
-        """__post_init__
+        '''__post_init__
 
         Check the values are in valid range.
-        """
+
+        '''
         self._assert_between(self.pi_batch_size, 0, self.num_steps_per_iteration, 'pi_batch_size')
         self._assert_positive(self.discriminator_learning_rate, "discriminator_learning_rate")
         self._assert_positive(self.discriminator_batch_size, "discriminator_batch_size")
         self._assert_positive(self.policy_update_frequency, "policy_update_frequency")
         self._assert_positive(self.discriminator_update_frequency, "discriminator_update_frequency")
         self._assert_positive(self.adversary_entropy_coef, "adversarial_entropy_coef")
         self._assert_between(self.gamma, 0.0, 1.0, 'gamma')
@@ -180,32 +178,16 @@
                      env_info: EnvironmentInfo,
                      algorithm_config: GAILConfig,
                      **kwargs) -> nn.solver.Solver:
         assert isinstance(algorithm_config, GAILConfig)
         return NS.Adam(alpha=algorithm_config.discriminator_learning_rate)
 
 
-class DefaultExplorerBuilder(ExplorerBuilder):
-    def build_explorer(self,  # type: ignore[override]
-                       env_info: EnvironmentInfo,
-                       algorithm_config: GAILConfig,
-                       algorithm: "GAIL",
-                       **kwargs) -> EnvironmentExplorer:
-        explorer_config = EE.RawPolicyExplorerConfig(
-            initial_step_num=algorithm.iteration_num,
-            timelimit_as_terminal=False
-        )
-        explorer = EE.RawPolicyExplorer(policy_action_selector=algorithm._exploration_action_selector,
-                                        env_info=env_info,
-                                        config=explorer_config)
-        return explorer
-
-
 class GAIL(Algorithm):
-    """Generative Adversarial Imitation Learning implementation.
+    '''Generative Adversarial Imitation Learning implementation.
 
     This class implements the Generative Adversarial Imitation Learning (GAIL) algorithm
     proposed by Jonathan Ho, et al. in the paper: "Generative Adversarial Imitation Learning"
     For detail see: https://arxiv.org/abs/1606.03476
 
     This algorithm only supports online training.
 
@@ -223,102 +205,99 @@
             builder of policy models
         reward_function_builder (:py:class:`ModelBuilder[RewardFunction] <nnabla_rl.builders.ModelBuilder>`):
             builder of reward function models
         reward_solver_builder (:py:class:`SolverBuilder <nnabla_rl.builders.SolverBuilder>`):
             builder for reward function solvers
         state_preprocessor_builder (None or :py:class:`PreprocessorBuilder <nnabla_rl.builders.PreprocessorBuilder>`):
             state preprocessor builder to preprocess the states
-        explorer_builder (:py:class:`ExplorerBuilder <nnabla_rl.builders.ExplorerBuilder>`):
-            builder of environment explorer
-    """
+    '''
 
     _config: GAILConfig
     _v_function: VFunction
     _v_function_solver: nn.solver.Solver
     _policy: StochasticPolicy
     _discriminator: RewardFunction
     _discriminator_solver: nn.solver.Solver
-    _explorer_builder: ExplorerBuilder
     _environment_explorer: EnvironmentExplorer
     _v_function_trainer: ModelTrainer
     _policy_trainer: ModelTrainer
     _discriminator_trainer: ModelTrainer
 
     _s_var_label: nn.Variable
     _s_next_var_label: nn.Variable
     _a_var_label: nn.Variable
     _reward: nn.Variable
 
     _v_function_trainer_state: Dict[str, Any]
     _policy_trainer_state: Dict[str, Any]
     _discriminator_trainer_state: Dict[str, Any]
 
-    _evaluation_actor: _StochasticPolicyActionSelector
-    _exploration_actor: _StochasticPolicyActionSelector
-
     def __init__(self, env_or_env_info: Union[gym.Env, EnvironmentInfo],
                  expert_buffer: ReplayBuffer,
                  config: GAILConfig = GAILConfig(),
                  v_function_builder: ModelBuilder[VFunction] = DefaultVFunctionBuilder(),
                  v_solver_builder: SolverBuilder = DefaultVFunctionSolverBuilder(),
                  policy_builder: ModelBuilder[StochasticPolicy] = DefaultPolicyBuilder(),
                  reward_function_builder: ModelBuilder[RewardFunction] = DefaultRewardFunctionBuilder(),
                  reward_solver_builder: SolverBuilder = DefaultRewardFunctionSolverBuilder(),
-                 state_preprocessor_builder: Optional[PreprocessorBuilder] = DefaultPreprocessorBuilder(),
-                 explorer_builder: ExplorerBuilder = DefaultExplorerBuilder()):
+                 state_preprocessor_builder: Optional[PreprocessorBuilder] = DefaultPreprocessorBuilder()):
         super(GAIL, self).__init__(env_or_env_info, config=config)
-
-        self._explorer_builder = explorer_builder
+        if self._env_info.is_discrete_action_env():
+            raise NotImplementedError
 
         with nn.context_scope(context.get_nnabla_context(self._config.gpu_id)):
             policy = policy_builder("pi", self._env_info, self._config)
             v_function = v_function_builder("v", self._env_info, self._config)
             discriminator = reward_function_builder("discriminator", self._env_info, self._config)
 
             if self._config.preprocess_state:
                 if state_preprocessor_builder is None:
                     raise ValueError('State preprocessing is enabled but no preprocessor builder is given')
                 pi_v_preprocessor = state_preprocessor_builder('pi_v_preprocessor', self._env_info, self._config)
                 v_function = _StatePreprocessedVFunction(v_function=v_function, preprocessor=pi_v_preprocessor)
-                policy = _StatePreprocessedStochasticPolicy(policy=policy, preprocessor=pi_v_preprocessor)
+                policy = _StatePreprocessedPolicy(policy=policy, preprocessor=pi_v_preprocessor)
                 r_preprocessor = state_preprocessor_builder('r_preprocessor', self._env_info, self._config)
                 discriminator = _StatePreprocessedRewardFunction(
                     reward_function=discriminator, preprocessor=r_preprocessor)
                 self._pi_v_state_preprocessor = pi_v_preprocessor
                 self._r_state_preprocessor = r_preprocessor
             self._v_function = v_function
             self._policy = policy
             self._discriminator = discriminator
 
             self._v_function_solver = v_solver_builder(self._env_info, self._config)
             self._discriminator_solver = reward_solver_builder(self._env_info, self._config)
 
             self._expert_buffer = expert_buffer
 
-        self._evaluation_actor = _StochasticPolicyActionSelector(
-            self._env_info, self._policy.shallowcopy(), deterministic=self._config.act_deterministic_in_eval)
-        self._exploration_actor = _StochasticPolicyActionSelector(
-            self._env_info, self._policy.shallowcopy(), deterministic=False)
-
     @eval_api
-    def compute_eval_action(self, state, *, begin_of_episode=False, extra_info={}):
+    def compute_eval_action(self, s):
         with nn.context_scope(context.get_nnabla_context(self._config.gpu_id)):
-            action, _ = self._evaluation_action_selector(state, begin_of_episode=begin_of_episode)
+            action, _ = self._compute_action(s, act_deterministic=self._config.act_deterministic_in_eval)
             return action
 
     def _before_training_start(self, env_or_buffer):
         # set context globally to ensure that the training runs on configured gpu
         context.set_nnabla_context(self._config.gpu_id)
         self._environment_explorer = self._setup_environment_explorer(env_or_buffer)
         self._v_function_trainer = self._setup_v_function_training(env_or_buffer)
         self._policy_trainer = self._setup_policy_training(env_or_buffer)
         self._discriminator_trainer = self._setup_reward_function_training(env_or_buffer)
 
     def _setup_environment_explorer(self, env_or_buffer):
-        return None if self._is_buffer(env_or_buffer) else self._explorer_builder(self._env_info, self._config, self)
+        if self._is_buffer(env_or_buffer):
+            return None
+        explorer_config = EE.RawPolicyExplorerConfig(
+            initial_step_num=self.iteration_num,
+            timelimit_as_terminal=False
+        )
+        explorer = EE.RawPolicyExplorer(policy_action_selector=self._compute_action,
+                                        env_info=self._env_info,
+                                        config=explorer_config)
+        return explorer
 
     def _setup_v_function_training(self, env_or_buffer):
         v_function_trainer_config = MT.v_value_trainers.MonteCarloVTrainerConfig(
             reduction_method='mean',
             v_loss_scalar=1.0
         )
         v_function_trainer = MT.v_value_trainers.MonteCarloVTrainer(
@@ -369,25 +348,28 @@
 
         self._gail_training(buffer)
 
     def _label_experience(self, experience):
         labeled_experience = []
         if not hasattr(self, '_s_var_label'):
             # build graph
-            self._s_var_label = create_variable(1, self._env_info.state_shape)
-            self._s_next_var_label = create_variable(1, self._env_info.state_shape)
-            self._a_var_label = create_variable(1, self._env_info.action_shape)
+            self._s_var_label = nn.Variable((1, *self._env_info.state_shape))
+            self._s_next_var_label = nn.Variable((1, *self._env_info.state_shape))
+            if self._env_info.is_discrete_action_env():
+                self._a_var_label = nn.Variable((1, 1))
+            else:
+                self._a_var_label = nn.Variable((1, self._env_info.action_dim))
             logits_fake = self._discriminator.r(self._s_var_label, self._a_var_label, self._s_next_var_label)
             self._reward = -NF.log(1. - NF.sigmoid(logits_fake) + 1e-8)
 
         for s, a, _, non_terminal, n_s, info in experience:
             # forward and get reward
-            set_data_to_variable(self._s_var_label, add_batch_dimension(s))
-            set_data_to_variable(self._a_var_label, add_batch_dimension(a))
-            set_data_to_variable(self._s_next_var_label, add_batch_dimension(n_s))
+            self._s_var_label.d = s.reshape((1, -1))
+            self._a_var_label.d = a.reshape((1, -1))
+            self._s_next_var_label.d = n_s.reshape((1, -1))
             self._reward.forward()
             transition = (s, a, self._reward.d, non_terminal, n_s, info)
             labeled_experience.append(transition)
 
         return labeled_experience
 
     def _run_offline_training_iteration(self, buffer):
@@ -512,19 +494,39 @@
         extra['s_next_expert'] = s_next_expert[:self._config.discriminator_batch_size]
 
         batch = TrainingBatch(batch_size=self._config.discriminator_batch_size,
                               extra=extra)
 
         self._discriminator_trainer_state = self._discriminator_trainer.train(batch)
 
-    def _evaluation_action_selector(self, s, *, begin_of_episode=False):
-        return self._evaluation_actor(s, begin_of_episode=begin_of_episode)
-
-    def _exploration_action_selector(self, s, *, begin_of_episode=False):
-        return self._exploration_actor(s, begin_of_episode=begin_of_episode)
+    @eval_api
+    def _compute_action(self, s, act_deterministic=False):
+        s = np.expand_dims(s, axis=0)
+        if not hasattr(self, '_eval_state_var'):
+            self._eval_state_var = nn.Variable(s.shape)
+            self._eval_a_distribution = self._policy.pi(self._eval_state_var)
+
+        if act_deterministic:
+            eval_a = self._deterministic_action()
+        else:
+            eval_a = self._probabilistic_action()
+
+        self._eval_state_var.d = s
+        eval_a.forward()
+        return np.squeeze(eval_a.d, axis=0), {}
+
+    def _deterministic_action(self):
+        if not hasattr(self, '_eval_deterministic_a'):
+            self._eval_deterministic_a = self._eval_a_distribution.choose_probable()
+        return self._eval_deterministic_a
+
+    def _probabilistic_action(self):
+        if not hasattr(self, '_eval_probabilistic_a'):
+            self._eval_probabilistic_a = self._eval_a_distribution.sample()
+        return self._eval_probabilistic_a
 
     def _models(self):
         models = {}
         models[self._policy.scope_name] = self._policy
         models[self._v_function.scope_name] = self._v_function
         models[self._discriminator.scope_name] = self._discriminator
         if self._config.preprocess_state and isinstance(self._r_state_preprocessor, Model):
@@ -535,30 +537,15 @@
 
     def _solvers(self):
         solvers = {}
         solvers[self._v_function.scope_name] = self._v_function_solver
         solvers[self._discriminator.scope_name] = self._discriminator_solver
         return solvers
 
-    @classmethod
-    def is_supported_env(cls, env_or_env_info):
-        env_info = EnvironmentInfo.from_env(env_or_env_info) if isinstance(env_or_env_info, gym.Env) \
-            else env_or_env_info
-        return not env_info.is_discrete_action_env() and not env_info.is_tuple_action_env()
-
     @property
     def latest_iteration_state(self):
         latest_iteration_state = super(GAIL, self).latest_iteration_state
         if hasattr(self, '_discriminator_trainer_state'):
-            latest_iteration_state['scalar'].update(
-                {'reward_loss': float(self._discriminator_trainer_state['reward_loss'])})
+            latest_iteration_state['scalar'].update({'reward_loss': self._discriminator_trainer_state['reward_loss']})
         if hasattr(self, '_v_function_trainer_state'):
-            latest_iteration_state['scalar'].update({'v_loss': float(self._v_function_trainer_state['v_loss'])})
+            latest_iteration_state['scalar'].update({'v_loss': self._v_function_trainer_state['v_loss']})
         return latest_iteration_state
-
-    @property
-    def trainers(self):
-        return {
-            "discriminator": self._discriminator_trainer,
-            "v_function": self._v_function_trainer,
-            "policy": self._policy_trainer,
-        }
```

## nnabla_rl/algorithms/icml2015_trpo.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -19,30 +19,28 @@
 import gym
 import numpy as np
 
 import nnabla as nn
 import nnabla_rl.environment_explorers as EE
 import nnabla_rl.model_trainers as MT
 from nnabla_rl.algorithm import Algorithm, AlgorithmConfig, eval_api
-from nnabla_rl.algorithms.common_utils import _StochasticPolicyActionSelector
-from nnabla_rl.builders import ExplorerBuilder, ModelBuilder
+from nnabla_rl.builders import ModelBuilder
 from nnabla_rl.environment_explorer import EnvironmentExplorer
 from nnabla_rl.environments.environment_info import EnvironmentInfo
 from nnabla_rl.model_trainers.model_trainer import ModelTrainer, TrainingBatch
 from nnabla_rl.models import ICML2015TRPOAtariPolicy, ICML2015TRPOMujocoPolicy, StochasticPolicy
 from nnabla_rl.replay_buffer import ReplayBuffer
 from nnabla_rl.replay_buffers.buffer_iterator import BufferIterator
 from nnabla_rl.utils import context
 from nnabla_rl.utils.data import marshal_experiences
 
 
 @dataclass
 class ICML2015TRPOConfig(AlgorithmConfig):
-    """List of configurations for ICML2015TRPO algorithm.
-
+    '''ICML2015TRPO config
     Args:
         gamma (float): Discount factor of rewards. Defaults to 0.99.
         num_steps_per_iteration (int): Number of steps per each training iteration for collecting on-policy experinces.\
             Increasing this step size is effective to get precise parameters of policy and value function updating, \
             but computational time of each iteration will increase. Defaults to 100000.
         batch_size (int): Trainig batch size of policy. \
             Usually, batch_size is the same as num_steps_per_iteration. Defaults to 100000.
@@ -50,29 +48,30 @@
             As long as gpu memory size is enough, this configuration should not be specified. If not specified,  \
             gpu_batch_size is the same as pi_batch_size. Defaults to None.
         sigma_kl_divergence_constraint (float): Constraint size of kl divergence \
             between previous policy and updated policy. Defaults to 0.01.
         maximum_backtrack_numbers (int): Maximum backtrack numbers of linesearch. Defaults to 10.
         conjugate_gradient_damping (float): Damping size of conjugate gradient method. Defaults to 0.1.
         conjugate_gradient_iterations (int): Number of iterations of conjugate gradient method. Defaults to 20.
-    """
+    '''
     gamma: float = 0.99
     num_steps_per_iteration: int = int(1e5)
     batch_size: int = int(1e5)
     gpu_batch_size: Optional[int] = None
     sigma_kl_divergence_constraint: float = 0.01
     maximum_backtrack_numbers: int = 10
     conjugate_gradient_damping: float = 0.001
     conjugate_gradient_iterations: int = 10
 
     def __post_init__(self):
-        """__post_init__
+        '''__post_init__
 
         Check the values are in valid range.
-        """
+
+        '''
         self._assert_between(self.gamma, 0.0, 1.0, 'gamma')
         self._assert_between(self.batch_size, 0, self.num_steps_per_iteration, 'batch_size')
         self._assert_positive(self.num_steps_per_iteration, 'num_steps_per_iteration')
         self._assert_positive(self.sigma_kl_divergence_constraint, 'sigma_kl_divergence_constraint')
         self._assert_positive(self.maximum_backtrack_numbers, 'maximum_backtrack_numbers')
         self._assert_positive(self.conjugate_gradient_damping, 'conjugate_gradient_damping')
 
@@ -99,91 +98,69 @@
                                        scope_name: str,
                                        env_info: EnvironmentInfo,
                                        algorithm_config: ICML2015TRPOConfig,
                                        **kwargs) -> StochasticPolicy:
         return ICML2015TRPOAtariPolicy(scope_name, env_info.action_dim)
 
 
-class DefaultExplorerBuilder(ExplorerBuilder):
-    def build_explorer(self,  # type: ignore[override]
-                       env_info: EnvironmentInfo,
-                       algorithm_config: ICML2015TRPOConfig,
-                       algorithm: "ICML2015TRPO",
-                       **kwargs) -> EnvironmentExplorer:
-        explorer_config = EE.RawPolicyExplorerConfig(initial_step_num=algorithm.iteration_num,
-                                                     timelimit_as_terminal=False)
-        explorer = EE.RawPolicyExplorer(policy_action_selector=algorithm._exploration_action_selector,
-                                        env_info=env_info,
-                                        config=explorer_config)
-        return explorer
-
-
 class ICML2015TRPO(Algorithm):
-    """Trust Region Policy Optimiation method with Single Path algorithm.
+    '''Trust Region Policy Optimiation method with Single Path algorithm.
 
     This class implements the Trust Region Policy Optimiation (TRPO)
     with Single Path algorithm proposed by J. Schulman, et al. in the paper: "Trust Region Policy Optimization"
     For detail see: https://arxiv.org/abs/1502.05477
 
     Args:
         env_or_env_info\
         (gym.Env or :py:class:`EnvironmentInfo <nnabla_rl.environments.environment_info.EnvironmentInfo>`):
             the environment to train or environment info
         config (:py:class:`ICML2015TRPOConfig <nnabla_rl.algorithms.icml2015_trpo.ICML2015TRPOConfig>`):
             configuration of ICML2015TRPO algorithm
         policy_builder (:py:class:`ModelBuilder[StochasicPolicy] <nnabla_rl.builders.ModelBuilder>`):
             builder of policy models
-        explorer_builder (:py:class:`ExplorerBuilder <nnabla_rl.builders.ExplorerBuilder>`):
-            builder of environment explorer
-    """
+    '''
 
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _config: ICML2015TRPOConfig
     _policy: StochasticPolicy
     _policy_trainer: ModelTrainer
-    _explorer_builder: ExplorerBuilder
     _environment_explorer: EnvironmentExplorer
+    _eval_state_var: nn.Variable
+    _eval_action: nn.Variable
 
     _policy_trainer_state: Dict[str, Any]
 
-    _evaluation_actor: _StochasticPolicyActionSelector
-    _exploration_actor: _StochasticPolicyActionSelector
-
     def __init__(self, env_or_env_info: Union[gym.Env, EnvironmentInfo],
                  config: ICML2015TRPOConfig = ICML2015TRPOConfig(),
-                 policy_builder: ModelBuilder[StochasticPolicy] = DefaultPolicyBuilder(),
-                 explorer_builder: ExplorerBuilder = DefaultExplorerBuilder()):
+                 policy_builder: ModelBuilder[StochasticPolicy] = DefaultPolicyBuilder()):
         super(ICML2015TRPO, self).__init__(env_or_env_info, config=config)
-
-        self._explorer_builder = explorer_builder
-
         with nn.context_scope(context.get_nnabla_context(self._config.gpu_id)):
             self._policy = policy_builder("pi", self._env_info, self._config)
 
-        self._evaluation_actor = _StochasticPolicyActionSelector(
-            self._env_info, self._policy.shallowcopy(), deterministic=False)
-        self._exploration_actor = _StochasticPolicyActionSelector(
-            self._env_info, self._policy.shallowcopy(), deterministic=False)
-
     @eval_api
-    def compute_eval_action(self, state, *, begin_of_episode=False, extra_info={}):
+    def compute_eval_action(self, s):
         with nn.context_scope(context.get_nnabla_context(self._config.gpu_id)):
-            action, _ = self._evaluation_action_selector(state, begin_of_episode=begin_of_episode)
+            action, _ = self._compute_action(s)
             return action
 
     def _before_training_start(self, env_or_buffer):
         # set context globally to ensure that the training runs on configured gpu
         context.set_nnabla_context(self._config.gpu_id)
         self._environment_explorer = self._setup_environment_explorer(env_or_buffer)
         self._policy_trainer = self._setup_policy_training(env_or_buffer)
 
     def _setup_environment_explorer(self, env_or_buffer):
-        return None if self._is_buffer(env_or_buffer) else self._explorer_builder(self._env_info, self._config, self)
+        if self._is_buffer(env_or_buffer):
+            return None
+        explorer_config = EE.RawPolicyExplorerConfig(initial_step_num=self.iteration_num, timelimit_as_terminal=False)
+        explorer = EE.RawPolicyExplorer(
+            policy_action_selector=self._compute_action, env_info=self._env_info, config=explorer_config)
+        return explorer
 
     def _setup_policy_training(self, env_or_buffer):
         policy_trainer_config = MT.policy_trainers.TRPOPolicyTrainerConfig(
             gpu_batch_size=self._config.gpu_batch_size,
             sigma_kl_divergence_constraint=self._config.sigma_kl_divergence_constraint,
             maximum_backtrack_numbers=self._config.maximum_backtrack_numbers,
             conjugate_gradient_damping=self._config.conjugate_gradient_damping,
@@ -259,35 +236,31 @@
         mask = left_justified_gamma_seqs != 0.
 
         gamma_seqs = np.zeros((episode_length, episode_length))
         gamma_seqs[np.triu_indices(episode_length)] = left_justified_gamma_seqs[mask]
 
         return np.sum(reward_sequence*gamma_seqs, axis=1, keepdims=True)
 
-    def _evaluation_action_selector(self, s, *, begin_of_episode=False):
-        return self._evaluation_actor(s, begin_of_episode=begin_of_episode)
-
-    def _exploration_action_selector(self, s, *, begin_of_episode=False):
-        return self._exploration_actor(s, begin_of_episode=begin_of_episode)
+    @eval_api
+    def _compute_action(self, s):
+        # evaluation input/action variables
+        s = np.expand_dims(s, axis=0)
+        if not hasattr(self, '_eval_state_var'):
+            self._eval_state_var = nn.Variable(s.shape)
+            distribution = self._policy.pi(self._eval_state_var)
+            self._eval_action = distribution.sample()
+        self._eval_state_var.d = s
+        self._eval_action.forward()
+        return np.squeeze(self._eval_action.d, axis=0), {}
 
     def _models(self):
         models = {}
         models[self._policy.scope_name] = self._policy
         return models
 
     def _solvers(self):
         return {}
 
-    @classmethod
-    def is_supported_env(cls, env_or_env_info):
-        env_info = EnvironmentInfo.from_env(env_or_env_info) if isinstance(env_or_env_info, gym.Env) \
-            else env_or_env_info
-        return not env_info.is_tuple_action_env()
-
     @property
     def latest_iteration_state(self):
         latest_iteration_state = super(ICML2015TRPO, self).latest_iteration_state
         return latest_iteration_state
-
-    @property
-    def trainers(self):
-        return {"policy": self._policy_trainer}
```

## nnabla_rl/algorithms/icml2018_sac.py

```diff
@@ -1,47 +1,49 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from dataclasses import dataclass
-from typing import Any, Dict, List, Union
+from typing import Any, Dict, List, Union, cast
 
 import gym
+import numpy as np
 
 import nnabla as nn
 import nnabla.solvers as NS
 import nnabla_rl.environment_explorers as EE
 import nnabla_rl.model_trainers as MT
 from nnabla_rl.algorithm import Algorithm, AlgorithmConfig, eval_api
-from nnabla_rl.algorithms.common_utils import _StochasticPolicyActionSelector
-from nnabla_rl.builders import ExplorerBuilder, ModelBuilder, ReplayBufferBuilder, SolverBuilder
+from nnabla_rl.builders import ModelBuilder, ReplayBufferBuilder, SolverBuilder
 from nnabla_rl.environment_explorer import EnvironmentExplorer
 from nnabla_rl.environments.environment_info import EnvironmentInfo
+from nnabla_rl.exceptions import UnsupportedEnvironmentException
 from nnabla_rl.model_trainers.model_trainer import ModelTrainer, TrainingBatch
 from nnabla_rl.models import QFunction, SACPolicy, SACQFunction, SACVFunction, StochasticPolicy, VFunction
 from nnabla_rl.replay_buffer import ReplayBuffer
 from nnabla_rl.utils import context
 from nnabla_rl.utils.data import marshal_experiences
 from nnabla_rl.utils.misc import sync_model
 
 
 @dataclass
 class ICML2018SACConfig(AlgorithmConfig):
-    """ICML2018SACConfig List of configurations for ICML2018SAC algorithm.
+    '''ICML2018SACConfig
+    List of configurations for ICML2018SAC algorithm.
 
     Args:
         gamma (float): discount factor of rewards. Defaults to 0.99.
         learning_rate (float): learning rate which is set to all solvers. \
             You can customize/override the learning rate for each solver by implementing the \
             (:py:class:`SolverBuilder <nnabla_rl.builders.SolverBuilder>`) by yourself. \
             Defaults to 0.0003.
@@ -50,89 +52,40 @@
         environment_steps (int): Number of steps to interact with the environment on each iteration. Defaults to 1.
         gradient_steps (int): Number of parameter updates to perform on each iteration. Defaults to 1.
         reward_scalar (float): Reward scaling factor. Obtained reward will be multiplied by this value. Defaults to 5.0.
         start_timesteps (int): the timestep when training starts.\
             The algorithm will collect experiences from the environment by acting randomly until this timestep.\
             Defaults to 10000.
         replay_buffer_size (int): capacity of the replay buffer. Defaults to 1000000.
-        num_steps (int): number of steps for N-step Q targets. Defaults to 1.
         target_update_interval (float): the interval of target v function parameter's update. Defaults to 1.
-        pi_unroll_steps (int): Number of steps to unroll policy's tranining network.\
-            The network will be unrolled even though the provided model doesn't have RNN layers.\
-            Defaults to 1.
-        pi_burn_in_steps (int): Number of burn-in steps to initiaze policy's recurrent layer states during training.\
-            This flag does not take effect if given model is not an RNN model.\
-            Defaults to 0.
-        pi_reset_rnn_on_terminal (bool): Reset policy's recurrent internal states to zero during training\
-            if episode ends. This flag does not take effect if given model is not an RNN model.\
-            Defaults to False.
-        q_unroll_steps (int): Number of steps to unroll q-function's tranining network.\
-            The network will be unrolled even though the provided model doesn't have RNN layers.\
-            Defaults to 1.
-        q_burn_in_steps (int): Number of burn-in steps to initiaze q-function's recurrent layer states\
-            during training. This flag does not take effect if given model is not an RNN model.\
-            Defaults to 0.
-        q_reset_rnn_on_terminal (bool): Reset q-function's recurrent internal states to zero during training\
-            if episode ends. This flag does not take effect if given model is not an RNN model.\
-            Defaults to False.
-        v_unroll_steps (int): Number of steps to unroll v-function's tranining network.\
-            The network will be unrolled even though the provided model doesn't have RNN layers.\
-            Defaults to 1.
-        v_burn_in_steps (int): Number of burn-in steps to initiaze v-function's recurrent layer states\
-            during training. This flag does not take effect if given model is not an RNN model.\
-            Defaults to 0.
-        v_reset_rnn_on_terminal (bool): Reset v-function's recurrent internal states to zero during training\
-            if episode ends. This flag does not take effect if given model is not an RNN model.\
-            Defaults to False.
-    """
+    '''
 
     gamma: float = 0.99
     learning_rate: float = 3.0*1e-4
     batch_size: int = 256
     tau: float = 0.005
     environment_steps: int = 1
     gradient_steps: int = 1
     reward_scalar: float = 5.0
     start_timesteps: int = 10000
     replay_buffer_size: int = 1000000
     target_update_interval: int = 1
-    num_steps: int = 1
-
-    # rnn model support
-    pi_unroll_steps: int = 1
-    pi_burn_in_steps: int = 0
-    pi_reset_rnn_on_terminal: bool = True
-
-    q_unroll_steps: int = 1
-    q_burn_in_steps: int = 0
-    q_reset_rnn_on_terminal: bool = True
-
-    v_unroll_steps: int = 1
-    v_burn_in_steps: int = 0
-    v_reset_rnn_on_terminal: bool = True
 
     def __post_init__(self):
-        """__post_init__
+        '''__post_init__
 
         Check the values are in valid range.
-        """
+
+        '''
         self._assert_between(self.tau, 0.0, 1.0, 'tau')
         self._assert_between(self.gamma, 0.0, 1.0, 'gamma')
         self._assert_positive(self.gradient_steps, 'gradient_steps')
         self._assert_positive(self.environment_steps, 'environment_steps')
         self._assert_positive(self.start_timesteps, 'start_timesteps')
         self._assert_positive(self.target_update_interval, 'target_update_interval')
-        self._assert_positive(self.num_steps, 'num_steps')
-
-        self._assert_positive(self.pi_unroll_steps, 'pi_unroll_steps')
-        self._assert_positive_or_zero(self.pi_burn_in_steps, 'pi_burn_in_steps')
-        self._assert_positive(self.q_unroll_steps, 'q_unroll_steps')
-        self._assert_positive_or_zero(self.q_burn_in_steps, 'q_burn_in_steps')
-        self._assert_positive(self.v_unroll_steps, 'v_unroll_steps')
-        self._assert_positive_or_zero(self.v_burn_in_steps, 'v_burn_in_steps')
 
 
 class DefaultVFunctionBuilder(ModelBuilder[VFunction]):
     def build_model(self,  # type: ignore[override]
                     scope_name: str,
                     env_info: EnvironmentInfo,
                     algorithm_config: ICML2018SACConfig,
@@ -172,34 +125,16 @@
                             env_info: EnvironmentInfo,
                             algorithm_config: ICML2018SACConfig,
                             **kwargs) -> ReplayBuffer:
         assert isinstance(algorithm_config, ICML2018SACConfig)
         return ReplayBuffer(capacity=algorithm_config.replay_buffer_size)
 
 
-class DefaultExplorerBuilder(ExplorerBuilder):
-    def build_explorer(self,  # type: ignore[override]
-                       env_info: EnvironmentInfo,
-                       algorithm_config: ICML2018SACConfig,
-                       algorithm: "ICML2018SAC",
-                       **kwargs) -> EnvironmentExplorer:
-        explorer_config = EE.RawPolicyExplorerConfig(
-            warmup_random_steps=algorithm_config.start_timesteps,
-            reward_scalar=algorithm_config.reward_scalar,
-            initial_step_num=algorithm.iteration_num,
-            timelimit_as_terminal=False
-        )
-        explorer = EE.RawPolicyExplorer(policy_action_selector=algorithm._exploration_action_selector,
-                                        env_info=env_info,
-                                        config=explorer_config)
-        return explorer
-
-
 class ICML2018SAC(Algorithm):
-    """Soft Actor-Critic (SAC) algorithm.
+    '''Soft Actor-Critic (SAC) algorithm.
 
     This class implements the ICML2018 version of Soft Actor Critic (SAC) algorithm proposed by T. Haarnoja, et al.
     in the paper: "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
     For detail see: https://arxiv.org/abs/1801.01290
 
     This implementation slightly differs from the implementation of Soft Actor-Critic algorithm presented
     also by T. Haarnoja, et al. in the following paper: https://arxiv.org/abs/1812.05905
@@ -222,31 +157,28 @@
             builder of q function solvers
         policy_builder (:py:class:`ModelBuilder[StochasticPolicy] <nnabla_rl.builders.ModelBuilder>`):
             builder of actor models
         policy_solver_builder (:py:class:`SolverBuilder <nnabla_rl.builders.SolverBuilder>`):
             builder of policy solvers
         replay_buffer_builder (:py:class:`ReplayBufferBuilder <nnabla_rl.builders.ReplayBufferBuilder>`):
             builder of replay_buffer
-        explorer_builder (:py:class:`ExplorerBuilder <nnabla_rl.builders.ExplorerBuilder>`):
-            builder of environment explorer
-    """
+    '''
 
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _config: ICML2018SACConfig
     _v: VFunction
     _v_solver: nn.solver.Solver
     _target_v: VFunction
     _q1: QFunction
     _q2: QFunction
     _train_q_functions: List[QFunction]
     _train_q_solvers: Dict[str, nn.solver.Solver]
     _replay_buffer: ReplayBuffer
-    _explorer_builder: ExplorerBuilder
     _environment_explorer: EnvironmentExplorer
     _policy_trainer: ModelTrainer
     _q_function_trainer: ModelTrainer
     _v_function_trainer: ModelTrainer
 
     _eval_state_var: nn.Variable
     _eval_deterministic_action: nn.Variable
@@ -260,24 +192,23 @@
                  config: ICML2018SACConfig = ICML2018SACConfig(),
                  v_function_builder: ModelBuilder[VFunction] = DefaultVFunctionBuilder(),
                  v_solver_builder: SolverBuilder = DefaultSolverBuilder(),
                  q_function_builder: ModelBuilder[QFunction] = DefaultQFunctionBuilder(),
                  q_solver_builder: SolverBuilder = DefaultSolverBuilder(),
                  policy_builder: ModelBuilder[StochasticPolicy] = DefaultPolicyBuilder(),
                  policy_solver_builder: SolverBuilder = DefaultSolverBuilder(),
-                 replay_buffer_builder: ReplayBufferBuilder = DefaultReplayBufferBuilder(),
-                 explorer_builder: ExplorerBuilder = DefaultExplorerBuilder()):
+                 replay_buffer_builder: ReplayBufferBuilder = DefaultReplayBufferBuilder()):
         super(ICML2018SAC, self).__init__(env_or_env_info, config=config)
-
-        self._explorer_builder = explorer_builder
+        if self._env_info.is_discrete_action_env():
+            raise UnsupportedEnvironmentException('{} only supports discrete action environment'.format(self.__name__))
 
         with nn.context_scope(context.get_nnabla_context(self._config.gpu_id)):
             self._v = v_function_builder(scope_name="v", env_info=self._env_info, algorithm_config=self._config)
             self._v_solver = v_solver_builder(env_info=self._env_info, algorithm_config=self._config)
-            self._target_v = self._v.deepcopy('target_' + self._v.scope_name)
+            self._target_v = cast(VFunction, self._v.deepcopy('target_' + self._v.scope_name))
 
             self._q1 = q_function_builder(scope_name="q1", env_info=self._env_info, algorithm_config=self._config)
             self._q2 = q_function_builder(scope_name="q2", env_info=self._env_info, algorithm_config=self._config)
 
             self._train_q_functions = [self._q1, self._q2]
             self._train_q_solvers = {}
             for q in self._train_q_functions:
@@ -285,43 +216,48 @@
                     env_info=self._env_info, algorithm_config=self._config)
 
             self._pi = policy_builder(scope_name="pi", env_info=self._env_info, algorithm_config=self._config)
             self._pi_solver = policy_solver_builder(env_info=self._env_info, algorithm_config=self._config)
 
             self._replay_buffer = replay_buffer_builder(env_info=self._env_info, algorithm_config=self._config)
 
-        self._evaluation_actor = _StochasticPolicyActionSelector(
-            self._env_info, self._pi.shallowcopy(), deterministic=True)
-        self._exploration_actor = _StochasticPolicyActionSelector(
-            self._env_info, self._pi.shallowcopy(), deterministic=False)
-
     @eval_api
-    def compute_eval_action(self, state, *, begin_of_episode=False, extra_info={}):
+    def compute_eval_action(self, state):
         with nn.context_scope(context.get_nnabla_context(self._config.gpu_id)):
-            action, _ = self._evaluation_action_selector(state, begin_of_episode=begin_of_episode)
+            action, _ = self._compute_greedy_action(state, deterministic=True)
             return action
 
     def _before_training_start(self, env_or_buffer):
         # set context globally to ensure that the training runs on configured gpu
         context.set_nnabla_context(self._config.gpu_id)
         self._environment_explorer = self._setup_environment_explorer(env_or_buffer)
         self._policy_trainer = self._setup_policy_training(env_or_buffer)
-        self._q_function_trainer = self._setup_q_function_training(env_or_buffer)
-        self._v_function_trainer = self._setup_v_function_training(env_or_buffer)
+        self._q_function_trainer = self._setup_q_function_training(
+            env_or_buffer)
+        self._v_function_trainer = self._setup_v_function_training(
+            env_or_buffer)
 
     def _setup_environment_explorer(self, env_or_buffer):
-        return None if self._is_buffer(env_or_buffer) else self._explorer_builder(self._env_info, self._config, self)
+        if self._is_buffer(env_or_buffer):
+            return None
+
+        explorer_config = EE.RawPolicyExplorerConfig(
+            warmup_random_steps=self._config.start_timesteps,
+            reward_scalar=self._config.reward_scalar,
+            initial_step_num=self.iteration_num,
+            timelimit_as_terminal=False
+        )
+        explorer = EE.RawPolicyExplorer(policy_action_selector=self._compute_greedy_action,
+                                        env_info=self._env_info,
+                                        config=explorer_config)
+        return explorer
 
     def _setup_policy_training(self, env_or_buffer):
         # NOTE: Fix temperature to 1.0. Because This version of SAC adjusts it by scaling the reward
-        policy_trainer_config = MT.policy_trainers.SoftPolicyTrainerConfig(
-            fixed_temperature=True,
-            unroll_steps=self._config.pi_unroll_steps,
-            burn_in_steps=self._config.pi_burn_in_steps,
-            reset_on_terminal=self._config.pi_reset_rnn_on_terminal)
+        policy_trainer_config = MT.policy_trainers.SoftPolicyTrainerConfig(fixed_temperature=True)
         temperature = MT.policy_trainers.soft_policy_trainer.AdjustableTemperature(
             scope_name='temperature',
             initial_value=1.0)
         policy_trainer = MT.policy_trainers.SoftPolicyTrainer(
             models=self._pi,
             solvers={self._pi.scope_name: self._pi_solver},
             q_functions=self._train_q_functions,
@@ -330,35 +266,29 @@
             env_info=self._env_info,
             config=policy_trainer_config)
         return policy_trainer
 
     def _setup_q_function_training(self, env_or_buffer):
         q_function_trainer_param = MT.q_value_trainers.VTargetedQTrainerConfig(
             reduction_method='mean',
-            q_loss_scalar=0.5,
-            num_steps=self._config.num_steps,
-            unroll_steps=self._config.q_unroll_steps,
-            burn_in_steps=self._config.q_burn_in_steps,
-            reset_on_terminal=self._config.q_reset_rnn_on_terminal)
+            q_loss_scalar=0.5)
         q_function_trainer = MT.q_value_trainers.VTargetedQTrainer(
             train_functions=self._train_q_functions,
             solvers=self._train_q_solvers,
             target_functions=self._target_v,
             env_info=self._env_info,
             config=q_function_trainer_param)
         return q_function_trainer
 
     def _setup_v_function_training(self, env_or_buffer):
-        v_function_trainer_config = MT.v_value_trainers.SoftVTrainerConfig(
+        v_function_trainer_config = MT.v_value.SoftVTrainerConfig(
             reduction_method='mean',
-            v_loss_scalar=0.5,
-            unroll_steps=self._config.v_unroll_steps,
-            burn_in_steps=self._config.v_burn_in_steps,
-            reset_on_terminal=self._config.v_reset_rnn_on_terminal)
-        v_function_trainer = MT.v_value_trainers.SoftVTrainer(
+            v_loss_scalar=0.5
+        )
+        v_function_trainer = MT.v_value.SoftVTrainer(
             train_functions=self._v,
             solvers={self._v.scope_name: self._v_solver},
             target_functions=self._train_q_functions,  # Set training q as target
             target_policy=self._pi,
             env_info=self._env_info,
             config=v_function_trainer_config)
         sync_model(self._v, self._target_v, 1.0)
@@ -379,88 +309,68 @@
         self._replay_buffer.append_all(experiences)
 
     def _run_gradient_step(self, replay_buffer):
         if self._config.start_timesteps < self.iteration_num:
             self._sac_training(replay_buffer)
 
     def _sac_training(self, replay_buffer):
-        pi_steps = self._config.pi_burn_in_steps + self._config.pi_unroll_steps
-        q_steps = self._config.num_steps + self._config.q_burn_in_steps + self._config.q_unroll_steps - 1
-        v_steps = self._config.v_burn_in_steps + self._config.v_unroll_steps
-        num_steps = max(pi_steps, max(q_steps, v_steps))
-        experiences_tuple, info = replay_buffer.sample(self._config.batch_size, num_steps=num_steps)
-        if num_steps == 1:
-            experiences_tuple = (experiences_tuple, )
-        assert len(experiences_tuple) == num_steps
-
-        batch = None
-        for experiences in reversed(experiences_tuple):
-            (s, a, r, non_terminal, s_next, rnn_states_dict, *_) = marshal_experiences(experiences)
-            rnn_states = rnn_states_dict['rnn_states'] if 'rnn_states' in rnn_states_dict else {}
-            batch = TrainingBatch(batch_size=self._config.batch_size,
-                                  s_current=s,
-                                  a_current=a,
-                                  gamma=self._config.gamma,
-                                  reward=r,
-                                  non_terminal=non_terminal,
-                                  s_next=s_next,
-                                  weight=info['weights'],
-                                  next_step_batch=batch,
-                                  rnn_states=rnn_states)
+        experiences, info = replay_buffer.sample(self._config.batch_size)
+        (s, a, r, non_terminal, s_next, *_) = marshal_experiences(experiences)
+        batch = TrainingBatch(batch_size=self._config.batch_size,
+                              s_current=s,
+                              a_current=a,
+                              gamma=self._config.gamma,
+                              reward=r,
+                              non_terminal=non_terminal,
+                              s_next=s_next,
+                              weight=info['weights'])
 
         # Train in the order of v -> q -> policy
         self._v_function_trainer_state = self._v_function_trainer.train(batch)
         self._q_function_trainer_state = self._q_function_trainer.train(batch)
         if self.iteration_num % self._config.target_update_interval == 0:
             sync_model(self._v, self._target_v, tau=self._config.tau)
         self._policy_trainer_state = self._policy_trainer.train(batch)
 
-        td_errors = self._q_function_trainer_state['td_errors']
+        td_errors = np.abs(self._q_function_trainer_state['td_errors'])
         replay_buffer.update_priorities(td_errors)
 
-    def _evaluation_action_selector(self, s, *, begin_of_episode=False):
-        return self._evaluation_actor(s, begin_of_episode=begin_of_episode)
-
-    def _exploration_action_selector(self, s, *, begin_of_episode=False):
-        return self._exploration_actor(s, begin_of_episode=begin_of_episode)
+    @eval_api
+    def _compute_greedy_action(self, s, deterministic=False):
+        # evaluation input/action variables
+        s = np.expand_dims(s, axis=0)
+        if not hasattr(self, '_eval_state_var'):
+            self._eval_state_var = nn.Variable(s.shape)
+            distribution = self._pi.pi(self._eval_state_var)
+            self._eval_deterministic_action = distribution.choose_probable()
+            self._eval_probabilistic_action = distribution.sample()
+        self._eval_state_var.d = s
+        if deterministic:
+            self._eval_deterministic_action.forward()
+            return np.squeeze(self._eval_deterministic_action.d, axis=0), {}
+        else:
+            self._eval_probabilistic_action.forward()
+            return np.squeeze(self._eval_probabilistic_action.d, axis=0), {}
 
     def _models(self):
         models = [self._v, self._target_v, self._q1, self._q2, self._pi]
         return {model.scope_name: model for model in models}
 
     def _solvers(self):
         solvers = {}
         solvers.update(self._train_q_solvers)
         solvers[self._v.scope_name] = self._v_solver
         solvers[self._pi.scope_name] = self._pi_solver
         return solvers
 
-    @classmethod
-    def is_rnn_supported(cls):
-        return True
-
-    @classmethod
-    def is_supported_env(cls, env_or_env_info):
-        env_info = EnvironmentInfo.from_env(env_or_env_info) if isinstance(env_or_env_info, gym.Env) \
-            else env_or_env_info
-        return not env_info.is_discrete_action_env() and not env_info.is_tuple_action_env()
-
     @property
     def latest_iteration_state(self):
         latest_iteration_state = super(ICML2018SAC, self).latest_iteration_state
         if hasattr(self, '_policy_trainer_state'):
-            latest_iteration_state['scalar'].update({'pi_loss': float(self._policy_trainer_state['pi_loss'])})
+            latest_iteration_state['scalar'].update({'pi_loss': self._policy_trainer_state['pi_loss']})
         if hasattr(self, '_v_function_trainer_state'):
-            latest_iteration_state['scalar'].update({'v_loss': float(self._v_function_trainer_state['v_loss'])})
+            latest_iteration_state['scalar'].update({'v_loss': self._v_function_trainer_state['v_loss']})
         if hasattr(self, '_q_function_trainer_state'):
-            latest_iteration_state['scalar'].update({'q_loss': float(self._q_function_trainer_state['q_loss'])})
+            latest_iteration_state['scalar'].update({'q_loss': self._q_function_trainer_state['q_loss']})
             latest_iteration_state['histogram'].update(
                 {'td_errors': self._q_function_trainer_state['td_errors'].flatten()})
         return latest_iteration_state
-
-    @property
-    def trainers(self):
-        return {
-            "v_function": self._v_function_trainer,
-            "q_function": self._q_function_trainer,
-            "policy": self._policy_trainer,
-        }
```

## nnabla_rl/algorithms/iqn.py

```diff
@@ -1,58 +1,58 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from dataclasses import dataclass
-from typing import Any, Dict, Union
+from typing import Any, Dict, Union, cast
 
 import gym
 import numpy as np
 
 import nnabla as nn
 import nnabla.solvers as NS
 import nnabla_rl.environment_explorers as EE
 import nnabla_rl.model_trainers as MT
 from nnabla_rl.algorithm import Algorithm, AlgorithmConfig, eval_api
-from nnabla_rl.algorithms.common_utils import _GreedyActionSelector
-from nnabla_rl.builders import ExplorerBuilder, ModelBuilder, ReplayBufferBuilder, SolverBuilder
+from nnabla_rl.builders import ModelBuilder, ReplayBufferBuilder, SolverBuilder
 from nnabla_rl.environment_explorer import EnvironmentExplorer
 from nnabla_rl.environment_explorers.epsilon_greedy_explorer import epsilon_greedy_action_selection
 from nnabla_rl.environments.environment_info import EnvironmentInfo
+from nnabla_rl.exceptions import UnsupportedEnvironmentException
 from nnabla_rl.model_trainers.model_trainer import ModelTrainer, TrainingBatch
 from nnabla_rl.models import IQNQuantileFunction, StateActionQuantileFunction
 from nnabla_rl.replay_buffer import ReplayBuffer
 from nnabla_rl.utils import context
 from nnabla_rl.utils.data import marshal_experiences
 from nnabla_rl.utils.misc import sync_model
 
 
 @dataclass
 class IQNConfig(AlgorithmConfig):
-    """List of configurations for IQN algorithm.
+    '''
+    List of configurations for IQN algorithm
 
     Args:
         gamma (float): discount factor of rewards. Defaults to 0.99.
         learning_rate (float): learning rate which is set to all solvers. \
             You can customize/override the learning rate for each solver by implementing the \
             (:py:class:`SolverBuilder <nnabla_rl.builders.SolverBuilder>`) by yourself. \
             Defaults to 0.00005.
-        batch_size (int): training batch size. Defaults to 32.
-        num_steps (int): number of steps for N-step Q targets. Defaults to 1.
+        batch_size (int): training atch size. Defaults to 32.
         start_timesteps (int): the timestep when training starts.\
             The algorithm will collect experiences from the environment by acting randomly until this timestep.
             Defaults to 50000.
         replay_buffer_size (int): the capacity of replay buffer. Defaults to 1000000.
         learner_update_frequency (int): the interval of learner update. Defaults to 4.
         target_update_frequency (int): the interval of target q-function update. Defaults to 10000.
         max_explore_steps (int): the number of steps decaying the epsilon value.\
@@ -64,89 +64,71 @@
         final_epsilon (float): the last epsilon value for -greedy explorer. Defaults to 0.01.
         test_epsilon (float): the epsilon value on testing. Defaults to 0.001.
         N (int): Number of samples to compute the current state's quantile values. Defaults to 64.
         N_prime (int): Number of samples to compute the target state's quantile values. Defaults to 64.
         K (int): Number of samples to compute greedy next action. Defaults to 32.
         kappa (float): threshold value of quantile huber loss. Defaults to 1.0.
         embedding_dim (int): dimension of embedding for the sample point. Defaults to 64.
-        unroll_steps (int): Number of steps to unroll tranining network.
-            The network will be unrolled even though the provided model doesn't have RNN layers.
-            Defaults to 1.
-        burn_in_steps (int): Number of burn-in steps to initiaze recurrent layer states during training.
-            This flag does not take effect if given model is not an RNN model.
-            Defaults to 0.
-        reset_rnn_on_terminal (bool): Reset recurrent internal states to zero during training if episode ends.
-            This flag does not take effect if given model is not an RNN model.
-            Defaults to True.
-    """
+    '''
     gamma: float = 0.99
     learning_rate: float = 0.00005
     batch_size: int = 32
-    num_steps: int = 1
     start_timesteps: int = 50000
     replay_buffer_size: int = 1000000
     learner_update_frequency: int = 4
     target_update_frequency: int = 10000
     max_explore_steps: int = 1000000
     initial_epsilon: float = 1.0
     final_epsilon: float = 0.01
     test_epsilon: float = 0.001
     N: int = 64
     N_prime: int = 64
     K: int = 32
     kappa: float = 1.0
     embedding_dim: int = 64
 
-    # rnn model support
-    unroll_steps: int = 1
-    burn_in_steps: int = 0
-    reset_rnn_on_terminal: bool = True
-
     def __post_init__(self):
-        """__post_init__
+        '''__post_init__
 
         Check that set values are in valid range.
-        """
+
+        '''
         self._assert_between(self.gamma, 0.0, 1.0, 'gamma')
         self._assert_positive(self.batch_size, 'batch_size')
         self._assert_positive(self.replay_buffer_size, 'replay_buffer_size')
-        self._assert_positive(self.num_steps, 'num_steps')
         self._assert_positive(self.learner_update_frequency, 'learner_update_frequency')
         self._assert_positive(self.target_update_frequency, 'target_update_frequency')
         self._assert_positive(self.max_explore_steps, 'max_explore_steps')
         self._assert_positive(self.learning_rate, 'learning_rate')
         self._assert_positive(self.initial_epsilon, 'initial_epsilon')
         self._assert_positive(self.final_epsilon, 'final_epsilon')
         self._assert_positive(self.test_epsilon, 'test_epsilon')
         self._assert_positive(self.N, 'N')
         self._assert_positive(self.N_prime, 'N_prime')
         self._assert_positive(self.K, 'K')
         self._assert_positive(self.kappa, 'kappa')
         self._assert_positive(self.embedding_dim, 'embedding_dim')
-        self._assert_positive(self.unroll_steps, 'unroll_steps')
-        self._assert_positive_or_zero(self.burn_in_steps, 'burn_in_steps')
 
 
 def risk_neutral_measure(tau):
     return tau
 
 
 class DefaultQuantileFunctionBuilder(ModelBuilder[StateActionQuantileFunction]):
     def build_model(self,  # type: ignore[override]
                     scope_name: str,
                     env_info: EnvironmentInfo,
                     algorithm_config: IQNConfig,
                     **kwargs) -> StateActionQuantileFunction:
         assert isinstance(algorithm_config, IQNConfig)
-        risk_measure_function = kwargs['risk_measure_function']
         return IQNQuantileFunction(scope_name,
                                    env_info.action_dim,
                                    algorithm_config.embedding_dim,
                                    K=algorithm_config.K,
-                                   risk_measure_function=risk_measure_function)
+                                   risk_measure_function=risk_neutral_measure)
 
 
 class DefaultSolverBuilder(SolverBuilder):
     def build_solver(self,  # type: ignore[override]
                      env_info: EnvironmentInfo,
                      algorithm_config: IQNConfig,
                      **kwargs) -> nn.solver.Solver:
@@ -159,37 +141,16 @@
                             env_info: EnvironmentInfo,
                             algorithm_config: IQNConfig,
                             **kwargs) -> ReplayBuffer:
         assert isinstance(algorithm_config, IQNConfig)
         return ReplayBuffer(capacity=algorithm_config.replay_buffer_size)
 
 
-class DefaultExplorerBuilder(ExplorerBuilder):
-    def build_explorer(self,  # type: ignore[override]
-                       env_info: EnvironmentInfo,
-                       algorithm_config: IQNConfig,
-                       algorithm: "IQN",
-                       **kwargs) -> EnvironmentExplorer:
-        explorer_config = EE.LinearDecayEpsilonGreedyExplorerConfig(
-            warmup_random_steps=algorithm_config.start_timesteps,
-            initial_step_num=algorithm.iteration_num,
-            initial_epsilon=algorithm_config.initial_epsilon,
-            final_epsilon=algorithm_config.final_epsilon,
-            max_explore_steps=algorithm_config.max_explore_steps
-        )
-        explorer = EE.LinearDecayEpsilonGreedyExplorer(
-            greedy_action_selector=algorithm._exploration_action_selector,
-            random_action_selector=algorithm._random_action_selector,
-            env_info=env_info,
-            config=explorer_config)
-        return explorer
-
-
 class IQN(Algorithm):
-    """Implicit Quantile Network algorithm.
+    '''Implicit Quantile Network algorithm.
 
     This class implements the Implicit Quantile Network (IQN) algorithm
     proposed by W. Dabney, et al. in the paper: "Implicit Quantile Networks for Distributional Reinforcement Learning"
     For details see: https://arxiv.org/pdf/1806.06923.pdf
 
     Args:
         env_or_env_info\
@@ -198,90 +159,91 @@
         config (:py:class:`IQNConfig <nnabla_rl.algorithms.iqn.IQNConfig>`): configuration of IQN algorithm
         quantile_function_builder (:py:class:`ModelBuilder[StateActionQuantileFunction] \
             <nnabla_rl.builders.ModelBuilder>`): buider of state-action quantile function models
         quantile_solver_builder (:py:class:`SolverBuilder <nnabla_rl.builders.SolverBuilder>`):
             builder for state action quantile function solvers
         replay_buffer_builder (:py:class:`ReplayBufferBuilder <nnabla_rl.builders.ReplayBufferBuilder>`):
             builder of replay_buffer
-        explorer_builder (:py:class:`ExplorerBuilder <nnabla_rl.builders.ExplorerBuilder>`):
-            builder of environment explorer
-    """
+    '''
 
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _config: IQNConfig
     _quantile_function: StateActionQuantileFunction
     _target_quantile_function: StateActionQuantileFunction
     _quantile_function_solver: nn.solver.Solver
     _replay_buffer: ReplayBuffer
 
-    _explorer_builder: ExplorerBuilder
     _environment_explorer: EnvironmentExplorer
     _quantile_function_trainer: ModelTrainer
 
+    _eval_state_var: nn.Variable
+    _a_greedy: nn.Variable
+
     _quantile_function_trainer_state: Dict[str, Any]
 
     def __init__(self, env_or_env_info: Union[gym.Env, EnvironmentInfo],
                  config: IQNConfig = IQNConfig(),
                  quantile_function_builder: ModelBuilder[StateActionQuantileFunction]
                  = DefaultQuantileFunctionBuilder(),
                  quantile_solver_builder: SolverBuilder = DefaultSolverBuilder(),
-                 replay_buffer_builder: ReplayBufferBuilder = DefaultReplayBufferBuilder(),
-                 explorer_builder: ExplorerBuilder = DefaultExplorerBuilder(),
-                 risk_measure_function=risk_neutral_measure):
+                 replay_buffer_builder: ReplayBufferBuilder = DefaultReplayBufferBuilder()):
         super(IQN, self).__init__(env_or_env_info, config=config)
 
-        self._explorer_builder = explorer_builder
+        if not self._env_info.is_discrete_action_env():
+            raise UnsupportedEnvironmentException('{} only supports discrete action environment'.format(self.__name__))
 
         with nn.context_scope(context.get_nnabla_context(self._config.gpu_id)):
-            kwargs = {}
-            kwargs['risk_measure_function'] = risk_measure_function
-            self._quantile_function = quantile_function_builder(
-                'quantile_function', self._env_info, self._config, **kwargs)
-            self._target_quantile_function = self._quantile_function.deepcopy('target_quantile_function')
+            self._quantile_function = quantile_function_builder('quantile_function', self._env_info, self._config)
+            self._target_quantile_function = cast(StateActionQuantileFunction,
+                                                  self._quantile_function.deepcopy('target_quantile_function'))
 
             self._quantile_function_solver = quantile_solver_builder(self._env_info, self._config)
 
             self._replay_buffer = replay_buffer_builder(self._env_info, self._config)
 
-        self._evaluation_actor = _GreedyActionSelector(
-            self._env_info, self._quantile_function.shallowcopy().as_q_function())
-        self._exploration_actor = _GreedyActionSelector(
-            self._env_info, self._quantile_function.shallowcopy().as_q_function())
-
     @eval_api
-    def compute_eval_action(self, state, *, begin_of_episode=False, extra_info={}):
+    def compute_eval_action(self, state):
         with nn.context_scope(context.get_nnabla_context(self._config.gpu_id)):
             (action, _), _ = epsilon_greedy_action_selection(state,
-                                                             self._evaluation_action_selector,
+                                                             self._greedy_action_selector,
                                                              self._random_action_selector,
-                                                             epsilon=self._config.test_epsilon,
-                                                             begin_of_episode=begin_of_episode)
+                                                             epsilon=self._config.test_epsilon)
             return action
 
     def _before_training_start(self, env_or_buffer):
         # set context globally to ensure that the training runs on configured gpu
         context.set_nnabla_context(self._config.gpu_id)
         self._environment_explorer = self._setup_environment_explorer(env_or_buffer)
         self._quantile_function_trainer = self._setup_quantile_function_training(env_or_buffer)
 
     def _setup_environment_explorer(self, env_or_buffer):
-        return None if self._is_buffer(env_or_buffer) else self._explorer_builder(self._env_info, self._config, self)
+        if self._is_buffer(env_or_buffer):
+            return None
+        explorer_config = EE.LinearDecayEpsilonGreedyExplorerConfig(
+            warmup_random_steps=self._config.start_timesteps,
+            initial_step_num=self.iteration_num,
+            initial_epsilon=self._config.initial_epsilon,
+            final_epsilon=self._config.final_epsilon,
+            max_explore_steps=self._config.max_explore_steps
+        )
+        explorer = EE.LinearDecayEpsilonGreedyExplorer(
+            greedy_action_selector=self._greedy_action_selector,
+            random_action_selector=self._random_action_selector,
+            env_info=self._env_info,
+            config=explorer_config)
+        return explorer
 
     def _setup_quantile_function_training(self, env_or_buffer):
         trainer_config = MT.q_value_trainers.IQNQTrainerConfig(
-            num_steps=self._config.num_steps,
             N=self._config.N,
             N_prime=self._config.N_prime,
             K=self._config.K,
-            kappa=self._config.kappa,
-            unroll_steps=self._config.unroll_steps,
-            burn_in_steps=self._config.burn_in_steps,
-            reset_on_terminal=self._config.reset_rnn_on_terminal)
+            kappa=self._config.kappa)
 
         quantile_function_trainer = MT.q_value_trainers.IQNQTrainer(
             train_functions=self._quantile_function,
             solvers={self._quantile_function.scope_name: self._quantile_function_solver},
             target_function=self._target_quantile_function,
             env_info=self._env_info,
             config=trainer_config)
@@ -299,72 +261,53 @@
             if self.iteration_num % self._config.learner_update_frequency == 0:
                 self._iqn_training(self._replay_buffer)
 
     def _run_offline_training_iteration(self, buffer):
         self._iqn_training(buffer)
 
     def _iqn_training(self, replay_buffer):
-        num_steps = self._config.num_steps + self._config.burn_in_steps + self._config.unroll_steps - 1
-        experiences_tuple, info = replay_buffer.sample(self._config.batch_size, num_steps=num_steps)
-        if num_steps == 1:
-            experiences_tuple = (experiences_tuple, )
-        assert len(experiences_tuple) == num_steps
-
-        batch = None
-        for experiences in reversed(experiences_tuple):
-            (s, a, r, non_terminal, s_next, rnn_states_dict, *_) = marshal_experiences(experiences)
-            rnn_states = rnn_states_dict['rnn_states'] if 'rnn_states' in rnn_states_dict else {}
-            batch = TrainingBatch(batch_size=self._config.batch_size,
-                                  s_current=s,
-                                  a_current=a,
-                                  gamma=self._config.gamma,
-                                  reward=r,
-                                  non_terminal=non_terminal,
-                                  s_next=s_next,
-                                  weight=info['weights'],
-                                  next_step_batch=batch,
-                                  rnn_states=rnn_states)
+        experiences, info = replay_buffer.sample(self._config.batch_size)
+        (s, a, r, non_terminal, s_next, *_) = marshal_experiences(experiences)
+        batch = TrainingBatch(batch_size=self._config.batch_size,
+                              s_current=s,
+                              a_current=a,
+                              gamma=self._config.gamma,
+                              reward=r,
+                              non_terminal=non_terminal,
+                              s_next=s_next,
+                              weight=info['weights'])
 
         self._quantile_function_trainer_state = self._quantile_function_trainer.train(batch)
         if self.iteration_num % self._config.target_update_frequency:
             sync_model(self._quantile_function, self._target_quantile_function)
 
-    def _evaluation_action_selector(self, s, *, begin_of_episode=False):
-        return self._evaluation_actor(s, begin_of_episode=begin_of_episode)
-
-    def _exploration_action_selector(self, s, *, begin_of_episode=False):
-        return self._exploration_actor(s, begin_of_episode=begin_of_episode)
+    @eval_api
+    def _greedy_action_selector(self, s):
+        s = np.expand_dims(s, axis=0)
+        if not hasattr(self, '_eval_state_var'):
+            self._eval_state_var = nn.Variable(s.shape)
+            q_function = self._quantile_function.as_q_function()
+            self._a_greedy = q_function.argmax_q(self._eval_state_var)
+        self._eval_state_var.d = s
+        self._a_greedy.forward()
+        return np.squeeze(self._a_greedy.d, axis=0), {}
 
-    def _random_action_selector(self, s, *, begin_of_episode=False):
+    def _random_action_selector(self, s):
         action = self._env_info.action_space.sample()
         return np.asarray(action).reshape((1, )), {}
 
     def _models(self):
         models = {}
         models[self._quantile_function.scope_name] = self._quantile_function
         return models
 
     def _solvers(self):
         solvers = {}
         solvers[self._quantile_function.scope_name] = self._quantile_function_solver
         return solvers
 
-    @classmethod
-    def is_supported_env(cls, env_or_env_info):
-        env_info = EnvironmentInfo.from_env(env_or_env_info) if isinstance(env_or_env_info, gym.Env) \
-            else env_or_env_info
-        return not env_info.is_continuous_action_env() and not env_info.is_tuple_action_env()
-
-    @classmethod
-    def is_rnn_supported(self):
-        return True
-
     @property
     def latest_iteration_state(self):
         latest_iteration_state = super(IQN, self).latest_iteration_state
         if hasattr(self, '_quantile_function_trainer_state'):
-            latest_iteration_state['scalar'].update({'q_loss': float(self._quantile_function_trainer_state['q_loss'])})
+            latest_iteration_state['scalar'].update({'q_loss': self._quantile_function_trainer_state['q_loss']})
         return latest_iteration_state
-
-    @property
-    def trainers(self):
-        return {"q_function": self._quantile_function_trainer}
```

## nnabla_rl/algorithms/munchausen_dqn.py

```diff
@@ -1,84 +1,146 @@
 # Copyright 2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from dataclasses import dataclass
-from typing import Union
+from typing import Any, Dict, Union, cast
 
 import gym
+import numpy as np
 
 import nnabla as nn
 import nnabla.solvers as NS
+import nnabla_rl.environment_explorers as EE
 import nnabla_rl.model_trainers as MT
-from nnabla_rl.algorithms.dqn import (DQN, DefaultExplorerBuilder, DefaultQFunctionBuilder, DefaultReplayBufferBuilder,
-                                      DQNConfig)
-from nnabla_rl.builders import ExplorerBuilder, ModelBuilder, ReplayBufferBuilder, SolverBuilder
+from nnabla_rl.algorithm import Algorithm, AlgorithmConfig, eval_api
+from nnabla_rl.builders import ModelBuilder, ReplayBufferBuilder, SolverBuilder
+from nnabla_rl.environment_explorer import EnvironmentExplorer
+from nnabla_rl.environment_explorers.epsilon_greedy_explorer import epsilon_greedy_action_selection
 from nnabla_rl.environments.environment_info import EnvironmentInfo
-from nnabla_rl.models import QFunction
+from nnabla_rl.exceptions import UnsupportedEnvironmentException
+from nnabla_rl.model_trainers.model_trainer import ModelTrainer, TrainingBatch
+from nnabla_rl.models import DQNQFunction, QFunction
+from nnabla_rl.replay_buffer import ReplayBuffer
+from nnabla_rl.utils import context
+from nnabla_rl.utils.data import marshal_experiences
 from nnabla_rl.utils.misc import sync_model
 
 
 @dataclass
-class MunchausenDQNConfig(DQNConfig):
-    """List of configurations for Munchausen DQN algorithm.
+class MunchausenDQNConfig(AlgorithmConfig):
+    """
+    List of configurations for Munchausen DQN algorithm
 
     Args:
+        gamma (float): discount factor of rewards. Defaults to 0.99.
         learning_rate (float): learning rate which is set to all solvers. \
             You can customize/override the learning rate for each solver by implementing the \
             (:py:class:`SolverBuilder <nnabla_rl.builders.SolverBuilder>`) by yourself. \
             Defaults to 0.00005.
+        batch_size (int): training atch size. Defaults to 32.
+        learner_update_frequency (int): the interval of learner update. Defaults to 4
+        target_update_frequency (int): the interval of target q-function update. Defaults to 10000.
+        start_timesteps (int): the timestep when training starts.\
+            The algorithm will collect experiences from the environment by acting randomly until this timestep.
+            Defaults to 50000.
+        replay_buffer_size (int): the capacity of replay buffer. Defaults to 1000000.
+        max_explore_steps (int): the number of steps decaying the epsilon value.\
+            The epsilon will be decayed linearly \
+            :math:`\\epsilon=\\epsilon_{init} - step\\times\\frac{\\epsilon_{init} - \
+            \\epsilon_{final}}{max\\_explore\\_steps}`.\
+            Defaults to 1000000.
+        initial_epsilon (float): the initial epsilon value for -greedy explorer. Defaults to 1.0.
         final_epsilon (float): the last epsilon value for -greedy explorer. Defaults to 0.01.
         test_epsilon (float): the epsilon value on testing. Defaults to 0.001.
         entropy_temperature (float): temperature parameter of softmax policy distribution. Defaults to 0.03.
         munchausen_scaling_term (float): scalar of scaled log policy. Defaults to 0.9.
         clipping_value (float): Lower value of the logarithm of policy distribution. Defaults to -1.
     """
 
-    # Parameters overridden from DQN
+    gamma: float = 0.99
     learning_rate: float = 0.00005
+    batch_size: int = 32
+    # network update
+    learner_update_frequency: float = 4
+    target_update_frequency: float = 10000
+    # buffers
+    start_timesteps: int = 50000
+    replay_buffer_size: int = 1000000
+    # explore
+    max_explore_steps: int = 1000000
+    initial_epsilon: float = 1.0
     final_epsilon: float = 0.01
     test_epsilon: float = 0.001
     # munchausen dqn training parameters
     entropy_temperature: float = 0.03
     munchausen_scaling_term: float = 0.9
     clipping_value: float = -1
 
     def __post_init__(self):
-        """__post_init__
+        '''__post_init__
 
         Check set values are in valid range.
-        """
-        super().__post_init__()
+
+        '''
+        self._assert_between(self.gamma, 0.0, 1.0, 'gamma')
+        self._assert_positive(self.batch_size, 'batch_size')
+        self._assert_positive(self.learning_rate, 'learning_rate')
+        self._assert_positive(self.learner_update_frequency, 'learner_update_frequency')
+        self._assert_positive(self.target_update_frequency, 'target_update_frequency')
+        self._assert_positive(self.start_timesteps, 'start_timesteps')
+        self._assert_smaller_than(self.start_timesteps, self.replay_buffer_size, 'start_timesteps')
+        self._assert_positive(self.replay_buffer_size, 'replay_buffer_size')
+        self._assert_between(self.initial_epsilon, 0.0, 1.0, 'initial_epsilon')
+        self._assert_between(self.final_epsilon, 0.0, 1.0, 'final_epsilon')
+        self._assert_between(self.test_epsilon, 0.0, 1.0, 'test_epsilon')
         self._assert_positive(self.max_explore_steps, 'max_explore_steps')
         self._assert_negative(self.clipping_value, 'clipping_value')
 
 
+class DefaultQFunctionBuilder(ModelBuilder[QFunction]):
+    def build_model(self,  # type: ignore[override]
+                    scope_name: str,
+                    env_info: EnvironmentInfo,
+                    algorithm_config: MunchausenDQNConfig,
+                    **kwargs) -> QFunction:
+        return DQNQFunction(scope_name, env_info.action_dim)
+
+
 class DefaultQSolverBuilder(SolverBuilder):
     def build_solver(self,  # type: ignore[override]
                      env_info: EnvironmentInfo,
                      algorithm_config: MunchausenDQNConfig,
                      **kwargs) -> nn.solvers.Solver:
         assert isinstance(algorithm_config, MunchausenDQNConfig)
         return NS.Adam(algorithm_config.learning_rate, eps=1e-2 / algorithm_config.batch_size)
 
 
-class MunchausenDQN(DQN):
-    """Munchausen-DQN algorithm.
+class DefaultReplayBufferBuilder(ReplayBufferBuilder):
+    def build_replay_buffer(self,  # type: ignore[override]
+                            env_info: EnvironmentInfo,
+                            algorithm_config: MunchausenDQNConfig,
+                            **kwargs) -> ReplayBuffer:
+        assert isinstance(algorithm_config, MunchausenDQNConfig)
+        return ReplayBuffer(capacity=algorithm_config.replay_buffer_size)
+
+
+class MunchausenDQN(Algorithm):
+    '''Munchausen-DQN algorithm.
 
     This class implements the Munchausen-DQN (Munchausen Deep Q Network) algorithm
     proposed by N. Vieillard, et al. in the paper: "Munchausen Reinforcement Learning"
     For details see: https://proceedings.neurips.cc/paper/2020/file/2c6a0bae0f071cbbf0bb3d5b11d90a82-Paper.pdf
 
     Args:
         env_or_env_info\
@@ -88,51 +150,155 @@
             configuration of MunchausenDQN algorithm
         q_func_builder (:py:class:`ModelBuilder[QFunction] <nnabla_rl.builders.ModelBuilder>`):
             builder of q-function models
         q_solver_builder (:py:class:`SolverBuilder <nnabla_rl.builders.SolverBuilder>`):
             builder for q-function solvers
         replay_buffer_builder (:py:class:`ReplayBufferBuilder <nnabla_rl.builders.ReplayBufferBuilder>`):
             builder of replay_buffer
-        explorer_builder (:py:class:`ExplorerBuilder <nnabla_rl.builders.ExplorerBuilder>`):
-            builder of environment explorer
-    """
+    '''
 
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _config: MunchausenDQNConfig
+    _q: QFunction
+    _target_q: QFunction
+    _q_solver: nn.solver.Solver
+    _replay_buffer: ReplayBuffer
+
+    _environment_explorer: EnvironmentExplorer
+    _q_function_trainer: ModelTrainer
+
+    _eval_state_var: nn.Variable
+    _a_greedy: nn.Variable
+
+    _q_function_trainer_state: Dict[str, Any]
 
     def __init__(self, env_or_env_info: Union[gym.Env, EnvironmentInfo],
                  config: MunchausenDQNConfig = MunchausenDQNConfig(),
                  q_func_builder: ModelBuilder[QFunction] = DefaultQFunctionBuilder(),
                  q_solver_builder: SolverBuilder = DefaultQSolverBuilder(),
-                 replay_buffer_builder: ReplayBufferBuilder = DefaultReplayBufferBuilder(),
-                 explorer_builder: ExplorerBuilder = DefaultExplorerBuilder()):
-        super(MunchausenDQN, self).__init__(env_or_env_info=env_or_env_info,
-                                            config=config,
-                                            q_func_builder=q_func_builder,
-                                            q_solver_builder=q_solver_builder,
-                                            replay_buffer_builder=replay_buffer_builder,
-                                            explorer_builder=explorer_builder)
+                 replay_buffer_builder: ReplayBufferBuilder = DefaultReplayBufferBuilder()):
+        super(MunchausenDQN, self).__init__(env_or_env_info, config=config)
+        if not self._env_info.is_discrete_action_env():
+            raise UnsupportedEnvironmentException('{} only supports discrete action environment'.format(self.__name__))
+
+        with nn.context_scope(context.get_nnabla_context(self._config.gpu_id)):
+            self._q = q_func_builder(scope_name='q', env_info=self._env_info, algorithm_config=self._config)
+            self._q_solver = q_solver_builder(env_info=self._env_info, algorithm_config=self._config)
+            self._target_q = cast(QFunction, self._q.deepcopy('target_' + self._q.scope_name))
+
+            self._replay_buffer = replay_buffer_builder(env_info=self._env_info, algorithm_config=self._config)
+
+    @eval_api
+    def compute_eval_action(self, s):
+        with nn.context_scope(context.get_nnabla_context(self._config.gpu_id)):
+            (action, _), _ = epsilon_greedy_action_selection(s,
+                                                             self._greedy_action_selector,
+                                                             self._random_action_selector,
+                                                             epsilon=self._config.test_epsilon)
+            return action
+
+    def _before_training_start(self, env_or_buffer):
+        # set context globally to ensure that the training runs on configured gpu
+        context.set_nnabla_context(self._config.gpu_id)
+        self._environment_explorer = self._setup_environment_explorer(env_or_buffer)
+        self._q_function_trainer = self._setup_q_function_training(env_or_buffer)
+
+    def _setup_environment_explorer(self, env_or_buffer):
+        if self._is_buffer(env_or_buffer):
+            return None
+
+        explorer_config = EE.LinearDecayEpsilonGreedyExplorerConfig(
+            warmup_random_steps=self._config.start_timesteps,
+            initial_step_num=self.iteration_num,
+            initial_epsilon=self._config.initial_epsilon,
+            final_epsilon=self._config.final_epsilon,
+            max_explore_steps=self._config.max_explore_steps
+        )
+        explorer = EE.LinearDecayEpsilonGreedyExplorer(
+            greedy_action_selector=self._greedy_action_selector,
+            random_action_selector=self._random_action_selector,
+            env_info=self._env_info,
+            config=explorer_config)
+        return explorer
 
     def _setup_q_function_training(self, env_or_buffer):
         trainer_config = MT.q_value_trainers.MunchausenDQNQTrainerConfig(
-            num_steps=self._config.num_steps,
             reduction_method='mean',
             q_loss_scalar=0.5,
             grad_clip=(-1.0, 1.0),
             tau=self._config.entropy_temperature,
             alpha=self._config.munchausen_scaling_term,
             clip_min=self._config.clipping_value,
-            clip_max=0.0,
-            unroll_steps=self._config.unroll_steps,
-            burn_in_steps=self._config.burn_in_steps,
-            reset_on_terminal=self._config.reset_rnn_on_terminal)
+            clip_max=0.0)
 
         q_function_trainer = MT.q_value_trainers.MunchausenDQNQTrainer(
             train_functions=self._q,
             solvers={self._q.scope_name: self._q_solver},
             target_function=self._target_q,
             env_info=self._env_info,
             config=trainer_config)
         sync_model(self._q, self._target_q)
         return q_function_trainer
+
+    def _run_online_training_iteration(self, env):
+        experiences = self._environment_explorer.step(env)
+        self._replay_buffer.append_all(experiences)
+        if self._config.start_timesteps < self.iteration_num:
+            if self.iteration_num % self._config.learner_update_frequency == 0:
+                self._m_dqn_training(self._replay_buffer)
+
+    def _run_offline_training_iteration(self, buffer):
+        self._m_dqn_training(buffer)
+
+    @eval_api
+    def _greedy_action_selector(self, s):
+        s = np.expand_dims(s, axis=0)
+        if not hasattr(self, '_eval_state_var'):
+            self._eval_state_var = nn.Variable(s.shape)
+            self._a_greedy = self._q.argmax_q(self._eval_state_var)
+        self._eval_state_var.d = s
+        self._a_greedy.forward()
+        return np.squeeze(self._a_greedy.d, axis=0), {}
+
+    def _random_action_selector(self, s):
+        action = self._env_info.action_space.sample()
+        return np.asarray(action).reshape((1, )), {}
+
+    def _m_dqn_training(self, replay_buffer):
+        experiences, info = replay_buffer.sample(self._config.batch_size)
+        (s, a, r, non_terminal, s_next, *_) = marshal_experiences(experiences)
+        batch = TrainingBatch(batch_size=self._config.batch_size,
+                              s_current=s,
+                              a_current=a,
+                              gamma=self._config.gamma,
+                              reward=r,
+                              non_terminal=non_terminal,
+                              s_next=s_next,
+                              weight=info['weights'])
+
+        self._q_function_trainer_state = self._q_function_trainer.train(batch)
+        if self.iteration_num % self._config.target_update_frequency == 0:
+            sync_model(self._q, self._target_q)
+
+        td_errors = np.abs(self._q_function_trainer_state['td_errors'])
+        replay_buffer.update_priorities(td_errors)
+
+    def _models(self):
+        models = {}
+        models[self._q.scope_name] = self._q
+        return models
+
+    def _solvers(self):
+        solvers = {}
+        solvers[self._q.scope_name] = self._q_solver
+        return solvers
+
+    @property
+    def latest_iteration_state(self):
+        latest_iteration_state = super(MunchausenDQN, self).latest_iteration_state
+        if hasattr(self, '_q_function_trainer_state'):
+            latest_iteration_state['scalar'].update({'q_loss': self._q_function_trainer_state['q_loss']})
+            latest_iteration_state['histogram'].update(
+                {'td_errors': self._q_function_trainer_state['td_errors'].flatten()})
+        return latest_iteration_state
```

## nnabla_rl/algorithms/munchausen_iqn.py

```diff
@@ -1,63 +1,167 @@
 # Copyright 2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from dataclasses import dataclass
-from typing import Union
+from typing import Any, Callable, Dict, Union, cast
 
 import gym
+import numpy as np
 
+import nnabla as nn
+import nnabla.solvers as NS
+import nnabla_rl.environment_explorers as EE
 import nnabla_rl.model_trainers as MT
-from nnabla_rl.algorithms.iqn import (IQN, DefaultExplorerBuilder, DefaultQuantileFunctionBuilder,
-                                      DefaultReplayBufferBuilder, DefaultSolverBuilder, IQNConfig, risk_neutral_measure)
-from nnabla_rl.builders import ExplorerBuilder, ModelBuilder, ReplayBufferBuilder, SolverBuilder
+from nnabla_rl.algorithm import Algorithm, AlgorithmConfig, eval_api
+from nnabla_rl.builders import ModelBuilder, ReplayBufferBuilder, SolverBuilder
+from nnabla_rl.environment_explorer import EnvironmentExplorer
+from nnabla_rl.environment_explorers.epsilon_greedy_explorer import epsilon_greedy_action_selection
 from nnabla_rl.environments.environment_info import EnvironmentInfo
-from nnabla_rl.models import StateActionQuantileFunction
+from nnabla_rl.exceptions import UnsupportedEnvironmentException
+from nnabla_rl.model_trainers.model_trainer import ModelTrainer, TrainingBatch
+from nnabla_rl.models import IQNQuantileFunction, StateActionQuantileFunction
+from nnabla_rl.replay_buffer import ReplayBuffer
+from nnabla_rl.utils import context
+from nnabla_rl.utils.data import marshal_experiences
 from nnabla_rl.utils.misc import sync_model
 
 
 @dataclass
-class MunchausenIQNConfig(IQNConfig):
-    """List of configurations for Munchausen IQN algorithm.
+class MunchausenIQNConfig(AlgorithmConfig):
+    """
+    List of configurations for Munchausen IQN algorithm
 
     Args:
+        gamma (float): discount factor of rewards. Defaults to 0.99.
+        learning_rate (float): learning rate which is set to all solvers. \
+            You can customize/override the learning rate for each solver by implementing the \
+            (:py:class:`SolverBuilder <nnabla_rl.builders.SolverBuilder>`) by yourself. \
+            Defaults to 0.00005.
+        batch_size (int): training atch size. Defaults to 32.
+        start_timesteps (int): the timestep when training starts.\
+            The algorithm will collect experiences from the environment by acting randomly until this timestep.
+            Defaults to 50000.
+        replay_buffer_size (int): the capacity of replay buffer. Defaults to 1000000.
+        learner_update_frequency (int): the interval of learner update. Defaults to 4.
+        target_update_frequency (int): the interval of target q-function update. Defaults to 10000.
+        max_explore_steps (int): the number of steps decaying the epsilon value.\
+            The epsilon will be decayed linearly \
+            :math:`\\epsilon=\\epsilon_{init} - step\\times\\frac{\\epsilon_{init} - \
+            \\epsilon_{final}}{max\\_explore\\_steps}`.\
+            Defaults to 1000000.
+        initial_epsilon (float): the initial epsilon value for -greedy explorer. Defaults to 1.0.
+        final_epsilon (float): the last epsilon value for -greedy explorer. Defaults to 0.01.
+        test_epsilon (float): the epsilon value on testing. Defaults to 0.001.
+        N (int): Number of samples to compute the current state's quantile values. Defaults to 64.
+        N_prime (int): Number of samples to compute the target state's quantile values. Defaults to 64.
+        K (int): Number of samples to compute greedy next action. Defaults to 32.
+        kappa (float): threshold value of quantile huber loss. Defaults to 1.0.
+        embedding_dim (int): dimension of embedding for the sample point. Defaults to 64.
         entropy_temperature (float): temperature parameter of softmax policy distribution. Defaults to 0.03.
         munchausen_scaling_term (float): scalar of scaled log policy. Defaults to 0.9.
         clipping_value (float): Lower value of the logarithm of policy distribution. Defaults to -1.
     """
 
+    gamma: float = 0.99
+    learning_rate: float = 0.00005
+    batch_size: int = 32
+    start_timesteps: int = 50000
+    replay_buffer_size: int = 1000000
+    learner_update_frequency: int = 4
+    target_update_frequency: int = 10000
+    max_explore_steps: int = 1000000
+    initial_epsilon: float = 1.0
+    final_epsilon: float = 0.01
+    test_epsilon: float = 0.001
+    N: int = 64
+    N_prime: int = 64
+    K: int = 32
+    kappa: float = 1.0
+    embedding_dim: int = 64
+
     # munchausen iqn training parameters
     entropy_temperature: float = 0.03
     munchausen_scaling_term: float = 0.9
     clipping_value: float = -1
 
     def __post_init__(self):
-        """__post_init__
+        '''__post_init__
 
         Check that set values are in valid range.
-        """
-        super().__post_init__()
+
+        '''
+        self._assert_between(self.gamma, 0.0, 1.0, 'gamma')
+        self._assert_positive(self.batch_size, 'batch_size')
+        self._assert_positive(self.replay_buffer_size, 'replay_buffer_size')
+        self._assert_positive(self.learner_update_frequency, 'learner_update_frequency')
+        self._assert_positive(self.target_update_frequency, 'target_update_frequency')
+        self._assert_positive(self.max_explore_steps, 'max_explore_steps')
+        self._assert_positive(self.learning_rate, 'learning_rate')
+        self._assert_positive(self.initial_epsilon, 'initial_epsilon')
+        self._assert_positive(self.final_epsilon, 'final_epsilon')
+        self._assert_positive(self.test_epsilon, 'test_epsilon')
+        self._assert_positive(self.N, 'N')
+        self._assert_positive(self.N_prime, 'N_prime')
+        self._assert_positive(self.K, 'K')
+        self._assert_positive(self.kappa, 'kappa')
         self._assert_positive(self.embedding_dim, 'embedding_dim')
         self._assert_negative(self.clipping_value, 'clipping_value')
 
 
-class MunchausenIQN(IQN):
-    """Munchausen-IQN algorithm implementation.
+def risk_neutral_measure(tau):
+    return tau
+
+
+class DefaultQuantileFunctionBuilder(ModelBuilder[StateActionQuantileFunction]):
+    def build_model(self,  # type: ignore[override]
+                    scope_name: str,
+                    env_info: EnvironmentInfo,
+                    algorithm_config: MunchausenIQNConfig,
+                    **kwargs) -> StateActionQuantileFunction:
+        assert isinstance(algorithm_config, MunchausenIQNConfig)
+        risk_measure_function = kwargs['risk_measure_function']
+        return IQNQuantileFunction(scope_name,
+                                   env_info.action_dim,
+                                   algorithm_config.embedding_dim,
+                                   K=algorithm_config.K,
+                                   risk_measure_function=risk_measure_function)
+
+
+class DefaultQuantileSolverBuilder(SolverBuilder):
+    def build_solver(self,  # type: ignore[override]
+                     env_info: EnvironmentInfo,
+                     algorithm_config: MunchausenIQNConfig,
+                     **kwargs) -> nn.solvers.Solver:
+        assert isinstance(algorithm_config, MunchausenIQNConfig)
+        return NS.Adam(algorithm_config.learning_rate, eps=1e-2 / algorithm_config.batch_size)
+
+
+class DefaultReplayBufferBuilder(ReplayBufferBuilder):
+    def build_replay_buffer(self,  # type: ignore[override]
+                            env_info: EnvironmentInfo,
+                            algorithm_config: MunchausenIQNConfig,
+                            **kwargs) -> ReplayBuffer:
+        assert isinstance(algorithm_config, MunchausenIQNConfig)
+        return ReplayBuffer(capacity=algorithm_config.replay_buffer_size)
+
+
+class MunchausenIQN(Algorithm):
+    '''Munchausen-IQN algorithm implementation.
 
     This class implements the Munchausen-IQN (Munchausen Implicit Quantile Network) algorithm
     proposed by N. Vieillard, et al. in the paper: "Munchausen Reinforcement Learning"
     For details see: https://proceedings.neurips.cc/paper/2020/file/2c6a0bae0f071cbbf0bb3d5b11d90a82-Paper.pdf
 
     Args:
         env_or_env_info\
@@ -68,59 +172,163 @@
         risk_measure_function (Callable[[nn.Variable], nn.Variable]): risk measure function to apply to the quantiles.
         quantile_function_builder (:py:class:`ModelBuilder[StateActionQuantileFunction] \
             <nnabla_rl.builders.ModelBuilder>`): builder of state-action quantile function models
         quantile_solver_builder (:py:class:`SolverBuilder <nnabla_rl.builders.SolverBuilder>`):
             builder for state action quantile function solvers
         replay_buffer_builder (:py:class:`ReplayBufferBuilder <nnabla_rl.builders.ReplayBufferBuilder>`):
             builder of replay_buffer
-        explorer_builder (:py:class:`ExplorerBuilder <nnabla_rl.builders.ExplorerBuilder>`):
-            builder of environment explorer
-    """
+    '''
 
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _config: MunchausenIQNConfig
+    _quantile_function: StateActionQuantileFunction
+    _target_quantile_function: StateActionQuantileFunction
+    _quantile_function_solver: nn.solver.Solver
+    _replay_buffer: ReplayBuffer
+
+    _environment_explorer: EnvironmentExplorer
+    _quantile_function_trainer: ModelTrainer
+
+    _eval_state_var: nn.Variable
+    _a_greedy: nn.Variable
+
+    _quantile_function_trainer_state: Dict[str, Any]
 
     def __init__(self,
                  env_or_env_info: Union[gym.Env, EnvironmentInfo],
                  config: MunchausenIQNConfig = MunchausenIQNConfig(),
-                 risk_measure_function=risk_neutral_measure,
+                 risk_measure_function: Callable[[nn.Variable], nn.Variable] = risk_neutral_measure,
                  quantile_function_builder: ModelBuilder[StateActionQuantileFunction]
                  = DefaultQuantileFunctionBuilder(),
-                 quantile_solver_builder: SolverBuilder = DefaultSolverBuilder(),
-                 replay_buffer_builder: ReplayBufferBuilder = DefaultReplayBufferBuilder(),
-                 explorer_builder: ExplorerBuilder = DefaultExplorerBuilder()):
-        super(MunchausenIQN, self).__init__(env_or_env_info, config=config,
-                                            risk_measure_function=risk_measure_function,
-                                            quantile_function_builder=quantile_function_builder,
-                                            quantile_solver_builder=quantile_solver_builder,
-                                            replay_buffer_builder=replay_buffer_builder,
-                                            explorer_builder=explorer_builder)
+                 quantile_solver_builder: SolverBuilder = DefaultQuantileSolverBuilder(),
+                 replay_buffer_builder: ReplayBufferBuilder = DefaultReplayBufferBuilder()):
+        super(MunchausenIQN, self).__init__(env_or_env_info, config=config)
+        if not self._env_info.is_discrete_action_env():
+            raise UnsupportedEnvironmentException('{} only supports discrete action environment'.format(self.__name__))
+
+        with nn.context_scope(context.get_nnabla_context(self._config.gpu_id)):
+            kwargs = {}
+            kwargs['risk_measure_function'] = risk_measure_function
+            self._quantile_function = quantile_function_builder(
+                'quantile_function', self._env_info, self._config, **kwargs)
+            self._target_quantile_function = cast(StateActionQuantileFunction,
+                                                  self._quantile_function.deepcopy('target_quantile_function'))
+
+            self._quantile_function_solver = quantile_solver_builder(self._env_info, self._config)
+
+            self._replay_buffer = replay_buffer_builder(self._env_info, self._config)
+
+    @eval_api
+    def compute_eval_action(self, state):
+        with nn.context_scope(context.get_nnabla_context(self._config.gpu_id)):
+            (action, _), _ = epsilon_greedy_action_selection(state,
+                                                             self._greedy_action_selector,
+                                                             self._random_action_selector,
+                                                             epsilon=self._config.test_epsilon)
+            return action
+
+    def _before_training_start(self, env_or_buffer):
+        # set context globally to ensure that the training runs on configured gpu
+        context.set_nnabla_context(self._config.gpu_id)
+        self._environment_explorer = self._setup_environment_explorer(env_or_buffer)
+        self._quantile_function_trainer = self._setup_quantile_function_training(env_or_buffer)
+
+    def _setup_environment_explorer(self, env_or_buffer):
+        if self._is_buffer(env_or_buffer):
+            return None
+        explorer_config = EE.LinearDecayEpsilonGreedyExplorerConfig(
+            warmup_random_steps=self._config.start_timesteps,
+            initial_step_num=self.iteration_num,
+            initial_epsilon=self._config.initial_epsilon,
+            final_epsilon=self._config.final_epsilon,
+            max_explore_steps=self._config.max_explore_steps
+        )
+        explorer = EE.LinearDecayEpsilonGreedyExplorer(
+            greedy_action_selector=self._greedy_action_selector,
+            random_action_selector=self._random_action_selector,
+            env_info=self._env_info,
+            config=explorer_config)
+        return explorer
 
     def _setup_quantile_function_training(self, env_or_buffer):
         trainer_config = MT.q_value_trainers.MunchausenIQNQTrainerConfig(
-            num_steps=self._config.num_steps,
             N=self._config.N,
             N_prime=self._config.N_prime,
             K=self._config.K,
             kappa=self._config.kappa,
             tau=self._config.entropy_temperature,
             alpha=self._config.munchausen_scaling_term,
             clip_min=self._config.clipping_value,
-            clip_max=0.0,
-            unroll_steps=self._config.unroll_steps,
-            burn_in_steps=self._config.burn_in_steps,
-            reset_on_terminal=self._config.reset_rnn_on_terminal)
+            clip_max=0.0)
 
         quantile_function_trainer = MT.q_value_trainers.MunchausenIQNQTrainer(
             train_functions=self._quantile_function,
             solvers={self._quantile_function.scope_name: self._quantile_function_solver},
             target_function=self._target_quantile_function,
             env_info=self._env_info,
             config=trainer_config)
 
         # NOTE: Copy initial parameters after setting up the training
         # Because the parameter is created after training graph construction
         sync_model(self._quantile_function, self._target_quantile_function)
 
         return quantile_function_trainer
+
+    def _run_online_training_iteration(self, env):
+        experiences = self._environment_explorer.step(env)
+        self._replay_buffer.append_all(experiences)
+        if self._config.start_timesteps < self.iteration_num:
+            if self.iteration_num % self._config.learner_update_frequency == 0:
+                self._m_iqn_training(self._replay_buffer)
+
+    def _run_offline_training_iteration(self, buffer):
+        self._m_iqn_training(buffer)
+
+    def _m_iqn_training(self, replay_buffer):
+        experiences, info = replay_buffer.sample(self._config.batch_size)
+        (s, a, r, non_terminal, s_next, *_) = marshal_experiences(experiences)
+        batch = TrainingBatch(batch_size=self._config.batch_size,
+                              s_current=s,
+                              a_current=a,
+                              gamma=self._config.gamma,
+                              reward=r,
+                              non_terminal=non_terminal,
+                              s_next=s_next,
+                              weight=info['weights'])
+
+        self._quantile_function_trainer_state = self._quantile_function_trainer.train(batch)
+        if self.iteration_num % self._config.target_update_frequency == 0:
+            sync_model(self._quantile_function, self._target_quantile_function)
+
+    @eval_api
+    def _greedy_action_selector(self, s):
+        s = np.expand_dims(s, axis=0)
+        if not hasattr(self, '_eval_state_var'):
+            self._eval_state_var = nn.Variable(s.shape)
+            q_function = self._quantile_function.as_q_function()
+            self._a_greedy = q_function.argmax_q(self._eval_state_var)
+        self._eval_state_var.d = s
+        self._a_greedy.forward()
+        return np.squeeze(self._a_greedy.d, axis=0), {}
+
+    def _random_action_selector(self, s):
+        action = self._env_info.action_space.sample()
+        return np.asarray(action).reshape((1, )), {}
+
+    def _models(self):
+        models = {}
+        models[self._quantile_function.scope_name] = self._quantile_function
+        return models
+
+    def _solvers(self):
+        solvers = {}
+        solvers[self._quantile_function.scope_name] = self._quantile_function_solver
+        return solvers
+
+    @property
+    def latest_iteration_state(self):
+        latest_iteration_state = super(MunchausenIQN, self).latest_iteration_state
+        if hasattr(self, '_quantile_function_trainer_state'):
+            latest_iteration_state['scalar'].update({'q_loss': self._quantile_function_trainer_state['q_loss']})
+        return latest_iteration_state
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## nnabla_rl/algorithms/ppo.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023,2024 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -11,50 +11,49 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import multiprocessing as mp
 import os
-import threading as th
 from collections import namedtuple
 from dataclasses import dataclass
-from typing import Any, Dict, List, NamedTuple, Optional, Tuple, Union
+from typing import Any, Dict, List, NamedTuple, Optional, Union
 
 import gym
 import numpy as np
 
 import nnabla as nn
 import nnabla.solvers as NS
 import nnabla_rl.environment_explorers as EE
 import nnabla_rl.model_trainers as MT
 import nnabla_rl.preprocessors as RP
 import nnabla_rl.utils.context as context
 from nnabla_rl.algorithm import Algorithm, AlgorithmConfig, eval_api
-from nnabla_rl.algorithms.common_utils import (_StatePreprocessedStochasticPolicy, _StatePreprocessedVFunction,
-                                               _StochasticPolicyActionSelector, compute_v_target_and_advantage)
+from nnabla_rl.algorithms.common_utils import (_StatePreprocessedPolicy, _StatePreprocessedVFunction,
+                                               compute_v_target_and_advantage)
 from nnabla_rl.builders import ModelBuilder, PreprocessorBuilder, SolverBuilder
 from nnabla_rl.environment_explorer import EnvironmentExplorer
 from nnabla_rl.environments.environment_info import EnvironmentInfo
 from nnabla_rl.model_trainers.model_trainer import ModelTrainer, TrainingBatch
 from nnabla_rl.models import (Model, PPOAtariPolicy, PPOAtariVFunction, PPOMujocoPolicy, PPOMujocoVFunction,
                               PPOSharedFunctionHead, StochasticPolicy, VFunction)
 from nnabla_rl.preprocessors.preprocessor import Preprocessor
 from nnabla_rl.replay_buffer import ReplayBuffer
 from nnabla_rl.replay_buffers import BufferIterator
-from nnabla_rl.utils.data import add_batch_dimension, marshal_experiences, set_data_to_variable, unzip
-from nnabla_rl.utils.misc import create_variable
+from nnabla_rl.utils.data import marshal_experiences, unzip
 from nnabla_rl.utils.multiprocess import (copy_mp_arrays_to_params, copy_params_to_mp_arrays, mp_array_from_np_array,
                                           mp_to_np_array, new_mp_arrays_from_params, np_to_mp_array)
 from nnabla_rl.utils.reproductions import set_global_seed
 
 
 @dataclass
 class PPOConfig(AlgorithmConfig):
-    """PPOConfig List of configurations for PPO algorithm.
+    '''PPOConfig
+    List of configurations for PPO algorithm
 
     Args:
         epsilon (float): PPO's probability ratio clipping range. Defaults to 0.1
         gamma (float): discount factor of rewards. Defaults to 0.99.
         learning_rate (float): learning rate which is set to all solvers. \
             You can customize/override the learning rate for each solver by implementing the \
             (:py:class:`SolverBuilder <nnabla_rl.builders.SolverBuilder>`) by yourself. \
@@ -71,15 +70,15 @@
             Defaults to True.
         timelimit_as_terminal (bool): Treat as done if the environment reaches the \
             `timelimit <https://github.com/openai/gym/blob/master/gym/wrappers/time_limit.py>`_.\
             Defaults to False.
         seed (int): base seed of random number generator used by the actors. Defaults to 1.
         preprocess_state (bool): Enable preprocessing the states in the collected experiences\
             before feeding as training batch. Defaults to True.
-    """
+    '''
 
     epsilon: float = 0.1
     gamma: float = 0.99
     learning_rate: float = 2.5*1e-4
     lmb: float = 0.95
     entropy_coefficient: float = 0.01
     value_coefficient: float = 1.0
@@ -90,18 +89,19 @@
     total_timesteps: int = 10000
     decrease_alpha: bool = True
     timelimit_as_terminal: bool = False
     seed: int = 1
     preprocess_state: bool = True
 
     def __post_init__(self):
-        """__post_init__
+        '''__post_init__
 
         Check the set values are in valid range.
-        """
+
+        '''
         self._assert_between(self.gamma, 0.0, 1.0, 'gamma')
         self._assert_positive(self.actor_num, 'actor num')
         self._assert_positive(self.epochs, 'epochs')
         self._assert_positive(self.batch_size, 'batch_size')
         self._assert_positive(self.actor_timesteps, 'actor_timesteps')
         self._assert_positive(self.total_timesteps, 'total_timesteps')
 
@@ -181,15 +181,15 @@
                            env_info: EnvironmentInfo,
                            algorithm_config: AlgorithmConfig,
                            **kwargs) -> Preprocessor:
         return RP.RunningMeanNormalizer('preprocessor', env_info.state_shape, value_clip=(-5.0, 5.0))
 
 
 class PPO(Algorithm):
-    """Proximal Policy Optimization (PPO) algorithm implementation.
+    '''Proximal Policy Optimization (PPO) algorithm implementation.
 
     This class implements the Proximal Policy Optimization (PPO) algorithm
     proposed by J. Schulman, et al. in the paper: "Proximal Policy Optimization Algorithms"
     For detail see: https://arxiv.org/abs/1707.06347
 
     This algorithm only supports online training.
 
@@ -202,15 +202,15 @@
             builder of v function models
         v_solver_builder (:py:class:`SolverBuilder <nnabla_rl.builders.SolverBuilder>`): builder for v function solvers
         policy_builder (:py:class:`ModelBuilder[StochasicPolicy] <nnabla_rl.builders.ModelBuilder>`):
             builder of policy models
         policy_solver_builder (:py:class:`SolverBuilder <nnabla_rl.builders.SolverBuilder>`): builder for policy solvers
         state_preprocessor_builder (None or :py:class:`PreprocessorBuilder <nnabla_rl.builders.PreprocessorBuilder>`):
             state preprocessor builder to preprocess the states
-    """
+    '''
 
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _config: PPOConfig
 
     _v_function: VFunction
@@ -222,15 +222,15 @@
     _policy_trainer: ModelTrainer
     _v_function_trainer: ModelTrainer
 
     _policy_solver_builder: SolverBuilder
     _v_solver_builder: SolverBuilder
 
     _actors: List['_PPOActor']
-    _actor_processes: List[Union[mp.Process, th.Thread]]
+    _actor_processes: List[mp.Process]
 
     _policy_trainer_state: Dict[str, Any]
     _v_function_trainer_state: Dict[str, Any]
 
     def __init__(self, env_or_env_info: Union[gym.Env, EnvironmentInfo],
                  config: PPOConfig = PPOConfig(),
                  v_function_builder: ModelBuilder[VFunction] = DefaultVFunctionBuilder(),
@@ -246,30 +246,26 @@
             self._policy = policy_builder('pi', self._env_info, self._config)
             self._state_preprocessor = None
 
             if self._config.preprocess_state and state_preprocessor_builder is not None:
                 preprocessor = state_preprocessor_builder('preprocessor', self._env_info, self._config)
                 assert preprocessor is not None
                 self._v_function = _StatePreprocessedVFunction(v_function=self._v_function, preprocessor=preprocessor)
-                self._policy = _StatePreprocessedStochasticPolicy(policy=self._policy, preprocessor=preprocessor)
+                self._policy = _StatePreprocessedPolicy(policy=self._policy, preprocessor=preprocessor)
                 self._state_preprocessor = preprocessor
 
             self._policy_solver = policy_solver_builder(self._env_info, self._config)
             self._policy_solver_builder = policy_solver_builder  # keep for later use
             self._v_function_solver = v_solver_builder(self._env_info, self._config)
             self._v_solver_builder = v_solver_builder  # keep for later use
 
-        self._evaluation_actor = _StochasticPolicyActionSelector(
-            self._env_info, self._policy.shallowcopy(), deterministic=False)
-
     @eval_api
-    def compute_eval_action(self, state, *, begin_of_episode=False, extra_info={}):
+    def compute_eval_action(self, state):
         with nn.context_scope(context.get_nnabla_context(self._config.gpu_id)):
-            action, *_ = self._evaluation_action_selector(state, begin_of_episode=begin_of_episode)
-            return action
+            return self._compute_action(state)
 
     def _before_training_start(self, env_or_buffer):
         if not self._is_env(env_or_buffer):
             raise ValueError('PPO only supports online training')
         env = env_or_buffer
 
         # FIXME: This setup is a workaround for creating underlying model parameters
@@ -352,72 +348,50 @@
     def _launch_actor_processes(self, env):
         actors = self._build_ppo_actors(env,
                                         v_function=self._v_function,
                                         policy=self._policy,
                                         state_preprocessor=self._state_preprocessor)
         processes = []
         for actor in actors:
-            if self._config.actor_num == 1:
-                # Run on same process when we have only 1 actor
-                p = th.Thread(target=actor, daemon=False)
-            else:
-                p = mp.Process(target=actor, daemon=True)
+            p = mp.Process(target=actor, daemon=True)
             p.start()
             processes.append(p)
         return actors, processes
 
     def _kill_actor_processes(self, process):
-        if isinstance(process, mp.Process):
-            process.terminate()
-        else:
-            # This is a thread. do nothing
-            pass
+        process.terminate()
         process.join()
 
     def _run_offline_training_iteration(self, buffer):
         raise NotImplementedError
 
     def _collect_experiences(self, actors):
-        def concat_result(result):
-            if isinstance(result[0], tuple):
-                num_items = len(result[0])
-                items = []
-                for i in range(num_items):
-                    concatenated = np.concatenate(tuple(item[i] for item in result), axis=0)
-                    items.append(concatenated)
-                return tuple(zip(*items))
-            else:
-                return np.concatenate(result, axis=0)
-
         for actor in self._actors:
-            if self._config.actor_num != 1:
-                actor.update_v_params(self._v_function.get_parameters())
-                actor.update_policy_params(self._policy.get_parameters())
-                if self._config.preprocess_state:
-                    actor.update_state_preprocessor_params(self._state_preprocessor.get_parameters())
-            else:
-                # Its running on same process. No need to synchronize parameters with multiprocessing arrays.
-                pass
+            actor.update_v_params(self._v_function.get_parameters())
+            actor.update_policy_params(self._policy.get_parameters())
+            if self._config.preprocess_state:
+                actor.update_state_preprocessor_params(self._state_preprocessor.get_parameters())
+
             actor.run_data_collection()
 
         results = []
         for actor in actors:
             result = actor.wait_data_collection()
             results.append(result)
-
-        return (concat_result(result) for result in unzip(results))
+        return (np.concatenate(item, axis=0) for item in unzip(results))
 
     def _ppo_training(self, experiences):
         if self._config.decrease_alpha:
             alpha = (1.0 - self.iteration_num / self._config.total_timesteps)
             alpha = np.maximum(alpha, 0.0)
         else:
             alpha = 1.0
 
         (s, a, _, _, _, log_prob, v_target, advantage) = marshal_experiences(experiences)
+
         extra = {}
         extra['log_prob'] = log_prob
         extra['advantage'] = advantage
         extra['alpha'] = alpha
         extra['v_target'] = v_target
         batch = TrainingBatch(batch_size=len(experiences),
                               s_current=s,
@@ -425,16 +399,28 @@
                               extra=extra)
 
         self._policy_trainer.set_learning_rate(self._config.learning_rate * alpha)
         self._policy_trainer_state = self._policy_trainer.train(batch)
         self._v_function_trainer.set_learning_rate(self._config.learning_rate * alpha)
         self._v_function_trainer_state = self._v_function_trainer.train(batch)
 
-    def _evaluation_action_selector(self, s, *, begin_of_episode=False):
-        return self._evaluation_actor(s, begin_of_episode=begin_of_episode)
+    @eval_api
+    def _compute_action(self, s):
+        s = np.expand_dims(s, axis=0)
+        if not hasattr(self, '_eval_state_var'):
+            self._eval_state_var = nn.Variable(s.shape)
+            distribution = self._policy.pi(self._eval_state_var)
+            self._eval_action = distribution.sample()
+        self._eval_state_var.d = s
+        self._eval_action.forward()
+        action = np.squeeze(self._eval_action.d, axis=0)
+        if self._env_info.is_discrete_action_env():
+            return np.int(action)
+        else:
+            return action
 
     def _models(self):
         models = {}
         models[self._v_function.scope_name] = self._v_function
         models[self._policy.scope_name] = self._policy
         if self._config.preprocess_state and isinstance(self._state_preprocessor, Model):
             models[self._state_preprocessor.scope_name] = self._state_preprocessor
@@ -442,20 +428,14 @@
 
     def _solvers(self):
         solvers = {}
         solvers[self._policy.scope_name] = self._policy_solver
         solvers[self._v_function.scope_name] = self._v_function_solver
         return solvers
 
-    @classmethod
-    def is_supported_env(cls, env_or_env_info):
-        env_info = EnvironmentInfo.from_env(env_or_env_info) if isinstance(env_or_env_info, gym.Env) \
-            else env_or_env_info
-        return not env_info.is_tuple_action_env()
-
     def _build_ppo_actors(self, env, v_function, policy, state_preprocessor):
         actors = []
         for i in range(self._config.actor_num):
             actor = _PPOActor(
                 actor_num=i,
                 env=env,
                 env_info=self._env_info,
@@ -466,23 +446,19 @@
             actors.append(actor)
         return actors
 
     @property
     def latest_iteration_state(self):
         latest_iteration_state = super(PPO, self).latest_iteration_state
         if hasattr(self, '_policy_trainer_state'):
-            latest_iteration_state['scalar'].update({'pi_loss': float(self._policy_trainer_state['pi_loss'])})
+            latest_iteration_state['scalar'].update({'pi_loss': self._policy_trainer_state['pi_loss']})
         if hasattr(self, '_v_function_trainer_state'):
-            latest_iteration_state['scalar'].update({'v_loss': float(self._v_function_trainer_state['v_loss'])})
+            latest_iteration_state['scalar'].update({'v_loss': self._v_function_trainer_state['v_loss']})
         return latest_iteration_state
 
-    @property
-    def trainers(self):
-        return {"v_function": self._v_function_trainer, "policy": self._policy_trainer}
-
 
 class _PPOActor(object):
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _actor_num: int
     _env: gym.Env
@@ -531,61 +507,65 @@
         obs_space = self._env.observation_space
         action_space = self._env.action_space
 
         MultiProcessingArrays = namedtuple('MultiProcessingArrays',
                                            ['state', 'action', 'reward', 'non_terminal',
                                             'next_state', 'log_prob', 'v_target', 'advantage'])
 
-        state_mp_array = self._prepare_state_mp_array(obs_space, env_info)
-        action_mp_array = self._prepare_action_mp_array(action_space, env_info)
+        state_mp_array_shape = (self._timesteps, *obs_space.shape)
+        state_mp_array = mp_array_from_np_array(
+            np.empty(shape=state_mp_array_shape, dtype=obs_space.dtype))
+        if env_info.is_discrete_action_env():
+            action_mp_array_shape = (self._timesteps, 1)
+            action_mp_array = mp_array_from_np_array(
+                np.empty(shape=action_mp_array_shape, dtype=action_space.dtype))
+        else:
+            action_mp_array_shape = (self._timesteps, action_space.shape[0])
+            action_mp_array = mp_array_from_np_array(
+                np.empty(shape=action_mp_array_shape, dtype=action_space.dtype))
 
         scalar_mp_array_shape = (self._timesteps, 1)
-        reward_mp_array = (mp_array_from_np_array(
-            np.empty(shape=scalar_mp_array_shape, dtype=np.float32)), scalar_mp_array_shape, np.float32)
-        non_terminal_mp_array = (mp_array_from_np_array(
-            np.empty(shape=scalar_mp_array_shape, dtype=np.float32)), scalar_mp_array_shape, np.float32)
-        next_state_mp_array = self._prepare_state_mp_array(obs_space, env_info)
-        log_prob_mp_array = (mp_array_from_np_array(
-            np.empty(shape=scalar_mp_array_shape, dtype=np.float32)), scalar_mp_array_shape, np.float32)
-        v_target_mp_array = (mp_array_from_np_array(
-            np.empty(shape=scalar_mp_array_shape, dtype=np.float32)), scalar_mp_array_shape, np.float32)
-        advantage_mp_array = (mp_array_from_np_array(
-            np.empty(shape=scalar_mp_array_shape, dtype=np.float32)), scalar_mp_array_shape, np.float32)
+        reward_mp_array = mp_array_from_np_array(
+            np.empty(shape=scalar_mp_array_shape, dtype=np.float32))
+        non_terminal_mp_array = mp_array_from_np_array(
+            np.empty(shape=scalar_mp_array_shape, dtype=np.float32))
+        next_state_mp_array = mp_array_from_np_array(
+            np.empty(shape=state_mp_array_shape, dtype=obs_space.dtype))
+        log_prob_mp_array = mp_array_from_np_array(
+            np.empty(shape=scalar_mp_array_shape, dtype=np.float32))
+        v_target_mp_array = mp_array_from_np_array(
+            np.empty(shape=scalar_mp_array_shape, dtype=np.float32))
+        advantage_mp_array = mp_array_from_np_array(
+            np.empty(shape=scalar_mp_array_shape, dtype=np.float32))
 
         self._mp_arrays = MultiProcessingArrays(
-            state_mp_array,
-            action_mp_array,
-            reward_mp_array,
-            non_terminal_mp_array,
-            next_state_mp_array,
-            log_prob_mp_array,
-            v_target_mp_array,
-            advantage_mp_array
+            (state_mp_array, state_mp_array_shape, obs_space.dtype),
+            (action_mp_array, action_mp_array_shape, action_space.dtype),
+            (reward_mp_array, scalar_mp_array_shape, np.float32),
+            (non_terminal_mp_array, scalar_mp_array_shape, np.float32),
+            (next_state_mp_array, state_mp_array_shape, obs_space.dtype),
+            (log_prob_mp_array, scalar_mp_array_shape, np.float32),
+            (v_target_mp_array, scalar_mp_array_shape, np.float32),
+            (advantage_mp_array, scalar_mp_array_shape, np.float32)
         )
 
     def __call__(self):
         self._run_actor_loop()
 
     def dispose(self):
-        self._disposed.value = True
+        self._disposed = True
         self._task_start_event.set()
 
     def run_data_collection(self):
         self._task_finish_event.clear()
         self._task_start_event.set()
 
     def wait_data_collection(self):
-        def _mp_to_np_array(mp_array):
-            if isinstance(mp_array[0], tuple):
-                # tupled state
-                return tuple(mp_to_np_array(*array) for array in mp_array)
-            else:
-                return mp_to_np_array(*mp_array)
         self._task_finish_event.wait()
-        return tuple(_mp_to_np_array(mp_array) for mp_array in self._mp_arrays)
+        return (mp_to_np_array(mp_array, shape, dtype) for (mp_array, shape, dtype) in self._mp_arrays)
 
     def update_v_params(self, params):
         self._update_params(src=params, dest=self._v_mp_arrays)
 
     def update_policy_params(self, params):
         self._update_params(src=params, dest=self._policy_mp_arrays)
 
@@ -601,21 +581,20 @@
 
         set_global_seed(seed)
         self._env.seed(seed)
         while (True):
             self._task_start_event.wait()
             if self._disposed.get_obj():
                 break
-            if self._config.actor_num != 1:
-                # Running on different process
-                # Sync parameters through multiproccess arrays
-                self._synchronize_v_params(self._v_function.get_parameters())
-                self._synchronize_policy_params(self._policy.get_parameters())
-                if self._config.preprocess_state:
-                    self._synchronize_preprocessor_params(self._state_preprocessor.get_parameters())
+
+            self._synchronize_v_params(self._v_function.get_parameters())
+            self._synchronize_policy_params(self._policy.get_parameters())
+            if self._config.preprocess_state:
+                self._synchronize_preprocessor_params(self._state_preprocessor.get_parameters())
+
             experiences, v_targets, advantages = self._run_data_collection()
             self._fill_result(experiences, v_targets, advantages)
 
             self._task_start_event.clear()
             self._task_finish_event.set()
 
     def _run_data_collection(self):
@@ -623,41 +602,43 @@
         experiences = [(s, a, r, non_terminal, s_next, info['log_prob'])
                        for (s, a, r, non_terminal, s_next, info) in experiences]
         v_targets, advantages = compute_v_target_and_advantage(
             self._v_function, experiences, gamma=self._gamma, lmb=self._lambda)
         return experiences, v_targets, advantages
 
     @eval_api
-    def _compute_action(self, s, *, begin_of_episode=False):
-        s = add_batch_dimension(s)
+    def _compute_action(self, s):
+        s = np.expand_dims(s, axis=0)
         if not hasattr(self, '_eval_state_var'):
-            self._eval_state_var = create_variable(1, self._env_info.state_shape)
+            self._eval_state_var = nn.Variable(s.shape)
             distribution = self._policy.pi(self._eval_state_var)
             self._eval_action, self._eval_log_prob = distribution.sample_and_compute_log_prob()
-        set_data_to_variable(self._eval_state_var, s)
+        self._eval_state_var.d = s
         nn.forward_all([self._eval_action, self._eval_log_prob])
         action = np.squeeze(self._eval_action.d, axis=0)
         log_prob = np.squeeze(self._eval_log_prob.d, axis=0)
         info = {}
         info['log_prob'] = log_prob
         if self._env_info.is_discrete_action_env():
-            return np.int32(action), info
+            return np.int(action), info
         else:
             return action, info
 
     def _fill_result(self, experiences, v_targets, advantages):
+        def array_and_dtype(mp_arrays_item):
+            return mp_arrays_item[0], mp_arrays_item[2]
         (s, a, r, non_terminal, s_next, log_prob) = marshal_experiences(experiences)
-        _copy_np_array_to_mp_array(s, self._mp_arrays.state)
-        _copy_np_array_to_mp_array(a, self._mp_arrays.action)
-        _copy_np_array_to_mp_array(r, self._mp_arrays.reward)
-        _copy_np_array_to_mp_array(non_terminal, self._mp_arrays.non_terminal)
-        _copy_np_array_to_mp_array(s_next, self._mp_arrays.next_state)
-        _copy_np_array_to_mp_array(log_prob, self._mp_arrays.log_prob)
-        _copy_np_array_to_mp_array(v_targets, self._mp_arrays.v_target)
-        _copy_np_array_to_mp_array(advantages, self._mp_arrays.advantage)
+        np_to_mp_array(s, *array_and_dtype(self._mp_arrays.state))
+        np_to_mp_array(a, *array_and_dtype(self._mp_arrays.action))
+        np_to_mp_array(r, *array_and_dtype(self._mp_arrays.reward))
+        np_to_mp_array(non_terminal, *array_and_dtype(self._mp_arrays.non_terminal))
+        np_to_mp_array(s_next, *array_and_dtype(self._mp_arrays.next_state))
+        np_to_mp_array(log_prob, *array_and_dtype(self._mp_arrays.log_prob))
+        np_to_mp_array(v_targets, *array_and_dtype(self._mp_arrays.v_target))
+        np_to_mp_array(advantages, *array_and_dtype(self._mp_arrays.advantage))
 
     def _update_params(self, src, dest):
         copy_params_to_mp_arrays(src, dest)
 
     def _synchronize_v_params(self, params):
         self._synchronize_params(src=self._v_mp_arrays, dest=params)
 
@@ -665,59 +646,7 @@
         self._synchronize_params(src=self._policy_mp_arrays, dest=params)
 
     def _synchronize_preprocessor_params(self, params):
         self._synchronize_params(src=self._state_preprocessor_mp_arrays, dest=params)
 
     def _synchronize_params(self, src, dest):
         copy_mp_arrays_to_params(src, dest)
-
-    def _prepare_state_mp_array(self, obs_space, env_info):
-        if env_info.is_tuple_state_env():
-            state_mp_arrays = []
-            state_mp_array_shapes = []
-            state_mp_array_dtypes = []
-            for space in obs_space:
-                state_mp_array_shape = (self._timesteps, *space.shape)
-                state_mp_array = mp_array_from_np_array(
-                    np.empty(shape=state_mp_array_shape, dtype=space.dtype))
-                state_mp_array_shapes.append(state_mp_array_shape)
-                state_mp_array_dtypes.append(space.dtype)
-                state_mp_arrays.append(state_mp_array)
-            return tuple(x for x in zip(state_mp_arrays, state_mp_array_shapes, state_mp_array_dtypes))
-        else:
-            state_mp_array_shape = (self._timesteps, *obs_space.shape)
-            state_mp_array = mp_array_from_np_array(
-                np.empty(shape=state_mp_array_shape, dtype=obs_space.dtype))
-            return (state_mp_array, state_mp_array_shape, obs_space.dtype)
-
-    def _prepare_action_mp_array(self, action_space, env_info):
-        if env_info.is_discrete_action_env():
-            action_mp_array_shape = (self._timesteps, 1)
-            action_mp_array = mp_array_from_np_array(
-                np.empty(shape=action_mp_array_shape, dtype=action_space.dtype))
-        else:
-            action_mp_array_shape = (self._timesteps, action_space.shape[0])
-            action_mp_array = mp_array_from_np_array(
-                np.empty(shape=action_mp_array_shape, dtype=action_space.dtype))
-        return (action_mp_array, action_mp_array_shape, action_space.dtype)
-
-
-def _copy_np_array_to_mp_array(
-    np_array: Union[np.ndarray, Tuple[np.ndarray]],
-    mp_array_shape_type: Union[Tuple[np.ndarray, Tuple[int], np.dtype], Tuple[Tuple[np.ndarray, Tuple[int], np.dtype]]],
-):
-    """Copy numpy array to multiprocessing array.
-
-    Args:
-        np_array (Union[np.ndarray, Tuple[np.ndarray]]): copy source of numpy array.
-        mp_array_shape_type
-            (Union[Tuple[np.ndarray, Tuple[int], np.dtype], Tuple[Tuple[np.ndarray, Tuple[int], np.dtype]]]):
-            copy target of multiprocessing array, shape and type.
-    """
-    if isinstance(np_array, tuple) and isinstance(mp_array_shape_type[0], tuple):
-        assert len(np_array) == len(mp_array_shape_type)
-        for np_ary, mp_ary_shape_type in zip(np_array, mp_array_shape_type):
-            np_to_mp_array(np_ary, mp_ary_shape_type[0], mp_ary_shape_type[2])
-    elif isinstance(np_array, np.ndarray) and isinstance(mp_array_shape_type[0], np.ndarray):
-        np_to_mp_array(np_array, mp_array_shape_type[0], mp_array_shape_type[2])
-    else:
-        raise ValueError("Invalid pair of np_array and mp_array!")
```

## nnabla_rl/algorithms/qrdqn.py

```diff
@@ -1,58 +1,58 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from dataclasses import dataclass
-from typing import Any, Dict, Union
+from typing import Any, Dict, Union, cast
 
 import gym
 import numpy as np
 
 import nnabla as nn
 import nnabla.solvers as NS
 import nnabla_rl.environment_explorers as EE
 import nnabla_rl.model_trainers as MT
 from nnabla_rl.algorithm import Algorithm, AlgorithmConfig, eval_api
-from nnabla_rl.algorithms.common_utils import _GreedyActionSelector
-from nnabla_rl.builders import ExplorerBuilder, ModelBuilder, ReplayBufferBuilder, SolverBuilder
+from nnabla_rl.builders import ModelBuilder, ReplayBufferBuilder, SolverBuilder
 from nnabla_rl.environment_explorer import EnvironmentExplorer
 from nnabla_rl.environment_explorers.epsilon_greedy_explorer import epsilon_greedy_action_selection
 from nnabla_rl.environments.environment_info import EnvironmentInfo
+from nnabla_rl.exceptions import UnsupportedEnvironmentException
 from nnabla_rl.model_trainers.model_trainer import ModelTrainer, TrainingBatch
 from nnabla_rl.models import QRDQNQuantileDistributionFunction, QuantileDistributionFunction
 from nnabla_rl.replay_buffer import ReplayBuffer
 from nnabla_rl.utils import context
 from nnabla_rl.utils.data import marshal_experiences
 from nnabla_rl.utils.misc import sync_model
 
 
 @dataclass
 class QRDQNConfig(AlgorithmConfig):
-    """List of configurations for QRDQN algorithm.
+    '''
+    List of configurations for QRDQN algorithm
 
     Args:
         gamma (float): discount factor of rewards. Defaults to 0.99.
         learning_rate (float): learning rate which is set to all solvers. \
             You can customize/override the learning rate for each solver by implementing the \
             (:py:class:`SolverBuilder <nnabla_rl.builders.SolverBuilder>`) by yourself. \
             Defaults to 0.00005.
-        batch_size (int): training batch size. Defaults to 32.
-        num_steps (int): number of steps for N-step Q targets. Defaults to 1.
+        batch_size (int): training atch size. Defaults to 32.
         learner_update_frequency (int): the interval of learner update. Defaults to 4.
         target_update_frequency (int): the interval of target q-function update. Defaults to 10000.
         start_timesteps (int): the timestep when training starts.\
             The algorithm will collect experiences from the environment by acting randomly until this timestep.
             Defaults to 50000.
         replay_buffer_size (int): the capacity of replay buffer. Defaults to 1000000.
         max_explore_steps (int): the number of steps decaying the epsilon value.\
@@ -61,65 +61,48 @@
             \\epsilon_{final}}{max\\_explore\\_steps}`.\
             Defaults to 1000000.
         initial_epsilon (float): the initial epsilon value for -greedy explorer. Defaults to 1.0.
         final_epsilon (float): the last epsilon value for -greedy explorer. Defaults to 0.01.
         test_epsilon (float): the epsilon value on testing. Defaults to 0.001.
         num_quantiles (int): Number of quantile points. Defaults to 200.
         kappa (float): threshold value of quantile huber loss. Defaults to 1.0.
-        unroll_steps (int): Number of steps to unroll tranining network.
-            The network will be unrolled even though the provided model doesn't have RNN layers.
-            Defaults to 1.
-        burn_in_steps (int): Number of burn-in steps to initiaze recurrent layer states during training.
-            This flag does not take effect if given model is not an RNN model.
-            Defaults to 0.
-        reset_rnn_on_terminal (bool): Reset recurrent internal states to zero during training if episode ends.
-            This flag does not take effect if given model is not an RNN model.
-            Defaults to True.
-    """
+    '''
 
     gamma: float = 0.99
     learning_rate: float = 0.00005
     batch_size: int = 32
-    num_steps: int = 1
     learner_update_frequency: int = 4
     target_update_frequency: int = 10000
     start_timesteps: int = 50000
     replay_buffer_size: int = 1000000
     max_explore_steps: int = 1000000
     initial_epsilon: float = 1.0
     final_epsilon: float = 0.01
     test_epsilon: float = 0.001
     num_quantiles: int = 200
     kappa: float = 1.0
 
-    # rnn model support
-    unroll_steps: int = 1
-    burn_in_steps: int = 0
-    reset_rnn_on_terminal: bool = True
-
     def __post_init__(self):
-        """__post_init__
+        '''__post_init__
 
         Check that set values are in valid range.
-        """
+
+        '''
         self._assert_between(self.gamma, 0.0, 1.0, 'gamma')
         self._assert_positive(self.batch_size, 'batch_size')
-        self._assert_positive(self.num_steps, 'num_steps')
         self._assert_positive(self.replay_buffer_size, 'replay_buffer_size')
         self._assert_positive(self.learner_update_frequency, 'learner_update_frequency')
         self._assert_positive(self.target_update_frequency, 'target_update_frequency')
         self._assert_positive(self.max_explore_steps, 'max_explore_steps')
         self._assert_positive(self.learning_rate, 'learning_rate')
         self._assert_positive(self.initial_epsilon, 'initial_epsilon')
         self._assert_positive(self.final_epsilon, 'final_epsilon')
         self._assert_positive(self.test_epsilon, 'test_epsilon')
         self._assert_positive(self.num_quantiles, 'num_quantiles')
         self._assert_positive(self.kappa, 'kappa')
-        self._assert_positive(self.unroll_steps, 'unroll_steps')
-        self._assert_positive_or_zero(self.burn_in_steps, 'burn_in_steps')
 
 
 class DefaultQuantileBuilder(ModelBuilder[QuantileDistributionFunction]):
     def build_model(self,  # type: ignore[override]
                     scope_name: str,
                     env_info: EnvironmentInfo,
                     algorithm_config: QRDQNConfig,
@@ -139,37 +122,16 @@
     def build_replay_buffer(self,  # type: ignore[override]
                             env_info: EnvironmentInfo,
                             algorithm_config: QRDQNConfig,
                             **kwargs) -> ReplayBuffer:
         return ReplayBuffer(capacity=algorithm_config.replay_buffer_size)
 
 
-class DefaultExplorerBuilder(ExplorerBuilder):
-    def build_explorer(self,  # type: ignore[override]
-                       env_info: EnvironmentInfo,
-                       algorithm_config: QRDQNConfig,
-                       algorithm: "QRDQN",
-                       **kwargs) -> EnvironmentExplorer:
-        explorer_config = EE.LinearDecayEpsilonGreedyExplorerConfig(
-            warmup_random_steps=algorithm_config.start_timesteps,
-            initial_step_num=algorithm.iteration_num,
-            initial_epsilon=algorithm_config.initial_epsilon,
-            final_epsilon=algorithm_config.final_epsilon,
-            max_explore_steps=algorithm_config.max_explore_steps
-        )
-        explorer = EE.LinearDecayEpsilonGreedyExplorer(
-            greedy_action_selector=algorithm._exploration_action_selector,
-            random_action_selector=algorithm._random_action_selector,
-            env_info=env_info,
-            config=explorer_config)
-        return explorer
-
-
 class QRDQN(Algorithm):
-    """Quantile Regression DQN algorithm.
+    '''Quantile Regression DQN algorithm.
 
     This class implements the Quantile Regression DQN algorithm
     proposed by W. Dabney, et al. in the paper: "Distributional Reinforcement Learning with Quantile Regression"
     For details see: https://arxiv.org/abs/1710.10044
 
     Args:
         env_or_env_info\
@@ -179,83 +141,87 @@
             configuration of QRDQN algorithm
         quantile_dist_function_builder (:py:class:`ModelBuilder[QuantileDistributionFunction] \
             <nnabla_rl.builders.ModelBuilder>`): builder of quantile distribution function models
         quantile_solver_builder (:py:class:`SolverBuilder <nnabla_rl.builders.SolverBuilder>`):
             builder for quantile distribution function solvers
         replay_buffer_builder (:py:class:`ReplayBufferBuilder <nnabla_rl.builders.ReplayBufferBuilder>`):
             builder of replay_buffer
-        explorer_builder (:py:class:`ExplorerBuilder <nnabla_rl.builders.ExplorerBuilder>`):
-            builder of environment explorer
-    """
+    '''
 
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _config: QRDQNConfig
 
     _quantile_dist: QuantileDistributionFunction
     _quantile_dist_solver: nn.solver.Solver
     _target_quantile_dist: QuantileDistributionFunction
 
     _replay_buffer: ReplayBuffer
-    _explorer_builder: ExplorerBuilder
     _environment_explorer: EnvironmentExplorer
     _quantile_dist_trainer: ModelTrainer
 
+    _eval_state_var: nn.Variable
+    _a_greedy: nn.Variable
+
     _quantile_dist_trainer_state: Dict[str, Any]
 
     def __init__(self, env_or_env_info: Union[gym.Env, EnvironmentInfo],
                  config: QRDQNConfig = QRDQNConfig(),
                  quantile_dist_function_builder: ModelBuilder[QuantileDistributionFunction] = DefaultQuantileBuilder(),
                  quantile_solver_builder: SolverBuilder = DefaultSolverBuilder(),
-                 replay_buffer_builder: ReplayBufferBuilder = DefaultReplayBufferBuilder(),
-                 explorer_builder: ExplorerBuilder = DefaultExplorerBuilder()):
+                 replay_buffer_builder: ReplayBufferBuilder = DefaultReplayBufferBuilder()):
         super(QRDQN, self).__init__(env_or_env_info, config=config)
-
-        self._explorer_builder = explorer_builder
+        if not self._env_info.is_discrete_action_env():
+            raise UnsupportedEnvironmentException('{} only supports discrete action environment'.format(self.__name__))
 
         with nn.context_scope(context.get_nnabla_context(self._config.gpu_id)):
             self._quantile_dist = quantile_dist_function_builder('quantile_dist_train', self._env_info, self._config)
             self._quantile_dist_solver = quantile_solver_builder(self._env_info, self._config)
-            self._target_quantile_dist = self._quantile_dist.deepcopy('quantile_dist_target')
+            self._target_quantile_dist = cast(QuantileDistributionFunction,
+                                              self._quantile_dist.deepcopy('quantile_dist_target'))
 
             self._replay_buffer = replay_buffer_builder(self._env_info, self._config)
 
-        self._evaluation_actor = _GreedyActionSelector(
-            self._env_info, self._quantile_dist.shallowcopy().as_q_function())
-        self._exploration_actor = _GreedyActionSelector(
-            self._env_info, self._quantile_dist.shallowcopy().as_q_function())
-
     @eval_api
-    def compute_eval_action(self, state, *, begin_of_episode=False, extra_info={}):
+    def compute_eval_action(self, state):
         with nn.context_scope(context.get_nnabla_context(self._config.gpu_id)):
             (action, _), _ = epsilon_greedy_action_selection(state,
-                                                             self._evaluation_action_selector,
+                                                             self._greedy_action_selector,
                                                              self._random_action_selector,
-                                                             epsilon=self._config.test_epsilon,
-                                                             begin_of_episode=begin_of_episode)
+                                                             epsilon=self._config.test_epsilon)
             return action
 
     def _before_training_start(self, env_or_buffer):
         # set context globally to ensure that the training runs on configured gpu
         context.set_nnabla_context(self._config.gpu_id)
         self._environment_explorer = self._setup_environment_explorer(env_or_buffer)
         self._quantile_dist_trainer = self._setup_quantile_function_training(env_or_buffer)
 
     def _setup_environment_explorer(self, env_or_buffer):
-        return None if self._is_buffer(env_or_buffer) else self._explorer_builder(self._env_info, self._config, self)
+        if self._is_buffer(env_or_buffer):
+            return None
+        explorer_config = EE.LinearDecayEpsilonGreedyExplorerConfig(
+            warmup_random_steps=self._config.start_timesteps,
+            initial_step_num=self.iteration_num,
+            initial_epsilon=self._config.initial_epsilon,
+            final_epsilon=self._config.final_epsilon,
+            max_explore_steps=self._config.max_explore_steps
+        )
+        explorer = EE.LinearDecayEpsilonGreedyExplorer(
+            greedy_action_selector=self._greedy_action_selector,
+            random_action_selector=self._random_action_selector,
+            env_info=self._env_info,
+            config=explorer_config)
+        return explorer
 
     def _setup_quantile_function_training(self, env_or_buffer):
-        trainer_config = MT.q_value_trainers.QRDQNQTrainerConfig(
-            num_steps=self._config.num_steps,
+        trainer_config = MT.q_value.QRDQNQTrainerConfig(
             num_quantiles=self._config.num_quantiles,
-            kappa=self._config.kappa,
-            unroll_steps=self._config.unroll_steps,
-            burn_in_steps=self._config.burn_in_steps,
-            reset_on_terminal=self._config.reset_rnn_on_terminal)
+            kappa=self._config.kappa)
 
         quantile_dist_trainer = MT.q_value_trainers.QRDQNQTrainer(
             train_functions=self._quantile_dist,
             solvers={self._quantile_dist.scope_name: self._quantile_dist_solver},
             target_function=self._target_quantile_dist,
             env_info=self._env_info,
             config=trainer_config)
@@ -272,72 +238,53 @@
             if self.iteration_num % self._config.learner_update_frequency == 0:
                 self._qrdqn_training(self._replay_buffer)
 
     def _run_offline_training_iteration(self, buffer):
         self._qrdqn_training(buffer)
 
     def _qrdqn_training(self, replay_buffer):
-        num_steps = self._config.num_steps + self._config.burn_in_steps + self._config.unroll_steps - 1
-        experiences_tuple, info = replay_buffer.sample(self._config.batch_size, num_steps=num_steps)
-        if num_steps == 1:
-            experiences_tuple = (experiences_tuple, )
-        assert len(experiences_tuple) == num_steps
-
-        batch = None
-        for experiences in reversed(experiences_tuple):
-            (s, a, r, non_terminal, s_next, rnn_states_dict, *_) = marshal_experiences(experiences)
-            rnn_states = rnn_states_dict['rnn_states'] if 'rnn_states' in rnn_states_dict else {}
-            batch = TrainingBatch(batch_size=self._config.batch_size,
-                                  s_current=s,
-                                  a_current=a,
-                                  gamma=self._config.gamma,
-                                  reward=r,
-                                  non_terminal=non_terminal,
-                                  s_next=s_next,
-                                  weight=info['weights'],
-                                  next_step_batch=batch,
-                                  rnn_states=rnn_states)
+        experiences, info = replay_buffer.sample(self._config.batch_size)
+        (s, a, r, non_terminal, s_next, *_) = marshal_experiences(experiences)
+        batch = TrainingBatch(batch_size=self._config.batch_size,
+                              s_current=s,
+                              a_current=a,
+                              gamma=self._config.gamma,
+                              reward=r,
+                              non_terminal=non_terminal,
+                              s_next=s_next,
+                              weight=info['weights'])
 
         self._quantile_dist_trainer_state = self._quantile_dist_trainer.train(batch)
         if self.iteration_num % self._config.target_update_frequency:
             sync_model(self._quantile_dist, self._target_quantile_dist)
 
-    def _evaluation_action_selector(self, s, *, begin_of_episode=False):
-        return self._evaluation_actor(s, begin_of_episode=begin_of_episode)
-
-    def _exploration_action_selector(self, s, *, begin_of_episode=False):
-        return self._exploration_actor(s, begin_of_episode=begin_of_episode)
+    @eval_api
+    def _greedy_action_selector(self, s):
+        s = np.expand_dims(s, axis=0)
+        if not hasattr(self, '_eval_state_var'):
+            self._eval_state_var = nn.Variable(s.shape)
+            q_function = self._quantile_dist.as_q_function()
+            self._a_greedy = q_function.argmax_q(self._eval_state_var)
+        self._eval_state_var.d = s
+        self._a_greedy.forward()
+        return np.squeeze(self._a_greedy.d, axis=0), {}
 
-    def _random_action_selector(self, s, *,  begin_of_episode=False):
+    def _random_action_selector(self, s):
         action = self._env_info.action_space.sample()
         return np.asarray(action).reshape((1, )), {}
 
     def _models(self):
         models = {}
         models[self._quantile_dist.scope_name] = self._quantile_dist
         return models
 
     def _solvers(self):
         solvers = {}
         solvers[self._quantile_dist.scope_name] = self._quantile_dist_solver
         return solvers
 
-    @classmethod
-    def is_supported_env(cls, env_or_env_info):
-        env_info = EnvironmentInfo.from_env(env_or_env_info) if isinstance(env_or_env_info, gym.Env) \
-            else env_or_env_info
-        return not env_info.is_continuous_action_env() and not env_info.is_tuple_action_env()
-
-    @classmethod
-    def is_rnn_supported(self):
-        return True
-
     @property
     def latest_iteration_state(self):
         latest_iteration_state = super(QRDQN, self).latest_iteration_state
         if hasattr(self, '_quantile_dist_trainer_state'):
-            latest_iteration_state['scalar'].update({'q_loss': float(self._quantile_dist_trainer_state['q_loss'])})
+            latest_iteration_state['scalar'].update({'q_loss': self._quantile_dist_trainer_state['q_loss']})
         return latest_iteration_state
-
-    @property
-    def trainers(self):
-        return {"q_function": self._quantile_dist_trainer}
```

## nnabla_rl/algorithms/reinforce.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -20,53 +20,52 @@
 import numpy as np
 
 import nnabla as nn
 import nnabla.solvers as NS
 import nnabla_rl.environment_explorers as EE
 import nnabla_rl.model_trainers as MT
 from nnabla_rl.algorithm import Algorithm, AlgorithmConfig, eval_api
-from nnabla_rl.algorithms.common_utils import _StochasticPolicyActionSelector
-from nnabla_rl.builders import ExplorerBuilder, ModelBuilder, SolverBuilder
+from nnabla_rl.builders import ModelBuilder, SolverBuilder
 from nnabla_rl.environment_explorer import EnvironmentExplorer
 from nnabla_rl.environments.environment_info import EnvironmentInfo
 from nnabla_rl.model_trainers.model_trainer import ModelTrainer, TrainingBatch
 from nnabla_rl.models import REINFORCEContinousPolicy, REINFORCEDiscretePolicy, StochasticPolicy
 from nnabla_rl.replay_buffer import ReplayBuffer
 from nnabla_rl.utils import context
 from nnabla_rl.utils.data import marshal_experiences
 
 
 @dataclass
 class REINFORCEConfig(AlgorithmConfig):
-    """List of configurations for REINFORCE algorithm.
-
+    '''REINFORCE config
     Args:
         reward_scale (float): Scale of reward. Defaults to 0.01.
         num_rollouts_per_train_iteration (int): Number of rollout per each training iteration \
             for collecting on-policy experinces.Increasing this step size is effective to get precise parameters \
             of policy function updating, but computational time of each iteration will increase. Defaults to 10.
         learning_rate (float): Learning rate which is set to the solvers of policy function. \
             You can customize/override the learning rate for each solver by implementing the \
             (:py:class:`SolverBuilder <nnabla_rl.builders.SolverBuilder>`) by yourself. Defaults to 0.001.
         clip_grad_norm (float): Clip to the norm of gradient to this value. Defaults to 1.0.
         fixed_ln_var (float): Fixed log variance of the policy.\
             This configuration is only valid when the enviroment is continuous. Defaults to 1.0.
-    """
+    '''
     reward_scale: float = 0.01
     num_rollouts_per_train_iteration: int = 10
     learning_rate: float = 1e-3
     clip_grad_norm: float = 1.0
     # this parameter is not used in discrete environment
     fixed_ln_var: float = np.log(0.1)
 
     def __post_init__(self):
-        """__post_init__
+        '''__post_init__
 
         Check the set values are in valid range.
-        """
+
+        '''
         self._assert_positive(self.reward_scale, 'reward_scale')
         self._assert_positive(self.num_rollouts_per_train_iteration, 'num_rollouts_per_train_iteration')
         self._assert_positive(self.learning_rate, 'learning_rate')
         self._assert_positive(self.clip_grad_norm, 'clip_grad_norm')
 
 
 class DefaultPolicyBuilder(ModelBuilder[StochasticPolicy]):
@@ -99,33 +98,16 @@
     def build_solver(self,  # type: ignore[override]
                      env_info: EnvironmentInfo,
                      algorithm_config: REINFORCEConfig,
                      **kwargs) -> nn.solver.Solver:
         return NS.Adam(alpha=algorithm_config.learning_rate)
 
 
-class DefaultExplorerBuilder(ExplorerBuilder):
-    def build_explorer(self,  # type: ignore[override]
-                       env_info: EnvironmentInfo,
-                       algorithm_config: REINFORCEConfig,
-                       algorithm: "REINFORCE",
-                       **kwargs) -> EnvironmentExplorer:
-        explorer_config = EE.RawPolicyExplorerConfig(
-            reward_scalar=algorithm_config.reward_scale,
-            initial_step_num=algorithm.iteration_num,
-            timelimit_as_terminal=False
-        )
-        explorer = EE.RawPolicyExplorer(policy_action_selector=algorithm._exploration_action_selector,
-                                        env_info=env_info,
-                                        config=explorer_config)
-        return explorer
-
-
 class REINFORCE(Algorithm):
-    """Episodic REINFORCE implementation.
+    '''episodic REINFORCE implementation.
 
     This class implements the episodic REINFORCE algorithm proposed by Ronald J. Williams.
     in the paper: "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"
     For detail see: https://link.springer.com/content/pdf/10.1007/BF00992696.pdf
 
     This algorithm only supports online training.
 
@@ -135,63 +117,62 @@
             the environment to train or environment info
         config (:py:class:`REINFORCEConfig <nnabla_rl.algorithms.reinforce.REINFORCEConfig>`):
             configuration of REINFORCE algorithm
         policy_builder (:py:class:`SolverBuilder <nnabla_rl.builders.SolverBuilder>`):
             builder for policy function solvers
         policy_builder (:py:class:`ModelBuilder[StochasicPolicy] <nnabla_rl.builders.ModelBuilder>`):
             builder of policy models
-        explorer_builder (:py:class:`ExplorerBuilder <nnabla_rl.builders.ExplorerBuilder>`):
-            builder of environment explorer
-    """
+    '''
     _config: REINFORCEConfig
     _policy: StochasticPolicy
     _policy_solver: nn.solver.Solver
 
-    _explorer_builder: ExplorerBuilder
     _environment_explorer: EnvironmentExplorer
     _policy_trainer: ModelTrainer
 
-    _evaluation_actor: _StochasticPolicyActionSelector
-    _exploration_actor: _StochasticPolicyActionSelector
+    _eval_state_var: nn.Variable
+    _eval_action: nn.Variable
 
     _policy_trainer_state: Dict[str, Any]
 
     def __init__(self,
                  env_or_env_info: Union[gym.Env, EnvironmentInfo],
                  config: REINFORCEConfig = REINFORCEConfig(),
                  policy_builder: ModelBuilder[StochasticPolicy] = DefaultPolicyBuilder(),
-                 policy_solver_builder: SolverBuilder = DefaultSolverBuilder(),
-                 explorer_builder: ExplorerBuilder = DefaultExplorerBuilder()):
+                 policy_solver_builder: SolverBuilder = DefaultSolverBuilder()):
         super(REINFORCE, self).__init__(env_or_env_info, config=config)
 
-        self._explorer_builder = explorer_builder
-
         with nn.context_scope(context.get_nnabla_context(self._config.gpu_id)):
             self._policy = policy_builder("pi", self._env_info, self._config)
             self._policy_solver = policy_solver_builder(self._env_info, self._config)
 
-        self._evaluation_actor = _StochasticPolicyActionSelector(
-            self._env_info, self._policy.shallowcopy(), deterministic=False)
-        self._exploration_actor = _StochasticPolicyActionSelector(
-            self._env_info, self._policy.shallowcopy(), deterministic=False)
-
     @eval_api
-    def compute_eval_action(self, state, *, begin_of_episode=False, extra_info={}):
+    def compute_eval_action(self, s):
         with nn.context_scope(context.get_nnabla_context(self._config.gpu_id)):
-            action, _ = self._evaluation_action_selector(state, begin_of_episode=begin_of_episode)
+            action, _ = self._compute_action(s)
             return action
 
     def _before_training_start(self, env_or_buffer):
         # set context globally to ensure that the training runs on configured gpu
         context.set_nnabla_context(self._config.gpu_id)
         self._environment_explorer = self._setup_environment_explorer(env_or_buffer)
         self._policy_trainer = self._setup_policy_training(env_or_buffer)
 
     def _setup_environment_explorer(self, env_or_buffer):
-        return None if self._is_buffer(env_or_buffer) else self._explorer_builder(self._env_info, self._config, self)
+        if self._is_buffer(env_or_buffer):
+            return None
+        explorer_config = EE.RawPolicyExplorerConfig(
+            reward_scalar=self._config.reward_scale,
+            initial_step_num=self.iteration_num,
+            timelimit_as_terminal=False
+        )
+        explorer = EE.RawPolicyExplorer(policy_action_selector=self._compute_action,
+                                        env_info=self._env_info,
+                                        config=explorer_config)
+        return explorer
 
     def _setup_policy_training(self, env_or_buffer):
         policy_trainer_config = MT.policy_trainers.REINFORCEPolicyTrainerConfig(
             pi_loss_scalar=1.0 / self._config.num_rollouts_per_train_iteration,
             grad_clip_norm=self._config.clip_grad_norm)
         policy_trainer = MT.policy_trainers.REINFORCEPolicyTrainer(
             models=self._policy,
@@ -212,18 +193,17 @@
     def _run_offline_training_iteration(self, buffer):
         raise NotImplementedError
 
     def _reinforce_training(self, buffer):
         # sample all experience in the buffer
         experiences, *_ = buffer.sample(buffer.capacity)
         s_batch, a_batch, target_return = self._align_experiences_and_compute_accumulated_reward(experiences)
-        batch_size = len(s_batch)
         extra = {}
-        extra['target_return'] = np.reshape(target_return, newshape=(batch_size, 1))
-        batch = TrainingBatch(batch_size,
+        extra['target_return'] = target_return
+        batch = TrainingBatch(batch_size=len(s_batch),
                               s_current=s_batch,
                               a_current=a_batch,
                               extra=extra)
 
         self._policy_trainer_state = self._policy_trainer.train(batch)
 
     def _align_experiences_and_compute_accumulated_reward(self, experiences):
@@ -244,39 +224,34 @@
             s_batch = np.concatenate((s_batch, s_seq), axis=0)
             a_batch = np.concatenate((a_batch, a_seq), axis=0)
             accumulated_reward_batch = np.concatenate(
                 (accumulated_reward_batch, accumulated_reward))
 
         return s_batch, a_batch, accumulated_reward_batch
 
-    def _evaluation_action_selector(self, s, *, begin_of_episode=False):
-        return self._evaluation_actor(s, begin_of_episode=begin_of_episode)
-
-    def _exploration_action_selector(self, s, *, begin_of_episode=False):
-        return self._exploration_actor(s, begin_of_episode=begin_of_episode)
+    @eval_api
+    def _compute_action(self, s):
+        s = np.expand_dims(s, axis=0)
+        if not hasattr(self, '_eval_state_var'):
+            self._eval_state_var = nn.Variable(s.shape)
+            distribution = self._policy.pi(self._eval_state_var)
+            self._eval_action = distribution.sample()
+        self._eval_state_var.d = s
+        self._eval_action.forward()
+        return np.squeeze(self._eval_action.d, axis=0), {}
 
     def _models(self):
         models = {}
         models[self._policy.scope_name] = self._policy
         return models
 
     def _solvers(self):
         solvers = {}
         solvers[self._policy.scope_name] = self._policy_solver
         return solvers
 
-    @classmethod
-    def is_supported_env(cls, env_or_env_info):
-        env_info = EnvironmentInfo.from_env(env_or_env_info) if isinstance(env_or_env_info, gym.Env) \
-            else env_or_env_info
-        return not env_info.is_tuple_action_env()
-
     @property
     def latest_iteration_state(self):
         latest_iteration_state = super(REINFORCE, self).latest_iteration_state
         if hasattr(self, '_policy_trainer_state'):
-            latest_iteration_state['scalar'].update({'pi_loss': float(self._policy_trainer_state['pi_loss'])})
+            latest_iteration_state['scalar'].update({'pi_loss': self._policy_trainer_state['pi_loss']})
         return latest_iteration_state
-
-    @property
-    def trainers(self):
-        return {"policy": self._policy_trainer}
```

## nnabla_rl/algorithms/sac.py

```diff
@@ -1,47 +1,49 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023,2024 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from dataclasses import dataclass
-from typing import Any, Dict, List, Optional, Union
+from typing import Any, Dict, List, Optional, Union, cast
 
 import gym
+import numpy as np
 
 import nnabla as nn
 import nnabla.solvers as NS
 import nnabla_rl.environment_explorers as EE
 import nnabla_rl.model_trainers as MT
 from nnabla_rl.algorithm import Algorithm, AlgorithmConfig, eval_api
-from nnabla_rl.algorithms.common_utils import _StochasticPolicyActionSelector
-from nnabla_rl.builders import ExplorerBuilder, ModelBuilder, ReplayBufferBuilder, SolverBuilder
+from nnabla_rl.builders import ModelBuilder, ReplayBufferBuilder, SolverBuilder
 from nnabla_rl.environment_explorer import EnvironmentExplorer
 from nnabla_rl.environments.environment_info import EnvironmentInfo
+from nnabla_rl.exceptions import UnsupportedEnvironmentException
 from nnabla_rl.model_trainers.model_trainer import ModelTrainer, TrainingBatch
 from nnabla_rl.models import QFunction, SACPolicy, SACQFunction, StochasticPolicy
 from nnabla_rl.replay_buffer import ReplayBuffer
 from nnabla_rl.utils import context
 from nnabla_rl.utils.data import marshal_experiences
 from nnabla_rl.utils.misc import sync_model
 
 
 @dataclass
 class SACConfig(AlgorithmConfig):
-    """SACConfig List of configurations for SAC algorithm.
+    '''SACConfig
+    List of configurations for SAC algorithm
 
     Args:
         gamma (float): discount factor of rewards. Defaults to 0.99.
         learning_rate (float): learning rate which is set to all solvers. \
             You can customize/override the learning rate for each solver by implementing the \
             (:py:class:`SolverBuilder <nnabla_rl.builders.SolverBuilder>`) by yourself. \
             Defaults to 0.0003.
@@ -52,73 +54,41 @@
         target_entropy (float, optional): Target entropy value. Defaults to None.
         initial_temperature (float, optional): Initial value of temperature parameter. Defaults to None.
         fix_temperature (bool): If true the temperature parameter will not be trained. Defaults to False.
         start_timesteps (int): the timestep when training starts.\
             The algorithm will collect experiences from the environment by acting randomly until this timestep.\
             Defaults to 10000.
         replay_buffer_size (int): capacity of the replay buffer. Defaults to 1000000.
-        num_steps (int): number of steps for N-step Q targets. Defaults to 1.
-        actor_unroll_steps (int): Number of steps to unroll actor's tranining network.\
-            The network will be unrolled even though the provided model doesn't have RNN layers.\
-            Defaults to 1.
-        actor_burn_in_steps (int): Number of burn-in steps to initiaze actor's recurrent layer states during training.\
-            This flag does not take effect if given model is not an RNN model.\
-            Defaults to 0.
-        actor_reset_rnn_on_terminal (bool): Reset actor's recurrent internal states to zero during training\
-            if episode ends. This flag does not take effect if given model is not an RNN model.\
-            Defaults to False.
-        critic_unroll_steps (int): Number of steps to unroll critic's tranining network.\
-            The network will be unrolled even though the provided model doesn't have RNN layers.\
-            Defaults to 1.
-        critic_burn_in_steps (int): Number of burn-in steps to initiaze critic's recurrent layer states\
-            during training. This flag does not take effect if given model is not an RNN model.\
-            Defaults to 0.
-        critic_reset_rnn_on_terminal (bool): Reset critic's recurrent internal states to zero during training\
-            if episode ends. This flag does not take effect if given model is not an RNN model.\
-            Defaults to False.
-    """
+    '''
 
     gamma: float = 0.99
     learning_rate: float = 3.0*1e-4
     batch_size: int = 256
     tau: float = 0.005
     environment_steps: int = 1
     gradient_steps: int = 1
     target_entropy: Optional[float] = None
     initial_temperature: Optional[float] = None
     fix_temperature: bool = False
     start_timesteps: int = 10000
     replay_buffer_size: int = 1000000
-    num_steps: int = 1
-
-    # rnn model support
-    actor_unroll_steps: int = 1
-    actor_burn_in_steps: int = 0
-    actor_reset_rnn_on_terminal: bool = True
-
-    critic_unroll_steps: int = 1
-    critic_burn_in_steps: int = 0
-    critic_reset_rnn_on_terminal: bool = True
 
     def __post_init__(self):
-        """__post_init__ Check set values are in valid range."""
+        '''__post_init__
+        Check set values are in valid range.
+        '''
         self._assert_between(self.tau, 0.0, 1.0, 'tau')
         self._assert_between(self.gamma, 0.0, 1.0, 'gamma')
         self._assert_positive(self.gradient_steps, 'gradient_steps')
         self._assert_positive(self.environment_steps, 'environment_steps')
         if self.initial_temperature is not None:
             self._assert_positive(
                 self.initial_temperature, 'initial_temperature')
         self._assert_positive(self.start_timesteps, 'start_timesteps')
 
-        self._assert_positive(self.critic_unroll_steps, 'critic_unroll_steps')
-        self._assert_positive_or_zero(self.critic_burn_in_steps, 'critic_burn_in_steps')
-        self._assert_positive(self.actor_unroll_steps, 'actor_unroll_steps')
-        self._assert_positive_or_zero(self.actor_burn_in_steps, 'actor_burn_in_steps')
-
 
 class DefaultQFunctionBuilder(ModelBuilder[QFunction]):
     def build_model(self,  # type: ignore[override]
                     scope_name: str,
                     env_info: EnvironmentInfo,
                     algorithm_config: SACConfig,
                     **kwargs) -> QFunction:
@@ -146,33 +116,16 @@
     def build_replay_buffer(self,  # type: ignore[override]
                             env_info: EnvironmentInfo,
                             algorithm_config: SACConfig,
                             **kwargs) -> ReplayBuffer:
         return ReplayBuffer(capacity=algorithm_config.replay_buffer_size)
 
 
-class DefaultExplorerBuilder(ExplorerBuilder):
-    def build_explorer(self,  # type: ignore[override]
-                       env_info: EnvironmentInfo,
-                       algorithm_config: SACConfig,
-                       algorithm: "SAC",
-                       **kwargs) -> EnvironmentExplorer:
-        explorer_config = EE.RawPolicyExplorerConfig(
-            warmup_random_steps=algorithm_config.start_timesteps,
-            initial_step_num=algorithm.iteration_num,
-            timelimit_as_terminal=False
-        )
-        explorer = EE.RawPolicyExplorer(policy_action_selector=algorithm._exploration_action_selector,
-                                        env_info=env_info,
-                                        config=explorer_config)
-        return explorer
-
-
 class SAC(Algorithm):
-    """Soft Actor-Critic (SAC) algorithm implementation.
+    '''Soft Actor-Critic (SAC) algorithm implementation.
 
     This class implements the extended version of Soft Actor Critic (SAC) algorithm
     proposed by T. Haarnoja, et al. in the paper: "Soft Actor-Critic Algorithms and Applications"
     For detail see: https://arxiv.org/abs/1812.05905
 
     This algorithm is slightly differs from the implementation of Soft Actor-Critic algorithm presented
     also by T. Haarnoja, et al. in the following paper:  https://arxiv.org/abs/1801.01290
@@ -180,138 +133,135 @@
     The temperature parameter is adjusted automatically instead of providing reward scalar as a
     hyper parameter.
 
     Args:
         env_or_env_info \
         (gym.Env or :py:class:`EnvironmentInfo <nnabla_rl.environments.environment_info.EnvironmentInfo>`):
             the environment to train or environment info
-        config (:py:class:`SACConfig <nnabla_rl.algorithms.sac.SACConfig>`): configuration of the SAC algorithm
+        config (:py:class:`SACConfig <nnabla_rl.algorithms.sac.ICML2018SACConfig>`): configuration of the SAC algorithm
         q_function_builder (:py:class:`ModelBuilder[QFunction] <nnabla_rl.builders.ModelBuilder>`):
             builder of q function models
         q_solver_builder (:py:class:`SolverBuilder <nnabla_rl.builders.SolverBuilder>`):
             builder of q function solvers
         policy_builder (:py:class:`ModelBuilder[StochasticPolicy] <nnabla_rl.builders.ModelBuilder>`):
             builder of actor models
         policy_solver_builder (:py:class:`SolverBuilder <nnabla_rl.builders.SolverBuilder>`):
             builder of policy solvers
         temperature_solver_builder (:py:class:`SolverBuilder <nnabla_rl.builders.SolverBuilder>`):
             builder of temperature solvers
         replay_buffer_builder (:py:class:`ReplayBufferBuilder <nnabla_rl.builders.ReplayBufferBuilder>`):
             builder of replay_buffer
-        explorer_builder (:py:class:`ExplorerBuilder <nnabla_rl.builders.ExplorerBuilder>`):
-            builder of environment explorer
-    """
+    '''
 
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _config: SACConfig
+    _q1: QFunction
+    _q2: QFunction
     _train_q_functions: List[QFunction]
     _train_q_solvers: Dict[str, nn.solver.Solver]
     _target_q_functions: List[QFunction]
 
     _pi: StochasticPolicy
     _temperature: MT.policy_trainers.soft_policy_trainer.AdjustableTemperature
     _temperature_solver: Optional[nn.solver.Solver]
     _replay_buffer: ReplayBuffer
 
-    _explorer_builder: ExplorerBuilder
     _environment_explorer: EnvironmentExplorer
     _policy_trainer: ModelTrainer
     _q_function_trainer: ModelTrainer
 
+    _eval_state_var: nn.Variable
+    _eval_deterministic_action: nn.Variable
+    _eval_probabilistic_action: nn.Variable
+
     _policy_trainer_state: Dict[str, Any]
     _q_function_trainer_state: Dict[str, Any]
 
     def __init__(self, env_or_env_info: Union[gym.Env, EnvironmentInfo],
                  config: SACConfig = SACConfig(),
                  q_function_builder: ModelBuilder[QFunction] = DefaultQFunctionBuilder(),
                  q_solver_builder: SolverBuilder = DefaultSolverBuilder(),
                  policy_builder: ModelBuilder[StochasticPolicy] = DefaultPolicyBuilder(),
                  policy_solver_builder: SolverBuilder = DefaultSolverBuilder(),
                  temperature_solver_builder: SolverBuilder = DefaultSolverBuilder(),
-                 replay_buffer_builder: ReplayBufferBuilder = DefaultReplayBufferBuilder(),
-                 explorer_builder: ExplorerBuilder = DefaultExplorerBuilder()):
+                 replay_buffer_builder: ReplayBufferBuilder = DefaultReplayBufferBuilder()):
         super(SAC, self).__init__(env_or_env_info, config=config)
-
-        self._explorer_builder = explorer_builder
+        if self._env_info.is_discrete_action_env():
+            raise UnsupportedEnvironmentException
 
         with nn.context_scope(context.get_nnabla_context(self._config.gpu_id)):
-            self._train_q_functions = self._build_q_functions(q_function_builder)
+            self._q1 = q_function_builder(scope_name="q1", env_info=self._env_info, algorithm_config=self._config)
+            self._q2 = q_function_builder(scope_name="q2", env_info=self._env_info, algorithm_config=self._config)
+            self._train_q_functions = [self._q1, self._q2]
             self._train_q_solvers = {q.scope_name: q_solver_builder(self._env_info, self._config)
                                      for q in self._train_q_functions}
-            self._target_q_functions = [q.deepcopy('target_' + q.scope_name) for q in self._train_q_functions]
+            self._target_q_functions = [cast(QFunction, q.deepcopy('target_' + q.scope_name))
+                                        for q in self._train_q_functions]
 
             self._pi = policy_builder(scope_name="pi", env_info=self._env_info, algorithm_config=self._config)
             self._pi_solver = policy_solver_builder(self._env_info, self._config)
 
-            self._temperature = self._setup_temperature_model()
+            self._temperature = MT.policy_trainers.soft_policy_trainer.AdjustableTemperature(
+                scope_name='temperature',
+                initial_value=self._config.initial_temperature)
             if not self._config.fix_temperature:
                 self._temperature_solver = temperature_solver_builder(self._env_info, self._config)
             else:
                 self._temperature_solver = None
 
             self._replay_buffer = replay_buffer_builder(self._env_info, self._config)
 
-        self._evaluation_actor = self._setup_evaluation_actor()
-        self._exploration_actor = self._setup_exploration_actor()
-
     @eval_api
-    def compute_eval_action(self, state, *, begin_of_episode=False, extra_info={}):
+    def compute_eval_action(self, state):
         with nn.context_scope(context.get_nnabla_context(self._config.gpu_id)):
-            action, _ = self._evaluation_action_selector(state, begin_of_episode=begin_of_episode)
+            action, _ = self._compute_greedy_action(state, deterministic=True)
             return action
 
     def _before_training_start(self, env_or_buffer):
         # set context globally to ensure that the training runs on configured gpu
         context.set_nnabla_context(self._config.gpu_id)
         self._environment_explorer = self._setup_environment_explorer(env_or_buffer)
         self._policy_trainer = self._setup_policy_training(env_or_buffer)
-        self._q_function_trainer = self._setup_q_function_training(env_or_buffer)
-
-    def _setup_evaluation_actor(self):
-        return _StochasticPolicyActionSelector(self._env_info, self._pi.shallowcopy(), deterministic=True)
-
-    def _setup_exploration_actor(self):
-        return _StochasticPolicyActionSelector(self._env_info, self._pi.shallowcopy(), deterministic=False)
+        self._q_function_trainer = self._setup_q_function_training(
+            env_or_buffer)
 
     def _setup_environment_explorer(self, env_or_buffer):
-        return None if self._is_buffer(env_or_buffer) else self._explorer_builder(self._env_info, self._config, self)
-
-    def _setup_temperature_model(self):
-        return MT.policy_trainers.soft_policy_trainer.AdjustableTemperature(
-            scope_name='temperature',
-            initial_value=self._config.initial_temperature)
+        if self._is_buffer(env_or_buffer):
+            return None
+        explorer_config = EE.RawPolicyExplorerConfig(
+            warmup_random_steps=self._config.start_timesteps,
+            initial_step_num=self.iteration_num,
+            timelimit_as_terminal=False
+        )
+        explorer = EE.RawPolicyExplorer(policy_action_selector=self._compute_greedy_action,
+                                        env_info=self._env_info,
+                                        config=explorer_config)
+        return explorer
 
     def _setup_policy_training(self, env_or_buffer):
         policy_trainer_config = MT.policy_trainers.SoftPolicyTrainerConfig(
             fixed_temperature=self._config.fix_temperature,
-            target_entropy=self._config.target_entropy,
-            unroll_steps=self._config.actor_unroll_steps,
-            burn_in_steps=self._config.actor_burn_in_steps,
-            reset_on_terminal=self._config.actor_reset_rnn_on_terminal)
+            target_entropy=self._config.target_entropy)
         policy_trainer = MT.policy_trainers.SoftPolicyTrainer(
             models=self._pi,
             solvers={self._pi.scope_name: self._pi_solver},
             temperature=self._temperature,
             temperature_solver=self._temperature_solver,
-            q_functions=self._train_q_functions,
+            q_functions=[self._q1, self._q2],
             env_info=self._env_info,
             config=policy_trainer_config)
         return policy_trainer
 
     def _setup_q_function_training(self, env_or_buffer):
         # training input/loss variables
         q_function_trainer_config = MT.q_value_trainers.SoftQTrainerConfig(
             reduction_method='mean',
-            grad_clip=None,
-            num_steps=self._config.num_steps,
-            unroll_steps=self._config.critic_unroll_steps,
-            burn_in_steps=self._config.critic_burn_in_steps,
-            reset_on_terminal=self._config.critic_reset_rnn_on_terminal)
+            grad_clip=None)
 
         q_function_trainer = MT.q_value_trainers.SoftQTrainer(
             train_functions=self._train_q_functions,
             solvers=self._train_q_solvers,
             target_functions=self._target_q_functions,
             target_policy=self._pi,
             temperature=self._policy_trainer.get_temperature(),
@@ -335,87 +285,65 @@
         self._replay_buffer.append_all(experiences)
 
     def _run_gradient_step(self, replay_buffer):
         if self._config.start_timesteps < self.iteration_num:
             self._sac_training(replay_buffer)
 
     def _sac_training(self, replay_buffer):
-        actor_steps = self._config.actor_burn_in_steps + self._config.actor_unroll_steps
-        critic_steps = self._config.num_steps + self._config.critic_burn_in_steps + self._config.critic_unroll_steps - 1
-        num_steps = max(actor_steps, critic_steps)
-        experiences_tuple, info = replay_buffer.sample(self._config.batch_size, num_steps=num_steps)
-        if num_steps == 1:
-            experiences_tuple = (experiences_tuple, )
-        assert len(experiences_tuple) == num_steps
-
-        batch = None
-        for experiences in reversed(experiences_tuple):
-            (s, a, r, non_terminal, s_next, rnn_states_dict, *_) = marshal_experiences(experiences)
-            rnn_states = rnn_states_dict['rnn_states'] if 'rnn_states' in rnn_states_dict else {}
-            batch = TrainingBatch(batch_size=self._config.batch_size,
-                                  s_current=s,
-                                  a_current=a,
-                                  gamma=self._config.gamma,
-                                  reward=r,
-                                  non_terminal=non_terminal,
-                                  s_next=s_next,
-                                  weight=info['weights'],
-                                  next_step_batch=batch,
-                                  rnn_states=rnn_states)
+        experiences, info = replay_buffer.sample(self._config.batch_size)
+        (s, a, r, non_terminal, s_next, *_) = marshal_experiences(experiences)
+        batch = TrainingBatch(batch_size=self._config.batch_size,
+                              s_current=s,
+                              a_current=a,
+                              gamma=self._config.gamma,
+                              reward=r,
+                              non_terminal=non_terminal,
+                              s_next=s_next,
+                              weight=info['weights'])
 
         self._q_function_trainer_state = self._q_function_trainer.train(batch)
         for q, target_q in zip(self._train_q_functions, self._target_q_functions):
             sync_model(q, target_q, tau=self._config.tau)
         self._policy_trainer_state = self._policy_trainer.train(batch)
 
-        td_errors = self._q_function_trainer_state['td_errors']
+        td_errors = np.abs(self._q_function_trainer_state['td_errors'])
         replay_buffer.update_priorities(td_errors)
 
-    def _evaluation_action_selector(self, s, *, begin_of_episode=False):
-        return self._evaluation_actor(s, begin_of_episode=begin_of_episode)
-
-    def _exploration_action_selector(self, s, *, begin_of_episode=False):
-        return self._exploration_actor(s, begin_of_episode=begin_of_episode)
-
-    def _build_q_functions(self, q_function_builder):
-        q_functions = []
-        for i in range(2):
-            q = q_function_builder(scope_name=f"q{i+1}", env_info=self._env_info, algorithm_config=self._config)
-            q_functions.append(q)
-        return q_functions
+    @eval_api
+    def _compute_greedy_action(self, s, deterministic=False):
+        # evaluation input/action variables
+        s = np.expand_dims(s, axis=0)
+        if not hasattr(self, '_eval_state_var'):
+            self._eval_state_var = nn.Variable(s.shape)
+            distribution = self._pi.pi(self._eval_state_var)
+            self._eval_deterministic_action = distribution.choose_probable()
+            self._eval_probabilistic_action = distribution.sample()
+        self._eval_state_var.d = s
+        if deterministic:
+            self._eval_deterministic_action.forward()
+            return np.squeeze(self._eval_deterministic_action.d, axis=0), {}
+        else:
+            self._eval_probabilistic_action.forward()
+            return np.squeeze(self._eval_probabilistic_action.d, axis=0), {}
 
     def _models(self):
-        models = [*self._train_q_functions, self._pi, self._temperature]
+        models = [self._q1, self._q2, self._pi, self._temperature]
         return {model.scope_name: model for model in models}
 
     def _solvers(self):
         solvers = {}
         solvers[self._pi.scope_name] = self._pi_solver
         solvers.update(self._train_q_solvers)
         if self._temperature_solver is not None:
             solvers[self._temperature.scope_name] = self._temperature_solver
         return solvers
 
-    @classmethod
-    def is_rnn_supported(self):
-        return True
-
-    @classmethod
-    def is_supported_env(cls, env_or_env_info):
-        env_info = EnvironmentInfo.from_env(env_or_env_info) if isinstance(env_or_env_info, gym.Env) \
-            else env_or_env_info
-        return not env_info.is_discrete_action_env() and not env_info.is_tuple_action_env()
-
     @property
     def latest_iteration_state(self):
         latest_iteration_state = super(SAC, self).latest_iteration_state
         if hasattr(self, '_policy_trainer_state'):
-            latest_iteration_state['scalar'].update({'pi_loss': float(self._policy_trainer_state['pi_loss'])})
+            latest_iteration_state['scalar'].update({'pi_loss': self._policy_trainer_state['pi_loss']})
         if hasattr(self, '_q_function_trainer_state'):
-            latest_iteration_state['scalar'].update({'q_loss': float(self._q_function_trainer_state['q_loss'])})
+            latest_iteration_state['scalar'].update({'q_loss': self._q_function_trainer_state['q_loss']})
             latest_iteration_state['histogram'].update(
                 {'td_errors': self._q_function_trainer_state['td_errors'].flatten()})
         return latest_iteration_state
-
-    @property
-    def trainers(self):
-        return {"q_function": self._q_function_trainer, "policy": self._policy_trainer}
```

## nnabla_rl/algorithms/td3.py

```diff
@@ -1,47 +1,49 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from dataclasses import dataclass
-from typing import Any, Dict, List, Union
+from typing import Any, Dict, List, Union, cast
 
 import gym
+import numpy as np
 
 import nnabla as nn
 import nnabla.solvers as NS
 import nnabla_rl.environment_explorers as EE
 import nnabla_rl.model_trainers as MT
 from nnabla_rl.algorithm import Algorithm, AlgorithmConfig, eval_api
-from nnabla_rl.algorithms.common_utils import _DeterministicPolicyActionSelector
-from nnabla_rl.builders import ExplorerBuilder, ModelBuilder, ReplayBufferBuilder, SolverBuilder
+from nnabla_rl.builders import ModelBuilder, ReplayBufferBuilder, SolverBuilder
 from nnabla_rl.environment_explorer import EnvironmentExplorer
 from nnabla_rl.environments.environment_info import EnvironmentInfo
+from nnabla_rl.exceptions import UnsupportedEnvironmentException
 from nnabla_rl.model_trainers.model_trainer import ModelTrainer, TrainingBatch
 from nnabla_rl.models import DeterministicPolicy, QFunction, TD3Policy, TD3QFunction
 from nnabla_rl.replay_buffer import ReplayBuffer
 from nnabla_rl.utils import context
 from nnabla_rl.utils.data import marshal_experiences
 from nnabla_rl.utils.misc import sync_model
 
 
 @dataclass
 class TD3Config(AlgorithmConfig):
-    """TD3Config List of configurations for TD3 algorithm.
+    '''TD3Config
+    List of configurations for TD3 algorithm
 
     Args:
         gamma (float): discount factor of rewards. Defaults to 0.99.
         learning_rate (float): learning rate which is set to all solvers. \
             You can customize/override the learning rate for each solver by implementing the \
             (:py:class:`SolverBuilder <nnabla_rl.builders.SolverBuilder>`) by yourself. \
             Defaults to 0.003.
@@ -52,76 +54,43 @@
             Defaults to 10000.
         replay_buffer_size (int): capacity of the replay buffer. Defaults to 1000000.
         d (int): Interval of the policy update. The policy will be updated every d q-function updates. Defaults to 2.
         exploration_noise_sigma (float): Standard deviation of the gaussian exploration noise. Defaults to 0.1.
         train_action_noise_sigma (float): Standard deviation of the gaussian action noise used in the training.\
             Defaults to 0.2.
         train_action_noise_abs (float): Absolute limit value of action noise used in the training. Defaults to 0.5.
-        num_steps (int): number of steps for N-step Q targets. Defaults to 1.
-        actor_unroll_steps (int): Number of steps to unroll actor's tranining network.\
-            The network will be unrolled even though the provided model doesn't have RNN layers.\
-            Defaults to 1.
-        actor_burn_in_steps (int): Number of burn-in steps to initiaze actor's recurrent layer states during training.\
-            This flag does not take effect if given model is not an RNN model.\
-            Defaults to 0.
-        actor_reset_rnn_on_terminal (bool): Reset actor's recurrent internal states to zero during training\
-            if episode ends. This flag does not take effect if given model is not an RNN model.\
-            Defaults to False.
-        critic_unroll_steps (int): Number of steps to unroll critic's tranining network.\
-            The network will be unrolled even though the provided model doesn't have RNN layers.\
-            Defaults to 1.
-        critic_burn_in_steps (int): Number of burn-in steps to initiaze critic's recurrent layer states\
-            during training. This flag does not take effect if given model is not an RNN model.\
-            Defaults to 0.
-        critic_reset_rnn_on_terminal (bool): Reset critic's recurrent internal states to zero during training\
-            if episode ends. This flag does not take effect if given model is not an RNN model.\
-            Defaults to False.
-    """
+    '''
 
     gamma: float = 0.99
     learning_rate: float = 1.0*1e-3
     batch_size: int = 100
     tau: float = 0.005
     start_timesteps: int = 10000
     replay_buffer_size: int = 1000000
     d: int = 2
     exploration_noise_sigma: float = 0.1
     train_action_noise_sigma: float = 0.2
     train_action_noise_abs: float = 0.5
-    num_steps: int = 1
-
-    # rnn model support
-    actor_unroll_steps: int = 1
-    actor_burn_in_steps: int = 0
-    actor_reset_rnn_on_terminal: bool = True
-
-    critic_unroll_steps: int = 1
-    critic_burn_in_steps: int = 0
-    critic_reset_rnn_on_terminal: bool = True
 
     def __post_init__(self):
-        """__post_init__
+        '''__post_init__
 
         Check the set values are in valid range.
-        """
+
+        '''
         self._assert_between(self.gamma, 0.0, 1.0, 'gamma')
         self._assert_positive(self.batch_size, 'batch_size')
         self._assert_between(self.tau, 0.0, 1.0, 'tau')
         self._assert_positive(self.start_timesteps, 'start_timesteps')
         self._assert_positive(self.replay_buffer_size, 'replay_buffer_size')
         self._assert_positive(self.d, 'd')
         self._assert_positive(self.exploration_noise_sigma, 'exploration_noise_sigma')
         self._assert_positive(self.train_action_noise_sigma, 'train_action_noise_sigma')
         self._assert_positive(self.train_action_noise_abs, 'train_action_noise_abs')
 
-        self._assert_positive(self.critic_unroll_steps, 'critic_unroll_steps')
-        self._assert_positive_or_zero(self.critic_burn_in_steps, 'critic_burn_in_steps')
-        self._assert_positive(self.actor_unroll_steps, 'actor_unroll_steps')
-        self._assert_positive_or_zero(self.actor_burn_in_steps, 'actor_burn_in_steps')
-
 
 class DefaultCriticBuilder(ModelBuilder[QFunction]):
     def build_model(self,  # type: ignore[override]
                     scope_name: str,
                     env_info: EnvironmentInfo,
                     algorithm_config: TD3Config,
                     **kwargs) -> QFunction:
@@ -131,15 +100,15 @@
 
 class DefaultActorBuilder(ModelBuilder[DeterministicPolicy]):
     def build_model(self,  # type: ignore[override]
                     scope_name: str,
                     env_info: EnvironmentInfo,
                     algorithm_config: TD3Config,
                     **kwargs) -> DeterministicPolicy:
-        max_action_value = float(env_info.action_high[0])
+        max_action_value = float(env_info.action_space.high[0])
         return TD3Policy(scope_name, env_info.action_dim, max_action_value=max_action_value)
 
 
 class DefaultSolverBuilder(SolverBuilder):
     def build_solver(self,  # type: ignore[override]
                      env_info: EnvironmentInfo,
                      algorithm_config: TD3Config,
@@ -151,36 +120,16 @@
     def build_replay_buffer(self,  # type: ignore[override]
                             env_info: EnvironmentInfo,
                             algorithm_config: TD3Config,
                             **kwargs) -> ReplayBuffer:
         return ReplayBuffer(capacity=algorithm_config.replay_buffer_size)
 
 
-class DefaultExplorerBuilder(ExplorerBuilder):
-    def build_explorer(self,  # type: ignore[override]
-                       env_info: EnvironmentInfo,
-                       algorithm_config: TD3Config,
-                       algorithm: "TD3",
-                       **kwargs) -> EnvironmentExplorer:
-        explorer_config = EE.GaussianExplorerConfig(
-            warmup_random_steps=algorithm_config.start_timesteps,
-            initial_step_num=algorithm.iteration_num,
-            timelimit_as_terminal=False,
-            action_clip_low=env_info.action_low,
-            action_clip_high=env_info.action_high,
-            sigma=algorithm_config.exploration_noise_sigma
-        )
-        explorer = EE.GaussianExplorer(policy_action_selector=algorithm._exploration_action_selector,
-                                       env_info=env_info,
-                                       config=explorer_config)
-        return explorer
-
-
 class TD3(Algorithm):
-    """Twin Delayed Deep Deterministic policy gradient (TD3) algorithm.
+    '''Twin Delayed Deep Deterministic policy gradient (TD3) algorithm.
 
     This class implements the Twin Delayed Deep Deteministic policy gradien (TD3) algorithm
     proposed by S. Fujimoto, et al. in the paper: "Addressing Function Approximation Error in Actor-Critic Methods"
     For detail see: https://arxiv.org/abs/1802.09477
 
     Args:
         env_or_env_info \
@@ -194,17 +143,15 @@
             builder of critic solvers
         actor_builder (:py:class:`ModelBuilder[DeterministicPolicy] <nnabla_rl.builders.ModelBuilder>`):
             builder of actor models
         actor_solver_builder (:py:class:`SolverBuilder <nnabla_rl.builders.SolverBuilder>`):
             builder of actor solvers
         replay_buffer_builder (:py:class:`ReplayBufferBuilder <nnabla_rl.builders.ReplayBufferBuilder>`):
             builder of replay_buffer
-        explorer_builder (:py:class:`ExplorerBuilder <nnabla_rl.builders.ExplorerBuilder>`):
-            builder of environment explorer
-    """
+    '''
 
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _config: TD3Config
     _q1: QFunction
     _q2: QFunction
@@ -212,15 +159,14 @@
     _train_q_solvers: Dict[str, QFunction]
     _target_q_functions: List[QFunction]
     _pi: DeterministicPolicy
     _pi_solver: nn.solvers.Solver
     _target_pi: DeterministicPolicy
     _replay_buffer: ReplayBuffer
 
-    _explorer_builder: ExplorerBuilder
     _environment_explorer: EnvironmentExplorer
     _policy_trainer: ModelTrainer
     _q_function_trainer: ModelTrainer
 
     _eval_state_var: nn.Variable
     _eval_action: nn.Variable
 
@@ -229,80 +175,83 @@
 
     def __init__(self, env_or_env_info: Union[gym.Env, EnvironmentInfo],
                  config: TD3Config = TD3Config(),
                  critic_builder: ModelBuilder[QFunction] = DefaultCriticBuilder(),
                  critic_solver_builder: SolverBuilder = DefaultSolverBuilder(),
                  actor_builder: ModelBuilder[DeterministicPolicy] = DefaultActorBuilder(),
                  actor_solver_builder: SolverBuilder = DefaultSolverBuilder(),
-                 replay_buffer_builder: ReplayBufferBuilder = DefaultReplayBufferBuilder(),
-                 explorer_builder: ExplorerBuilder = DefaultExplorerBuilder()):
+                 replay_buffer_builder: ReplayBufferBuilder = DefaultReplayBufferBuilder()):
         super(TD3, self).__init__(env_or_env_info, config=config)
-
-        self._explorer_builder = explorer_builder
+        if self._env_info.is_discrete_action_env():
+            raise UnsupportedEnvironmentException
 
         with nn.context_scope(context.get_nnabla_context(self._config.gpu_id)):
             self._q1 = critic_builder(scope_name="q1", env_info=self._env_info, algorithm_config=self._config)
             self._q2 = critic_builder(scope_name="q2", env_info=self._env_info, algorithm_config=self._config)
             self._train_q_functions = [self._q1, self._q2]
             self._train_q_solvers = {q.scope_name: critic_solver_builder(
                 env_info=self._env_info, algorithm_config=self._config) for q in self._train_q_functions}
-            self._target_q_functions = [q.deepcopy('target_' + q.scope_name) for q in self._train_q_functions]
+            self._target_q_functions = [cast(QFunction, q.deepcopy('target_' + q.scope_name))
+                                        for q in self._train_q_functions]
 
             self._pi = actor_builder(scope_name="pi", env_info=self._env_info, algorithm_config=self._config)
             self._pi_solver = actor_solver_builder(env_info=self._env_info, algorithm_config=self._config)
-            self._target_pi = self._pi.deepcopy('target_' + self._pi.scope_name)
+            self._target_pi = cast(DeterministicPolicy, self._pi.deepcopy('target_' + self._pi.scope_name))
 
             self._replay_buffer = replay_buffer_builder(env_info=self._env_info, algorithm_config=self._config)
 
-        self._evaluation_actor = _DeterministicPolicyActionSelector(self._env_info, self._pi.shallowcopy())
-        self._exploration_actor = _DeterministicPolicyActionSelector(self._env_info, self._pi.shallowcopy())
-
     @eval_api
-    def compute_eval_action(self, state, *, begin_of_episode=False, extra_info={}):
+    def compute_eval_action(self, state):
         with nn.context_scope(context.get_nnabla_context(self._config.gpu_id)):
-            action, _ = self._evaluation_action_selector(state, begin_of_episode=begin_of_episode)
+            action, _ = self._compute_greedy_action(state)
             return action
 
     def _before_training_start(self, env_or_buffer):
         # set context globally to ensure that the training runs on configured gpu
         context.set_nnabla_context(self._config.gpu_id)
         self._environment_explorer = self._setup_environment_explorer(env_or_buffer)
         self._q_function_trainer = self._setup_q_function_training(env_or_buffer)
         self._policy_trainer = self._setup_policy_training(env_or_buffer)
 
     def _setup_environment_explorer(self, env_or_buffer):
-        return None if self._is_buffer(env_or_buffer) else self._explorer_builder(self._env_info, self._config, self)
+        if self._is_buffer(env_or_buffer):
+            return None
+        explorer_config = EE.GaussianExplorerConfig(
+            warmup_random_steps=self._config.start_timesteps,
+            initial_step_num=self.iteration_num,
+            timelimit_as_terminal=False,
+            action_clip_low=self._env_info.action_space.low,
+            action_clip_high=self._env_info.action_space.high,
+            sigma=self._config.exploration_noise_sigma
+        )
+        explorer = EE.GaussianExplorer(policy_action_selector=self._compute_greedy_action,
+                                       env_info=self._env_info,
+                                       config=explorer_config)
+        return explorer
 
     def _setup_q_function_training(self, env_or_buffer):
         # training input/loss variables
         q_function_trainer_config = MT.q_value_trainers.TD3QTrainerConfig(
             reduction_method='mean',
             grad_clip=None,
             train_action_noise_sigma=self._config.train_action_noise_sigma,
-            train_action_noise_abs=self._config.train_action_noise_abs,
-            num_steps=self._config.num_steps,
-            unroll_steps=self._config.critic_unroll_steps,
-            burn_in_steps=self._config.critic_burn_in_steps,
-            reset_on_terminal=self._config.critic_reset_rnn_on_terminal)
+            train_action_noise_abs=self._config.train_action_noise_abs)
         q_function_trainer = MT.q_value_trainers.TD3QTrainer(
             train_functions=self._train_q_functions,
             solvers=self._train_q_solvers,
             target_functions=self._target_q_functions,
             target_policy=self._target_pi,
             env_info=self._env_info,
             config=q_function_trainer_config)
         for q, target_q in zip(self._train_q_functions, self._target_q_functions):
             sync_model(q, target_q)
         return q_function_trainer
 
     def _setup_policy_training(self, env_or_buffer):
-        policy_trainer_config = MT.policy_trainers.DPGPolicyTrainerConfig(
-            unroll_steps=self._config.actor_unroll_steps,
-            burn_in_steps=self._config.actor_burn_in_steps,
-            reset_on_terminal=self._config.actor_reset_rnn_on_terminal)
+        policy_trainer_config = MT.policy_trainers.DPGPolicyTrainerConfig()
         policy_trainer = MT.policy_trainers.DPGPolicyTrainer(
             models=self._pi,
             solvers={self._pi.scope_name: self._pi_solver},
             q_function=self._q1,
             env_info=self._env_info,
             config=policy_trainer_config)
         sync_model(self._pi, self._target_pi, 1.0)
@@ -316,55 +265,47 @@
         if self._config.start_timesteps < self.iteration_num:
             self._td3_training(self._replay_buffer)
 
     def _run_offline_training_iteration(self, buffer):
         self._td3_training(buffer)
 
     def _td3_training(self, replay_buffer):
-        actor_steps = self._config.actor_burn_in_steps + self._config.actor_unroll_steps
-        critic_steps = self._config.num_steps + self._config.critic_burn_in_steps + self._config.critic_unroll_steps - 1
-        num_steps = max(actor_steps, critic_steps)
-        experiences_tuple, info = replay_buffer.sample(self._config.batch_size, num_steps=num_steps)
-        if num_steps == 1:
-            experiences_tuple = (experiences_tuple, )
-        assert len(experiences_tuple) == num_steps
-
-        batch = None
-        for experiences in reversed(experiences_tuple):
-            (s, a, r, non_terminal, s_next, rnn_states_dict, *_) = marshal_experiences(experiences)
-            rnn_states = rnn_states_dict['rnn_states'] if 'rnn_states' in rnn_states_dict else {}
-            batch = TrainingBatch(batch_size=self._config.batch_size,
-                                  s_current=s,
-                                  a_current=a,
-                                  gamma=self._config.gamma,
-                                  reward=r,
-                                  non_terminal=non_terminal,
-                                  s_next=s_next,
-                                  weight=info['weights'],
-                                  next_step_batch=batch,
-                                  rnn_states=rnn_states)
+        experiences, info = replay_buffer.sample(self._config.batch_size)
+        (s, a, r, non_terminal, s_next, *_) = marshal_experiences(experiences)
+        batch = TrainingBatch(batch_size=self._config.batch_size,
+                              s_current=s,
+                              a_current=a,
+                              gamma=self._config.gamma,
+                              reward=r,
+                              non_terminal=non_terminal,
+                              s_next=s_next,
+                              weight=info['weights'])
 
         self._q_function_trainer_state = self._q_function_trainer.train(batch)
-        td_errors = self._q_function_trainer_state['td_errors']
+        td_errors = np.abs(self._q_function_trainer_state['td_errors'])
         replay_buffer.update_priorities(td_errors)
 
         if self.iteration_num % self._config.d == 0:
             # Optimize actor
             self._policy_trainer_state = self._policy_trainer.train(batch)
 
             # parameter update
             for q, target_q in zip(self._train_q_functions, self._target_q_functions):
                 sync_model(q, target_q, tau=self._config.tau)
             sync_model(self._pi, self._target_pi, tau=self._config.tau)
 
-    def _evaluation_action_selector(self, s, *, begin_of_episode=False):
-        return self._evaluation_actor(s, begin_of_episode=begin_of_episode)
-
-    def _exploration_action_selector(self, s, *, begin_of_episode=False):
-        return self._exploration_actor(s, begin_of_episode=begin_of_episode)
+    @eval_api
+    def _compute_greedy_action(self, s):
+        s = np.expand_dims(s, axis=0)
+        if not hasattr(self, '_eval_state_var'):
+            self._eval_state_var = nn.Variable(s.shape)
+            self._eval_action = self._pi.pi(self._eval_state_var)
+        self._eval_state_var.d = s
+        self._eval_action.forward()
+        return np.squeeze(self._eval_action.d, axis=0), {}
 
     def _models(self):
         models = {}
         models[self._q1.scope_name] = self._q1
         models[self._q2.scope_name] = self._q2
         models[self._pi.scope_name] = self._pi
         models[self._target_pi.scope_name] = self._target_pi
@@ -372,30 +313,17 @@
 
     def _solvers(self):
         solvers = {}
         solvers[self._pi.scope_name] = self._pi_solver
         solvers.update(self._train_q_solvers)
         return solvers
 
-    @classmethod
-    def is_rnn_supported(self):
-        return True
-
-    @classmethod
-    def is_supported_env(cls, env_or_env_info):
-        env_info = EnvironmentInfo.from_env(env_or_env_info) if isinstance(env_or_env_info, gym.Env) \
-            else env_or_env_info
-        return not env_info.is_discrete_action_env() and not env_info.is_tuple_action_env()
-
     @property
     def latest_iteration_state(self):
         latest_iteration_state = super(TD3, self).latest_iteration_state
         if hasattr(self, '_policy_trainer_state'):
-            latest_iteration_state['scalar'].update({'pi_loss': float(self._policy_trainer_state['pi_loss'])})
+            latest_iteration_state['scalar'].update({'pi_loss': self._policy_trainer_state['pi_loss']})
         if hasattr(self, '_q_function_trainer_state'):
-            latest_iteration_state['scalar'].update({'q_loss': float(self._q_function_trainer_state['q_loss'])})
+            latest_iteration_state['scalar'].update({'q_loss': self._q_function_trainer_state['q_loss']})
             latest_iteration_state['histogram'].update(
                 {'td_errors': self._q_function_trainer_state['td_errors'].flatten()})
         return latest_iteration_state
-
-    def trainers(self):
-        return {"q_function": self._q_function_trainer, "policy": self._policy_trainer}
```

## nnabla_rl/algorithms/trpo.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -21,32 +21,31 @@
 
 import nnabla as nn
 import nnabla.solvers as NS
 import nnabla_rl.environment_explorers as EE
 import nnabla_rl.model_trainers as MT
 import nnabla_rl.preprocessors as RP
 from nnabla_rl.algorithm import Algorithm, AlgorithmConfig, eval_api
-from nnabla_rl.algorithms.common_utils import (_StatePreprocessedStochasticPolicy, _StatePreprocessedVFunction,
-                                               _StochasticPolicyActionSelector, compute_v_target_and_advantage)
-from nnabla_rl.builders import ExplorerBuilder, ModelBuilder, PreprocessorBuilder, SolverBuilder
+from nnabla_rl.algorithms.common_utils import (_StatePreprocessedPolicy, _StatePreprocessedVFunction,
+                                               compute_v_target_and_advantage)
+from nnabla_rl.builders import ModelBuilder, PreprocessorBuilder, SolverBuilder
 from nnabla_rl.environment_explorer import EnvironmentExplorer
 from nnabla_rl.environments.environment_info import EnvironmentInfo
 from nnabla_rl.model_trainers.model_trainer import ModelTrainer, TrainingBatch
 from nnabla_rl.models import Model, StochasticPolicy, TRPOPolicy, TRPOVFunction, VFunction
 from nnabla_rl.preprocessors import Preprocessor
 from nnabla_rl.replay_buffer import ReplayBuffer
 from nnabla_rl.replay_buffers.buffer_iterator import BufferIterator
 from nnabla_rl.utils import context
 from nnabla_rl.utils.data import marshal_experiences
 
 
 @dataclass
 class TRPOConfig(AlgorithmConfig):
-    """List of configurations for TRPO algorithm.
-
+    '''TRPO config
     Args:
         gamma (float): Discount factor of rewards. Defaults to 0.995.
         lmb (float): Scalar of lambda return's computation in GAE. Defaults to 0.97.\
             This configuration is related to bias and variance of estimated value. \
             If it is close to 0, estimated value is low-variance but biased.\
             If it is close to 1, estimated value is unbiased but high-variance.
         num_steps_per_iteration (int): Number of steps per each training iteration for collecting on-policy experinces.\
@@ -66,15 +65,15 @@
             (:py:class:`SolverBuilder <nnabla_rl.builders.SolverBuilder>`) by yourself. \
             Defaults to 0.001.
         preprocess_state (bool): Enable preprocessing the states in the collected experiences \
             before feeding as training batch. Defaults to True.
         gpu_batch_size (int, optional): Actual batch size to reduce one forward gpu calculation memory. \
             As long as gpu memory size is enough, this configuration should not be specified. If not specified,  \
             gpu_batch_size is the same as pi_batch_size. Defaults to None.
-    """
+    '''
     gamma: float = 0.995
     lmb: float = 0.97
     num_steps_per_iteration: int = 5000
     pi_batch_size: int = 5000
     sigma_kl_divergence_constraint: float = 0.01
     maximum_backtrack_numbers: int = 10
     conjugate_gradient_damping: float = 0.1
@@ -82,18 +81,19 @@
     vf_epochs: int = 5
     vf_batch_size: int = 64
     vf_learning_rate: float = 1e-3
     preprocess_state: bool = True
     gpu_batch_size: Optional[int] = None
 
     def __post_init__(self):
-        """__post_init__
+        '''__post_init__
 
         Check the values are in valid range.
-        """
+
+        '''
         self._assert_between(self.pi_batch_size, 0, self.num_steps_per_iteration, 'pi_batch_size')
         self._assert_between(self.gamma, 0.0, 1.0, 'gamma')
         self._assert_between(self.lmb, 0.0, 1.0, 'lmb')
         self._assert_positive(self.num_steps_per_iteration, 'num_steps_per_iteration')
         self._assert_between(self.pi_batch_size, 0, self.num_steps_per_iteration, 'pi_batch_size')
         self._assert_positive(self.sigma_kl_divergence_constraint, 'sigma_kl_divergence_constraint')
         self._assert_positive(self.maximum_backtrack_numbers, 'maximum_backtrack_numbers')
@@ -135,33 +135,16 @@
                            scope_name: str,
                            env_info: EnvironmentInfo,
                            algorithm_config: TRPOConfig,
                            **kwargs) -> Preprocessor:
         return RP.RunningMeanNormalizer(scope_name, env_info.state_shape, value_clip=(-5.0, 5.0))
 
 
-class DefaultExplorerBuilder(ExplorerBuilder):
-    def build_explorer(self,  # type: ignore[override]
-                       env_info: EnvironmentInfo,
-                       algorithm_config: TRPOConfig,
-                       algorithm: "TRPO",
-                       **kwargs) -> EnvironmentExplorer:
-        explorer_config = EE.RawPolicyExplorerConfig(
-            initial_step_num=algorithm.iteration_num,
-            timelimit_as_terminal=False
-        )
-        explorer = EE.RawPolicyExplorer(policy_action_selector=algorithm._exploration_action_selector,
-                                        env_info=env_info,
-                                        config=explorer_config)
-        return explorer
-
-
 class TRPO(Algorithm):
-    """Trust Region Policy Optimiation method with Generalized Advantage
-    Estimation (GAE) implementation.
+    '''Trust Region Policy Optimiation method with Generalized Advantage Estimation (GAE) implementation.
 
     This class implements the Trust Region Policy Optimiation (TRPO)
     with Generalized Advantage Estimation (GAE) algorithm proposed by J. Schulman, et al.
     in the paper: "Trust Region Policy Optimization" and
     "High-Dimensional Continuous Control Using Generalized Advantage Estimation"
     For detail see: https://arxiv.org/abs/1502.05477 and https://arxiv.org/abs/1506.02438
 
@@ -175,27 +158,24 @@
         v_function_builder (:py:class:`ModelBuilder[VFunction] <nnabla_rl.builders.ModelBuilder>`):
             builder of v function models
         v_solver_builder (:py:class:`SolverBuilder <nnabla_rl.builders.SolverBuilder>`): builder for v function solvers
         policy_builder (:py:class:`ModelBuilder[StochasicPolicy] <nnabla_rl.builders.ModelBuilder>`):
             builder of policy models
         state_preprocessor_builder (None or :py:class:`PreprocessorBuilder <nnabla_rl.builders.PreprocessorBuilder>`):
             state preprocessor builder to preprocess the states
-        explorer_builder (:py:class:`ExplorerBuilder <nnabla_rl.builders.ExplorerBuilder>`):
-            builder of environment explorer
-    """
+    '''
 
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _config: TRPOConfig
     _policy: StochasticPolicy
     _v_function: VFunction
     _v_function_solver: nn.solvers.Solver
     _state_preprocessor: Optional[Preprocessor]
-    _explorer_builder: ExplorerBuilder
     _environment_explorer: EnvironmentExplorer
     _policy_trainer: ModelTrainer
     _v_function_trainer: ModelTrainer
     _eval_state_var: nn.Variable
     _eval_action: nn.Variable
 
     _policy_trainer_state: Dict[str, Any]
@@ -203,53 +183,56 @@
 
     def __init__(self,
                  env_or_env_info: Union[gym.Env, EnvironmentInfo],
                  config: TRPOConfig = TRPOConfig(),
                  v_function_builder: ModelBuilder[VFunction] = DefaultVFunctionBuilder(),
                  v_solver_builder: SolverBuilder = DefaultSolverBuilder(),
                  policy_builder: ModelBuilder[StochasticPolicy] = DefaultPolicyBuilder(),
-                 state_preprocessor_builder: Optional[PreprocessorBuilder] = DefaultPreprocessorBuilder(),
-                 explorer_builder: ExplorerBuilder = DefaultExplorerBuilder()):
+                 state_preprocessor_builder: Optional[PreprocessorBuilder] = DefaultPreprocessorBuilder()):
         super(TRPO, self).__init__(env_or_env_info, config=config)
-
-        self._explorer_builder = explorer_builder
+        if self._env_info.is_discrete_action_env():
+            raise NotImplementedError
 
         with nn.context_scope(context.get_nnabla_context(self._config.gpu_id)):
             self._v_function = v_function_builder('v', self._env_info, self._config)
             self._policy = policy_builder('pi', self._env_info, self._config)
 
             self._preprocessor: Optional[Preprocessor] = None
             if self._config.preprocess_state and state_preprocessor_builder is not None:
                 preprocessor = state_preprocessor_builder('preprocessor', self._env_info, self._config)
                 assert preprocessor is not None
                 self._v_function = _StatePreprocessedVFunction(v_function=self._v_function, preprocessor=preprocessor)
-                self._policy = _StatePreprocessedStochasticPolicy(policy=self._policy, preprocessor=preprocessor)
+                self._policy = _StatePreprocessedPolicy(policy=self._policy, preprocessor=preprocessor)
                 self._state_preprocessor = preprocessor
             self._v_function_solver = v_solver_builder(self._env_info, self._config)
 
-        self._evaluation_actor = _StochasticPolicyActionSelector(
-            self._env_info, self._policy.shallowcopy(), deterministic=False)
-        self._exploration_actor = _StochasticPolicyActionSelector(
-            self._env_info, self._policy.shallowcopy(), deterministic=False)
-
     @eval_api
-    def compute_eval_action(self, state, *, begin_of_episode=False, extra_info={}):
+    def compute_eval_action(self, s):
         with nn.context_scope(context.get_nnabla_context(self._config.gpu_id)):
-            action, _ = self._evaluation_action_selector(state, begin_of_episode=begin_of_episode)
+            action, _ = self._compute_action(s)
             return action
 
     def _before_training_start(self, env_or_buffer):
         # set context globally to ensure that the training runs on configured gpu
         context.set_nnabla_context(self._config.gpu_id)
         self._environment_explorer = self._setup_environment_explorer(env_or_buffer)
         self._v_function_trainer = self._setup_v_function_training(env_or_buffer)
         self._policy_trainer = self._setup_policy_training(env_or_buffer)
 
     def _setup_environment_explorer(self, env_or_buffer):
-        return None if self._is_buffer(env_or_buffer) else self._explorer_builder(self._env_info, self._config, self)
+        if self._is_buffer(env_or_buffer):
+            return None
+        explorer_config = EE.RawPolicyExplorerConfig(
+            initial_step_num=self.iteration_num,
+            timelimit_as_terminal=False
+        )
+        explorer = EE.RawPolicyExplorer(policy_action_selector=self._compute_action,
+                                        env_info=self._env_info,
+                                        config=explorer_config)
+        return explorer
 
     def _setup_v_function_training(self, env_or_buffer):
         v_function_trainer_config = MT.v_value.MonteCarloVTrainerConfig(
             reduction_method='mean',
             v_loss_scalar=1.0
         )
         v_function_trainer = MT.v_value.MonteCarloVTrainer(
@@ -338,15 +321,15 @@
         buffer_iterator.reset()
         for experiences, _ in buffer_iterator:
             # length of experiences is 1
             s_seq, a_seq, *_ = marshal_experiences(experiences[0])
             s_batch.append(s_seq)
             a_batch.append(a_seq)
 
-        s_batch = np.concatenate(s_batch, axis=0) if not isinstance(s_batch, tuple) else s_batch
+        s_batch = np.concatenate(s_batch, axis=0)
         a_batch = np.concatenate(a_batch, axis=0)
         return s_batch, a_batch
 
     def _v_function_training(self, s, v_target):
         num_iterations_per_epoch = self._config.num_steps_per_iteration // self._config.vf_batch_size
 
         for _ in range(self._config.vf_epochs * num_iterations_per_epoch):
@@ -363,41 +346,37 @@
         batch = TrainingBatch(batch_size=self._config.pi_batch_size,
                               s_current=s[:self._config.pi_batch_size],
                               a_current=a[:self._config.pi_batch_size],
                               extra=extra)
 
         self._policy_trainer_state = self._policy_trainer.train(batch)
 
-    def _evaluation_action_selector(self, s, *, begin_of_episode=False):
-        return self._evaluation_actor(s, begin_of_episode=begin_of_episode)
-
-    def _exploration_action_selector(self, s, *, begin_of_episode=False):
-        return self._exploration_actor(s, begin_of_episode=begin_of_episode)
+    @eval_api
+    def _compute_action(self, s):
+        s = np.expand_dims(s, axis=0)
+        if not hasattr(self, '_eval_state_var'):
+            self._eval_state_var = nn.Variable(s.shape)
+            distribution = self._policy.pi(self._eval_state_var)
+            self._eval_action = distribution.sample()
+        self._eval_state_var.d = s
+        self._eval_action.forward()
+        return np.squeeze(self._eval_action.d, axis=0), {}
 
     def _models(self):
         models = {}
         models[self._policy.scope_name] = self._policy
         models[self._v_function.scope_name] = self._v_function
         if self._config.preprocess_state and isinstance(self._state_preprocessor, Model):
             models[self._state_preprocessor.scope_name] = self._state_preprocessor
         return models
 
     def _solvers(self):
         solvers = {}
         solvers[self._v_function.scope_name] = self._v_function_solver
         return solvers
 
-    @classmethod
-    def is_supported_env(cls, env_or_env_info):
-        env_info = EnvironmentInfo.from_env(env_or_env_info) if isinstance(env_or_env_info, gym.Env) \
-            else env_or_env_info
-        return not env_info.is_discrete_action_env() and not env_info.is_tuple_action_env()
-
     @property
     def latest_iteration_state(self):
         latest_iteration_state = super(TRPO, self).latest_iteration_state
         if hasattr(self, '_v_function_trainer_state'):
-            latest_iteration_state['scalar'].update({'v_loss': float(self._v_function_trainer_state['v_loss'])})
+            latest_iteration_state['scalar'].update({'v_loss': self._v_function_trainer_state['v_loss']})
         return latest_iteration_state
-
-    def trainers(self):
-        return {"v_function": self._v_function_trainer, "policy": self._policy_trainer}
```

## nnabla_rl/builders/__init__.py

```diff
@@ -1,21 +1,19 @@
 # Copyright 2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from nnabla_rl.builders.explorer_builder import ExplorerBuilder  # noqa
-from nnabla_rl.builders.lr_scheduler_builder import LearningRateSchedulerBuilder  # noqa
 from nnabla_rl.builders.model_builder import ModelBuilder  # noqa
 from nnabla_rl.builders.preprocessor_builder import PreprocessorBuilder  # noqa
 from nnabla_rl.builders.replay_buffer_builder import ReplayBufferBuilder  # noqa
 from nnabla_rl.builders.solver_builder import SolverBuilder  # noqa
```

## nnabla_rl/distributions/__init__.py

```diff
@@ -1,22 +1,19 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from nnabla_rl.distributions.distribution import Distribution, DiscreteDistribution, ContinuosDistribution  # noqa
-from nnabla_rl.distributions.bernoulli import Bernoulli  # noqa
+from nnabla_rl.distributions.distribution import Distribution  # noqa
 from nnabla_rl.distributions.squashed_gaussian import SquashedGaussian  # noqa
 from nnabla_rl.distributions.gaussian import Gaussian  # noqa
-from nnabla_rl.distributions.one_hot_softmax import OneHotSoftmax  # noqa
 from nnabla_rl.distributions.softmax import Softmax  # noqa
-from nnabla_rl.distributions.gmm import GMM  # noqa
```

## nnabla_rl/distributions/distribution.py

```diff
@@ -1,138 +1,137 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from abc import ABCMeta, abstractmethod
-from typing import Optional, Tuple, Union
-
-import numpy as np
+from typing import Optional, Tuple
 
 import nnabla as nn
 
 
 class Distribution(metaclass=ABCMeta):
     @abstractmethod
-    def sample(self, noise_clip: Optional[Tuple[float, float]] = None) -> Union[nn.Variable, np.ndarray]:
-        """Sample a value from the distribution. If noise_clip is specified,
-        the sampled value will be clipped in the given range. Applicability of
-        noise_clip depends on underlying implementation.
+    def sample(self, noise_clip: Optional[Tuple[float, float]] = None) -> nn.Variable:
+        '''
+        Sample a value from the distribution.
+        If noise_clip is specified, the sampled value will be clipped in the given range.
+        Applicability of noise_clip depends on underlying implementation.
 
         Args:
             noise_clip(Tuple[float, float], optional):
                 float tuple of size 2 which contains the min and max value of the noise.
 
         Returns:
-            Union[nn.Variable, np.ndarray]: Sampled value
-        """
+             nn.Variable: Sampled value
+        '''
         raise NotImplementedError
 
     @property
     def ndim(self) -> int:
-        """The number of dimensions of the distribution."""
+        '''
+        The number of dimensions of the distribution
+        '''
         raise NotImplementedError
 
-    def sample_multiple(self, num_samples: int, noise_clip: Optional[Tuple[float, float]] = None
-                        ) -> Union[nn.Variable, np.ndarray]:
-        """Sample mutiple value from the distribution New axis will be added
-        between the first and second axis. Thefore, the returned value shape
-        for mean and variance with shape (batch_size, data_shape) will be
-        changed to (batch_size, num_samples, data_shape)
+    def sample_multiple(self, num_samples: int, noise_clip: Optional[Tuple[float, float]] = None) -> nn.Variable:
+        '''
+        Sample mutiple value from the distribution
+        New axis will be added between the first and second axis.
+        Thefore, the returned value shape for mean and variance with shape (batch_size, data_shape)
+        will be changed to (batch_size, num_samples, data_shape)
 
         If noise_clip is specified, sampled values will be clipped in the given range.
         Applicability of noise_clip depends on underlying implementation.
 
         Args:
             num_samples(int): number of samples per batch
             noise_clip(Tuple[float, float], optional):
                 float tuple of size 2 which contains the min and max value of the noise.
 
         Returns:
-            Union[nn.Variable, np.ndarray]: Sampled value.
-        """
+             nn.Variable: Sampled value.
+        '''
         raise NotImplementedError
 
-    def choose_probable(self) -> Union[nn.Variable, np.ndarray]:
-        """Compute the most probable action of the distribution.
+    def choose_probable(self) -> nn.Variable:
+        '''
+        Compute the most probable action of the distribution
 
         Returns:
-            Union[nn.Variable, np.ndarray]: Probable action of the distribution
-        """
+             nnabla.Variable: Probable action of the distribution
+        '''
         raise NotImplementedError
 
-    def mean(self) -> Union[nn.Variable, np.ndarray]:
-        """Compute the mean of the distribution (if exist)
+    def mean(self) -> nn.Variable:
+        '''
+        Compute the mean of the distribution (if exist)
 
         Returns:
-            Union[nn.Variable, np.ndarray]: mean of the distribution
+             nn.Variable: mean of the distribution
 
         Raises:
              NotImplementedError: The distribution does not have mean
-        """
+        '''
         raise NotImplementedError
 
-    def log_prob(self, x: Union[nn.Variable, np.ndarray]) -> Union[nn.Variable, np.ndarray]:
-        """Compute the log probability of given input.
+    def log_prob(self, x: nn.Variable) -> nn.Variable:
+        '''
+        Compute the log probability of given input
 
         Args:
-            x (Union[nn.Variable, np.ndarray]): Target value to compute the log probability
+            x (nn.Variable): Target value to compute the log probability
 
         Returns:
-            Union[nn.Variable, np.ndarray]: Log probability of given input
-        """
+            nn.Variable: Log probability of given input
+        '''
         raise NotImplementedError
 
     def sample_and_compute_log_prob(self, noise_clip: Optional[Tuple[float, float]] = None) \
-            -> Union[Tuple[nn.Variable, nn.Variable], Tuple[np.ndarray, np.ndarray]]:
-        """Sample a value from the distribution and compute its log
-        probability.
+            -> Tuple[nn.Variable, nn.Variable]:
+        '''
+        Sample a value from the distribution and compute its log probability.
 
         Args:
             noise_clip(Tuple[float, float], optional):
                 float tuple of size 2 which contains the min and max value of the noise.
 
         Returns:
-            Union[Tuple[nn.Variable, nn.Variable], Tuple[np.ndarray, np.ndarray]]: Sampled value and its log probabilty
-        """
+            Tuple[nn.Variable, nn.Variable]: Sampled value and its log probabilty
+        '''
         raise NotImplementedError
 
-    def entropy(self) -> Union[nn.Variable, np.ndarray]:
-        """Compute the entropy of the distribution.
+    def entropy(self) -> nn.Variable:
+        '''
+        Compute the entropy of the distribution
 
         Returns:
-            Union[nn.Variable, np.ndarray]: Entropy of the distribution
-        """
+            nn.Variable: Entropy of the distribution
+        '''
         raise NotImplementedError
 
-    def kl_divergence(self, q: 'Distribution') -> Union[nn.Variable, np.ndarray]:
-        """Compute the kullback leibler divergence between given distribution.
+    def kl_divergence(self, q: 'Distribution') -> nn.Variable:
+        '''
+        Compute the kullback leibler divergence between given distribution.
         This function will compute KL(self||q)
 
         Args:
             q(nnabla_rl.distributions.Distribution): target distribution to compute the kl_divergence
 
         Returns:
-            Union[nn.Variable, np.ndarray]: Kullback leibler divergence
+            nn.Variable: Kullback leibler divergence
 
         Raises:
             ValueError: target distribution's type does not match with current distribution type.
-        """
-        raise NotImplementedError
-
 
-class DiscreteDistribution(Distribution):
-    pass
-
-
-class ContinuosDistribution(Distribution):
-    pass
+        '''
+        raise NotImplementedError
```

## nnabla_rl/distributions/gaussian.py

```diff
@@ -1,103 +1,48 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import warnings
-from typing import Union
-
 import numpy as np
 
 import nnabla as nn
 import nnabla.functions as NF
 import nnabla_rl.functions as RF
-from nnabla_rl.distributions import common_utils
-from nnabla_rl.distributions.distribution import ContinuosDistribution, Distribution
+from nnabla_rl.distributions import Distribution, common_utils
 
 
-class Gaussian(ContinuosDistribution):
-    """Gaussian distribution.
+class Gaussian(Distribution):
+    '''
+    Gaussian distribution
 
     :math:`\\mathcal{N}(\\mu,\\,\\sigma^{2})`
 
     Args:
         mean (nn.Variable): mean :math:`\\mu` of gaussian distribution.
         ln_var (nn.Variable): logarithm of the variance :math:`\\sigma^{2}`. (i.e. ln_var is :math:`\\log{\\sigma^{2}}`)
-    """
-
-    _delegate: Union["NumpyGaussian", "NnablaGaussian"]
+    '''
 
-    def __init__(self, mean: Union[nn.Variable, np.ndarray], ln_var: Union[nn.Variable, np.ndarray]):
+    def __init__(self, mean, ln_var):
         super(Gaussian, self).__init__()
-        if isinstance(mean, np.ndarray) and isinstance(ln_var, np.ndarray):
-            warnings.warn(
-                "Numpy ndarrays are given as mean and ln_var.\n"
-                "From v0.12.0, if numpy.ndarray is given, "
-                "all Gaussian class methods return numpy.ndarray not nnabla.Variable")
-            self._delegate = NumpyGaussian(mean, ln_var)
-        elif isinstance(mean, nn.Variable) and isinstance(ln_var, nn.Variable):
-            self._delegate = NnablaGaussian(mean, ln_var)
-        else:
-            raise ValueError(
-                f"Invalid type or a pair of types, mean type is {type(mean)} and ln type is {type(ln_var)}")
-
-    @property
-    def ndim(self):
-        return self._delegate.ndim
-
-    def sample(self, noise_clip=None):
-        return self._delegate.sample(noise_clip)
-
-    def sample_multiple(self, num_samples, noise_clip=None):
-        return self._delegate.sample_multiple(num_samples, noise_clip)
-
-    def sample_and_compute_log_prob(self, noise_clip=None):
-        return self._delegate.sample_and_compute_log_prob(noise_clip)
-
-    def sample_multiple_and_compute_log_prob(self, num_samples, noise_clip=None):
-        return self._delegate.sample_multiple_and_compute_log_prob(num_samples, noise_clip)
-
-    def choose_probable(self):
-        return self._delegate.choose_probable()
-
-    def mean(self):
-        return self._delegate.mean()
-
-    def var(self):
-        return self._delegate.var()
+        if not isinstance(mean, nn.Variable):
+            mean = nn.Variable.from_numpy_array(mean)
+        if not isinstance(ln_var, nn.Variable):
+            ln_var = nn.Variable.from_numpy_array(ln_var)
 
-    def log_prob(self, x):
-        return self._delegate.log_prob(x)
-
-    def entropy(self):
-        return self._delegate.entropy()
-
-    def kl_divergence(self, q):
-        assert isinstance(q, Gaussian)
-        return self._delegate.kl_divergence(q._delegate)
-
-
-class NnablaGaussian(Distribution):
-    _mean: nn.Variable
-    _var: nn.Variable
-
-    def __init__(self, mean: nn.Variable, ln_var: nn.Variable):
-        super(Distribution, self).__init__()
-        assert mean.shape == ln_var.shape
         self._mean = mean
         self._var = NF.exp(ln_var)
         self._ln_var = ln_var
         self._batch_size = mean.shape[0]
         self._data_dim = mean.shape[1:]
         self._ndim = mean.shape[-1]
 
@@ -139,80 +84,19 @@
 
     def choose_probable(self):
         return self._mean
 
     def mean(self):
         return self._mean
 
-    def var(self):
-        return self._var
-
     def log_prob(self, x):
         return common_utils.gaussian_log_prob(x, self._mean, self._var, self._ln_var)
 
     def entropy(self):
         return NF.sum(0.5 + 0.5 * np.log(2.0 * np.pi) + 0.5 * self._ln_var, axis=1, keepdims=True)
 
     def kl_divergence(self, q):
-        assert isinstance(q, NnablaGaussian)
+        assert isinstance(q, Gaussian)
         p = self
         return 0.5 * NF.sum(q._ln_var - p._ln_var + (p._var + (p._mean - q._mean) ** 2.0) / q._var - 1,
                             axis=1,
                             keepdims=True)
-
-
-class NumpyGaussian(Distribution):
-    _mean: np.ndarray
-    _var: np.ndarray
-
-    def __init__(self, mean: np.ndarray, ln_var: np.ndarray) -> None:
-        super(Distribution, self).__init__()
-        self._dim = mean.shape[0]
-        assert (self._dim, ) == mean.shape
-        assert (self._dim, self._dim) == ln_var.shape
-        self._mean = mean
-        self._var = np.exp(ln_var)
-        self._inv_var = np.linalg.inv(self._var)
-
-    def log_prob(self, x):
-        log_det_term = np.log(np.linalg.det(2.0 * np.pi * self._var))
-        diff = self._mean - x
-        quadratic_term = diff.T.dot(self._inv_var).dot(diff)
-        return -0.5 * (log_det_term + quadratic_term)
-
-    def mean(self):
-        return self._mean
-
-    def var(self):
-        return self._var
-
-    def sample(self, noise_clip=None):
-        if noise_clip is not None:
-            raise NotImplementedError
-        return np.random.multivariate_normal(self._mean, self._var)
-
-    def sample_and_compute_log_prob(self, noise_clip=None):
-        raise NotImplementedError
-
-    def sample_multiple(self, num_samples, noise_clip=None):
-        raise NotImplementedError
-
-    def sample_multiple_and_compute_log_prob(self, num_samples, noise_clip=None):
-        raise NotImplementedError
-
-    def kl_divergence(self, q: 'Distribution'):
-        if not isinstance(q, NumpyGaussian):
-            raise NotImplementedError
-
-        p_mean = self._mean
-        p_var = self._var
-
-        q_mean = q.mean()
-        q_var = q.var()
-        q_var_inv = np.linalg.inv(q.var())
-
-        trace_term = np.trace(q_var_inv.dot(p_var))
-        diff = q_mean - p_mean
-        quadratic_term = diff.T.dot(q_var_inv).dot(diff)
-        dimension = self._dim
-        log_det_term = np.log(np.linalg.det(q_var)) - np.log(np.linalg.det(p_var))
-        return 0.5 * (trace_term + quadratic_term - dimension + log_det_term)
```

## nnabla_rl/distributions/softmax.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -14,75 +14,72 @@
 # limitations under the License.
 
 import numpy as np
 
 import nnabla as nn
 import nnabla.functions as NF
 import nnabla_rl.functions as RF
-from nnabla_rl.distributions import DiscreteDistribution
+from nnabla_rl.distributions import Distribution
 
 
-class Softmax(DiscreteDistribution):
-    """Softmax distribution which samples a class index :math:`i` according to
-    the following probability.
+class Softmax(Distribution):
+    '''
+    Softmax distribution which samples a class index :math:`i` according to the following probability.
 
     :math:`i \\sim \\frac{\\exp{z_{i}}}{\\sum_{j}\\exp{z_{j}}}`.
 
     Args:
         z (nn.Variable): logits :math:`z`. Logits' dimension should be same as the number of class to sample.
-    """
+    '''
 
     def __init__(self, z):
         super(Softmax, self).__init__()
         if not isinstance(z, nn.Variable):
             z = nn.Variable.from_numpy_array(z)
 
-        self._distribution = NF.softmax(x=z, axis=len(z.shape) - 1)
-        self._log_distribution = NF.log_softmax(x=z, axis=len(z.shape) - 1)
+        self._distribution = NF.softmax(x=z, axis=1)
+        self._log_distribution = NF.log_softmax(x=z, axis=1)
         self._batch_size = z.shape[0]
         self._num_class = z.shape[-1]
 
         labels = np.array(
-            [label for label in range(self._num_class)], dtype=np.int32)
+            [label for label in range(self._num_class)], dtype=np.int)
         self._labels = nn.Variable.from_numpy_array(labels)
-        self._actions = self._labels
-        for size in reversed(z.shape[0:-1]):
-            self._actions = NF.stack(*[self._actions for _ in range(size)])
+        self._actions = NF.stack(
+            *[self._labels for _ in range(self._batch_size)])
 
     @property
     def ndim(self):
         return 1
 
     def sample(self, noise_clip=None):
-        # NOTE: nnabla's random_choice backpropagetes through distribution
         return NF.random_choice(self._actions, w=self._distribution)
 
     def sample_multiple(self, num_samples, noise_clip=None):
         raise NotImplementedError
 
     def sample_and_compute_log_prob(self, noise_clip=None):
-        # NOTE: nnabla's random_choice backpropagetes through distribution
         sample = NF.random_choice(self._actions, w=self._distribution)
         log_prob = self.log_prob(sample)
         return sample, log_prob
 
     def choose_probable(self):
-        # NOTE: nnabla's argmax backpropagetes through distribution
-        return RF.argmax(self._distribution, axis=len(self._distribution.shape) - 1)
+        return RF.argmax(self._distribution, axis=1)
 
     def mean(self):
         raise NotImplementedError
 
     def log_prob(self, x):
+        log_pi = self._log_distribution
         one_hot_action = NF.one_hot(x, shape=(self._num_class, ))
-        return NF.sum(self._log_distribution * one_hot_action, axis=len(self._distribution.shape) - 1, keepdims=True)
+        return NF.sum(log_pi * one_hot_action, axis=1, keepdims=True)
 
     def entropy(self):
         plogp = self._distribution * self._log_distribution
-        return -NF.sum(plogp, axis=len(plogp.shape) - 1, keepdims=True)
+        return -NF.sum(plogp, axis=1, keepdims=True)
 
     def kl_divergence(self, q):
         if not isinstance(q, Softmax):
             raise ValueError("Invalid q to compute kl divergence")
         return NF.sum(self._distribution * (self._log_distribution - q._log_distribution),
-                      axis=len(self._distribution.shape) - 1,
+                      axis=1,
                       keepdims=True)
```

## nnabla_rl/distributions/squashed_gaussian.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -14,31 +14,31 @@
 # limitations under the License.
 
 import numpy as np
 
 import nnabla as nn
 import nnabla.functions as NF
 import nnabla_rl.functions as RF
-from nnabla_rl.distributions import common_utils
-from nnabla_rl.distributions.distribution import ContinuosDistribution
+from nnabla_rl.distributions import Distribution, common_utils
 
 
-class SquashedGaussian(ContinuosDistribution):
-    """Gaussian distribution which its output is squashed with tanh.
+class SquashedGaussian(Distribution):
+    '''
+    Gaussian distribution which its output is squashed with tanh.
 
     :math:`z \\sim \\mathcal{N}(\\mu,\\,\\sigma^{2})`. :math:`out = \\tanh{z}`.
 
     Args:
         mean (nn.Variable): mean :math:`\\mu` of underlying gaussian distribution.
         ln_var (nn.Variable): logarithm of the variance :math:`\\sigma^{2}`. (i.e. ln_var is :math:`\\log{\\sigma^{2}}`)
 
     Note:
         The log probability and kl_divergence of this distribution is different from
         :py:class:`Gaussian distribution <nnabla_rl.distributions.Gaussian>` because the output is squashed.
-    """
+    '''
 
     def __init__(self, mean, ln_var):
         super(SquashedGaussian, self).__init__()
         if not isinstance(mean, nn.Variable):
             mean = nn.Variable.from_numpy_array(mean)
         if not isinstance(ln_var, nn.Variable):
             ln_var = nn.Variable.from_numpy_array(ln_var)
@@ -62,31 +62,33 @@
         x = RF.sample_gaussian_multiple(self._mean,
                                         self._ln_var,
                                         num_samples,
                                         noise_clip=noise_clip)
         return NF.tanh(x)
 
     def sample_and_compute_log_prob(self, noise_clip=None):
-        """NOTE: In order to avoid sampling different random values for sample
-        and log_prob, you'll need to use nnabla.forward_all(sample, log_prob)
-        If you forward the two variables independently, you'll get a log_prob
-        for different sample, since different random variables are sampled
-        internally."""
+        '''
+        NOTE: In order to avoid sampling different random values for sample and log_prob,
+        you'll need to use nnabla.forward_all(sample, log_prob)
+        If you forward the two variables independently, you'll get a log_prob for different sample,
+        since different random variables are sampled internally.
+        '''
         x = RF.sample_gaussian(
             mean=self._mean, ln_var=self._ln_var, noise_clip=noise_clip)
         log_prob = self._log_prob_internal(
             x, self._mean, self._var, self._ln_var)
         return NF.tanh(x), log_prob
 
     def sample_multiple_and_compute_log_prob(self, num_samples, noise_clip=None):
-        """NOTE: In order to avoid sampling different random values for sample
-        and log_prob, you'll need to use nnabla.forward_all(sample, log_prob)
-        If you forward the two variables independently, you'll get a log_prob
-        for different sample, since different random variables are sampled
-        internally."""
+        '''
+        NOTE: In order to avoid sampling different random values for sample and log_prob,
+        you'll need to use nnabla.forward_all(sample, log_prob)
+        If you forward the two variables independently, you'll get a log_prob for different sample,
+        since different random variables are sampled internally.
+        '''
         x = RF.sample_gaussian_multiple(self._mean,
                                         self._ln_var,
                                         num_samples=num_samples,
                                         noise_clip=noise_clip)
         mean = RF.expand_dims(self._mean, axis=1)
         var = RF.expand_dims(self._var, axis=1)
         ln_var = RF.expand_dims(self._ln_var, axis=1)
```

## nnabla_rl/environment_explorers/__init__.py

```diff
@@ -9,14 +9,12 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from nnabla_rl.environment_explorers.epsilon_greedy_explorer import (NoDecayEpsilonGreedyExplorer,  # noqa
-                                                                     NoDecayEpsilonGreedyExplorerConfig,
-                                                                     LinearDecayEpsilonGreedyExplorer,
+from nnabla_rl.environment_explorers.epsilon_greedy_explorer import (LinearDecayEpsilonGreedyExplorer,  # noqa
                                                                      LinearDecayEpsilonGreedyExplorerConfig)
 
 from nnabla_rl.environment_explorers.gaussian_explorer import GaussianExplorer, GaussianExplorerConfig  # noqa
 from nnabla_rl.environment_explorers.raw_policy_explorer import RawPolicyExplorer, RawPolicyExplorerConfig  # noqa
```

## nnabla_rl/environment_explorers/epsilon_greedy_explorer.py

```diff
@@ -1,144 +1,69 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023,2024 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from dataclasses import dataclass
-from typing import Dict, Tuple
+from typing import Callable, Dict, Tuple
 
 import numpy as np
 
 from nnabla_rl.environment_explorer import EnvironmentExplorer, EnvironmentExplorerConfig
 from nnabla_rl.environments.environment_info import EnvironmentInfo
-from nnabla_rl.typing import ActionSelector
 
 
-def epsilon_greedy_action_selection(state: np.ndarray,
-                                    greedy_action_selector: ActionSelector,
-                                    random_action_selector: ActionSelector,
-                                    epsilon: float,
-                                    *,
-                                    begin_of_episode: bool = False):
+def epsilon_greedy_action_selection(state, greedy_action_selector, random_action_selector, epsilon):
     if np.random.rand() > epsilon:
         # optimal action
-        return greedy_action_selector(state, begin_of_episode=begin_of_episode), True
+        return greedy_action_selector(state), True
     else:
         # random action
-        return random_action_selector(state, begin_of_episode=begin_of_episode), False
-
-
-@dataclass
-class NoDecayEpsilonGreedyExplorerConfig(EnvironmentExplorerConfig):
-    epsilon: float = 1.0
-
-    def __post_init__(self):
-        self._assert_between(self.epsilon, 0.0, 1.0, 'epsilon')
-
-
-class NoDecayEpsilonGreedyExplorer(EnvironmentExplorer):
-    # type declarations to type check with mypy
-    # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
-    # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
-    _config: NoDecayEpsilonGreedyExplorerConfig
-
-    def __init__(self,
-                 greedy_action_selector: ActionSelector,
-                 random_action_selector: ActionSelector,
-                 env_info: EnvironmentInfo,
-                 config: NoDecayEpsilonGreedyExplorerConfig = NoDecayEpsilonGreedyExplorerConfig()):
-        super().__init__(env_info, config)
-        self._greedy_action_selector = greedy_action_selector
-        self._random_action_selector = random_action_selector
-
-    def action(self, step: int, state: np.ndarray, *, begin_of_episode: bool = False) -> Tuple[np.ndarray, Dict]:
-        epsilon = self._config.epsilon
-        (action, info), _ = epsilon_greedy_action_selection(state,
-                                                            self._greedy_action_selector,
-                                                            self._random_action_selector,
-                                                            epsilon,
-                                                            begin_of_episode=begin_of_episode)
-        return action, info
+        return random_action_selector(state), False
 
 
 @dataclass
 class LinearDecayEpsilonGreedyExplorerConfig(EnvironmentExplorerConfig):
-    """List of configurations for Linear decay epsilon-greedy explorer.
-
-    Args:
-        initial_epsilon (float): Initial value of epsilon. Defaults to 1.0.
-        final_epsilon (float): Final value of epsilon after max_explore_steps.
-            This value must be smaller than initial_epsilon. Defaults to 0.05.
-        max_explore_steps (int): Number of steps to decay epsilon from initial_epsilon to final_epsilon.
-            Defaults to 1000000.
-        append_explorer_info (bool): Flag for appending explorer info to the action info. \
-            The explore info includes whether the action is greedy or not, and explore rate. Defaults to False.
-    """
-
     initial_epsilon: float = 1.0
     final_epsilon: float = 0.05
     max_explore_steps: float = 1000000
-    append_explorer_info: bool = False
 
     def __post_init__(self):
         self._assert_between(self.initial_epsilon, 0.0, 1.0, 'initial_epsilon')
         self._assert_between(self.final_epsilon, 0.0, 1.0, 'final_epsilon')
         self._assert_descending_order([self.initial_epsilon, self.final_epsilon], 'initial/final epsilon')
         self._assert_positive(self.max_explore_steps, 'max_explore_steps')
 
 
 class LinearDecayEpsilonGreedyExplorer(EnvironmentExplorer):
-    """Linear decay epsilon-greedy explorer.
-
-    Epsilon-greedy style explorer. Epsilon is linearly decayed until max_eplore_steps set in the config.
-
-    Args:
-        greedy_action_selector (:py:class:`ActionSelector <nnabla_rl.typing.ActionSelector>`):
-            callable which computes greedy action with respect to current state.
-        random_action_selector (:py:class:`ActionSelector <nnabla_rl.typing.ActionSelector>`):
-            callable which computes random action that can be executed in the environment.
-        env_info (:py:class:`EnvironmentInfo <nnabla_rl.environments.environment_info.EnvironmentInfo>`):
-            environment info
-        config (:py:class:`LinearDecayEpsilonGreedyExplorerConfig\
-            <nnabla_rl.environment_explorers.LinearDecayEpsilonGreedyExplorerConfig>`): the config of this class.
-    """
-
-    # type declarations to type check with mypy
-    # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
-    # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
-    _config: LinearDecayEpsilonGreedyExplorerConfig
-
     def __init__(self,
-                 greedy_action_selector: ActionSelector,
-                 random_action_selector: ActionSelector,
+                 greedy_action_selector: Callable[[np.array], Tuple[np.array, Dict]],
+                 random_action_selector: Callable[[np.array], Tuple[np.array, Dict]],
                  env_info: EnvironmentInfo,
                  config: LinearDecayEpsilonGreedyExplorerConfig = LinearDecayEpsilonGreedyExplorerConfig()):
         super().__init__(env_info, config)
         self._greedy_action_selector = greedy_action_selector
         self._random_action_selector = random_action_selector
 
-    def action(self, step: int, state: np.ndarray, *, begin_of_episode: bool = False) -> Tuple[np.ndarray, Dict]:
+    def action(self, step, state):
         epsilon = self._compute_epsilon(step)
-        (action, info), is_greedy_action = epsilon_greedy_action_selection(state,
-                                                                           self._greedy_action_selector,
-                                                                           self._random_action_selector,
-                                                                           epsilon,
-                                                                           begin_of_episode=begin_of_episode)
-        if self._config.append_explorer_info:
-            info.update({"greedy_action": is_greedy_action, "explore_rate": epsilon})
+        (action, info), _ = epsilon_greedy_action_selection(state,
+                                                            self._greedy_action_selector,
+                                                            self._random_action_selector,
+                                                            epsilon)
         return action, info
 
     def _compute_epsilon(self, step):
         assert 0 <= step
         delta_epsilon = step / self._config.max_explore_steps \
             * (self._config.initial_epsilon - self._config.final_epsilon)
         epsilon = self._config.initial_epsilon - delta_epsilon
```

## nnabla_rl/environment_explorers/gaussian_explorer.py

```diff
@@ -1,78 +1,50 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+import sys
 from dataclasses import dataclass
-from typing import Dict, Tuple, cast
+from typing import Callable, Dict, Tuple
 
 import numpy as np
 
 from nnabla_rl.environment_explorer import EnvironmentExplorer, EnvironmentExplorerConfig
 from nnabla_rl.environments.environment_info import EnvironmentInfo
-from nnabla_rl.typing import ActionSelector
 
 
 @dataclass
 class GaussianExplorerConfig(EnvironmentExplorerConfig):
-    """List of configurations for gaussian explorer.
-
-    Args:
-        action_clip_low (float): Minimum noise value. Noise below this value will be clipped.
-            Defaults to np.finfo(np.float).min
-        action_clip_high (float): Maximum noise value. Noise above this value will be clipped.
-            Defaults to np.finfo(np.float).max
-        sigma (float): Standard deviation of gaussian noise. Must be positive. Defaults to 1.0.
-    """
-
-    action_clip_low: float = cast(float, np.finfo(np.float32).min)
-    action_clip_high: float = cast(float, np.finfo(np.float32).max)
+    action_clip_low: float = sys.float_info.min
+    action_clip_high: float = sys.float_info.max
     sigma: float = 1.0
 
     def __post_init__(self):
         self._assert_positive(self.sigma, 'sigma')
 
 
 class GaussianExplorer(EnvironmentExplorer):
-    """Gaussian explorer.
-
-    Explore using policy's action with gaussian noise appended to it. Policy's action must be continuous action.
-
-    Args:
-        policy_action_selector (:py:class:`ActionSelector <nnabla_rl.typing.ActionSelector>`):
-            callable which computes current policy's action with respect to current state.
-        env_info (:py:class:`EnvironmentInfo <nnabla_rl.environments.environment_info.EnvironmentInfo>`):
-            environment info
-        config (:py:class:`LinearDecayEpsilonGreedyExplorerConfig\
-            <nnabla_rl.environment_explorers.LinearDecayEpsilonGreedyExplorerConfig>`): the config of this class.
-    """
-
-    # type declarations to type check with mypy
-    # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
-    # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
-    _config: GaussianExplorerConfig
-
     def __init__(self,
-                 policy_action_selector: ActionSelector,
+                 policy_action_selector: Callable[[np.array], Tuple[np.array, Dict]],
                  env_info: EnvironmentInfo,
                  config: GaussianExplorerConfig = GaussianExplorerConfig()):
         super().__init__(env_info, config)
         self._policy_action_selector = policy_action_selector
 
-    def action(self, step: int, state: np.ndarray, *, begin_of_episode: bool = False) -> Tuple[np.ndarray, Dict]:
-        (action, info) = self._policy_action_selector(state, begin_of_episode=begin_of_episode)
+    def action(self, step, state):
+        (action, info) = self._policy_action_selector(state)
         return self._append_noise(action, self._config.action_clip_low, self._config.action_clip_high), info
 
     def _append_noise(self, action, low, high):
         noise = np.random.normal(loc=0.0, scale=self._config.sigma, size=action.shape).astype(np.float32)
         return np.clip(action + noise, low, high)
```

## nnabla_rl/environment_explorers/raw_policy_explorer.py

```diff
@@ -1,53 +1,39 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from dataclasses import dataclass
-from typing import Dict, Tuple
+from typing import Callable, Dict, Tuple
 
 import numpy as np
 
 from nnabla_rl.environment_explorer import EnvironmentExplorer, EnvironmentExplorerConfig
 from nnabla_rl.environments.environment_info import EnvironmentInfo
-from nnabla_rl.typing import ActionSelector
 
 
 @dataclass
 class RawPolicyExplorerConfig(EnvironmentExplorerConfig):
     pass
 
 
 class RawPolicyExplorer(EnvironmentExplorer):
-    """Raw policy explorer.
-
-    Explore using policy's action without any changes.
-
-    Args:
-        policy_action_selector (:py:class:`ActionSelector <nnabla_rl.typing.ActionSelector>`):
-            callable which computes current policy's action with respect to current state.
-        env_info (:py:class:`EnvironmentInfo <nnabla_rl.environments.environment_info.EnvironmentInfo>`):
-            environment info
-        config (:py:class:`LinearDecayEpsilonGreedyExplorerConfig\
-            <nnabla_rl.environment_explorers.RawPolicyExplorerConfig>`): the config of this class.
-    """
-
     def __init__(self,
-                 policy_action_selector: ActionSelector,
+                 policy_action_selector: Callable[[np.array], Tuple[np.array, Dict]],
                  env_info: EnvironmentInfo,
                  config: RawPolicyExplorerConfig = RawPolicyExplorerConfig()):
         super().__init__(env_info, config)
         self._policy_action_selector = policy_action_selector
 
-    def action(self, step: int, state: np.ndarray, *, begin_of_episode: bool = False) -> Tuple[np.ndarray, Dict]:
-        return self._policy_action_selector(state, begin_of_episode=begin_of_episode)
+    def action(self, step, state):
+        return self._policy_action_selector(state)
```

## nnabla_rl/environments/__init__.py

```diff
@@ -1,121 +1,31 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023,2024 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from gym.envs.registration import register
-from gymnasium.envs.registration import register as gymnasium_register
 
-from nnabla_rl.environments.dummy import (DummyAtariEnv, DummyContinuous, DummyContinuousActionGoalEnv, DummyDiscrete,  # noqa
-                                          DummyDiscreteActionGoalEnv, DummyDiscreteImg, DummyContinuousImg,
-                                          DummyFactoredContinuous, DummyMujocoEnv,
-                                          DummyTupleContinuous, DummyTupleDiscrete, DummyTupleMixed,
-                                          DummyTupleStateContinuous, DummyTupleStateDiscrete,
-                                          DummyTupleActionContinuous, DummyTupleActionDiscrete,
-                                          DummyHybridEnv, DummyAMPEnv, DummyAMPGoalEnv,
-                                          DummyGymnasiumAtariEnv, DummyGymnasiumMujocoEnv)
+from nnabla_rl.environments.dummy import (DummyAtariEnv, DummyContinuous, DummyDiscrete,  # noqa
+                                          DummyDiscreteImg, DummyMujocoEnv)
 
 register(
     id='FakeMujocoNNablaRL-v1',
     entry_point='nnabla_rl.environments.dummy:DummyMujocoEnv',
     max_episode_steps=10
 )
 
 register(
-    id='FakeDMControlNNablaRL-v1',
-    entry_point='nnabla_rl.environments.dummy:DummyDMControlEnv',
-    max_episode_steps=10
-)
-
-register(
     id='FakeAtariNNablaRLNoFrameskip-v1',
     entry_point='nnabla_rl.environments.dummy:DummyAtariEnv',
     max_episode_steps=10
 )
-
-register(
-    id='FakeGoalConditionedNNablaRL-v1',
-    entry_point='nnabla_rl.environments.dummy:DummyContinuousActionGoalEnv',
-    max_episode_steps=10
-)
-
-register(
-    id='FactoredLunarLanderContinuousV2NNablaRL-v1',
-    entry_point='nnabla_rl.environments.factored_envs:FactoredLunarLanderV2',
-    kwargs={"continuous": True},
-    max_episode_steps=1000,
-    reward_threshold=200.0,
-)
-
-register(
-    id='FactoredAntV4NNablaRL-v1',
-    entry_point='nnabla_rl.environments.factored_envs:FactoredAntV4',
-    max_episode_steps=1000,
-    reward_threshold=6000.0,
-)
-
-register(
-    id='FactoredHopperV4NNablaRL-v1',
-    entry_point='nnabla_rl.environments.factored_envs:FactoredHopperV4',
-    max_episode_steps=1000,
-    reward_threshold=3800.0,
-)
-
-register(
-    id='FactoredHalfCheetahV4NNablaRL-v1',
-    entry_point='nnabla_rl.environments.factored_envs:FactoredHalfCheetahV4',
-    max_episode_steps=1000,
-    reward_threshold=4800.0,
-)
-
-register(
-    id='FactoredWalker2dV4NNablaRL-v1',
-    entry_point='nnabla_rl.environments.factored_envs:FactoredWalker2dV4',
-    max_episode_steps=1000,
-)
-
-register(
-    id='FactoredHumanoidV4NNablaRL-v1',
-    entry_point='nnabla_rl.environments.factored_envs:FactoredHumanoidV4',
-    max_episode_steps=1000,
-)
-
-register(
-    id='FakeHybridNNablaRL-v1',
-    entry_point='nnabla_rl.environments.dummy:DummyHybridEnv',
-    max_episode_steps=10
-)
-
-register(
-    id='FakeAMPNNablaRL-v1',
-    entry_point='nnabla_rl.environments.dummy:DummyAMPEnv',
-    max_episode_steps=10
-)
-
-register(
-    id='FakeAMPGoalConditionedNNablaRL-v1',
-    entry_point='nnabla_rl.environments.dummy:DummyAMPGoalEnv',
-    max_episode_steps=10
-)
-
-gymnasium_register(
-    id='FakeGymnasiumMujocoNNablaRL-v1',
-    entry_point='nnabla_rl.environments.dummy:DummyGymnasiumMujocoEnv',
-    max_episode_steps=10
-)
-
-gymnasium_register(
-    id='FakeGymnasiumAtariNNablaRLNoFrameskip-v1',
-    entry_point='nnabla_rl.environments.dummy:DummyGymnasiumAtariEnv',
-    max_episode_steps=10
-)
```

## nnabla_rl/environments/dummy.py

```diff
@@ -1,194 +1,79 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023,2024 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from typing import TYPE_CHECKING, List, cast
-
 import gym
-import gym.spaces
-import gymnasium
 import numpy as np
 from gym.envs.registration import EnvSpec
-from gymnasium.envs.registration import EnvSpec as GymnasiumEnvSpec
-
-if TYPE_CHECKING:
-    from gym.utils.seeding import RandomNumberGenerator
-
-import nnabla_rl
-from nnabla_rl.environments.amp_env import AMPEnv, AMPGoalEnv, TaskResult
-from nnabla_rl.external.goal_env import GoalEnv
 
 
 class AbstractDummyEnv(gym.Env):
     def __init__(self, max_episode_steps):
-        self.spec = EnvSpec('dummy-v0', max_episode_steps=max_episode_steps)
-        self._episode_steps = 0
+        self.spec = EnvSpec(id='dummy-v0', max_episode_steps=max_episode_steps)
 
     def reset(self):
-        self._episode_steps = 0
         return self.observation_space.sample()
 
     def step(self, a):
         next_state = self.observation_space.sample()
         reward = np.random.randn()
-        done = False if self.spec.max_episode_steps is None else bool(self._episode_steps < self.spec.max_episode_steps)
-        info = {'rnn_states': {'dummy_scope': {'dummy_state1': 1, 'dummy_state2': 2}}}
-        self._episode_steps += 1
+        done = False
+        info = {}
         return next_state, reward, done, info
 
 
 class DummyContinuous(AbstractDummyEnv):
-    def __init__(self, max_episode_steps=None, observation_shape=(5, ), action_shape=(5, )):
+    def __init__(self, max_episode_steps=None):
         super(DummyContinuous, self).__init__(
             max_episode_steps=max_episode_steps)
-        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=observation_shape)
-        self.action_space = gym.spaces.Box(low=-1.0, high=5.0, shape=action_shape)
-
-
-class DummyFactoredContinuous(DummyContinuous):
-    def __init__(self, max_episode_steps=None, observation_shape=(5, ), action_shape=(5, ), reward_dimension=1):
-        super(DummyFactoredContinuous, self).__init__(
-            max_episode_steps=max_episode_steps,
-            observation_shape=observation_shape,
-            action_shape=action_shape)
-        self.reward_dimension = reward_dimension
-
-    def step(self, a):
-        s, _, d, i = super().step(a)
-        return s, np.random.normal(size=self.reward_dimension), d, i
+        self.action_space = gym.spaces.Box(low=0.0, high=1.0, shape=(5, ))
+        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(5, ))
 
 
 class DummyDiscrete(AbstractDummyEnv):
     def __init__(self, max_episode_steps=None):
         super(DummyDiscrete, self).__init__(
             max_episode_steps=max_episode_steps)
         self.action_space = gym.spaces.Discrete(4)
-        self.observation_space = gym.spaces.Discrete(5)
-
-
-class DummyTupleContinuous(AbstractDummyEnv):
-    def __init__(self, max_episode_steps=None):
-        super(DummyTupleContinuous, self).__init__(
-            max_episode_steps=max_episode_steps)
-        self.action_space = gym.spaces.Tuple((gym.spaces.Box(low=0.0, high=1.0, shape=(2, )),
-                                              gym.spaces.Box(low=0.0, high=1.0, shape=(3, ))))
-        self.observation_space = gym.spaces.Tuple((gym.spaces.Box(low=0.0, high=1.0, shape=(4, )),
-                                                   gym.spaces.Box(low=0.0, high=1.0, shape=(5, ))))
-
-    def step(self, a):
-        for a, action_space in zip(a, self.action_space):
-            assert a.shape == action_space.shape
-        return super().step(a)
-
-
-class DummyTupleDiscrete(AbstractDummyEnv):
-    def __init__(self, max_episode_steps=None):
-        super(DummyTupleDiscrete, self).__init__(
-            max_episode_steps=max_episode_steps)
-        self.action_space = gym.spaces.Tuple((gym.spaces.Discrete(2), gym.spaces.Discrete(3)))
-        self.observation_space = gym.spaces.Tuple((gym.spaces.Discrete(4), gym.spaces.Discrete(5)))
-
-    def step(self, a):
-        for a, action_space in zip(a, self.action_space):
-            assert isinstance(a, int) or a.shape == action_space.shape
-        return super().step(a)
-
-
-class DummyTupleMixed(AbstractDummyEnv):
-    def __init__(self, max_episode_steps=None):
-        super(DummyTupleMixed, self).__init__(
-            max_episode_steps=max_episode_steps)
-        self.action_space = gym.spaces.Tuple((gym.spaces.Discrete(2),
-                                              gym.spaces.Box(low=0.0, high=1.0, shape=(3, ))))
-        self.observation_space = gym.spaces.Tuple((gym.spaces.Discrete(4),
-                                                   gym.spaces.Box(low=0.0, high=1.0, shape=(5, ))))
-
-    def step(self, a):
-        for a, action_space in zip(a, self.action_space):
-            assert isinstance(a, int) or a.shape == action_space.shape
-        return super().step(a)
-
-
-class DummyTupleStateContinuous(AbstractDummyEnv):
-    def __init__(self, max_episode_steps=None):
-        super(DummyTupleStateContinuous, self).__init__(
-            max_episode_steps=max_episode_steps)
-        self.action_space = gym.spaces.Box(low=0.0, high=1.0, shape=(2, ))
-        self.observation_space = gym.spaces.Tuple((gym.spaces.Box(low=0.0, high=1.0, shape=(4, )),
-                                                   gym.spaces.Box(low=0.0, high=1.0, shape=(5, ))))
-
-
-class DummyTupleStateDiscrete(AbstractDummyEnv):
-    def __init__(self, max_episode_steps=None):
-        super(DummyTupleStateDiscrete, self).__init__(
-            max_episode_steps=max_episode_steps)
-        self.action_space = gym.spaces.Discrete(2)
-        self.observation_space = gym.spaces.Tuple((gym.spaces.Discrete(4), gym.spaces.Discrete(5)))
-
-
-class DummyTupleActionContinuous(AbstractDummyEnv):
-    def __init__(self, max_episode_steps=None):
-        super(DummyTupleActionContinuous, self).__init__(
-            max_episode_steps=max_episode_steps)
-        self.action_space = gym.spaces.Tuple((gym.spaces.Box(low=0.0, high=1.0, shape=(2, )),
-                                              gym.spaces.Box(low=0.0, high=1.0, shape=(3, ))))
-        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(4, ))
-
-
-class DummyTupleActionDiscrete(AbstractDummyEnv):
-    def __init__(self, max_episode_steps=None):
-        super(DummyTupleActionDiscrete, self).__init__(
-            max_episode_steps=max_episode_steps)
-        self.action_space = gym.spaces.Tuple((gym.spaces.Discrete(2), gym.spaces.Discrete(3)))
-        self.observation_space = gym.spaces.Discrete(4)
+        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(5, ))
 
 
 class DummyDiscreteImg(AbstractDummyEnv):
     def __init__(self, max_episode_steps=None):
         super(DummyDiscreteImg, self).__init__(
             max_episode_steps=max_episode_steps)
         self.observation_space = gym.spaces.Box(
             low=0.0, high=1.0, shape=(4, 84, 84))
         self.action_space = gym.spaces.Discrete(4)
 
 
-class DummyContinuousImg(AbstractDummyEnv):
-    def __init__(self, image_shape=(3, 64, 64), max_episode_steps=None):
-        super(DummyContinuousImg, self).__init__(
-            max_episode_steps=max_episode_steps)
-        self.observation_space = gym.spaces.Box(
-            low=0.0, high=1.0, shape=image_shape)
-        self.action_space = gym.spaces.Box(low=0.0, high=1.0, shape=(2, ))
-
-
 class DummyAtariEnv(AbstractDummyEnv):
     class DummyALE(object):
         def __init__(self):
             self._lives = 100
 
         def lives(self):
             self._lives -= 1
             if self._lives < 0:
                 self._lives = 100
             return self._lives
 
-    # seeding.np_random outputs np_random and seed
-    np_random = cast("RandomNumberGenerator", nnabla_rl.random.drng)
+    np_random = np.random
 
     def __init__(self, done_at_random=True, max_episode_length=None):
         super(DummyAtariEnv, self).__init__(
             max_episode_steps=max_episode_length)
         self.action_space = gym.spaces.Discrete(4)
         self.observation_space = gym.spaces.Box(
             low=0, high=255, shape=(84, 84, 3), dtype=np.uint8)
@@ -198,15 +83,15 @@
         self._episode_length = None
 
     def step(self, action):
         assert self._episode_length is not None
         observation = self.observation_space.sample()
         self._episode_length += 1
         if self._done_at_random:
-            done = bool(self.np_random.integers(10) == 0)
+            done = (np.random.randint(10) == 0)
         else:
             done = False
         if self._max_episode_length is not None:
             done = (self._max_episode_length <= self._episode_length) or done
         return observation, 1.0, done, {'needs_reset': False}
 
     def reset(self):
@@ -221,297 +106,13 @@
     def __init__(self, max_episode_steps=None):
         super(DummyMujocoEnv, self).__init__(max_episode_steps=max_episode_steps)
         self.action_space = gym.spaces.Box(low=0.0, high=1.0, shape=(5, ))
         self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(5, ))
 
     def get_dataset(self):
         dataset = {}
-        datasize = 2000
-        dataset['observations'] = np.stack([self.observation_space.sample() for _ in range(datasize)], axis=0)
-        dataset['actions'] = np.stack([self.action_space.sample() for _ in range(datasize)], axis=0)
-        dataset['rewards'] = np.random.randn(datasize, 1)
-        dataset['terminals'] = np.random.randint(2, size=(datasize, 1))
-        dataset['timeouts'] = np.zeros((datasize, 1))
-        return dataset
-
-
-class DummyDMControlEnv(DummyMujocoEnv):
-    pass
-
-
-class DummyContinuousActionGoalEnv(GoalEnv):
-    def __init__(self, max_episode_steps=10):
-        self.spec = EnvSpec('dummy-continuou-action-goal-v0', max_episode_steps=max_episode_steps)
-        self.observation_space = gym.spaces.Dict({'observation': gym.spaces.Box(low=0.0, high=1.0, shape=(5, )),
-                                                  'achieved_goal': gym.spaces.Box(low=0.0, high=1.0, shape=(2, )),
-                                                 'desired_goal': gym.spaces.Box(low=0.0, high=1.0, shape=(2, ))})
-        self.action_space = gym.spaces.Box(low=0.0, high=1.0, shape=(2, ))
-        self._max_episode_length = max_episode_steps
-        self._episode_length = 0
-        self._desired_goal = None
-
-    def reset(self):
-        super(DummyContinuousActionGoalEnv, self).reset()
-        self._episode_length = 0
-        state = self.observation_space.sample()
-        self._desired_goal = state['desired_goal']
-        return state
-
-    def step(self, a):
-        next_state = self.observation_space.sample()
-        next_state['desired_goal'] = self._desired_goal
-        reward = self.compute_reward(next_state['achieved_goal'], next_state['desired_goal'], {})
-        self._episode_length += 1
-        info = {'is_success': reward}
-        if self._episode_length >= self._max_episode_length:
-            done = True
-        else:
-            done = False
-        return next_state, reward, done, info
-
-    def compute_reward(self, achieved_goal, desired_goal, info):
-        if np.linalg.norm(achieved_goal - desired_goal) < 0.1:
-            return 1
-        else:
-            return 0
-
-
-class DummyDiscreteActionGoalEnv(GoalEnv):
-    def __init__(self, max_episode_steps=10):
-        self.spec = EnvSpec('dummy-discrete-action-goal-v0', max_episode_steps=max_episode_steps)
-        self.observation_space = gym.spaces.Dict({'observation': gym.spaces.Box(low=0.0, high=1.0, shape=(5, )),
-                                                  'achieved_goal': gym.spaces.Box(low=0.0, high=1.0, shape=(2, )),
-                                                 'desired_goal': gym.spaces.Box(low=0.0, high=1.0, shape=(2, ))})
-        self.action_space = gym.spaces.Discrete(n=3)
-        self._max_episode_length = max_episode_steps
-        self._episode_length = 0
-        self._desired_goal = None
-
-    def reset(self):
-        super(DummyDiscreteActionGoalEnv, self).reset()
-        self._episode_length = 0
-        state = self.observation_space.sample()
-        self._desired_goal = state['desired_goal']
-        return state
-
-    def step(self, a):
-        next_state = self.observation_space.sample()
-        next_state['desired_goal'] = self._desired_goal
-        reward = self.compute_reward(next_state['achieved_goal'], next_state['desired_goal'], {})
-        self._episode_length += 1
-        info = {'is_success': reward}
-        if self._episode_length >= self._max_episode_length:
-            done = True
-        else:
-            done = False
-        return next_state, reward, done, info
-
-    def compute_reward(self, achieved_goal, desired_goal, info):
-        if np.linalg.norm(achieved_goal - desired_goal) < 0.1:
-            return 1
-        else:
-            return 0
-
-
-class DummyHybridEnv(AbstractDummyEnv):
-    def __init__(self, max_episode_steps=None):
-        super(DummyHybridEnv, self).__init__(max_episode_steps=max_episode_steps)
-        self.action_space = gym.spaces.Tuple((gym.spaces.Discrete(5), gym.spaces.Box(low=0.0, high=1.0, shape=(5, ))))
-        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(5, ))
-
-
-class DummyAMPEnv(AMPEnv):
-    def __init__(self, max_episode_steps=10):
-        self.spec = EnvSpec('dummy-amp-v0', max_episode_steps=max_episode_steps)
-        self.action_space = gym.spaces.Box(low=0.0, high=1.0, shape=(4, ))
-        self.observation_space = gym.spaces.Tuple(
-            [gym.spaces.Box(low=0.0, high=1.0, shape=(2, )),
-             gym.spaces.Box(low=0.0, high=1.0, shape=(5, )),
-             gym.spaces.Box(low=0.0, high=1.0, shape=(1, ))])
-        self.reward_range = (0.0, 1.0)
-        self.observation_mean = tuple([np.zeros(2, dtype=np.float32), np.zeros(
-            5, dtype=np.float32), np.zeros(1, dtype=np.float32)])
-        self.observation_var = tuple([np.ones(2, dtype=np.float32), np.ones(
-            5, dtype=np.float32), np.ones(1, dtype=np.float32)])
-        self.action_mean = np.zeros((4,), dtype=np.float32)
-        self.action_var = np.ones((4,), dtype=np.float32)
-        self.reward_at_task_fail = 0.0
-        self.reward_at_task_success = 10.0
-        self._episode_steps = 0
-
-    def reset(self):
-        self._episode_steps = 0
-        state = list(self.observation_space.sample())
-        return tuple(state)
-
-    def task_result(self, state, reward, done, info) -> TaskResult:
-        return TaskResult(TaskResult.UNKNOWN.value)
-
-    def is_valid_episode(self, state, reward, done, info) -> bool:
-        return True
-
-    def expert_experience(self, state, reward, done, info):
-        state = list(self.observation_space.sample())
-        action = self.action_space.sample()
-        next_state = list(self.observation_space.sample())
-        return tuple(state), action, 0.0, False, tuple(next_state), {}
-
-    def _step(self, a):
-        self._episode_steps += 1
-        next_state = list(self.observation_space.sample())
-        reward = np.random.randn()
-        done = self._episode_steps >= self.spec.max_episode_steps
-        info = {'rnn_states': {'dummy_scope': {'dummy_state1': 1, 'dummy_state2': 2}}}
-        return tuple(next_state), reward, done, info
-
-
-class DummyAMPGoalEnv(AMPGoalEnv):
-    def __init__(self, max_episode_steps=10):
-        self.spec = EnvSpec('dummy-amp-goal-v0', max_episode_steps=max_episode_steps)
-        self.action_space = gym.spaces.Box(low=0.0, high=1.0, shape=(4, ))
-        observation_space = gym.spaces.Tuple(
-            [gym.spaces.Box(low=0.0, high=1.0, shape=(2, )),
-             gym.spaces.Box(low=0.0, high=1.0, shape=(5, )),
-             gym.spaces.Box(low=0.0, high=1.0, shape=(1, ))])
-        goal_state_space = gym.spaces.Tuple([gym.spaces.Box(low=-np.inf,
-                                                            high=np.inf,
-                                                            shape=(3,),
-                                                            dtype=np.float32),
-                                             gym.spaces.Box(low=0.0,
-                                                            high=1.0,
-                                                            shape=(1,),
-                                                            dtype=np.float32)])
-        self.observation_space = gym.spaces.Dict({"observation": observation_space,
-                                                  "desired_goal": goal_state_space,
-                                                  "achieved_goal": goal_state_space})
-
-        self.reward_range = (0.0, 1.0)
-        self.observation_mean = tuple([np.zeros(2, dtype=np.float32), np.zeros(
-            5, dtype=np.float32), np.zeros(1, dtype=np.float32)])
-        self.observation_var = tuple([np.ones(2, dtype=np.float32), np.ones(
-            5, dtype=np.float32), np.ones(1, dtype=np.float32)])
-        self.action_mean = np.zeros((4,), dtype=np.float32)
-        self.action_var = np.ones((4,), dtype=np.float32)
-        self.reward_at_task_fail = 0.0
-        self.reward_at_task_success = 10.0
-        self._episode_steps = 0
-
-    def reset(self):
-        super().reset()
-        self._episode_steps = 0
-        return self.observation_space.sample()
-
-    def task_result(self, state, reward, done, info) -> TaskResult:
-        return TaskResult(TaskResult.UNKNOWN.value)
-
-    def is_valid_episode(self, state, reward, done, info) -> bool:
-        return True
-
-    def expert_experience(self, state, reward, done, info):
-        action = self.action_space.sample()
-        return (self._generate_dummy_goal_env_flatten_state(), action, 0.0,
-                False, self._generate_dummy_goal_env_flatten_state(), {})
-
-    def _generate_dummy_goal_env_flatten_state(self):
-        state: List[np.ndarray] = []
-        sample = self.observation_space.sample()
-        for key in ["observation", "desired_goal", "achieved_goal"]:
-            s = sample[key]
-            if isinstance(s, tuple):
-                state.extend(s)
-            else:
-                state.append(s)
-        state = list(map(lambda v: v * 0.0, state))
-        return tuple(state)
-
-    def _step(self, a):
-        self._episode_steps += 1
-        next_state = self.observation_space.sample()
-        reward = np.random.randn()
-        done = self._episode_steps >= self.spec.max_episode_steps
-        info = {'rnn_states': {'dummy_scope': {'dummy_state1': 1, 'dummy_state2': 2}}}
-        return next_state, reward, done, info
-
-
-# =========== gymnasium ==========
-class AbstractDummyGymnasiumEnv(gymnasium.Env):
-    def __init__(self, max_episode_steps):
-        self.spec = GymnasiumEnvSpec('dummy-v0', max_episode_steps=max_episode_steps)
-        self._episode_steps = 0
-
-    def reset(self):
-        self._episode_steps = 0
-        return self.observation_space.sample(), {}
-
-    def step(self, a):
-        next_state = self.observation_space.sample()
-        reward = np.random.randn()
-        terminated = False
-        if self.spec.max_episode_steps is None:
-            truncated = False
-        else:
-            truncated = bool(self._episode_steps < self.spec.max_episode_steps)
-        info = {'rnn_states': {'dummy_scope': {'dummy_state1': 1, 'dummy_state2': 2}}}
-        self._episode_steps += 1
-        return next_state, reward, terminated, truncated, info
-
-
-class DummyGymnasiumAtariEnv(AbstractDummyGymnasiumEnv):
-    class DummyALE(object):
-        def __init__(self):
-            self._lives = 100
-
-        def lives(self):
-            self._lives -= 1
-            if self._lives < 0:
-                self._lives = 100
-            return self._lives
-
-    # seeding.np_random outputs np_random and seed
-    np_random = cast("RandomNumberGenerator", nnabla_rl.random.drng)
-
-    def __init__(self, done_at_random=True, max_episode_length=None):
-        super(DummyGymnasiumAtariEnv, self).__init__(
-            max_episode_steps=max_episode_length)
-        self.action_space = gymnasium.spaces.Discrete(4)
-        self.observation_space = gymnasium.spaces.Box(
-            low=0, high=255, shape=(84, 84, 3), dtype=np.uint8)
-        self.ale = DummyGymnasiumAtariEnv.DummyALE()
-        self._done_at_random = done_at_random
-        self._max_episode_length = max_episode_length
-        self._episode_length = None
-
-    def step(self, action):
-        assert self._episode_length is not None
-        observation = self.observation_space.sample()
-        self._episode_length += 1
-        if self._done_at_random:
-            done = bool(self.np_random.integers(10) == 0)
-        else:
-            done = False
-        if self._max_episode_length is not None:
-            done = (self._max_episode_length <= self._episode_length) or done
-        return observation, 1.0, done, {'needs_reset': False}
-
-    def reset(self):
-        self._episode_length = 0
-        return self.observation_space.sample()
-
-    def get_action_meanings(self):
-        return ['NOOP', 'FIRE', 'LEFT', 'RIGHT']
-
-
-class DummyGymnasiumMujocoEnv(AbstractDummyGymnasiumEnv):
-    def __init__(self, max_episode_steps=None):
-        super(DummyGymnasiumMujocoEnv, self).__init__(max_episode_steps=max_episode_steps)
-        self.action_space = gymnasium.spaces.Box(low=0.0, high=1.0, shape=(5, ))
-        self.observation_space = gymnasium.spaces.Box(low=0.0, high=1.0, shape=(5, ))
-
-    def get_dataset(self):
-        dataset = {}
-        datasize = 2000
+        datasize = 100
         dataset['observations'] = np.stack([self.observation_space.sample() for _ in range(datasize)], axis=0)
         dataset['actions'] = np.stack([self.action_space.sample() for _ in range(datasize)], axis=0)
         dataset['rewards'] = np.random.randn(datasize, 1)
-        dataset['terminals'] = np.random.randint(2, size=(datasize, 1))
-        dataset['timeouts'] = np.zeros((datasize, 1))
+        dataset['terminals'] = np.zeros((datasize, 1))
         return dataset
```

## nnabla_rl/environments/environment_info.py

```diff
@@ -1,64 +1,41 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023,2024 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from dataclasses import dataclass
-from typing import Any, Callable, Dict, Optional
 
 import gym
-
-from nnabla_rl.environments.gym_utils import (extract_max_episode_steps, get_space_dim, get_space_high, get_space_low,
-                                              get_space_shape, is_same_space_type)
-from nnabla_rl.external.goal_env import GoalEnv
+import numpy as np
 
 
 @dataclass
 class EnvironmentInfo(object):
-    """Environment Information class.
+    """Environment Information class
 
-    This class contains the basic information of the target training
-    environment.
+    This class contains the basic information of the target training environment.
     """
     observation_space: gym.spaces.Space
     action_space: gym.spaces.Space
     max_episode_steps: int
 
-    def __init__(self,
-                 observation_space,
-                 action_space,
-                 max_episode_steps,
-                 unwrapped_env,
-                 reward_function: Optional[Callable[[Any, Any, Dict], int]] = None):
-        self.observation_space = observation_space
-        self.action_space = action_space
-        self.max_episode_steps = max_episode_steps
-        self.unwrapped_env = unwrapped_env
-        self.reward_function = reward_function
-
-        if not (self.is_discrete_state_env() or self.is_continuous_state_env() or self.is_tuple_state_env()):
-            raise ValueError(f"Unsupported state space: {observation_space}")
-
-        if not (self.is_discrete_action_env() or self.is_continuous_action_env() or self.is_tuple_action_env()):
-            raise ValueError(f"Unsupported action space: {action_space}")
-
     @staticmethod
     def from_env(env):
-        """Create env_info from environment.
+        """Create env_info from environment
 
         Args:
             env (gym.Env): the environment
 
         Returns:
             EnvironmentInfo\
                 (:py:class:`EnvironmentInfo <nnabla_rl.environments.environment_info.EnvironmentInfo>`)
@@ -67,164 +44,66 @@
             >>> import gym
             >>> from nnabla_rl.environments.environment_info import EnvironmentInfo
             >>> env = gym.make("CartPole-v0")
             >>> env_info = EnvironmentInfo.from_env(env)
             >>> env_info.state_shape
             (4,)
         """
-        reward_function = env.compute_reward if hasattr(env, 'compute_reward') else None
-        unwrapped_env = env.unwrapped
         return EnvironmentInfo(observation_space=env.observation_space,
                                action_space=env.action_space,
-                               max_episode_steps=extract_max_episode_steps(env),
-                               unwrapped_env=unwrapped_env,
-                               reward_function=reward_function)
+                               max_episode_steps=EnvironmentInfo._extract_max_episode_steps(env))
 
     def is_discrete_action_env(self):
-        """Check whether the action to execute in the environment is discrete
-        or not.
+        '''
+        Check whether the action to execute in the environment is discrete or not
 
         Returns:
             bool: True if the action to execute in the environment is discrete. Otherwise False.
-                Note that if the action is gym.spaces.Tuple and all of the element are discrete, it returns True.
-        """
-        return is_same_space_type(self.action_space, gym.spaces.Discrete)
+        '''
+        return isinstance(self.action_space, gym.spaces.Discrete)
 
     def is_continuous_action_env(self):
-        """Check whether the action to execute in the environment is continuous
-        or not.
+        '''
+        Check whether the action to execute in the environment is continuous or not
 
         Returns:
             bool: True if the action to execute in the environment is continuous. Otherwise False.
-                Note that if the action is gym.spaces.Tuple and all of the element are continuous, it returns True.
-        """
-        return is_same_space_type(self.action_space, gym.spaces.Box)
-
-    def is_mixed_action_env(self):
-        """Check whether the action of the environment consists of either
-        continuous or discrete action.
-
-        Returns:
-            bool: True if the action of the environment is either continuous or discrete. Otherwise False.
-                Note that if the action is not a gym.spaces.Tuple, then returns False.
-        """
-        if not self.is_tuple_action_env():
-            return False
-        return all(isinstance(a, gym.spaces.Discrete) or isinstance(a, gym.spaces.Box) for a in self.action_space)
-
-    def is_tuple_action_env(self):
-        """Check whether the action of the environment is tuple or not.
-
-        Returns:
-            bool: True if the action of the environment is tuple. Otherwise False.
-        """
-        return isinstance(self.action_space, gym.spaces.Tuple)
-
-    def is_discrete_state_env(self):
-        """Check whether the state of the environment is discrete or not.
-
-        Returns:
-            bool: True if the state of the environment is discrete. Otherwise False.
-                Note that if the state is gym.spaces.Tuple and all of the element are discrete, it returns True.
-        """
-        return is_same_space_type(self.observation_space, gym.spaces.Discrete)
-
-    def is_continuous_state_env(self):
-        """Check whether the state of the environment is continuous or not.
-
-        Returns:
-            bool: True if the state of the environment is continuous. Otherwise False.
-                Note that if the state is gym.spaces.Tuple and all of the element are continuous, it returns True.
-        """
-        return is_same_space_type(self.observation_space, gym.spaces.Box)
-
-    def is_mixed_state_env(self):
-        """Check whether the state of the environment consists of either
-        continuous or discrete state.
-
-        Returns:
-            bool: True if the state of the environment is either continuous or discrete. Otherwise False.
-                Note that if the state is not a gym.spaces.Tuple, then returns False.
-        """
-        if not self.is_tuple_state_env():
-            return False
-        return all(isinstance(s, gym.spaces.Discrete) or isinstance(s, gym.spaces.Box) for s in self.observation_space)
-
-    def is_tuple_state_env(self):
-        """Check whether the state of the environment is tuple or not.
-
-        Returns:
-            bool: True if the state of the environment is tuple. Otherwise False.
-        """
-        return isinstance(self.observation_space, gym.spaces.Tuple)
-
-    def is_goal_conditioned_env(self):
-        """Check whether the environment is gym.GoalEnv or not.
-
-        Returns:
-            bool: True if the environment is GoalEnv. Otherwise False.
-        """
-        return issubclass(self.unwrapped_env.__class__, GoalEnv)
+        '''
+        return not self.is_discrete_action_env()
 
     @property
     def state_shape(self):
-        """The shape of observation space."""
-        if self.is_tuple_state_env():
-            return tuple(map(get_space_shape, self.observation_space))
-        else:
-            return get_space_shape(self.observation_space)
+        '''
+        The shape of observation space
+        '''
+        return self.observation_space.shape
 
     @property
     def state_dim(self):
-        """The dimension of state assuming that the state is flatten."""
-        if self.is_tuple_state_env():
-            return tuple(map(get_space_dim, self.observation_space))
-        else:
-            return get_space_dim(self.observation_space)
-
-    @property
-    def state_high(self):
-        """The upper limit of observation space."""
-        if self.is_tuple_state_env():
-            return tuple(map(get_space_high, self.observation_space))
-        else:
-            return get_space_high(self.observation_space)
-
-    @property
-    def state_low(self):
-        """The lower limit of observation space."""
-        if self.is_tuple_state_env():
-            return tuple(map(get_space_low, self.observation_space))
-        else:
-            return get_space_low(self.observation_space)
-
-    @property
-    def action_high(self):
-        """The upper limit of action space."""
-        if self.is_tuple_action_env():
-            return tuple(map(get_space_high, self.action_space))
-        else:
-            return get_space_high(self.action_space)
-
-    @property
-    def action_low(self):
-        """The lower limit of action space."""
-        if self.is_tuple_action_env():
-            return tuple(map(get_space_low, self.action_space))
-        else:
-            return get_space_low(self.action_space)
+        '''
+        The dimension of state assuming that the state is flatten.
+        '''
+        return np.prod(self.observation_space.shape)
 
     @property
     def action_shape(self):
-        """The shape of action space."""
-        if self.is_tuple_action_env():
-            return tuple(map(get_space_shape, self.action_space))
-        else:
-            return get_space_shape(self.action_space)
+        '''
+        The shape of action space
+        '''
+        return self.action_space.shape
 
     @property
     def action_dim(self):
-        """The dimension of action assuming that the action is flatten."""
-        if self.is_tuple_action_env():
-            return tuple(map(get_space_dim, self.action_space))
+        '''
+        The dimension of action assuming that the action is flatten.
+        '''
+        if self.is_discrete_action_env():
+            return self.action_space.n
+        else:
+            return np.prod(self.action_space.shape)
+
+    @staticmethod
+    def _extract_max_episode_steps(env):
+        if env.spec is None or env.spec.max_episode_steps is None:
+            return float("inf")
         else:
-            return get_space_dim(self.action_space)
+            return env.spec.max_episode_steps
```

## nnabla_rl/environments/wrappers/__init__.py

```diff
@@ -1,23 +1,17 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023,2024 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from nnabla_rl.environments.wrappers.common import (Float32RewardEnv, HWCToCHWEnv, NumpyFloat32Env,  # noqa
-                                                    ScreenRenderEnv, TimestepAsStateEnv, FlattenNestedTupleStateWrapper)
-
-from nnabla_rl.environments.wrappers.mujoco import EndlessEnv  # noqa
+from nnabla_rl.environments.wrappers.common import NumpyFloat32Env, ScreenRenderEnv  # noqa
 from nnabla_rl.environments.wrappers.atari import make_atari, wrap_deepmind  # noqa
-from nnabla_rl.environments.wrappers.hybrid_env import (EmbedActionWrapper, FlattenActionWrapper,  # noqa
-                                                        RemoveStepWrapper, ScaleActionWrapper, ScaleStateWrapper)
-from nnabla_rl.environments.wrappers.gymnasium import Gymnasium2GymWrapper  # noqa
```

## nnabla_rl/environments/wrappers/atari.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023,2024 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -12,41 +12,23 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 from collections import deque
 
 import cv2
 import gym
-import gymnasium
 import numpy as np
 from gym import spaces
 
-import nnabla_rl as rl
-from nnabla_rl.environments.wrappers.gymnasium import Gymnasium2GymWrapper
 from nnabla_rl.external.atari_wrappers import (ClipRewardEnv, EpisodicLifeEnv, FireResetEnv, MaxAndSkipEnv,
                                                NoopResetEnv, ScaledFloatFrame)
 
 cv2.ocl.setUseOpenCL(False)
 
 
-class FlickerFrame(gym.ObservationWrapper):
-    """Obscure (blackout) screen with flicker_probability."""
-
-    def __init__(self, env, flicker_probability=0.5):
-        gym.ObservationWrapper.__init__(self, env)
-        self.width = 84
-        self.height = 84
-        obs_shape = (1, self.height, self.width)  # 'chw' order
-        self.observation_space = spaces.Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)
-        self.flicker_probability = flicker_probability
-
-    def observation(self, frame):
-        return frame * float(self.flicker_probability < rl.random.drng.uniform())
-
-
 class CHWWarpFrame(gym.ObservationWrapper):
     def __init__(self, env):
         gym.ObservationWrapper.__init__(self, env)
         self.width = 84
         self.height = 84
         obs_shape = (1, self.height, self.width)  # 'chw' order
         self.observation_space = spaces.Box(
@@ -95,47 +77,30 @@
     def __array__(self, dtype=None):
         out = np.concatenate(self._frames, axis=0)
         if dtype is not None:
             out = out.astype(dtype)
         return out
 
 
-def make_atari(env_id, max_frames_per_episode=None, use_gymnasium=False):
-    if use_gymnasium:
-        env = gymnasium.make(env_id)
-        env = Gymnasium2GymWrapper(env)
-        # gymnasium env is not wrapped TimeLimit wrapper
-        env = gym.wrappers.TimeLimit(env, max_episode_steps=env.spec.kwargs["max_num_frames_per_episode"])
-    else:
-        env = gym.make(env_id)
-    if max_frames_per_episode is not None:
-        env = env.unwrapped
-        env = gym.wrappers.TimeLimit(env, max_episode_steps=max_frames_per_episode)
+def make_atari(env_id):
+    env = gym.make(env_id)
     assert 'NoFrameskip' in env.spec.id
     assert isinstance(env, gym.wrappers.TimeLimit)
     env = NoopResetEnv(env, noop_max=30)
     env = MaxAndSkipEnv(env, skip=4)
     return env
 
 
-def wrap_deepmind(env,
-                  episode_life=True,
-                  clip_rewards=True,
-                  normalize=True,
-                  frame_stack=True,
-                  fire_reset=False,
-                  flicker_probability=0.0):
+def wrap_deepmind(env, episode_life=True, clip_rewards=True, normalize=True, frame_stack=True, fire_reset=False):
     """Configure environment for DeepMind-style Atari."""
     if episode_life:
         env = EpisodicLifeEnv(env)
     if fire_reset and 'FIRE' in env.unwrapped.get_action_meanings():
         env = FireResetEnv(env)
     env = CHWWarpFrame(env)
-    if 0.0 < flicker_probability:
-        env = FlickerFrame(env, flicker_probability=flicker_probability)
     if normalize:
         env = ScaledFloatFrame(env)
     if clip_rewards:
         env = ClipRewardEnv(env)
     if frame_stack:
         env = CHWFrameStack(env, 4)
     return env
```

## nnabla_rl/environments/wrappers/common.py

```diff
@@ -1,116 +1,55 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023,2024 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import cv2
 import gym
 import numpy as np
-from gym import spaces
-from packaging.version import parse
-
-from nnabla_rl.logger import logger
 
 
 class Float32ObservationEnv(gym.ObservationWrapper):
     def __init__(self, env):
         super(Float32ObservationEnv, self).__init__(env)
         self.dtype = np.float32
-        if isinstance(env.observation_space, spaces.Tuple):
-            self.observation_space = spaces.Tuple(
-                [self._create_observation_space(observation_space)
-                 for observation_space in env.observation_space]
-            )
-        else:
-            self.observation_space = self._create_observation_space(env.observation_space)
-
-    def _create_observation_space(self, observation_space):
-        if isinstance(observation_space, spaces.Box):
-            return spaces.Box(
-                low=observation_space.low,
-                high=observation_space.high,
-                shape=observation_space.shape,
-                dtype=self.dtype
-            )
-        elif isinstance(observation_space, spaces.Discrete):
-            return spaces.Discrete(n=observation_space.n)
-        else:
-            raise NotImplementedError
+        self.observation_space.dtype = np.dtype(np.float32)
 
     def observation(self, observation):
-        if isinstance(observation, tuple):
-            return tuple(map(self.dtype, observation))
-        else:
-            return self.dtype(observation)
+        return self.dtype(observation)
 
 
 class Float32RewardEnv(gym.RewardWrapper):
     def __init__(self, env):
         super(Float32RewardEnv, self).__init__(env)
         self.dtype = np.float32
 
     def reward(self, reward):
         return self.dtype(reward)
 
 
 class Float32ActionEnv(gym.ActionWrapper):
     def __init__(self, env):
         super(Float32ActionEnv, self).__init__(env)
-        self.continuous_dtype = np.float32
-        self.discrete_dtype = np.int32
-        if isinstance(env.action_space, spaces.Tuple):
-            self.action_space = spaces.Tuple(
-                [self._create_action_space(action_space) for action_space in env.action_space]
-            )
-        else:
-            self.action_space = self._create_action_space(env.action_space)
+        self.dtype = np.float32
 
     def action(self, action):
-        def _action(action, action_space):
-            if isinstance(action_space, spaces.Discrete):
-                if isinstance(action, np.ndarray):
-                    action = action[0]
-                return self.discrete_dtype(action)
-            else:
-                return self.continuous_dtype(action)
-
-        if isinstance(action, tuple):
-            return tuple(_action(a, action_space) for a, action_space in zip(action, self.env.action_space))
-        else:
-            return self.continuous_dtype(action)
+        return self.dtype(action)
 
     def reverse_action(self, action):
-        if isinstance(action, tuple):
-            return tuple(action_space.dtype(action) for action_space in self.env.action_space)
-        else:
-            return self.env.action_space.dtype(action)
-
-    def _create_action_space(self, action_space):
-        if isinstance(action_space, spaces.Box):
-            return spaces.Box(
-                low=action_space.low,
-                high=action_space.high,
-                shape=action_space.shape,
-                dtype=self.continuous_dtype
-            )
-        elif isinstance(action_space, spaces.Discrete):
-            return spaces.Discrete(n=action_space.n)
-        else:
-            raise NotImplementedError
+        return self.env.action_space.dtype(action)
 
 
 class Int32ActionEnv(gym.ActionWrapper):
     def __init__(self, env):
         super(Int32ActionEnv, self).__init__(env)
         self.dtype = np.int32
 
@@ -134,168 +73,21 @@
 
         super(NumpyFloat32Env, self).__init__(env)
 
 
 class ScreenRenderEnv(gym.Wrapper):
     def __init__(self, env):
         super(ScreenRenderEnv, self).__init__(env)
-        self._installed_gym_version = parse(gym.__version__)
-        self._gym_version25 = parse('0.25.0')
-        self._env_name = "Unknown" if env.unwrapped.spec is None else env.unwrapped.spec.id
+        self._env_name = env.unwrapped.spec.id
 
     def step(self, action):
-        results = self.env.step(action)
-        self._render_env()
-        return results
+        self.env.render()
+        return self.env.step(action)
 
     def reset(self):
         if 'Bullet' in self._env_name:
-            self._render_env()
+            self.env.render()
             state = self.env.reset()
         else:
             state = self.env.reset()
-            self._render_env()
-        return state
-
-    def _render_env(self):
-        if self._gym_version25 <= self._installed_gym_version:
-            # 0.25.0 <= gym version
-            rgb_array = self.env.render(mode='rgb_array')
-            cv2.imshow(f'{self._env_name}', cv2.cvtColor(rgb_array, cv2.COLOR_RGB2BGR))
-            cv2.waitKey(1)
-        else:
-            # old gym version
             self.env.render()
-
-
-class PrintEpisodeResultEnv(gym.Wrapper):
-    def __init__(self, env):
-        super().__init__(env)
-        self._episode_rewards = []
-        self._episode_num = 0
-
-    def step(self, action):
-        s_next, reward, done, info = self.env.step(action)
-        self._episode_rewards.append(reward)
-        if done:
-            self._episode_num += 1
-            episode_steps = len(self._episode_rewards)
-            episode_return = np.sum(self._episode_rewards)
-            logger.info(f'Episode #{self._episode_num} finished.')
-            logger.info(f'Episode steps: {episode_steps}. Total return: {episode_return}.')
-            self._episode_rewards.clear()
-        return s_next, reward, done, info
-
-    def reset(self):
-        self._episode_rewards.clear()
-        return self.env.reset()
-
-
-class TimestepAsStateEnv(gym.Wrapper):
-    """Timestep as state environment wrapper.
-
-    This wrapper adds the timestep to original state. The concatenated
-    state provides in TupleState type.
-    """
-
-    def __init__(self, env):
-        super(TimestepAsStateEnv, self).__init__(env)
-        self._timestep = 0
-        obs_space = self.observation_space
-        timestep_obs_space = spaces.Box(low=0., high=np.inf, shape=(1, ), dtype=np.float32)
-        self.observation_space = spaces.Tuple([obs_space, timestep_obs_space])
-
-    def reset(self):
-        observation = self.env.reset()
-        self._timestep = 0
-        return self.observation(observation)
-
-    def step(self, action):
-        observation, reward, done, info = self.env.step(action)
-        self._timestep += 1
-        return self.observation(observation), reward, done, info
-
-    def observation(self, observation):
-        return (observation, np.ones(1, dtype=np.int32) * self._timestep)
-
-
-class HWCToCHWEnv(gym.ObservationWrapper):
-    """HWC to CHW env wrapper.
-
-    This wrapper changes the order of the image, from (height, width,
-    channel) to (channel, height, width)
-    """
-
-    def __init__(self, env):
-        gym.ObservationWrapper.__init__(self, env)
-        height, width, channel = self.observation_space.shape
-        self.observation_space = spaces.Box(low=0, high=255, shape=(channel, height, width), dtype=np.uint8)
-
-    def observation(self, obs):
-        return np.transpose(obs, [2, 0, 1])
-
-
-class FlattenNestedTupleStateWrapper(gym.ObservationWrapper):
-    """Flatten a nested tuple state observation wrapper.
-
-    This wapper flattens a state.
-    For example, if the original env_info.observation_shape is
-
-
-    ```
-    Tuple(Tuple(Box(-inf, inf, (2,), float32), Box(-inf, inf, (6,), float32)),
-          Tuple(Box(-inf, inf, (1,), float32), Box(-inf, inf, (3,), float32)))
-    ```,
-
-    then the wrapped observation_shape is
-
-    ```
-    Tuple(Box(-inf, inf, (2,), float32),
-          Box(-inf, inf, (6,), float32),
-          Box(-inf, inf, (1,), float32),
-          Box(-inf, inf, (3,), float32))
-    ```.
-    """
-
-    def __init__(self, env):
-        super().__init__(env)
-        original_observation_space = env.observation_space
-        assert isinstance(original_observation_space, gym.spaces.Tuple)
-        self.observation_space = self._flatten_observation_space(original_observation_space)
-
-    def _flatten_observation_space(self, observation_space):
-        flattened_obs = []
-        for space in observation_space:
-            if isinstance(space, gym.spaces.Tuple):
-                space = self._flatten_tuple_space(space)
-                flattened_obs.extend(space)
-            else:
-                flattened_obs.append(space)
-        return gym.spaces.Tuple(flattened_obs)
-
-    def _flatten_tuple_space(self, tuple_space):
-        flattened = []
-        for item in tuple_space:
-            if isinstance(item, gym.spaces.Tuple):
-                flattened.extend(self._flatten_tuple_space(item))
-            else:
-                flattened.append(item)
-        return flattened
-
-    def observation(self, observation):
-        flattened_obs = []
-        for obs in observation:
-            if isinstance(obs, tuple):
-                obs = self._flatten_nested_array(obs)
-                flattened_obs.extend(obs)
-            else:
-                flattened_obs.append(obs)
-        return tuple(flattened_obs)
-
-    def _flatten_nested_array(self, arr):
-        flattened = []
-        for item in arr:
-            if isinstance(item, tuple):
-                flattened.extend(self._flatten_nested_array(item))
-            else:
-                flattened.append(item)
-        return flattened
+        return state
```

## nnabla_rl/external/atari_wrappers.py

```diff
@@ -33,50 +33,44 @@
 
 cv2.ocl.setUseOpenCL(False)
 
 
 class NoopResetEnv(gym.Wrapper):
     def __init__(self, env, noop_max=30):
         """Sample initial states by taking random number of no-ops on reset.
-
         No-op is assumed to be action 0.
         """
         gym.Wrapper.__init__(self, env)
         self.noop_max = noop_max
         self.override_num_noops = None
         self.noop_action = 0
-        if hasattr(self.unwrapped.np_random, 'randint'):
-            self._randint = self.unwrapped.np_random.randint
-        else:
-            self._randint = self.unwrapped.np_random.integers
         assert env.unwrapped.get_action_meanings()[0] == 'NOOP'
 
     def reset(self, **kwargs):
-        """Do no-op action for a number of steps in [1, noop_max]."""
+        """ Do no-op action for a number of steps in [1, noop_max]."""
         self.env.reset(**kwargs)
         if self.override_num_noops is not None:
             noops = self.override_num_noops
         else:
-            noops = self._randint(1, self.noop_max + 1)  # pylint: disable=E1101
+            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1)  # pylint: disable=E1101
         assert noops > 0
         obs = None
         for _ in range(noops):
             obs, _, done, _ = self.env.step(self.noop_action)
             if done:
                 obs = self.env.reset(**kwargs)
         return obs
 
     def step(self, ac):
         return self.env.step(ac)
 
 
 class FireResetEnv(gym.Wrapper):
     def __init__(self, env):
-        """Take action on reset for environments that are fixed until
-        firing."""
+        """Take action on reset for environments that are fixed until firing."""
         gym.Wrapper.__init__(self, env)
         assert env.unwrapped.get_action_meanings()[1] == 'FIRE'
         assert len(env.unwrapped.get_action_meanings()) >= 3
 
     def reset(self, **kwargs):
         self.env.reset(**kwargs)
         obs, _, done, _ = self.env.step(1)
@@ -89,19 +83,16 @@
 
     def step(self, ac):
         return self.env.step(ac)
 
 
 class EpisodicLifeEnv(gym.Wrapper):
     def __init__(self, env):
-        """Make end-of-life == end-of-episode, but only reset on true game
-        over.
-
-        Done by DeepMind for the DQN and co. since it helps value
-        estimation.
+        """Make end-of-life == end-of-episode, but only reset on true game over.
+        Done by DeepMind for the DQN and co. since it helps value estimation.
         """
         gym.Wrapper.__init__(self, env)
         self.lives = 0
         self.was_real_done = True
 
     def step(self, action):
         obs, reward, done, info = self.env.step(action)
@@ -115,31 +106,29 @@
             # the environment advertises done.
             done = True
         self.lives = lives
         return obs, reward, done, info
 
     def reset(self, **kwargs):
         """Reset only when lives are exhausted.
-
-        This way all states are still reachable even though lives are
-        episodic, and the learner need not know about any of this
-        behind-the-scenes.
+        This way all states are still reachable even though lives are episodic,
+        and the learner need not know about any of this behind-the-scenes.
         """
         if self.was_real_done:
             obs = self.env.reset(**kwargs)
         else:
             # no-op step to advance from terminal/lost life state
             obs, _, _, _ = self.env.step(0)
         self.lives = self.env.unwrapped.ale.lives()
         return obs
 
 
 class MaxAndSkipEnv(gym.Wrapper):
     def __init__(self, env, skip=4):
-        """Return only every `skip`-th frame."""
+        """Return only every `skip`-th frame"""
         gym.Wrapper.__init__(self, env)
         # most recent raw observations (for max pooling across time steps)
         self._obs_buffer = np.zeros((2,)+env.observation_space.shape, dtype=np.uint8)
         self._skip = skip
 
     def step(self, action):
         """Repeat action, sum reward, and max over last observations."""
@@ -171,18 +160,17 @@
     def reward(self, reward):
         """Bin reward to {+1, 0, -1} by its sign."""
         return np.sign(reward)
 
 
 class WarpFrame(gym.ObservationWrapper):
     def __init__(self, env, width=84, height=84, grayscale=True, dict_space_key=None):
-        """Warp frames to 84x84 as done in the Nature paper and later work.
-
-        If the environment uses dictionary observations,
-        `dict_space_key` can be specified which indicates which
+        """
+        Warp frames to 84x84 as done in the Nature paper and later work.
+        If the environment uses dictionary observations, `dict_space_key` can be specified which indicates which
         observation should be warped.
         """
         super().__init__(env)
         self._width = width
         self._height = height
         self._grayscale = grayscale
         self._key = dict_space_key
@@ -225,17 +213,16 @@
             obs = obs.copy()
             obs[self._key] = frame
         return obs
 
 
 class FrameStack(gym.Wrapper):
     def __init__(self, env, k):
-        """Stack k last frames. Returns lazy array, which is much more memory
-        efficient.
-
+        """Stack k last frames.
+        Returns lazy array, which is much more memory efficient.
         See Also
         --------
         baselines.common.atari_wrappers.LazyFrames
         """
         gym.Wrapper.__init__(self, env)
         self.k = k
         self.frames = deque([], maxlen=k)
@@ -268,22 +255,19 @@
         # careful! This undoes the memory optimization, use
         # with smaller replay buffers only.
         return np.array(observation).astype(np.float32) / 255.0
 
 
 class LazyFrames(object):
     def __init__(self, frames):
-        """This object ensures that common frames between the observations are
-        only stored once.
-
-        It exists purely to optimize memory usage which can be huge for
-        DQN's 1M frames replay buffers. This object should only be
-        converted to numpy array before being passed to the model. You'd
-        not believe how complex the previous solution was.
-        """
+        """This object ensures that common frames between the observations are only stored once.
+        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay
+        buffers.
+        This object should only be converted to numpy array before being passed to the model.
+        You'd not believe how complex the previous solution was."""
         self._frames = frames
         self._out = None
 
     def _force(self):
         if self._out is None:
             self._out = np.concatenate(self._frames, axis=-1)
             self._frames = None
@@ -316,15 +300,16 @@
     env = MaxAndSkipEnv(env, skip=4)
     if max_episode_steps is not None:
         env = gym.wrappers.TimeLimit(env, max_episode_steps=max_episode_steps)
     return env
 
 
 def wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=False, scale=False):
-    """Configure environment for DeepMind-style Atari."""
+    """Configure environment for DeepMind-style Atari.
+    """
     if episode_life:
         env = EpisodicLifeEnv(env)
     if 'FIRE' in env.unwrapped.get_action_meanings():
         env = FireResetEnv(env)
     env = WarpFrame(env)
     if scale:
         env = ScaledFloatFrame(env)
```

## nnabla_rl/hooks/__init__.py

```diff
@@ -1,23 +1,20 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from nnabla_rl.hooks.epoch_num_hook import EpochNumHook  # noqa
 from nnabla_rl.hooks.evaluation_hook import EvaluationHook  # noqa
 from nnabla_rl.hooks.save_snapshot_hook import SaveSnapshotHook  # noqa
 from nnabla_rl.hooks.iteration_num_hook import IterationNumHook  # noqa
 from nnabla_rl.hooks.iteration_state_hook import IterationStateHook  # noqa
 from nnabla_rl.hooks.time_measuring_hook import TimeMeasuringHook  # noqa
-from nnabla_rl.hooks.computational_graph_hook import TrainingGraphHook  # noqa
-from nnabla_rl.hooks.progress_bar_hook import ProgressBarHook  # noqa
```

## nnabla_rl/hooks/evaluation_hook.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -17,25 +17,26 @@
 
 from nnabla_rl.hook import Hook
 from nnabla_rl.logger import logger
 from nnabla_rl.utils.evaluator import EpisodicEvaluator
 
 
 class EvaluationHook(Hook):
-    """Hook to run evaluation during training.
+    '''
+    Hook to run evaluation during training.
 
     Args:
         env (gym.Env): Environment to run the evaluation
         evaluator (Callable[[nnabla_rl.algorithm.Algorithm, gym.Env], List[float]]):
             Evaluator which runs the actual evaluation.
             Defaults to :py:class:`EpisodicEvaluator <nnabla_rl.utils.evaluator.EpisodicEvaluator>`.
         timing (int): Evaluation interval. Defaults to 1000 iteration.
         writer (nnabla_rl.writer.Writer, optional): Writer instance to save/print the evaluation results.
             Defaults to None.
-    """
+    '''
 
     def __init__(self, env, evaluator=EpisodicEvaluator(), timing=1000, writer=None):
         super(EvaluationHook, self).__init__(timing=timing)
         self._env = env
         self._evaluator = evaluator
         self._timing = timing
         self._writer = writer
```

## nnabla_rl/hooks/iteration_num_hook.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -14,19 +14,20 @@
 # limitations under the License.
 
 from nnabla_rl.hook import Hook
 from nnabla_rl.logger import logger
 
 
 class IterationNumHook(Hook):
-    """Hook to print the iteration number periodically. This hook just prints
-    the iteration number of training.
+    '''
+    Hook to print the iteration number periodically.
+    This hook just prints the iteration number of training.
 
     Args:
         timing (int): Printing interval. Defaults to 1 iteration.
-    """
+    '''
 
     def __init__(self, timing=1):
         super(IterationNumHook, self).__init__(timing=timing)
 
     def on_hook_called(self, algorithm):
         logger.info("current iteration: {}".format(algorithm.iteration_num))
```

## nnabla_rl/hooks/iteration_state_hook.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -16,22 +16,22 @@
 import pprint
 
 from nnabla_rl.hook import Hook
 from nnabla_rl.logger import logger
 
 
 class IterationStateHook(Hook):
-    """Hook which retrieves the iteration state to print/save the training
-    status through writer.
+    '''
+    Hook which retrieves the iteration state to print/save the training status through writer.
 
     Args:
         timing (int): Retriving interval. Defaults to 1000 iteration.
         writer (nnabla_rl.writer.Writer, optional): Writer instance to save/print the iteration states.
             Defaults to None.
-    """
+    '''
 
     def __init__(self, writer=None, timing=1000):
         self._timing = timing
         self._writer = writer
 
     def on_hook_called(self, algorithm):
         logger.info('Iteration state at iteration {}'.format(
```

## nnabla_rl/hooks/save_snapshot_hook.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -14,19 +14,20 @@
 # limitations under the License.
 
 from nnabla_rl.hook import Hook
 from nnabla_rl.utils.serializers import save_snapshot
 
 
 class SaveSnapshotHook(Hook):
-    """Hook to save the training snapshot of current algorithm.
+    '''
+    Hook to save the training snapshot of current algorithm.
 
     Args:
         timing (int): Saving interval. Defaults to 1000 iteration.
-    """
+    '''
 
     def __init__(self, outdir, timing=1000):
         super(SaveSnapshotHook, self).__init__(timing=timing)
         self._outdir = outdir
 
     def on_hook_called(self, algorithm):
         save_snapshot(self._outdir, algorithm)
```

## nnabla_rl/hooks/time_measuring_hook.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -16,19 +16,20 @@
 import time
 
 from nnabla_rl.hook import Hook
 from nnabla_rl.logger import logger
 
 
 class TimeMeasuringHook(Hook):
-    """Hook to measure and print the actual time spent to run the iteration(s).
+    '''
+    Hook to measure and print the actual time spent to run the iteration(s).
 
     Args:
         timing (int): Measuring interval. Defaults to 1 iteration.
-    """
+    '''
 
     def __init__(self, timing=1):
         super(TimeMeasuringHook, self).__init__(timing=timing)
         self._prev_time = time.time()
 
     def on_hook_called(self, algorithm):
         current_time = time.time()
```

## nnabla_rl/model_trainers/__init__.py

```diff
@@ -1,24 +1,21 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023,2024 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from nnabla_rl.model_trainers import decision_transformer as dt_trainers  # noqa
-from nnabla_rl.model_trainers import dynamics as dynamics_trainers  # noqa
-from nnabla_rl.model_trainers import encoder as encoder_trainers  # noqa
-from nnabla_rl.model_trainers import hybrid as hybrid_trainers  # noqa
 from nnabla_rl.model_trainers import perturbator as perturbator_trainers  # noqa
 from nnabla_rl.model_trainers import policy as policy_trainers  # noqa
 from nnabla_rl.model_trainers import q_value as q_value_trainers  # noqa
 from nnabla_rl.model_trainers import v_value as v_value_trainers  # noqa
+from nnabla_rl.model_trainers import encoder as encoder_trainers  # noqa
 from nnabla_rl.model_trainers import reward as reward_trainiers  # noqa
```

## nnabla_rl/model_trainers/model_trainer.py

```diff
@@ -1,123 +1,83 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023,2024 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import contextlib
 from abc import ABCMeta, abstractmethod
 from dataclasses import dataclass
-from enum import Enum
-from typing import Dict, Optional, Sequence, Tuple, Union
+from typing import Dict, Optional, Sequence, Union
 
 import numpy as np
 
 import nnabla as nn
 from nnabla_rl.configuration import Configuration
 from nnabla_rl.environments.environment_info import EnvironmentInfo
 from nnabla_rl.models import Model
 from nnabla_rl.utils.data import convert_to_list_if_not_list
-from nnabla_rl.utils.misc import retrieve_internal_states
-
-
-@contextlib.contextmanager
-def rnn_support(model: Model,
-                prev_rnn_states: Dict[str, Dict[str, nn.Variable]],
-                train_rnn_states: Dict[str, Dict[str, nn.Variable]],
-                training_variables: 'TrainingVariables',
-                config: 'TrainerConfig'):
-    def stop_backprop(rnn_states):
-        for value in rnn_states.values():
-            value.need_grad = False
-    try:
-        if model.is_recurrent():
-            scope_name = model.scope_name
-            internal_states = retrieve_internal_states(
-                scope_name, prev_rnn_states, train_rnn_states, training_variables, config.reset_on_terminal)
-            model.set_internal_states(internal_states)
-        yield
-    finally:
-        if model.is_recurrent():
-            rnn_states = model.get_internal_states()
-            if training_variables.step_index() < config.burn_in_steps:
-                stop_backprop(rnn_states)
-            prev_rnn_states[model.scope_name] = rnn_states
-
-
-class LossIntegration(Enum):
-    ALL_TIMESTEPS = 1, 'Computed loss is summed over all timesteps'
-    LAST_TIMESTEP_ONLY = 2, 'Only the last timestep\'s loss is used.'
 
 
 @dataclass
 class TrainerConfig(Configuration):
-    """Configuration class for ModelTrainer."""
-    unroll_steps: int = 1
-    burn_in_steps: int = 0
-    reset_on_terminal: bool = True  # Reset internal rnn state to given state if previous state is terminal.
-    loss_integration: LossIntegration = LossIntegration.ALL_TIMESTEPS
+    """Configuration class for ModelTrainer
+    """
 
     def __post_init__(self):
         super(TrainerConfig, self).__post_init__()
-        self._assert_positive(self.unroll_steps, 'unroll_steps')
-        self._assert_positive_or_zero(self.burn_in_steps, 'burn_in_steps')
 
 
 class TrainingBatch():
-    """Mini-Batch class for train.
+    """Mini-Batch class for train
 
     Args:
        batch_size (int): the size of mini-batch
-       s_current (Optional[np.ndarray]): the current state array
-       a_current (Optional[np.ndarray]): the current action array
-       reward (Optional[np.ndarray]): the reward value array
+       s_current (Optional[np.array]): the current state array
+       a_current (Optional[np.array]): the current action array
+       reward (Optional[np.array]): the reward value array
        gamma (Optional[float]): gamma value
-       non_terminal (Optional[np.ndarray]): the non_terminal flag array
-       s_next (Optional[np.ndarray]): the next state array
-       weight (Optional[np.ndarray]): the weight of loss array
-       extra (Dict[str, np.ndarray]): the extra information
+       non_terminal (Optional[np.array]): the non_terminal flag array
+       s_next (Optional[np.array]): the next state array
+       weight (Optional[np.array]): the weight of loss array
+       extra (Dict[str, np.array]): the extra information
        next_step_batch (Optional[:py:class:`TrainingBatch <nnabla_rl.model_trainers.model_trainer.TrainingBatch>`]):\
            the mini-batch for next step (used in n-step learning)
-       rnn_states (Dict[str, Dict[str, np.array]]): the rnn internal state values
     """
     batch_size: int
-    s_current: Union[np.ndarray, Tuple[np.ndarray, ...]]
-    a_current: np.ndarray
-    reward: np.ndarray
+    s_current: np.array
+    a_current: np.array
+    reward: np.array
     gamma: float
-    non_terminal: np.ndarray
-    s_next: Union[np.ndarray, Tuple[np.ndarray, ...]]
-    weight: np.ndarray
-    extra: Dict[str, np.ndarray]
-    # Used in n-step/rnn learning
+    non_terminal: np.array
+    s_next: np.array
+    weight: np.array
+    extra: Dict[str, np.array]
+    # Used in n-step learning
     next_step_batch: Optional['TrainingBatch']
-    rnn_states: Dict[str, Dict[str, np.ndarray]]
 
     def __init__(self,
                  batch_size: int,
-                 s_current: Optional[Union[np.ndarray, Tuple[np.ndarray, ...]]] = None,
-                 a_current: Optional[np.ndarray] = None,
-                 reward: Optional[np.ndarray] = None,
+                 s_current: Optional[np.array] = None,
+                 a_current: Optional[np.array] = None,
+                 reward: Optional[np.array] = None,
                  gamma: Optional[float] = None,
-                 non_terminal: Optional[np.ndarray] = None,
-                 s_next: Optional[Union[np.ndarray, Tuple[np.ndarray, ...]]] = None,
-                 weight: Optional[np.ndarray] = None,
-                 extra: Optional[Dict[str, np.ndarray]] = None,
-                 next_step_batch: Optional['TrainingBatch'] = None,
-                 rnn_states: Optional[Dict[str, Dict[str, np.ndarray]]] = None):
+                 non_terminal: Optional[np.array] = None,
+                 s_next: Optional[np.array] = None,
+                 weight: Optional[np.array] = None,
+                 extra: Dict[str, np.array] = {},
+                 next_step_batch: Optional['TrainingBatch'] = None):
         assert 0 < batch_size
         self.batch_size = batch_size
         if s_current is not None:
             self.s_current = s_current
         if a_current is not None:
             self.a_current = a_current
         if reward is not None:
@@ -126,71 +86,38 @@
             self.gamma = gamma
         if non_terminal is not None:
             self.non_terminal = non_terminal
         if s_next is not None:
             self.s_next = s_next
         if weight is not None:
             self.weight = weight
-        self.extra: Dict[str, np.ndarray] = {} if extra is None else extra
+        self.extra: Dict[str, np.array] = extra
         self.next_step_batch = next_step_batch
-        self.rnn_states = {} if rnn_states is None else rnn_states
-
-    def __getitem__(self, index):
-        num_steps = len(self)
-        if num_steps <= index:
-            raise IndexError
-
-        batch = self
-        for _ in range(index):
-            batch = batch.next_step_batch
-        return batch
-
-    def __iter__(self):
-        batch = self
-        while batch is not None:
-            yield batch
-            batch = batch.next_step_batch
-
-    def __len__(self):
-        num_steps = 1
-        batch = self.next_step_batch
-        while batch is not None:
-            num_steps += 1
-            batch = batch.next_step_batch
-        return num_steps
 
 
 class TrainingVariables():
     batch_size: int
-    s_current: Union[nn.Variable, Tuple[nn.Variable, ...]]
+    s_current: nn.Variable
     a_current: nn.Variable
     reward: nn.Variable
     gamma: nn.Variable
     non_terminal: nn.Variable
-    s_next: Union[nn.Variable, Tuple[nn.Variable, ...]]
+    s_next: nn.Variable
     weight: nn.Variable
-    extra: Dict[str, nn.Variable]
-    rnn_states: Dict[str, Dict[str, nn.Variable]]
-
-    # Used in rnn learning
-    _next_step_variables: Optional['TrainingVariables']
-    _prev_step_variables: Optional['TrainingVariables']
 
     def __init__(self,
                  batch_size: int,
-                 s_current: Optional[Union[nn.Variable, Tuple[nn.Variable, ...]]] = None,
+                 s_current: Optional[nn.Variable] = None,
                  a_current: Optional[nn.Variable] = None,
                  reward: Optional[nn.Variable] = None,
                  gamma: Optional[nn.Variable] = None,
                  non_terminal: Optional[nn.Variable] = None,
-                 s_next: Optional[Union[nn.Variable, Tuple[nn.Variable, ...]]] = None,
+                 s_next: Optional[nn.Variable] = None,
                  weight: Optional[nn.Variable] = None,
-                 extra: Optional[Dict[str, nn.Variable]] = None,
-                 next_step_variables: Optional["TrainingVariables"] = None,
-                 rnn_states: Optional[Dict[str, Dict[str, nn.Variable]]] = None):
+                 extra: Dict[str, nn.Variable] = {}):
         assert 0 < batch_size
         self.batch_size = batch_size
         if s_current is not None:
             self.s_current = s_current
         if a_current is not None:
             self.a_current = a_current
         if reward is not None:
@@ -199,125 +126,15 @@
             self.gamma = gamma
         if non_terminal is not None:
             self.non_terminal = non_terminal
         if s_next is not None:
             self.s_next = s_next
         if weight is not None:
             self.weight = weight
-        self.extra: Dict[str, nn.Variable] = {} if extra is None else extra
-        self.next_step_variables = next_step_variables
-        self._prev_step_variables = None
-        self.rnn_states = {} if rnn_states is None else rnn_states
-
-    @property
-    def next_step_variables(self) -> Optional["TrainingVariables"]:
-        return self._next_step_variables
-
-    @next_step_variables.setter
-    def next_step_variables(self, value: Optional["TrainingVariables"]) -> None:
-        self._next_step_variables = value
-        if self._next_step_variables is None:
-            return
-        if self._next_step_variables.prev_step_variables is not self:
-            self._next_step_variables.prev_step_variables = self
-
-    @property
-    def prev_step_variables(self) -> Optional["TrainingVariables"]:
-        return self._prev_step_variables
-
-    @prev_step_variables.setter
-    def prev_step_variables(self, value: Optional["TrainingVariables"]) -> None:
-        self._prev_step_variables = value
-        if self._prev_step_variables is None:
-            return
-        if self._prev_step_variables.next_step_variables is not self:
-            self._prev_step_variables.next_step_variables = self
-
-    def __getitem__(self, item: int) -> "TrainingVariables":
-        num_steps = len(self)
-        if num_steps <= item:
-            raise IndexError
-
-        variable = self
-        for _ in range(item):
-            assert variable.next_step_variables
-            variable = variable.next_step_variables
-        assert variable
-        return variable
-
-    def __iter__(self):
-        variable = self
-        while variable is not None:
-            yield variable
-            variable = variable.next_step_variables
-
-    def __len__(self):
-        num_steps = 1
-        variable = self.next_step_variables
-        while variable is not None:
-            num_steps += 1
-            variable = variable.next_step_variables
-        return num_steps
-
-    def is_initial_step(self) -> bool:
-        return self.prev_step_variables is None
-
-    def step_index(self):
-        if self._prev_step_variables is None:
-            return 0
-        else:
-            return 1 + self._prev_step_variables.step_index()
-
-    def get_variables(self, depth: int = 0) -> Dict[str, nn.Variable]:
-        variables = {}
-
-        prefix = f"step_{depth}_" if depth > 0 else ""
-
-        def _append_variable(name: str, variable: nn.Variable) -> None:
-            if variable is None:
-                return
-            if isinstance(variable, nn.Variable):
-                variables[f"{prefix}{name}"] = variable
-            elif isinstance(variable, (list, tuple)):
-                for i, v in enumerate(variable):
-                    _append_variable(f"{prefix}{name}_{i}", v)
-            else:
-                raise ValueError(f"invalid variable type: {type(variable)}")
-
-        # add standard variables
-        if hasattr(self, "s_current"):
-            _append_variable("s_current", self.s_current)
-        if hasattr(self, "a_current"):
-            _append_variable("a_current", self.a_current)
-        if hasattr(self, "reward"):
-            _append_variable("reward", self.reward)
-        if hasattr(self, "gamma"):
-            _append_variable("gamma", self.gamma)
-        if hasattr(self, "non_terminal"):
-            _append_variable("non_terminal", self.non_terminal)
-        if hasattr(self, "s_next"):
-            _append_variable("s_next", self.s_next)
-        if hasattr(self, "weight"):
-            _append_variable("weight", self.weight)
-
-        # recursively append next step variables
-        if self.next_step_variables:
-            next_step_variables = self.next_step_variables.get_variables(depth + 1)
-            variables.update(next_step_variables)
-
-        # add extra variables
-        for name, variable in self.extra.items():
-            _append_variable(name, variable)
-
-        # add rnn state variables
-        for state_name, state in self.rnn_states.items():
-            for variable_name, variable in state.items():
-                _append_variable(f"{state_name}_{variable_name}", variable)
-
-        return variables
+        self.extra: Dict[str, nn.Variable] = extra
 
 
 class ModelTrainer(metaclass=ABCMeta):
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _env_info: EnvironmentInfo
@@ -334,93 +151,55 @@
                  config: TrainerConfig):
         self._env_info = env_info
         self._config = config
 
         self._train_count = 0
 
         self._models = convert_to_list_if_not_list(models)
-        self._assert_no_duplicate_model(self._models)
-        if self._need_rnn_support(self._models) and not self.support_rnn():
-            raise NotImplementedError(f'{self.__name__} does not support RNN models!')
         self._solvers = solvers
 
         # Initially create training variables with batch_size 1.
         # The batch_size will be updated later depending on the given experience data
         # This procedure is a workaround to initialize model parameters (it it is not created).
-        total_timesteps = self._total_timesteps()
-        next_step_variables = None
-        for _ in range(total_timesteps):
-            training_variables = self._setup_training_variables(1)
-            training_variables.next_step_variables = next_step_variables
-            next_step_variables = training_variables
-        self._training_variables = training_variables
-        self._assert_variable_length_equals_total_timesteps()
+        self._training_variables = self._setup_training_variables(1)
 
         self._build_training_graph(self._models, self._training_variables)
 
         self._setup_solver()
 
-    @property
-    def __name__(self):
-        return self.__class__.__name__
-
-    def train(self, batch: TrainingBatch, **kwargs) -> Dict[str, np.ndarray]:
+    def train(self, batch: TrainingBatch, **kwargs) -> Dict[str, np.array]:
         if self._models is None:
             raise RuntimeError('Call setup_training() first. Model is not set!')
         self._train_count += 1
 
         batch = self._setup_batch(batch)
         new_batch_size = batch.batch_size
         prev_batch_size = self._training_variables.batch_size
         if new_batch_size != prev_batch_size:
-            total_timesteps = self._total_timesteps()
-            assert 0 < total_timesteps
-            next_step_variables = None
-            for _ in range(total_timesteps):
-                training_variables = self._setup_training_variables(new_batch_size)
-                training_variables.next_step_variables = next_step_variables
-                next_step_variables = training_variables
-            self._training_variables = training_variables
-            self._assert_variable_length_equals_total_timesteps()
-
+            self._training_variables = self._setup_training_variables(new_batch_size)
             self._build_training_graph(self._models, self._training_variables)
 
         trainer_state = self._update_model(self._models, self._solvers, batch, self._training_variables, **kwargs)
 
         return trainer_state
 
-    @property
-    @abstractmethod
-    def loss_variables(self) -> Dict[str, nn.Variable]:
-        raise NotImplementedError
-
-    @property
-    def training_variables(self) -> TrainingVariables:
-        return self._training_variables
-
     def set_learning_rate(self, new_learning_rate):
         for solver in self._solvers.values():
             solver.set_learning_rate(new_learning_rate)
 
-    def support_rnn(self) -> bool:
-        return False
-
-    def _total_timesteps(self) -> int:
-        return self._config.unroll_steps + self._config.burn_in_steps
-
-    def _setup_batch(self, training_batch: TrainingBatch) -> TrainingBatch:
-        return training_batch
+    def _setup_batch(self, batch: TrainingBatch) -> TrainingBatch:
+        return batch
 
     @abstractmethod
     def _update_model(self,
                       models: Sequence[Model],
                       solvers: Dict[str, nn.solver.Solver],
                       batch: TrainingBatch,
                       training_variables: TrainingVariables,
-                      **kwargs) -> Dict[str, np.ndarray]:
+                      **kwargs) -> Dict[str, np.array]:
         raise NotImplementedError
 
     @abstractmethod
     def _build_training_graph(self,
                               models: Sequence[Model],
                               training_variables: TrainingVariables):
         raise NotImplementedError
@@ -431,28 +210,7 @@
 
     def _setup_solver(self):
         for model in self._models:
             if model.scope_name in self._solvers.keys():
                 solver = self._solvers[model.scope_name]
                 # Set retain_state = True and prevent overwriting loaded state (If it is loaded)
                 solver.set_parameters(model.get_parameters(), reset=False, retain_state=True)
-
-    def _assert_variable_length_equals_total_timesteps(self):
-        total_timesptes = self._total_timesteps()
-        if len(self._training_variables) != total_timesptes:
-            raise RuntimeError(f'Training variables length and rnn unroll + burn-in steps does not match!. \
-                                   {len(self._training_variables)} != {total_timesptes}. \
-                                   Check that the training method supports recurrent networks.')
-
-    @classmethod
-    def _assert_no_duplicate_model(cls, models):
-        scope_names = set()
-        for model in models:
-            scope_name = model.scope_name
-            assert scope_name not in scope_names
-            scope_names.add(scope_name)
-
-    def _need_rnn_support(self, models: Sequence[Model]):
-        for model in models:
-            if model.is_recurrent():
-                return True
-        return False
```

## nnabla_rl/model_trainers/encoder/__init__.py

```diff
@@ -1,18 +1,16 @@
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from nnabla_rl.model_trainers.encoder.hyar_vae_trainer import (  # noqa
-    HyARVAETrainer, HyARVAETrainerConfig)
 from nnabla_rl.model_trainers.encoder.kld_variational_auto_encoder_trainer import (  # noqa
     KLDVariationalAutoEncoderTrainer, KLDVariationalAutoEncoderTrainerConfig)
```

## nnabla_rl/model_trainers/encoder/kld_variational_auto_encoder_trainer.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021,2022 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -18,19 +18,16 @@
 import numpy as np
 
 import nnabla as nn
 import nnabla.functions as NF
 import nnabla_rl.functions as RNF
 from nnabla_rl.distributions import Gaussian
 from nnabla_rl.environments.environment_info import EnvironmentInfo
-from nnabla_rl.model_trainers.model_trainer import (LossIntegration, ModelTrainer, TrainerConfig, TrainingBatch,
-                                                    TrainingVariables)
+from nnabla_rl.model_trainers.model_trainer import ModelTrainer, TrainerConfig, TrainingBatch, TrainingVariables
 from nnabla_rl.models import Model, VariationalAutoEncoder
-from nnabla_rl.utils.data import set_data_to_variable
-from nnabla_rl.utils.misc import create_variable
 
 
 @dataclass
 class KLDVariationalAutoEncoderTrainerConfig(TrainerConfig):
     pass
 
 
@@ -49,66 +46,50 @@
         super(KLDVariationalAutoEncoderTrainer, self).__init__(models, solvers, env_info, config)
 
     def _update_model(self,
                       models: Iterable[Model],
                       solvers: Dict[str, nn.solver.Solver],
                       batch: TrainingBatch,
                       training_variables: TrainingVariables,
-                      **kwargs) -> Dict[str, np.ndarray]:
-        for t, b in zip(training_variables, batch):
-            set_data_to_variable(t.s_current, b.s_current)
-            set_data_to_variable(t.a_current, b.a_current)
+                      **kwargs) -> Dict[str, np.array]:
+        training_variables.s_current.d = batch.s_current
+        training_variables.a_current.d = batch.a_current
 
         # update model
         for solver in solvers.values():
             solver.zero_grad()
         self._encoder_loss.forward(clear_no_need_grad=True)
         self._encoder_loss.backward(clear_buffer=True)
         for solver in solvers.values():
             solver.update()
 
         trainer_state = {}
-        trainer_state['encoder_loss'] = self._encoder_loss.d.copy()
+        trainer_state['encoder_loss'] = float(self._encoder_loss.d.copy())
         return trainer_state
 
-    def _build_training_graph(self, models: Union[Model, Sequence[Model]], training_variables: TrainingVariables):
-        self._encoder_loss = 0
-        models = cast(Sequence[VariationalAutoEncoder], models)
-        ignore_intermediate_loss = self._config.loss_integration is LossIntegration.LAST_TIMESTEP_ONLY
-        for step_index, variables in enumerate(training_variables):
-            is_burn_in_steps = step_index < self._config.burn_in_steps
-            is_intermediate_steps = step_index < self._config.burn_in_steps + self._config.unroll_steps - 1
-            ignore_loss = is_burn_in_steps or (is_intermediate_steps and ignore_intermediate_loss)
-            self._build_one_step_graph(models, variables, ignore_loss=ignore_loss)
-
-    def _build_one_step_graph(self,
-                              models: Sequence[Model],
-                              training_variables: TrainingVariables,
-                              ignore_loss: bool):
+    def _build_training_graph(self,
+                              models: Iterable[Model],
+                              training_variables: TrainingVariables):
+        models = cast(Iterable[VariationalAutoEncoder], models)
         batch_size = training_variables.batch_size
 
-        models = cast(Sequence[VariationalAutoEncoder], models)
+        self._encoder_loss = 0
         for vae in models:
             latent_distribution, reconstructed_action = vae.encode_and_decode(training_variables.s_current,
                                                                               action=training_variables.a_current)
 
             latent_shape = (batch_size, latent_distribution.ndim)
-            target_latent_distribution = Gaussian(
-                mean=nn.Variable.from_numpy_array(np.zeros(shape=latent_shape, dtype=np.float32)),
-                ln_var=nn.Variable.from_numpy_array(np.zeros(shape=latent_shape, dtype=np.float32))
-            )
+            target_latent_distribution = Gaussian(mean=np.zeros(shape=latent_shape, dtype=np.float32),
+                                                  ln_var=np.zeros(shape=latent_shape, dtype=np.float32))
 
             reconstruction_loss = RNF.mean_squared_error(training_variables.a_current, reconstructed_action)
             kl_divergence = latent_distribution.kl_divergence(target_latent_distribution)
             latent_loss = 0.5 * NF.mean(kl_divergence)
-            self._encoder_loss += 0.0 if ignore_loss else reconstruction_loss + latent_loss
+            self._encoder_loss += reconstruction_loss + latent_loss
 
     def _setup_training_variables(self, batch_size) -> TrainingVariables:
         # Training input variables
-        s_current_var = create_variable(batch_size, self._env_info.state_shape)
-        a_current_var = create_variable(batch_size, self._env_info.action_shape)
-
-        return TrainingVariables(batch_size, s_current_var, a_current_var)
+        s_current_var = nn.Variable((batch_size, *self._env_info.state_shape))
+        a_current_var = nn.Variable((batch_size, self._env_info.action_dim))
 
-    @property
-    def loss_variables(self) -> Dict[str, nn.Variable]:
-        return {"encoder_loss": self._encoder_loss}
+        training_variables = TrainingVariables(batch_size, s_current_var, a_current_var)
+        return training_variables
```

## nnabla_rl/model_trainers/perturbator/bcq_perturbator_trainer.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021,2022 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -14,19 +14,16 @@
 
 from dataclasses import dataclass
 from typing import Dict, Sequence, Union, cast
 
 import nnabla as nn
 import nnabla.functions as NF
 from nnabla_rl.environments.environment_info import EnvironmentInfo
-from nnabla_rl.model_trainers.model_trainer import (LossIntegration, ModelTrainer, TrainerConfig, TrainingBatch,
-                                                    TrainingVariables)
+from nnabla_rl.model_trainers.model_trainer import ModelTrainer, TrainerConfig, TrainingBatch, TrainingVariables
 from nnabla_rl.models import Model, Perturbator, QFunction, VariationalAutoEncoder
-from nnabla_rl.utils.data import set_data_to_variable
-from nnabla_rl.utils.misc import create_variable
 
 
 @dataclass
 class BCQPerturbatorTrainerConfig(TrainerConfig):
     '''
     Args:
         phi(float): action perturbator noise coefficient
@@ -56,59 +53,45 @@
 
     def _update_model(self,
                       models: Sequence[Model],
                       solvers: Dict[str, nn.solver.Solver],
                       batch: TrainingBatch,
                       training_variables: TrainingVariables,
                       **kwargs) -> Dict:
-        for t, b in zip(training_variables, batch):
-            set_data_to_variable(t.s_current, b.s_current)
+        training_variables.s_current.d = batch.s_current
 
         # update model
         for solver in solvers.values():
             solver.zero_grad()
         self._perturbator_loss.forward(clear_no_need_grad=True)
         self._perturbator_loss.backward(clear_buffer=True)
         for solver in solvers.values():
             solver.update()
 
         trainer_state = {}
         trainer_state['perturbator_loss'] = float(self._perturbator_loss.d.copy())
         return trainer_state
 
-    def _build_training_graph(self, models: Union[Model, Sequence[Model]], training_variables: TrainingVariables):
-        self._perturbator_loss = 0
-        models = cast(Sequence[Perturbator], models)
-        ignore_intermediate_loss = self._config.loss_integration is LossIntegration.LAST_TIMESTEP_ONLY
-        for step_index, variables in enumerate(training_variables):
-            is_burn_in_steps = step_index < self._config.burn_in_steps
-            is_intermediate_steps = step_index < self._config.burn_in_steps + self._config.unroll_steps - 1
-            ignore_loss = is_burn_in_steps or (is_intermediate_steps and ignore_intermediate_loss)
-            self._build_one_step_graph(models, variables, ignore_loss=ignore_loss)
-
-    def _build_one_step_graph(self,
+    def _build_training_graph(self,
                               models: Sequence[Model],
-                              training_variables: TrainingVariables,
-                              ignore_loss: bool):
+                              training_variables: TrainingVariables):
         assert training_variables.s_current is not None
+        models = cast(Sequence[Perturbator], models)
         batch_size = training_variables.batch_size
 
-        models = cast(Sequence[Perturbator], models)
+        self._perturbator_loss = 0
         for perturbator in models:
             action = self._vae.decode(z=None, state=training_variables.s_current)
             action.need_grad = False
 
             noise = perturbator.generate_noise(training_variables.s_current, action, phi=self._config.phi)
 
             xi_loss = -self._q_function.q(training_variables.s_current, action + noise)
             assert xi_loss.shape == (batch_size, 1)
 
-            self._perturbator_loss += 0.0 if ignore_loss else NF.mean(xi_loss)
+            self._perturbator_loss += NF.mean(xi_loss)
 
     def _setup_training_variables(self, batch_size) -> TrainingVariables:
         # Training input variables
-        s_current_var = create_variable(batch_size, self._env_info.state_shape)
-        return TrainingVariables(batch_size, s_current_var)
-
-    @property
-    def loss_variables(self) -> Dict[str, nn.Variable]:
-        return {"perturbator_loss": self._perturbator_loss}
+        s_current_var = nn.Variable((batch_size, *self._env_info.state_shape))
+        training_variables = TrainingVariables(batch_size, s_current_var)
+        return training_variables
```

## nnabla_rl/model_trainers/policy/__init__.py

```diff
@@ -1,27 +1,21 @@
-# Copyright 2021,2022,2023,2024 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from nnabla_rl.model_trainers.policy.a2c_policy_trainer import A2CPolicyTrainer, A2CPolicyTrainerConfig  # noqa
-from nnabla_rl.model_trainers.policy.amp_policy_trainer import AMPPolicyTrainer, AMPPolicyTrainerConfig  # noqa
 from nnabla_rl.model_trainers.policy.bear_policy_trainer import BEARPolicyTrainer, BEARPolicyTrainerConfig  # noqa
-from nnabla_rl.model_trainers.policy.demme_policy_trainer import DEMMEPolicyTrainer, DEMMEPolicyTrainerConfig  # noqa
 from nnabla_rl.model_trainers.policy.dpg_policy_trainer import DPGPolicyTrainer, DPGPolicyTrainerConfig  # noqa
-from nnabla_rl.model_trainers.policy.her_policy_trainer import HERPolicyTrainer, HERPolicyTrainerConfig  # noqa
-from nnabla_rl.model_trainers.policy.hyar_policy_trainer import HyARPolicyTrainer, HyARPolicyTrainerConfig  # noqa
 from nnabla_rl.model_trainers.policy.ppo_policy_trainer import PPOPolicyTrainer, PPOPolicyTrainerConfig  # noqa
 from nnabla_rl.model_trainers.policy.soft_policy_trainer import SoftPolicyTrainer, SoftPolicyTrainerConfig  # noqa
 from nnabla_rl.model_trainers.policy.reinforce_policy_trainer import REINFORCEPolicyTrainer, REINFORCEPolicyTrainerConfig  # noqa
 from nnabla_rl.model_trainers.policy.trpo_policy_trainer import TRPOPolicyTrainer, TRPOPolicyTrainerConfig  # noqa
-from nnabla_rl.model_trainers.policy.xql_forward_policy_trainer import XQLForwardPolicyTrainer, XQLForwardPolicyTrainerConfig  # noqa
-from nnabla_rl.model_trainers.policy.xql_reverse_policy_trainer import XQLReversePolicyTrainer, XQLReversePolicyTrainerConfig  # noqa
```

## nnabla_rl/model_trainers/policy/a2c_policy_trainer.py

```diff
@@ -1,43 +1,43 @@
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from dataclasses import dataclass
-from typing import Dict, Sequence, Union, cast
+from typing import Dict, Optional, Sequence, Union, cast
 
 import numpy as np
 
 import nnabla as nn
 import nnabla.functions as NF
 from nnabla_rl.environments.environment_info import EnvironmentInfo
-from nnabla_rl.model_trainers.model_trainer import (LossIntegration, ModelTrainer, TrainerConfig, TrainingBatch,
-                                                    TrainingVariables)
+from nnabla_rl.model_trainers.model_trainer import ModelTrainer, TrainerConfig, TrainingBatch, TrainingVariables
 from nnabla_rl.models import Model, StochasticPolicy
-from nnabla_rl.utils.data import set_data_to_variable
-from nnabla_rl.utils.misc import create_variable
+from nnabla_rl.utils.misc import clip_grad_by_global_norm
 
 
 @dataclass
 class A2CPolicyTrainerConfig(TrainerConfig):
     entropy_coefficient: float = 0.01
+    max_grad_norm: Optional[float] = 0.5
 
 
 class A2CPolicyTrainer(ModelTrainer):
-    """Advantaged Actor Critic style Policy Trainer."""
+    '''Advantaged Actor Critic style Policy Trainer
+    '''
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _config: A2CPolicyTrainerConfig
     _pi_loss: nn.Variable
 
     def __init__(self,
@@ -48,61 +48,48 @@
         super(A2CPolicyTrainer, self).__init__(models, solvers, env_info, config)
 
     def _update_model(self,
                       models: Sequence[Model],
                       solvers: Dict[str, nn.solver.Solver],
                       batch: TrainingBatch,
                       training_variables: TrainingVariables,
-                      **kwargs) -> Dict[str, np.ndarray]:
-        for t, b in zip(training_variables, batch):
-            set_data_to_variable(t.s_current, b.s_current)
-            set_data_to_variable(t.a_current, b.a_current)
-            set_data_to_variable(t.extra['advantage'], b.extra['advantage'])
+                      **kwargs) -> Dict[str, np.array]:
+        training_variables.s_current.d = batch.s_current
+        training_variables.a_current.d = batch.a_current
+        training_variables.extra['advantage'].d = batch.extra['advantage']
 
         # update model
         for solver in solvers.values():
             solver.zero_grad()
         self._pi_loss.forward()
         self._pi_loss.backward()
         for solver in solvers.values():
+            if self._config.max_grad_norm is not None:
+                clip_grad_by_global_norm(solver, self._config.max_grad_norm)
             solver.update()
 
         trainer_state = {}
-        trainer_state['pi_loss'] = self._pi_loss.d.copy()
+        trainer_state['pi_loss'] = float(self._pi_loss.d.copy())
         return trainer_state
 
     def _build_training_graph(self, models: Sequence[Model], training_variables: TrainingVariables):
         models = cast(Sequence[StochasticPolicy], models)
         self._pi_loss = 0
-        ignore_intermediate_loss = self._config.loss_integration is LossIntegration.LAST_TIMESTEP_ONLY
-        for step_index, variables in enumerate(training_variables):
-            is_burn_in_steps = step_index < self._config.burn_in_steps
-            is_intermediate_steps = step_index < self._config.burn_in_steps + self._config.unroll_steps - 1
-            ignore_loss = is_burn_in_steps or (is_intermediate_steps and ignore_intermediate_loss)
-            self._build_one_step_graph(models, variables, ignore_loss=ignore_loss)
-
-    def _build_one_step_graph(self,
-                              models: Sequence[Model],
-                              training_variables: TrainingVariables,
-                              ignore_loss: bool):
-        models = cast(Sequence[StochasticPolicy], models)
         for policy in models:
             distribution = policy.pi(training_variables.s_current)
             log_prob = distribution.log_prob(training_variables.a_current)
             entropy = distribution.entropy()
             advantage = training_variables.extra['advantage']
 
             self._pi_loss += NF.mean(-advantage * log_prob - self._config.entropy_coefficient * entropy)
 
     def _setup_training_variables(self, batch_size):
         # Training input variables
-        s_current_var = create_variable(batch_size, self._env_info.state_shape)
-        a_current_var = create_variable(batch_size, self._env_info.action_shape)
-        advantage_var = create_variable(batch_size, 1)
-
+        s_current_var = nn.Variable((batch_size, *self._env_info.state_shape))
+        if self._env_info.is_discrete_action_env():
+            a_current_var = nn.Variable((batch_size, 1))
+        else:
+            a_current_var = nn.Variable((batch_size, self._env_info.action_dim))
+        advantage_var = nn.Variable((batch_size, 1))
         extra = {}
         extra['advantage'] = advantage_var
         return TrainingVariables(batch_size, s_current_var, a_current_var, extra=extra)
-
-    @property
-    def loss_variables(self) -> Dict[str, nn.Variable]:
-        return {"pi_loss": self._pi_loss}
```

## nnabla_rl/model_trainers/policy/bear_policy_trainer.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -17,19 +17,16 @@
 
 import numpy as np
 
 import nnabla as nn
 import nnabla.functions as NF
 import nnabla_rl.functions as RF
 from nnabla_rl.environments.environment_info import EnvironmentInfo
-from nnabla_rl.model_trainers.model_trainer import (LossIntegration, ModelTrainer, TrainerConfig, TrainingBatch,
-                                                    TrainingVariables)
+from nnabla_rl.model_trainers.model_trainer import ModelTrainer, TrainerConfig, TrainingBatch, TrainingVariables
 from nnabla_rl.models import Model, QFunction, StochasticPolicy, VariationalAutoEncoder
-from nnabla_rl.utils.data import set_data_to_variable
-from nnabla_rl.utils.misc import create_variable
 
 
 class AdjustableLagrangeMultiplier(Model):
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _log_lagrange: nn.Variable
@@ -71,16 +68,16 @@
     warmup_iterations: int = 20000
 
     def __post_init__(self):
         self._assert_one_of(self.mmd_type, ['gaussian', 'laplacian'], 'mmd_type')
 
 
 class BEARPolicyTrainer(ModelTrainer):
-    """Bootstrapping Error Accumulation Reduction (BEAR) style Policy
-    Trainer."""
+    '''Bootstrapping Error Accumulation Reduction (BEAR) style Policy Trainer
+    '''
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _config: BEARPolicyTrainerConfig
     _mmd_loss: nn.Variable
     _pi_loss: nn.Variable
     _pi_warmup_loss: nn.Variable
@@ -97,29 +94,27 @@
                  q_ensembles: Sequence[QFunction],
                  vae: VariationalAutoEncoder,
                  lagrange_multiplier: AdjustableLagrangeMultiplier,
                  lagrange_solver: Optional[nn.solver.Solver],
                  env_info: EnvironmentInfo,
                  config: BEARPolicyTrainerConfig = BEARPolicyTrainerConfig()):
         self._q_ensembles = q_ensembles
-
         self._vae = vae
 
         self._lagrange = lagrange_multiplier
         self._lagrange_solver = lagrange_solver
         super(BEARPolicyTrainer, self).__init__(models, solvers, env_info, config)
 
     def _update_model(self,
                       models: Sequence[Model],
                       solvers: Dict[str, nn.solver.Solver],
                       batch: TrainingBatch,
                       training_variables: TrainingVariables,
-                      **kwargs) -> Dict[str, np.ndarray]:
-        for t, b in zip(training_variables, batch):
-            set_data_to_variable(t.s_current, b.s_current)
+                      **kwargs) -> Dict[str, np.array]:
+        training_variables.s_current.d = batch.s_current
 
         # Optimize actor
         # Always forward pi loss to update the graph
         pi_loss = self._pi_warmup_loss if self._train_count < self._config.warmup_iterations else self._pi_loss
         for solver in solvers.values():
             solver.zero_grad()
         nn.forward_all([pi_loss, self._lagrange_loss])
@@ -132,111 +127,76 @@
             assert self._lagrange_solver is not None
             self._lagrange_solver.zero_grad()
             self._lagrange_loss.backward()
             self._lagrange_solver.update()
             self._lagrange.clip(-5.0, 10.0)
 
         trainer_state = {}
-        trainer_state['pi_loss'] = self._pi_loss.d.copy()
+        trainer_state['pi_loss'] = float(self._pi_loss.d.copy())
         return trainer_state
 
-    def _repeat_state(self, s_var: nn.Variable, batch_size: int) -> nn.Variable:
-        s_hat = RF.expand_dims(s_var, axis=0)
-        s_hat = RF.repeat(s_hat, repeats=self._config.num_mmd_actions, axis=0)
-        s_hat = NF.reshape(s_hat, shape=(batch_size * self._config.num_mmd_actions,
-                                         s_var.shape[-1]))
-        return s_hat
-
     def _build_training_graph(self, models: Sequence[Model], training_variables: TrainingVariables):
         models = cast(Sequence[StochasticPolicy], models)
+        batch_size = training_variables.batch_size
         self._pi_loss = 0
         self._pi_warmup_loss = 0
-        self._lagrange_loss = 0
-        ignore_intermediate_loss = self._config.loss_integration is LossIntegration.LAST_TIMESTEP_ONLY
-        for step_index, variables in enumerate(training_variables):
-            is_burn_in_steps = step_index < self._config.burn_in_steps
-            is_intermediate_steps = step_index < self._config.burn_in_steps + self._config.unroll_steps - 1
-            ignore_loss = is_burn_in_steps or (is_intermediate_steps and ignore_intermediate_loss)
-            self._build_one_step_graph(models, variables, ignore_loss=ignore_loss)
-
-    def _build_one_step_graph(self,
-                              models: Sequence[Model],
-                              training_variables: TrainingVariables,
-                              ignore_loss: bool):
-        models = cast(Sequence[StochasticPolicy], models)
-        batch_size = training_variables.batch_size
-
         for policy in models:
             sampled_actions = self._vae.decode_multiple(z=None,
                                                         decode_num=self._config.num_mmd_actions,
                                                         state=training_variables.s_current)
             policy_distribution = policy.pi(training_variables.s_current)
-
             policy_actions = policy_distribution.sample_multiple(
                 num_samples=self._config.num_mmd_actions, noise_clip=(-0.5, 0.5))
 
             if self._config.mmd_type == 'gaussian':
                 mmd_loss = _compute_gaussian_mmd(sampled_actions, policy_actions, sigma=self._config.mmd_sigma)
             elif self._config.mmd_type == 'laplacian':
                 mmd_loss = _compute_laplacian_mmd(sampled_actions, policy_actions, sigma=self._config.mmd_sigma)
             else:
                 raise ValueError(
                     'Unknown mmd type: {}'.format(self._config.mmd_type))
             assert mmd_loss.shape == (batch_size, 1)
 
-            if isinstance(training_variables.s_current, tuple):
-                s_hat = tuple(self._repeat_state(s_var, batch_size) for s_var in training_variables.s_current)
-            else:
-                s_hat = self._repeat_state(training_variables.s_current, batch_size)
-
+            s_hat = RF.expand_dims(training_variables.s_current, axis=0)
+            s_hat = RF.repeat(s_hat, repeats=self._config.num_mmd_actions, axis=0)
+            s_hat = NF.reshape(s_hat, shape=(batch_size * self._config.num_mmd_actions,
+                                             training_variables.s_current.shape[-1]))
             action_shape = policy_actions.shape[-1]
             a_hat = NF.transpose(policy_actions, axes=(1, 0, 2))
             a_hat = NF.reshape(a_hat, shape=(batch_size * self._config.num_mmd_actions, action_shape))
 
-            q_values = []
-            for q in self._q_ensembles:
-                q_value = q.q(s_hat, a_hat)
-                q_values.append(q_value)
-
-            q_values = NF.stack(*q_values)
             num_q_ensembles = len(self._q_ensembles)
-            assert isinstance(q_values, nn.Variable)
+            q_values = NF.stack(*(q.q(s_hat, a_hat) for q in self._q_ensembles))
             assert q_values.shape == (num_q_ensembles, self._config.num_mmd_actions * batch_size, 1)
             q_values = NF.reshape(q_values, shape=(num_q_ensembles, self._config.num_mmd_actions, batch_size, 1))
             # Compute mean among sampled actions
             q_values = NF.mean(q_values, axis=1)
-            assert isinstance(q_values, nn.Variable)
             assert q_values.shape == (num_q_ensembles, batch_size, 1)
 
             # Compute the minimum among ensembles
             q_min = NF.min(q_values, axis=0)
 
             assert q_min.shape == (batch_size, 1)
 
-            self._pi_loss += 0.0 if ignore_loss else NF.mean(-q_min + self._lagrange() * mmd_loss)
-            self._pi_warmup_loss += 0.0 if ignore_loss else NF.mean(self._lagrange() * mmd_loss)
+            self._pi_loss += NF.mean(-q_min + self._lagrange() * mmd_loss)
+            self._pi_warmup_loss += NF.mean(self._lagrange() * mmd_loss)
 
         # Must forward pi_loss before forwarding lagrange_loss
-        self._lagrange_loss += 0.0 if ignore_loss else - \
-            NF.mean(-q_min + self._lagrange() * (mmd_loss - self._config.epsilon))
+        self._lagrange_loss = -NF.mean(-q_min + self._lagrange() * (mmd_loss - self._config.epsilon))
 
     def _setup_training_variables(self, batch_size):
         # Training input variables
-        s_current_var = create_variable(batch_size, self._env_info.state_shape)
+        s_current_var = nn.Variable((batch_size, *self._env_info.state_shape))
         return TrainingVariables(batch_size, s_current_var)
 
     def _setup_solver(self):
         super()._setup_solver()
         if not self._config.fix_lagrange_multiplier:
             self._lagrange_solver.set_parameters(self._lagrange.get_parameters(), reset=False, retain_state=True)
 
-    @property
-    def loss_variables(self) -> Dict[str, nn.Variable]:
-        return {"pi_loss": self._pi_loss, "lagrange_loss": self._lagrange_loss}
-
 
 def _compute_gaussian_mmd(samples1, samples2, sigma):
     n = samples1.shape[1]
     m = samples2.shape[1]
 
     k_xx = RF.expand_dims(x=samples1, axis=2) - RF.expand_dims(x=samples1, axis=1)
     last_axis = len(k_xx.shape) - 1
```

## nnabla_rl/model_trainers/policy/dpg_policy_trainer.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -16,135 +16,67 @@
 from typing import Dict, Sequence, Union, cast
 
 import numpy as np
 
 import nnabla as nn
 import nnabla.functions as NF
 from nnabla_rl.environments.environment_info import EnvironmentInfo
-from nnabla_rl.model_trainers.model_trainer import (LossIntegration, ModelTrainer, TrainerConfig, TrainingBatch,
-                                                    TrainingVariables, rnn_support)
+from nnabla_rl.model_trainers.model_trainer import ModelTrainer, TrainerConfig, TrainingBatch, TrainingVariables
 from nnabla_rl.models import DeterministicPolicy, Model, QFunction
-from nnabla_rl.utils.data import convert_to_list_if_not_list, set_data_to_variable
-from nnabla_rl.utils.misc import create_variable, create_variables
 
 
 @dataclass
 class DPGPolicyTrainerConfig(TrainerConfig):
     pass
 
 
 class DPGPolicyTrainer(ModelTrainer):
-    """Deterministic Policy Gradient (DPG) style Policy Trainer."""
+    '''Deterministic Policy Gradient (DPG) style Policy Trainer
+    '''
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _config: DPGPolicyTrainerConfig
     _pi_loss: nn.Variable
     _q_function: QFunction
-    _prev_policy_rnn_states: Dict[str, Dict[str, nn.Variable]]
-    _prev_q_rnn_states: Dict[str, Dict[str, Dict[str, nn.Variable]]]
 
     def __init__(self,
                  models: Union[DeterministicPolicy, Sequence[DeterministicPolicy]],
                  solvers: Dict[str, nn.solver.Solver],
                  q_function: QFunction,
                  env_info: EnvironmentInfo,
                  config: DPGPolicyTrainerConfig = DPGPolicyTrainerConfig()):
         self._q_function = q_function
-        self._prev_policy_rnn_states = {}
-        self._prev_q_rnn_states = {}
-        for model in convert_to_list_if_not_list(models):
-            self._prev_q_rnn_states[model.scope_name] = {}
-
         super(DPGPolicyTrainer, self).__init__(models, solvers, env_info, config)
 
-    def support_rnn(self) -> bool:
-        return True
-
     def _update_model(self,
                       models: Sequence[Model],
                       solvers: Dict[str, nn.solver.Solver],
                       batch: TrainingBatch,
                       training_variables: TrainingVariables,
-                      **kwargs) -> Dict[str, np.ndarray]:
-        for t, b in zip(training_variables, batch):
-            set_data_to_variable(t.s_current, b.s_current)
-            set_data_to_variable(t.non_terminal, b.non_terminal)
-
-            for model in models:
-                if not model.is_recurrent():
-                    continue
-                # Check batch keys. Because it can be empty.
-                # If batch does not provide rnn states, train with zero initial state.
-                if model.scope_name not in batch.rnn_states.keys():
-                    continue
-                b_rnn_states = b.rnn_states[model.scope_name]
-                t_rnn_states = t.rnn_states[model.scope_name]
-
-                for state_name in t_rnn_states.keys():
-                    set_data_to_variable(t_rnn_states[state_name], b_rnn_states[state_name])
-            if self._q_function.is_recurrent() and self._q_function.scope_name in batch.rnn_states.keys():
-                b_rnn_states = b.rnn_states[self._q_function.scope_name]
-                t_rnn_states = t.rnn_states[self._q_function.scope_name]
-                for state_name in t_rnn_states.keys():
-                    set_data_to_variable(t_rnn_states[state_name], b_rnn_states[state_name])
+                      **kwargs) -> Dict[str, np.array]:
+        training_variables.s_current.d = batch.s_current
 
         # update model
         for solver in solvers.values():
             solver.zero_grad()
         self._pi_loss.forward(clear_no_need_grad=True)
         self._pi_loss.backward(clear_buffer=True)
         for solver in solvers.values():
             solver.update()
 
         trainer_state = {}
-        trainer_state['pi_loss'] = self._pi_loss.d.copy()
+        trainer_state['pi_loss'] = float(self._pi_loss.d.copy())
         return trainer_state
 
     def _build_training_graph(self, models: Sequence[Model], training_variables: TrainingVariables):
         models = cast(Sequence[DeterministicPolicy], models)
         self._pi_loss = 0
-        ignore_intermediate_loss = self._config.loss_integration is LossIntegration.LAST_TIMESTEP_ONLY
-        for step_index, variables in enumerate(training_variables):
-            is_burn_in_steps = step_index < self._config.burn_in_steps
-            is_intermediate_steps = step_index < self._config.burn_in_steps + self._config.unroll_steps - 1
-            ignore_loss = is_burn_in_steps or (is_intermediate_steps and ignore_intermediate_loss)
-            self._build_one_step_graph(models, variables, ignore_loss=ignore_loss)
-
-    def _build_one_step_graph(self,
-                              models: Sequence[Model],
-                              training_variables: TrainingVariables,
-                              ignore_loss: bool):
-        models = cast(Sequence[DeterministicPolicy], models)
-        train_rnn_states = training_variables.rnn_states
         for policy in models:
-            prev_rnn_states = self._prev_policy_rnn_states
-            with rnn_support(policy, prev_rnn_states, train_rnn_states, training_variables, self._config):
-                action = policy.pi(training_variables.s_current)
-
-            prev_rnn_states = self._prev_q_rnn_states[policy.scope_name]
-            with rnn_support(self._q_function, prev_rnn_states, train_rnn_states, training_variables, self._config):
-                q = self._q_function.q(training_variables.s_current, action)
-            self._prev_q_rnn_states[policy.scope_name] = prev_rnn_states
-            self._pi_loss += 0.0 if ignore_loss else -NF.mean(q)
+            action = policy.pi(training_variables.s_current)
+            q = self._q_function.q(training_variables.s_current, action)
+            self._pi_loss += -NF.mean(q)
 
     def _setup_training_variables(self, batch_size):
         # Training input variables
-        s_current_var = create_variable(batch_size, self._env_info.state_shape)
-        non_terminal_var = create_variable(batch_size, 1)
-
-        rnn_states = {}
-        for policy in self._models:
-            if policy.is_recurrent():
-                shapes = policy.internal_state_shapes()
-                rnn_state_variables = create_variables(batch_size, shapes)
-                rnn_states[policy.scope_name] = rnn_state_variables
-        if self._q_function.is_recurrent():
-            shapes = self._q_function.internal_state_shapes()
-            rnn_state_variables = create_variables(batch_size, shapes)
-            rnn_states[self._q_function.scope_name] = rnn_state_variables
-
-        return TrainingVariables(batch_size, s_current_var, non_terminal=non_terminal_var, rnn_states=rnn_states)
-
-    @property
-    def loss_variables(self) -> Dict[str, nn.Variable]:
-        return {"pi_loss": self._pi_loss}
+        s_current_var = nn.Variable((batch_size, *self._env_info.state_shape))
+        return TrainingVariables(batch_size, s_current_var)
```

## nnabla_rl/model_trainers/policy/ppo_policy_trainer.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -16,29 +16,27 @@
 from typing import Dict, Sequence, Union, cast
 
 import numpy as np
 
 import nnabla as nn
 import nnabla.functions as NF
 from nnabla_rl.environments.environment_info import EnvironmentInfo
-from nnabla_rl.model_trainers.model_trainer import (LossIntegration, ModelTrainer, TrainerConfig, TrainingBatch,
-                                                    TrainingVariables)
+from nnabla_rl.model_trainers.model_trainer import ModelTrainer, TrainerConfig, TrainingBatch, TrainingVariables
 from nnabla_rl.models import Model, StochasticPolicy
-from nnabla_rl.utils.data import set_data_to_variable
-from nnabla_rl.utils.misc import create_variable
 
 
 @dataclass
 class PPOPolicyTrainerConfig(TrainerConfig):
     entropy_coefficient: float = 0.01
     epsilon: float = 0.1
 
 
 class PPOPolicyTrainer(ModelTrainer):
-    """Proximal Policy Optimization (PPO) style Policy Trainer."""
+    '''Proximal Policy Optimization (PPO) style Policy Trainer
+    '''
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _config: PPOPolicyTrainerConfig
     _pi_loss: nn.Variable
 
     def __init__(self,
@@ -49,49 +47,36 @@
         super(PPOPolicyTrainer, self).__init__(models, solvers, env_info, config)
 
     def _update_model(self,
                       models: Sequence[Model],
                       solvers: Dict[str, nn.solver.Solver],
                       batch: TrainingBatch,
                       training_variables: TrainingVariables,
-                      **kwargs) -> Dict[str, np.ndarray]:
-        for t, b in zip(training_variables, batch):
-            set_data_to_variable(t.s_current, b.s_current)
-            set_data_to_variable(t.a_current, b.a_current)
-            set_data_to_variable(t.extra['log_prob'], b.extra['log_prob'])
-            set_data_to_variable(t.extra['advantage'], b.extra['advantage'])
-            set_data_to_variable(t.extra['alpha'], b.extra['alpha'])
+                      **kwargs) -> Dict[str, np.array]:
+        training_variables.s_current.d = batch.s_current
+        training_variables.a_current.d = batch.a_current
+        training_variables.extra['log_prob'].d = batch.extra['log_prob']
+        training_variables.extra['advantage'].d = batch.extra['advantage']
+        training_variables.extra['alpha'].d = batch.extra['alpha']
 
         # update model
         for solver in solvers.values():
             solver.zero_grad()
         self._pi_loss.forward(clear_no_need_grad=True)
         self._pi_loss.backward(clear_buffer=True)
         for solver in solvers.values():
             solver.update()
 
         trainer_state = {}
-        trainer_state['pi_loss'] = self._pi_loss.d.copy()
+        trainer_state['pi_loss'] = float(self._pi_loss.d.copy())
         return trainer_state
 
     def _build_training_graph(self, models: Sequence[Model], training_variables: TrainingVariables):
         models = cast(Sequence[StochasticPolicy], models)
         self._pi_loss = 0
-        ignore_intermediate_loss = self._config.loss_integration is LossIntegration.LAST_TIMESTEP_ONLY
-        for step_index, variables in enumerate(training_variables):
-            is_burn_in_steps = step_index < self._config.burn_in_steps
-            is_intermediate_steps = step_index < self._config.burn_in_steps + self._config.unroll_steps - 1
-            ignore_loss = is_burn_in_steps or (is_intermediate_steps and ignore_intermediate_loss)
-            self._build_one_step_graph(models, variables, ignore_loss=ignore_loss)
-
-    def _build_one_step_graph(self,
-                              models: Sequence[Model],
-                              training_variables: TrainingVariables,
-                              ignore_loss: bool):
-        models = cast(Sequence[StochasticPolicy], models)
         for policy in models:
             distribution = policy.pi(training_variables.s_current)
             log_prob_new = distribution.log_prob(training_variables.a_current)
             log_prob_old = training_variables.extra['log_prob']
             probability_ratio = NF.exp(log_prob_new - log_prob_old)
             alpha = training_variables.extra['alpha']
             clipped_ratio = NF.clip_by_value(probability_ratio,
@@ -100,26 +85,26 @@
             advantage = training_variables.extra['advantage']
             lower_bounds = NF.minimum2(probability_ratio * advantage, clipped_ratio * advantage)
             clip_loss = NF.mean(lower_bounds)
 
             entropy = distribution.entropy()
             entropy_loss = NF.mean(entropy)
 
-            self._pi_loss += 0.0 if ignore_loss else (-clip_loss - self._config.entropy_coefficient * entropy_loss)
+            self._pi_loss += -clip_loss - self._config.entropy_coefficient * entropy_loss
 
     def _setup_training_variables(self, batch_size) -> TrainingVariables:
         # Training input variables
-        s_current_var = create_variable(batch_size, self._env_info.state_shape)
-        a_current_var = create_variable(batch_size, self._env_info.action_shape)
-        log_prob_var = create_variable(batch_size, 1)
-        advantage_var = create_variable(batch_size, 1)
-        alpha_var = create_variable(batch_size, 1)
+        s_current_var = nn.Variable((batch_size, *self._env_info.state_shape))
+        if self._env_info.is_discrete_action_env():
+            action_shape = (batch_size, 1)
+        else:
+            action_shape = (batch_size, self._env_info.action_dim)
+        a_current_var = nn.Variable(action_shape)
+        log_prob_var = nn.Variable((batch_size, 1))
+        advantage_var = nn.Variable((batch_size, 1))
+        alpha_var = nn.Variable((batch_size, 1))
 
         extra = {}
         extra['log_prob'] = log_prob_var
         extra['advantage'] = advantage_var
         extra['alpha'] = alpha_var
         return TrainingVariables(batch_size, s_current_var, a_current_var, extra=extra)
-
-    @property
-    def loss_variables(self) -> Dict[str, nn.Variable]:
-        return {"pi_loss": self._pi_loss}
```

## nnabla_rl/model_trainers/policy/reinforce_policy_trainer.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -18,52 +18,45 @@
 import numpy as np
 
 import nnabla as nn
 from nnabla_rl.environments.environment_info import EnvironmentInfo
 from nnabla_rl.model_trainers.model_trainer import TrainingBatch, TrainingVariables
 from nnabla_rl.model_trainers.policy.spg_policy_trainer import SPGPolicyTrainer, SPGPolicyTrainerConfig
 from nnabla_rl.models import StochasticPolicy
-from nnabla_rl.models.model import Model
-from nnabla_rl.utils.data import set_data_to_variable
-from nnabla_rl.utils.misc import create_variable
 
 
 @dataclass
 class REINFORCEPolicyTrainerConfig(SPGPolicyTrainerConfig):
     pass
 
 
 class REINFORCEPolicyTrainer(SPGPolicyTrainer):
-    """REINFORCE style Stochastic Policy Trainer."""
+    '''REINFORCE style Stochastic Policy Trainer
+    '''
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _config: REINFORCEPolicyTrainerConfig
+    _target_return: nn.Variable
 
     def __init__(self,
                  models: Union[StochasticPolicy, Sequence[StochasticPolicy]],
                  solvers: Dict[str, nn.solver.Solver],
                  env_info: EnvironmentInfo,
                  config: REINFORCEPolicyTrainerConfig = REINFORCEPolicyTrainerConfig()):
         super(REINFORCEPolicyTrainer, self).__init__(models, solvers, env_info, config)
 
-    def _update_model(self,
-                      models: Sequence[Model],
-                      solvers: Dict[str, nn.solver.Solver],
-                      batch: TrainingBatch,
-                      training_variables: TrainingVariables,
-                      **kwargs) -> Dict[str, np.ndarray]:
-        for t, b in zip(training_variables, batch):
-            set_data_to_variable(t.extra['target_return'], b.extra['target_return'])
-        return super()._update_model(models, solvers, batch, training_variables, **kwargs)
+    def _setup_batch(self, batch: TrainingBatch):
+        target_return = batch.extra['target_return']
+        prev_batch_size = self._target_return.shape[0]
+        new_batch_size = target_return.shape[0]
+        if prev_batch_size != new_batch_size:
+            self._target_return = nn.Variable((new_batch_size, 1))
+        target_return = np.reshape(target_return, self._target_return.shape)
+        self._target_return.d = target_return
+        return batch
 
     def _compute_target(self, training_variables: TrainingVariables) -> nn.Variable:
-        return training_variables.extra['target_return']
-
-    def _setup_training_variables(self, batch_size) -> TrainingVariables:
-        training_variables = super()._setup_training_variables(batch_size)
-
-        extra = {}
-        extra['target_return'] = create_variable(batch_size, 1)
-        training_variables.extra.update(extra)
-
-        return training_variables
+        batch_size = training_variables.batch_size
+        if not hasattr(self, '_target_return') or self._target_return.shape[0] != batch_size:
+            self._target_return = nn.Variable((batch_size, 1))
+        return self._target_return
```

## nnabla_rl/model_trainers/policy/soft_policy_trainer.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -17,19 +17,16 @@
 
 import numpy as np
 
 import nnabla as nn
 import nnabla.functions as NF
 import nnabla_rl.functions as RNF
 from nnabla_rl.environments.environment_info import EnvironmentInfo
-from nnabla_rl.model_trainers.model_trainer import (LossIntegration, ModelTrainer, TrainerConfig, TrainingBatch,
-                                                    TrainingVariables, rnn_support)
+from nnabla_rl.model_trainers.model_trainer import ModelTrainer, TrainerConfig, TrainingBatch, TrainingVariables
 from nnabla_rl.models import Model, QFunction, StochasticPolicy
-from nnabla_rl.utils.data import convert_to_list_if_not_list, set_data_to_variable
-from nnabla_rl.utils.misc import create_variable, create_variables
 
 
 class AdjustableTemperature(Model):
     def __init__(self, scope_name, initial_value=None):
         super(AdjustableTemperature, self).__init__(scope_name=scope_name)
         if initial_value:
             initial_value = np.log(initial_value)
@@ -52,26 +49,25 @@
     target_entropy: Optional[float] = None
 
     def __post_init__(self):
         super(SoftPolicyTrainerConfig, self).__post_init__()
 
 
 class SoftPolicyTrainer(ModelTrainer):
-    """Soft Policy Gradient style Policy Trainer."""
+    '''Soft Policy Gradient style Policy Trainer
+    '''
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _q_functions: Sequence[QFunction]
     _temperature_loss: nn.Variable
     _temperature: AdjustableTemperature
     _temperature_solver: Optional[nn.solver.Solver]
     _config: SoftPolicyTrainerConfig
     _pi_loss: nn.Variable
-    _prev_policy_rnn_states: Dict[str, Dict[str, nn.Variable]]
-    _prev_q_rnn_states: Dict[str, Dict[str, Dict[str, nn.Variable]]]
 
     def __init__(self,
                  models: Union[StochasticPolicy, Sequence[StochasticPolicy]],
                  solvers: Dict[str, nn.solver.Solver],
                  q_functions: Sequence[QFunction],
                  temperature: AdjustableTemperature,
                  temperature_solver: Optional[nn.solver.Solver],
@@ -81,60 +77,25 @@
             raise ValueError('Must provide at least 2 Q-functions for soft-training')
         self._q_functions = q_functions
         if not config.fixed_temperature and temperature_solver is None:
             raise ValueError('Please set solver for temperature model')
         self._temperature = temperature
         self._temperature_solver = temperature_solver
 
-        self._prev_policy_rnn_states = {}
-        self._prev_q_rnn_states = {}
-        for model in convert_to_list_if_not_list(models):
-            self._prev_q_rnn_states[model.scope_name] = {}
-
         if config.target_entropy is None:
             config.target_entropy = -env_info.action_dim
         super(SoftPolicyTrainer, self).__init__(models, solvers, env_info, config)
 
-    def support_rnn(self) -> bool:
-        return True
-
     def _update_model(self,
                       models: Sequence[Model],
                       solvers: Dict[str, nn.solver.Solver],
                       batch: TrainingBatch,
                       training_variables: TrainingVariables,
-                      **kwargs) -> Dict[str, np.ndarray]:
-        for t, b in zip(training_variables, batch):
-            set_data_to_variable(t.s_current, b.s_current)
-            set_data_to_variable(t.non_terminal, b.non_terminal)
-
-            for model in models:
-                if not model.is_recurrent():
-                    continue
-                # Check batch keys. Because it can be empty.
-                # If batch does not provide rnn states, train with zero initial state.
-                if model.scope_name not in batch.rnn_states.keys():
-                    continue
-                b_rnn_states = b.rnn_states[model.scope_name]
-                t_rnn_states = t.rnn_states[model.scope_name]
-
-                for state_name in t_rnn_states.keys():
-                    set_data_to_variable(t_rnn_states[state_name], b_rnn_states[state_name])
-            for q_function in self._q_functions:
-                if not q_function.is_recurrent():
-                    continue
-                # Check batch keys. Because it can be empty.
-                # If batch does not provide rnn states, train with zero initial state.
-                if q_function.scope_name not in batch.rnn_states.keys():
-                    continue
-                b_rnn_states = b.rnn_states[q_function.scope_name]
-                t_rnn_states = t.rnn_states[q_function.scope_name]
-
-                for state_name in t_rnn_states.keys():
-                    set_data_to_variable(t_rnn_states[state_name], b_rnn_states[state_name])
+                      **kwargs) -> Dict[str, np.array]:
+        training_variables.s_current.d = batch.s_current
 
         # update model
         for solver in solvers.values():
             solver.zero_grad()
         self._pi_loss.forward()
         self._pi_loss.backward()
         for solver in solvers.values():
@@ -145,79 +106,43 @@
             assert self._temperature_loss is not None
             self._temperature_solver.zero_grad()
             self._temperature_loss.forward()
             self._temperature_loss.backward()
             self._temperature_solver.update()
 
         trainer_state = {}
-        trainer_state['pi_loss'] = self._pi_loss.d.copy()
+        trainer_state['pi_loss'] = float(self._pi_loss.d.copy())
         return trainer_state
 
     def get_temperature(self) -> nn.Variable:
         # Will return exponentiated log temperature. To keep temperature always positive
         return self._temperature()
 
     def _build_training_graph(self,
                               models: Sequence[Model],
                               training_variables: TrainingVariables):
         self._pi_loss = 0
-        ignore_intermediate_loss = self._config.loss_integration is LossIntegration.LAST_TIMESTEP_ONLY
-        for step_index, variables in enumerate(training_variables):
-            is_burn_in_steps = step_index < self._config.burn_in_steps
-            is_intermediate_steps = step_index < self._config.burn_in_steps + self._config.unroll_steps - 1
-            ignore_loss = is_burn_in_steps or (is_intermediate_steps and ignore_intermediate_loss)
-            self._build_one_step_graph(models, variables, ignore_loss=ignore_loss)
-
-    def _build_one_step_graph(self,
-                              models: Sequence[Model],
-                              training_variables: TrainingVariables,
-                              ignore_loss: bool):
-        train_rnn_states = training_variables.rnn_states
         for policy in models:
             assert isinstance(policy, StochasticPolicy)
             # Actor optimization graph
-            prev_rnn_states = self._prev_policy_rnn_states
-            with rnn_support(policy, prev_rnn_states, train_rnn_states, training_variables, self._config):
-                policy_distribution = policy.pi(training_variables.s_current)
+            policy_distribution = policy.pi(self._training_variables.s_current)
             action_var, log_pi = policy_distribution.sample_and_compute_log_prob()
             q_values = []
-            prev_rnn_states = self._prev_q_rnn_states[policy.scope_name]
             for q_function in self._q_functions:
-                with rnn_support(q_function, prev_rnn_states, train_rnn_states, training_variables, self._config):
-                    q_values.append(q_function.q(training_variables.s_current, action_var))
-            self._prev_q_rnn_states[policy.scope_name] = prev_rnn_states
+                q_values.append(q_function.q(self._training_variables.s_current, action_var))
             min_q = RNF.minimum_n(q_values)
             self._pi_loss += NF.mean(self.get_temperature() * log_pi - min_q)
 
         if not self._config.fixed_temperature:
-            assert isinstance(log_pi, nn.Variable)
             log_pi_unlinked = log_pi.get_unlinked_variable()
             self._temperature_loss = -NF.mean(self.get_temperature() *
                                               (log_pi_unlinked + self._config.target_entropy))
 
     def _setup_training_variables(self, batch_size):
         # Training input variables
-        s_current_var = create_variable(batch_size, self._env_info.state_shape)
-        non_terminal_var = create_variable(batch_size, 1)
-
-        rnn_states = {}
-        for policy in self._models:
-            if policy.is_recurrent():
-                shapes = policy.internal_state_shapes()
-                rnn_state_variables = create_variables(batch_size, shapes)
-                rnn_states[policy.scope_name] = rnn_state_variables
-        for q_function in self._q_functions:
-            if q_function.is_recurrent():
-                shapes = q_function.internal_state_shapes()
-                rnn_state_variables = create_variables(batch_size, shapes)
-                rnn_states[q_function.scope_name] = rnn_state_variables
-
-        return TrainingVariables(batch_size, s_current_var, non_terminal=non_terminal_var, rnn_states=rnn_states)
+        s_current_var = nn.Variable((batch_size, *self._env_info.state_shape))
+        return TrainingVariables(batch_size, s_current_var)
 
     def _setup_solver(self):
         super()._setup_solver()
         if not self._config.fixed_temperature:
             self._temperature_solver.set_parameters(self._temperature.get_parameters(), reset=False, retain_state=True)
-
-    @property
-    def loss_variables(self) -> Dict[str, nn.Variable]:
-        return {"pi_loss": self._pi_loss, "temperature_loss": self._temperature_loss}
```

## nnabla_rl/model_trainers/policy/spg_policy_trainer.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -16,105 +16,100 @@
 from typing import Dict, Optional, Sequence, Union, cast
 
 import numpy as np
 
 import nnabla as nn
 import nnabla.functions as NF
 from nnabla_rl.environments.environment_info import EnvironmentInfo
-from nnabla_rl.model_trainers.model_trainer import (LossIntegration, ModelTrainer, TrainerConfig, TrainingBatch,
-                                                    TrainingVariables)
+from nnabla_rl.model_trainers.model_trainer import ModelTrainer, TrainerConfig, TrainingBatch, TrainingVariables
 from nnabla_rl.models import Model, StochasticPolicy
-from nnabla_rl.utils.data import set_data_to_variable
-from nnabla_rl.utils.misc import create_variable
 
 
 @dataclass
 class SPGPolicyTrainerConfig(TrainerConfig):
     pi_loss_scalar: float = 1.0
     grad_clip_norm: Optional[float] = None
 
 
 class SPGPolicyTrainer(ModelTrainer):
-    """Stochastic Policy Gradient (SPG) style Policy Trainer Stochastic Policy
-    Gradient is widely known as 'Policy Gradient algorithm'."""
+    '''Stochastic Policy Gradient (SPG) style Policy Trainer
+    Stochastic Policy Gradient is widely known as 'Policy Gradient algorithm'
+    '''
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _config: SPGPolicyTrainerConfig
     _pi_loss: nn.Variable
+    _target_return: nn.Variable
 
     def __init__(self,
                  models: Union[StochasticPolicy, Sequence[StochasticPolicy]],
                  solvers: Dict[str, nn.solver.Solver],
                  env_info: EnvironmentInfo,
                  config: SPGPolicyTrainerConfig = SPGPolicyTrainerConfig()):
         super(SPGPolicyTrainer, self).__init__(models, solvers, env_info, config)
 
+    def setup_batch(self, batch: TrainingBatch):
+        target_return = batch.extra['target_return']
+        prev_batch_size = self._target_return.shape[0]
+        new_batch_size = target_return.shape[0]
+        if prev_batch_size != new_batch_size:
+            self._target_return = nn.Variable((new_batch_size, 1))
+        target_return = np.reshape(target_return, self._target_return.shape)
+        self._target_return.d = target_return
+        return batch
+
     def _update_model(self,
                       models: Sequence[Model],
                       solvers: Dict[str, nn.solver.Solver],
                       batch: TrainingBatch,
                       training_variables: TrainingVariables,
-                      **kwargs) -> Dict[str, np.ndarray]:
-        for t, b in zip(training_variables, batch):
-            set_data_to_variable(t.s_current, b.s_current)
-            set_data_to_variable(t.a_current, b.a_current)
+                      **kwargs) -> Dict[str, np.array]:
+        training_variables.s_current.d = batch.s_current
+        training_variables.a_current.d = batch.a_current
 
         # update model
         for solver in solvers.values():
             solver.zero_grad()
         self._pi_loss.forward(clear_no_need_grad=True)
         self._pi_loss.backward(clear_buffer=True)
 
         for solver in solvers.values():
             if self._config.grad_clip_norm is not None:
                 solver.clip_grad_by_norm(self._config.grad_clip_norm)
             solver.update()
 
         trainer_state = {}
-        trainer_state['pi_loss'] = self._pi_loss.d.copy()
+        trainer_state['pi_loss'] = float(self._pi_loss.d.copy())
         return trainer_state
 
     def _build_training_graph(self, models: Sequence[Model], training_variables: TrainingVariables):
         models = cast(Sequence[StochasticPolicy], models)
-        self._pi_loss = 0
-        ignore_intermediate_loss = self._config.loss_integration is LossIntegration.LAST_TIMESTEP_ONLY
-        for step_index, variables in enumerate(training_variables):
-            is_burn_in_steps = step_index < self._config.burn_in_steps
-            is_intermediate_steps = step_index < self._config.burn_in_steps + self._config.unroll_steps - 1
-            ignore_loss = is_burn_in_steps or (is_intermediate_steps and ignore_intermediate_loss)
-            self._build_one_step_graph(models, variables, ignore_loss=ignore_loss)
-
-    def _build_one_step_graph(self,
-                              models: Sequence[Model],
-                              training_variables: TrainingVariables,
-                              ignore_loss: bool):
-        models = cast(Sequence[StochasticPolicy], models)
+
         # Actor optimization graph
         target_value = self._compute_target(training_variables)
         target_value.need_grad = False
 
+        self._pi_loss = 0
         for policy in models:
-            self._pi_loss += 0.0 if ignore_loss else self._compute_loss(policy, target_value, training_variables)
+            self._pi_loss += self._compute_loss(policy, target_value, training_variables)
 
     def _compute_loss(self,
                       model: StochasticPolicy,
                       target_value: nn.Variable,
                       training_variables: TrainingVariables) -> nn.Variable:
         distribution = model.pi(training_variables.s_current)
         log_prob = distribution.log_prob(training_variables.a_current)
         return NF.sum(-log_prob * target_value) * self._config.pi_loss_scalar
 
     def _compute_target(self, training_variables: TrainingVariables) -> nn.Variable:
         raise NotImplementedError
 
     def _setup_training_variables(self, batch_size) -> TrainingVariables:
         # Training input variables
-        s_current_var = create_variable(batch_size, self._env_info.state_shape)
-        a_current_var = create_variable(batch_size, self._env_info.action_shape)
-        return TrainingVariables(batch_size,
-                                 s_current_var,
-                                 a_current_var)
-
-    @property
-    def loss_variables(self) -> Dict[str, nn.Variable]:
-        return {"pi_loss": self._pi_loss}
+        s_current_var = nn.Variable((batch_size, *self._env_info.state_shape))
+        if self._env_info.is_discrete_action_env():
+            action_shape = (batch_size, 1)
+        else:
+            action_shape = (batch_size, self._env_info.action_dim)
+        a_current_var = nn.Variable(action_shape)
+        return TrainingVariables(batch_size, s_current_var, a_current_var)
```

## nnabla_rl/model_trainers/policy/trpo_policy_trainer.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -19,66 +19,65 @@
 
 import nnabla as nn
 import nnabla.functions as NF
 from nnabla_rl.environments.environment_info import EnvironmentInfo
 from nnabla_rl.logger import logger
 from nnabla_rl.model_trainers.model_trainer import ModelTrainer, TrainerConfig, TrainingBatch, TrainingVariables
 from nnabla_rl.models import Model, StochasticPolicy
-from nnabla_rl.utils.data import set_data_to_variable
-from nnabla_rl.utils.misc import copy_network_parameters, create_variable
+from nnabla_rl.utils.misc import copy_network_parameters
 from nnabla_rl.utils.optimization import conjugate_gradient
 
 
 def _hessian_vector_product(flat_grads, params, vector):
-    """Compute multiplied vector hessian of parameters and vector.
+    ''' Compute multiplied vector hessian of parameters and vector
 
     Args:
         flat_grads (nn.Variable): gradient of parameters, should be flattened
         params (list[nn.Variable]): list of parameters
         vector (numpy.ndarray): input vector, shape is the same as flat_grads
     Returns:
         hessian_vector (numpy.ndarray): multiplied vector of hessian of parameters and vector
     See:
         https://www.telesens.co/2018/06/09/efficiently-computing-the-fisher-vector-product-in-trpo/
-    """
+    '''
     assert flat_grads.shape[0] == len(vector)
     if isinstance(vector, np.ndarray):
         vector = nn.Variable.from_numpy_array(vector)
     hessian_multiplied_vector_loss = NF.sum(flat_grads * vector)
     hessian_multiplied_vector_loss.forward()
     for param in params:
         param.grad.zero()
     hessian_multiplied_vector_loss.backward()
     hessian_multiplied_vector = [param.g.copy().flatten() for param in params]
     return np.concatenate(hessian_multiplied_vector)
 
 
 def _concat_network_params_in_ndarray(params):
-    """Concatenate network parameters in numpy.ndarray, this function returns
-    copied parameters.
+    ''' Concatenate network parameters in numpy.ndarray,
+        this function returns copied parameters
 
     Args:
         params (OrderedDict): parameters
     Returns:
         flat_params (numpy.ndarray): flatten parameters in numpy.ndarray type
-    """
+    '''
     flat_params = []
     for param in params.values():
         flat_param = param.d.copy().flatten()
         flat_params.append(flat_param)
     return np.concatenate(flat_params)
 
 
 def _update_network_params_by_flat_params(params, new_flat_params):
-    """Update Network parameters by hand.
+    ''' Update Network parameters by hand
 
     Args:
         params (OrderedDict): parameteres
         new_flat_params (numpy.ndarray): flattened new parameters
-    """
+    '''
     if not isinstance(new_flat_params, np.ndarray):
         raise ValueError("Invalid new_flat_params")
     total_param_numbers = 0
     for param in params.values():
         param_shape = param.shape
         param_numbers = len(param.d.flatten())
         new_param = new_flat_params[total_param_numbers:total_param_numbers +
@@ -91,15 +90,14 @@
 @dataclass
 class TRPOPolicyTrainerConfig(TrainerConfig):
     gpu_batch_size: Optional[int] = None
     sigma_kl_divergence_constraint: float = 0.01
     maximum_backtrack_numbers: int = 10
     conjugate_gradient_damping: float = 0.1
     conjugate_gradient_iterations: int = 20
-    backtrack_coefficient: float = 0.5
 
     def __post_init__(self):
         self._assert_positive(self.sigma_kl_divergence_constraint, 'sigma_kl_divergence_constraint')
         self._assert_positive(self.maximum_backtrack_numbers, 'maximum_backtrack_numbers')
         self._assert_positive(self.conjugate_gradient_damping, 'conjugate_gradient_damping')
         self._assert_positive(self.conjugate_gradient_iterations, 'conjugate_gradient_iterations')
 
@@ -122,15 +120,15 @@
         super(TRPOPolicyTrainer, self).__init__(model, {}, env_info, config)
 
     def _update_model(self,
                       models: Sequence[Model],
                       solvers: Dict[str, nn.solver.Solver],
                       batch: TrainingBatch,
                       training_variables: TrainingVariables,
-                      **kwargs) -> Dict[str, np.ndarray]:
+                      **kwargs) -> Dict[str, np.array]:
         s = batch.s_current
         a = batch.a_current
         advantage = batch.extra['advantage']
 
         policy = models[0]
 
         old_policy = self._old_policy
@@ -158,15 +156,15 @@
 
     def _build_training_graph(self, models: Sequence[Model], training_variables: TrainingVariables):
         if len(models) != 1:
             raise RuntimeError('TRPO training only support 1 model for training')
         models = cast(Sequence[StochasticPolicy], models)
         policy = models[0]
         if not hasattr(self, '_old_policy'):
-            self._old_policy = policy.deepcopy('old_policy')
+            self._old_policy = cast(StochasticPolicy, policy.deepcopy('old_policy'))
         old_policy = self._old_policy
 
         # policy learning
         distribution = policy.pi(training_variables.s_current)
         old_distribution = old_policy.pi(training_variables.s_current)
 
         self._kl_divergence = NF.mean(old_distribution.kl_divergence(distribution))
@@ -216,16 +214,16 @@
     def _fisher_vector_product(self, policy, s_batch, a_batch, vector, training_variables):
         sum_hessian_multiplied_vector = 0
         gpu_batch_size = self._gpu_batch_size(len(s_batch))
         total_blocks = len(s_batch) // gpu_batch_size
 
         for block_index in range(total_blocks):
             start_idx = block_index * gpu_batch_size
-            set_data_to_variable(training_variables.s_current, s_batch[start_idx:start_idx+gpu_batch_size])
-            set_data_to_variable(training_variables.a_current, a_batch[start_idx:start_idx+gpu_batch_size])
+            training_variables.s_current.d = s_batch[start_idx:start_idx+gpu_batch_size]
+            training_variables.a_current.d = a_batch[start_idx:start_idx+gpu_batch_size]
 
             for param in policy.get_parameters().values():
                 param.grad.zero()
             self._kl_divergence_flat_grads.forward()
             hessian_vector_product = _hessian_vector_product(self._kl_divergence_flat_grads,
                                                              policy.get_parameters().values(),
                                                              vector)
@@ -236,15 +234,15 @@
     def _linesearch_and_update_params(
             self, policy, s_batch, a_batch, adv_batch, full_step_params_update, training_variables):
         current_flat_params = _concat_network_params_in_ndarray(policy.get_parameters())
 
         current_approximate_return, _, _ = self._forward_all_variables(
             s_batch, a_batch, adv_batch, training_variables)
 
-        for step_size in self._config.backtrack_coefficient**np.arange(self._config.maximum_backtrack_numbers):
+        for step_size in 0.5**np.arange(self._config.maximum_backtrack_numbers):
             new_flat_params = current_flat_params + step_size * full_step_params_update
             _update_network_params_by_flat_params(policy.get_parameters(), new_flat_params)
 
             approximate_return, kl_divergence, _ = self._forward_all_variables(
                 s_batch, a_batch, adv_batch, training_variables)
 
             improved = approximate_return - current_approximate_return > 0.
@@ -267,16 +265,16 @@
         sum_kl_divergence = 0.0
         sum_approximate_return_flat_grad = 0.0
         gpu_batch_size = self._gpu_batch_size(len(s_batch))
         total_blocks = len(s_batch) // gpu_batch_size
 
         for block_index in range(total_blocks):
             start_idx = block_index * gpu_batch_size
-            set_data_to_variable(training_variables.s_current, s_batch[start_idx:start_idx+gpu_batch_size])
-            set_data_to_variable(training_variables.a_current, a_batch[start_idx:start_idx+gpu_batch_size])
+            training_variables.s_current.d = s_batch[start_idx:start_idx+gpu_batch_size]
+            training_variables.a_current.d = a_batch[start_idx:start_idx+gpu_batch_size]
             training_variables.extra['advantage'].d = adv_batch[start_idx:start_idx+gpu_batch_size]
 
             nn.forward_all([self._approximate_return,
                             self._kl_divergence,
                             self._approximate_return_flat_grads])
 
             sum_approximate_return += float(self._approximate_return.d)
@@ -286,17 +284,16 @@
         approximate_return = sum_approximate_return / total_blocks
         approximate_return_flat_grads = sum_approximate_return_flat_grad / total_blocks
         kl_divergence = sum_kl_divergence / total_blocks
         return approximate_return, kl_divergence, approximate_return_flat_grads
 
     def _setup_training_variables(self, batch_size: int) -> TrainingVariables:
         gpu_batch_size = self._gpu_batch_size(batch_size)
-        s_current_var = create_variable(gpu_batch_size, self._env_info.state_shape)
-        a_current_var = create_variable(gpu_batch_size, self._env_info.action_shape)
-        advantage_var = create_variable(gpu_batch_size, 1)
+        s_current_var = nn.Variable((gpu_batch_size, *self._env_info.state_shape))
+        if self._env_info.is_discrete_action_env():
+            a_current_var = nn.Variable((gpu_batch_size, 1))
+        else:
+            a_current_var = nn.Variable((gpu_batch_size, self._env_info.action_dim))
+        advantage_var = nn.Variable((gpu_batch_size, 1))
         extra = {}
         extra['advantage'] = advantage_var
         return TrainingVariables(gpu_batch_size, s_current_var, a_current_var, extra=extra)
-
-    @property
-    def loss_variables(self) -> Dict[str, nn.Variable]:
-        return {"kl_divergence_flat_grads": self._kl_divergence_flat_grads}
```

## nnabla_rl/model_trainers/q_value/__init__.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -12,39 +12,27 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from nnabla_rl.model_trainers.q_value.bcq_q_trainer import (  # noqa
     BCQQTrainer, BCQQTrainerConfig)
 from nnabla_rl.model_trainers.q_value.categorical_dqn_q_trainer import (  # noqa
     CategoricalDQNQTrainer, CategoricalDQNQTrainerConfig)
-from nnabla_rl.model_trainers.q_value.categorical_ddqn_q_trainer import (  # noqa
-    CategoricalDDQNQTrainer, CategoricalDDQNQTrainerConfig)
 from nnabla_rl.model_trainers.q_value.clipped_double_q_trainer import (  # noqa
     ClippedDoubleQTrainer, ClippedDoubleQTrainerConfig)
 from nnabla_rl.model_trainers.q_value.ddpg_q_trainer import (  # noqa
     DDPGQTrainer, DDPGQTrainerConfig)
 from nnabla_rl.model_trainers.q_value.ddqn_q_trainer import (  # noqa
     DDQNQTrainer, DDQNQTrainerConfig)
 from nnabla_rl.model_trainers.q_value.dqn_q_trainer import (  # noqa
     DQNQTrainer, DQNQTrainerConfig)
-from nnabla_rl.model_trainers.q_value.her_q_trainer import (  # noqa
-    HERQTrainer, HERQTrainerConfig)
-from nnabla_rl.model_trainers.q_value.hyar_q_trainer import (  # noqa
-    HyARQTrainer, HyARQTrainerConfig)
 from nnabla_rl.model_trainers.q_value.iqn_q_trainer import (  # noqa
     IQNQTrainer, IQNQTrainerConfig)
 from nnabla_rl.model_trainers.q_value.munchausen_rl_q_trainer import (  # noqa
     MunchausenIQNQTrainer, MunchausenIQNQTrainerConfig, MunchausenDQNQTrainer, MunchausenDQNQTrainerConfig)
 from nnabla_rl.model_trainers.q_value.qrdqn_q_trainer import (  # noqa
     QRDQNQTrainer, QRDQNQTrainerConfig)
-from nnabla_rl.model_trainers.q_value.qrsac_q_trainer import (  # noqa
-    QRSACQTrainer, QRSACQTrainerConfig)
-from nnabla_rl.model_trainers.q_value.redq_q_trainer import (  # noqa
-    REDQQTrainer, REDQQTrainerConfig)
 from nnabla_rl.model_trainers.q_value.soft_q_trainer import (  # noqa
     SoftQTrainer, SoftQTrainerConfig)
-from nnabla_rl.model_trainers.q_value.soft_q_decomposition_trainer import (  # noqa
-    SoftQDTrainer, SoftQDTrainerConfig)
 from nnabla_rl.model_trainers.q_value.td3_q_trainer import (  # noqa
     TD3QTrainer, TD3QTrainerConfig)
 from nnabla_rl.model_trainers.q_value.v_targeted_q_trainer import (  # noqa
     VTargetedQTrainer, VTargetedQTrainerConfig)
```

## nnabla_rl/model_trainers/q_value/bcq_q_trainer.py

```diff
@@ -15,97 +15,57 @@
 from dataclasses import dataclass
 from typing import Dict, Sequence, Union
 
 import nnabla as nn
 import nnabla.functions as NF
 import nnabla_rl.functions as RNF
 from nnabla_rl.environments.environment_info import EnvironmentInfo
-from nnabla_rl.model_trainers.model_trainer import TrainingVariables, rnn_support
+from nnabla_rl.model_trainers.model_trainer import TrainingVariables
 from nnabla_rl.model_trainers.q_value.squared_td_q_function_trainer import (SquaredTDQFunctionTrainer,
                                                                             SquaredTDQFunctionTrainerConfig)
 from nnabla_rl.models import DeterministicPolicy, QFunction
 from nnabla_rl.utils.data import convert_to_list_if_not_list
-from nnabla_rl.utils.misc import create_variables
 
 
 @dataclass
 class BCQQTrainerConfig(SquaredTDQFunctionTrainerConfig):
     lmb: float = 0.75
     num_action_samples: int = 10
 
 
 class BCQQTrainer(SquaredTDQFunctionTrainer):
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _target_functions: Sequence[QFunction]
     _config: BCQQTrainerConfig
-    _prev_target_rnn_states: Dict[str, Dict[str, nn.Variable]]
-    _prev_q_rnn_states: Dict[str, Dict[str, nn.Variable]]
 
     def __init__(self,
                  train_functions: Union[QFunction, Sequence[QFunction]],
                  solvers: Dict[str, nn.solver.Solver],
                  target_functions: Union[QFunction, Sequence[QFunction]],
                  target_policy: DeterministicPolicy,
                  env_info: EnvironmentInfo,
                  config: BCQQTrainerConfig = BCQQTrainerConfig()):
         self._target_functions = convert_to_list_if_not_list(target_functions)
-        self._assert_no_duplicate_model(self._target_functions)
         self._target_policy = target_policy
-        self._prev_target_rnn_states = {}
-        self._prev_q_rnn_states = {}
         super(BCQQTrainer, self).__init__(train_functions, solvers, env_info, config)
 
-    def support_rnn(self) -> bool:
-        return True
-
     def _compute_target(self, training_variables: TrainingVariables) -> nn.Variable:
         gamma = training_variables.gamma
         reward = training_variables.reward
         non_terminal = training_variables.non_terminal
         s_next = training_variables.s_next
 
         batch_size = training_variables.batch_size
         s_next_rep = RNF.repeat(x=s_next, repeats=self._config.num_action_samples, axis=0)
-
-        train_rnn_states = training_variables.rnn_states
-        prev_rnn_states = self._prev_target_rnn_states
-        with rnn_support(self._target_policy, prev_rnn_states, train_rnn_states, training_variables, self._config):
-            a_next_rep = self._target_policy.pi(s_next_rep)
-
-        q_values = []
-        prev_rnn_states = self._prev_q_rnn_states
-        for target_q_function in self._target_functions:
-            with rnn_support(target_q_function, prev_rnn_states, train_rnn_states, training_variables, self._config):
-                q_value = target_q_function.q(s_next_rep, a_next_rep)
-                q_values.append(q_value)
-        q_values = NF.stack(*q_values)
-
+        a_next_rep = self._target_policy.pi(s_next_rep)
+        q_values = NF.stack(*(q_target.q(s_next_rep, a_next_rep) for q_target in self._target_functions))
         num_q_ensembles = len(self._target_functions)
-        assert isinstance(q_values, nn.Variable)
         assert q_values.shape == (num_q_ensembles, batch_size * self._config.num_action_samples, 1)
         weighted_q_minmax = self._config.lmb * NF.min(q_values, axis=0) + \
             (1.0 - self._config.lmb) * NF.max(q_values, axis=0)
         assert weighted_q_minmax.shape == (batch_size * self._config.num_action_samples, 1)
 
         next_q_value = NF.max(NF.reshape(weighted_q_minmax, shape=(batch_size, -1)), axis=1, keepdims=True)
         assert next_q_value.shape == (batch_size, 1)
         return reward + gamma * non_terminal * next_q_value
-
-    def _setup_training_variables(self, batch_size) -> TrainingVariables:
-        training_variables = super()._setup_training_variables(batch_size)
-
-        rnn_states = {}
-        for target_function in self._target_functions:
-            if target_function.is_recurrent():
-                shapes = target_function.internal_state_shapes()
-                rnn_state_variables = create_variables(batch_size, shapes)
-                rnn_states[target_function.scope_name] = rnn_state_variables
-
-        if self._target_policy.is_recurrent():
-            shapes = self._target_policy.internal_state_shapes()
-            rnn_state_variables = create_variables(batch_size, shapes)
-            rnn_states[self._target_policy.scope_name] = rnn_state_variables
-
-        training_variables.rnn_states.update(rnn_states)
-        return training_variables
```

## nnabla_rl/model_trainers/q_value/categorical_dqn_q_trainer.py

```diff
@@ -16,61 +16,52 @@
 from typing import Dict, Sequence, Union
 
 import numpy as np
 
 import nnabla as nn
 import nnabla.functions as NF
 from nnabla_rl.environments.environment_info import EnvironmentInfo
-from nnabla_rl.model_trainers.model_trainer import TrainingVariables, rnn_support
+from nnabla_rl.model_trainers.model_trainer import TrainingVariables
 from nnabla_rl.model_trainers.q_value.value_distribution_function_trainer import (
     ValueDistributionFunctionTrainer, ValueDistributionFunctionTrainerConfig)
 from nnabla_rl.models import ValueDistributionFunction
-from nnabla_rl.utils.misc import create_variables
 
 
 @dataclass
 class CategoricalDQNQTrainerConfig(ValueDistributionFunctionTrainerConfig):
     pass
 
 
 class CategoricalDQNQTrainer(ValueDistributionFunctionTrainer):
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _target_function: ValueDistributionFunction
-    _prev_target_rnn_states: Dict[str, Dict[str, nn.Variable]]
 
     def __init__(self,
                  train_functions: Union[ValueDistributionFunction, Sequence[ValueDistributionFunction]],
                  solvers: Dict[str, nn.solver.Solver],
                  target_function: ValueDistributionFunction,
                  env_info: EnvironmentInfo,
                  config: CategoricalDQNQTrainerConfig = CategoricalDQNQTrainerConfig()):
         self._target_function = target_function
-        self._prev_target_rnn_states = {}
         super(CategoricalDQNQTrainer, self).__init__(train_functions, solvers, env_info, config)
 
-    def support_rnn(self) -> bool:
-        return True
-
     def _compute_target(self, training_variables: TrainingVariables, **kwargs) -> nn.Variable:
         batch_size = training_variables.batch_size
         gamma = training_variables.gamma
         reward = training_variables.reward
         non_terminal = training_variables.non_terminal
         s_next = training_variables.s_next
 
         N = self._target_function._n_atom
         v_max = self._config.v_max
         v_min = self._config.v_min
 
-        prev_rnn_states = self._prev_target_rnn_states
-        train_rnn_states = training_variables.rnn_states
-        with rnn_support(self._target_function, prev_rnn_states, train_rnn_states, training_variables, self._config):
-            pj = self._target_function.max_q_probs(s_next)
+        pj = self._target_function.max_q_probs(s_next)
 
         delta_z = (v_max - v_min) / (N - 1)
         z = np.asarray([v_min + i * delta_z for i in range(N)])
         z = np.broadcast_to(array=z, shape=(batch_size, N))
         z = nn.Variable.from_numpy_array(z)
         target = reward + non_terminal * gamma * z
         Tz = NF.clip_by_value(target, v_min, v_max)
@@ -97,19 +88,7 @@
         # upper - bj = (1 + lower) - bj
         upper = 1 + lower
 
         result_upper = NF.scatter_add(mi, ml_indices, pj * (upper - bj), axis=-1)
         result_lower = NF.scatter_add(mi, mu_indices, pj * (bj - lower), axis=-1)
 
         return (result_upper + result_lower)
-
-    def _setup_training_variables(self, batch_size: int) -> TrainingVariables:
-        training_variables = super()._setup_training_variables(batch_size)
-
-        rnn_states = {}
-        if self._target_function.is_recurrent():
-            shapes = self._target_function.internal_state_shapes()
-            rnn_state_variables = create_variables(batch_size, shapes)
-            rnn_states[self._target_function.scope_name] = rnn_state_variables
-
-        training_variables.rnn_states.update(rnn_states)
-        return training_variables
```

## nnabla_rl/model_trainers/q_value/clipped_double_q_trainer.py

```diff
@@ -14,79 +14,48 @@
 
 from dataclasses import dataclass
 from typing import Dict, Sequence, Union
 
 import nnabla as nn
 import nnabla_rl.functions as RNF
 from nnabla_rl.environments.environment_info import EnvironmentInfo
-from nnabla_rl.model_trainers.model_trainer import TrainingVariables, rnn_support
+from nnabla_rl.model_trainers.model_trainer import TrainingVariables
 from nnabla_rl.model_trainers.q_value.squared_td_q_function_trainer import (SquaredTDQFunctionTrainer,
                                                                             SquaredTDQFunctionTrainerConfig)
 from nnabla_rl.models import QFunction
-from nnabla_rl.utils.data import convert_to_list_if_not_list
-from nnabla_rl.utils.misc import create_variables
 
 
 @dataclass
 class ClippedDoubleQTrainerConfig(SquaredTDQFunctionTrainerConfig):
     pass
 
 
 class ClippedDoubleQTrainer(SquaredTDQFunctionTrainer):
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _target_functions: Sequence[QFunction]
-    _prev_q0_rnn_states: Dict[str, Dict[str, nn.Variable]]
-    _prev_q_rnn_states: Dict[str, Dict[str, nn.Variable]]
 
     def __init__(self,
                  train_functions: Union[QFunction, Sequence[QFunction]],
                  solvers: Dict[str, nn.solver.Solver],
                  target_functions: Sequence[QFunction],
                  env_info: EnvironmentInfo,
                  config: ClippedDoubleQTrainerConfig = ClippedDoubleQTrainerConfig()):
         if len(target_functions) < 2:
             raise ValueError('Must have at least 2 target functions for training')
-        self._target_functions = convert_to_list_if_not_list(target_functions)
-        self._assert_no_duplicate_model(self._target_functions)
-        self._prev_q0_rnn_states = {}
-        self._prev_q_rnn_states = {}
+        self._target_functions = target_functions
         super(ClippedDoubleQTrainer, self).__init__(train_functions, solvers, env_info, config)
 
-    def support_rnn(self) -> bool:
-        return True
-
     def _compute_target(self, training_variables: TrainingVariables, **kwargs) -> nn.Variable:
         gamma = training_variables.gamma
         reward = training_variables.reward
         non_terminal = training_variables.non_terminal
         s_next = training_variables.s_next
 
-        train_rnn_states = training_variables.rnn_states
-        prev_rnn_states = self._prev_q0_rnn_states
-        target_q0_function = self._target_functions[0]
-        with rnn_support(target_q0_function, prev_rnn_states, train_rnn_states, training_variables, self._config):
-            a_next = target_q0_function.argmax_q(s_next)
-
         q_values = []
-        prev_rnn_states = self._prev_q_rnn_states
-        for target_q_function in self._target_functions:
-            with rnn_support(target_q_function, prev_rnn_states, train_rnn_states, training_variables, self._config):
-                q_value = target_q_function.q(s_next, a_next)
-                q_values.append(q_value)
-
+        a_next = self._target_functions[0].argmax_q(s_next)
+        for target_function in self._target_functions:
+            q_value = target_function.q(s_next, a_next)
+            q_values.append(q_value)
         target_q = RNF.minimum_n(q_values)
         return reward + gamma * non_terminal * target_q
-
-    def _setup_training_variables(self, batch_size) -> TrainingVariables:
-        training_variables = super()._setup_training_variables(batch_size)
-
-        rnn_states = {}
-        for target_function in self._target_functions:
-            if target_function.is_recurrent():
-                shapes = target_function.internal_state_shapes()
-                rnn_state_variables = create_variables(batch_size, shapes)
-                rnn_states[target_function.scope_name] = rnn_state_variables
-
-        training_variables.rnn_states.update(rnn_states)
-        return training_variables
```

## nnabla_rl/model_trainers/q_value/ddpg_q_trainer.py

```diff
@@ -14,84 +14,52 @@
 
 from dataclasses import dataclass
 from typing import Dict, Sequence, Union
 
 import nnabla as nn
 import nnabla_rl.functions as RNF
 from nnabla_rl.environments.environment_info import EnvironmentInfo
-from nnabla_rl.model_trainers.model_trainer import TrainingVariables, rnn_support
+from nnabla_rl.model_trainers.model_trainer import TrainingVariables
 from nnabla_rl.model_trainers.q_value.squared_td_q_function_trainer import (SquaredTDQFunctionTrainer,
                                                                             SquaredTDQFunctionTrainerConfig)
 from nnabla_rl.models import DeterministicPolicy, QFunction
 from nnabla_rl.utils.data import convert_to_list_if_not_list
-from nnabla_rl.utils.misc import create_variables
 
 
 @dataclass
 class DDPGQTrainerConfig(SquaredTDQFunctionTrainerConfig):
     pass
 
 
 class DDPGQTrainer(SquaredTDQFunctionTrainer):
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _target_functions: Sequence[QFunction]
     _target_policy: DeterministicPolicy
-    _prev_target_rnn_states: Dict[str, Dict[str, nn.Variable]]
-    _prev_q_rnn_states: Dict[str, Dict[str, nn.Variable]]
 
     def __init__(self,
                  train_functions: Union[QFunction, Sequence[QFunction]],
                  solvers: Dict[str, nn.solver.Solver],
                  target_functions: Union[QFunction, Sequence[QFunction]],
                  target_policy: DeterministicPolicy,
                  env_info: EnvironmentInfo,
                  config: DDPGQTrainerConfig = DDPGQTrainerConfig()):
         self._target_policy = target_policy
         self._target_functions = convert_to_list_if_not_list(target_functions)
-        self._assert_no_duplicate_model(self._target_functions)
-        self._prev_target_rnn_states = {}
-        self._prev_q_rnn_states = {}
         super(DDPGQTrainer, self).__init__(train_functions, solvers, env_info, config)
 
-    def support_rnn(self) -> bool:
-        return True
-
     def _compute_target(self, training_variables: TrainingVariables, **kwargs) -> nn.Variable:
         gamma = training_variables.gamma
         reward = training_variables.reward
         non_terminal = training_variables.non_terminal
         s_next = training_variables.s_next
 
-        train_rnn_states = training_variables.rnn_states
-        prev_rnn_states = self._prev_target_rnn_states
-        with rnn_support(self._target_policy, prev_rnn_states, train_rnn_states, training_variables, self._config):
-            a_next = self._target_policy.pi(s_next)
-        a_next.need_grad = False
-
         q_values = []
-        prev_rnn_states = self._prev_q_rnn_states
+        a_next = self._target_policy.pi(s_next)
+        a_next.need_grad = False
         for target_q_function in self._target_functions:
-            with rnn_support(target_q_function, prev_rnn_states, train_rnn_states, training_variables, self._config):
-                q_value = target_q_function.q(s_next, a_next)
-                q_values.append(q_value)
+            q_value = target_q_function.q(s_next, a_next)
+            q_values.append(q_value)
         # Use the minimum among computed q_values by default
         target_q = RNF.minimum_n(q_values)
         return reward + gamma * non_terminal * target_q
-
-    def _setup_training_variables(self, batch_size: int) -> TrainingVariables:
-        training_variables = super()._setup_training_variables(batch_size)
-
-        rnn_states = {}
-        for target_function in self._target_functions:
-            if target_function.is_recurrent():
-                shapes = target_function.internal_state_shapes()
-                rnn_state_variables = create_variables(batch_size, shapes)
-                rnn_states[target_function.scope_name] = rnn_state_variables
-        if self._target_policy.is_recurrent():
-            shapes = self._target_policy.internal_state_shapes()
-            rnn_state_variables = create_variables(batch_size, shapes)
-            rnn_states[self._target_policy.scope_name] = rnn_state_variables
-
-        training_variables.rnn_states.update(rnn_states)
-        return training_variables
```

## nnabla_rl/model_trainers/q_value/ddqn_q_trainer.py

```diff
@@ -13,76 +13,48 @@
 # limitations under the License.
 
 from dataclasses import dataclass
 from typing import Dict
 
 import nnabla as nn
 from nnabla_rl.environments.environment_info import EnvironmentInfo
-from nnabla_rl.model_trainers.model_trainer import TrainingVariables, rnn_support
+from nnabla_rl.model_trainers.model_trainer import TrainingVariables
 from nnabla_rl.model_trainers.q_value.squared_td_q_function_trainer import (SquaredTDQFunctionTrainer,
                                                                             SquaredTDQFunctionTrainerConfig)
 from nnabla_rl.models import QFunction
-from nnabla_rl.utils.misc import create_variables
 
 
 @dataclass
 class DDQNQTrainerConfig(SquaredTDQFunctionTrainerConfig):
     pass
 
 
 class DDQNQTrainer(SquaredTDQFunctionTrainer):
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _train_function: QFunction
     _target_function: QFunction
-    _prev_train_rnn_states: Dict[str, Dict[str, nn.Variable]]
-    _prev_target_rnn_states: Dict[str, Dict[str, nn.Variable]]
 
     def __init__(self,
                  train_function: QFunction,
                  solvers: Dict[str, nn.solver.Solver],
                  target_function: QFunction,
                  env_info: EnvironmentInfo,
                  config: DDQNQTrainerConfig = DDQNQTrainerConfig()):
         self._train_function = train_function
         self._target_function = target_function
-        self._prev_train_rnn_states = {}
-        self._prev_target_rnn_states = {}
         super(DDQNQTrainer, self).__init__(train_function, solvers, env_info, config)
 
-    def support_rnn(self) -> bool:
-        return True
-
     def _compute_target(self, training_variables: TrainingVariables, **kwargs) -> nn.Variable:
         gamma = training_variables.gamma
         reward = training_variables.reward
         non_terminal = training_variables.non_terminal
         s_next = training_variables.s_next
         assert gamma is not None
         assert reward is not None
         assert non_terminal is not None
         assert s_next is not None
 
-        train_rnn_states = training_variables.rnn_states
-        prev_rnn_states = self._prev_train_rnn_states
-        with rnn_support(self._train_function, prev_rnn_states, train_rnn_states, training_variables, self._config):
-            a_next = self._train_function.argmax_q(s_next)
-
-        prev_rnn_states = self._prev_target_rnn_states
-        with rnn_support(self._target_function, prev_rnn_states, train_rnn_states,  training_variables, self._config):
-            double_q_target = self._target_function.q(s_next, a_next)
-
+        a_next = self._train_function.argmax_q(s_next)
+        double_q_target = self._target_function.q(s_next, a_next)
         return reward + gamma * non_terminal * double_q_target
-
-    def _setup_training_variables(self, batch_size: int) -> TrainingVariables:
-        training_variables = super()._setup_training_variables(batch_size)
-
-        rnn_states = {}
-        if self._target_function.is_recurrent():
-            shapes = self._target_function.internal_state_shapes()
-            rnn_state_variables = create_variables(batch_size, shapes)
-            rnn_states[self._target_function.scope_name] = rnn_state_variables
-        # NOTE: rnn_states for train_function is already generated by parent class
-
-        training_variables.rnn_states.update(rnn_states)
-        return training_variables
```

## nnabla_rl/model_trainers/q_value/dqn_q_trainer.py

```diff
@@ -13,62 +13,41 @@
 # limitations under the License.
 
 from dataclasses import dataclass
 from typing import Dict, Sequence, Union
 
 import nnabla as nn
 from nnabla_rl.environments.environment_info import EnvironmentInfo
-from nnabla_rl.model_trainers.model_trainer import TrainingVariables, rnn_support
+from nnabla_rl.model_trainers.model_trainer import TrainingVariables
 from nnabla_rl.model_trainers.q_value.squared_td_q_function_trainer import (SquaredTDQFunctionTrainer,
                                                                             SquaredTDQFunctionTrainerConfig)
 from nnabla_rl.models import QFunction
-from nnabla_rl.utils.misc import create_variables
 
 
 @dataclass
 class DQNQTrainerConfig(SquaredTDQFunctionTrainerConfig):
     pass
 
 
 class DQNQTrainer(SquaredTDQFunctionTrainer):
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _target_function: QFunction
-    _prev_target_rnn_states: Dict[str, Dict[str, nn.Variable]]
 
     def __init__(self,
                  train_functions: Union[QFunction, Sequence[QFunction]],
                  solvers: Dict[str, nn.solver.Solver],
                  target_function: QFunction,
                  env_info: EnvironmentInfo,
                  config: DQNQTrainerConfig = DQNQTrainerConfig()):
         self._target_function = target_function
-        self._prev_target_rnn_states = {}
         super(DQNQTrainer, self).__init__(train_functions, solvers, env_info, config)
 
-    def support_rnn(self) -> bool:
-        return True
-
     def _compute_target(self, training_variables: TrainingVariables) -> nn.Variable:
         gamma = training_variables.gamma
         reward = training_variables.reward
         non_terminal = training_variables.non_terminal
         s_next = training_variables.s_next
 
-        prev_rnn_states = self._prev_target_rnn_states
-        train_rnn_states = training_variables.rnn_states
-        with rnn_support(self._target_function, prev_rnn_states, train_rnn_states, training_variables, self._config):
-            max_q_value = self._target_function.max_q(s_next)
+        max_q_value = self._target_function.max_q(s_next)
         return reward + gamma * non_terminal * max_q_value
-
-    def _setup_training_variables(self, batch_size: int) -> TrainingVariables:
-        training_variables = super()._setup_training_variables(batch_size)
-
-        rnn_states = {}
-        if self._target_function.is_recurrent():
-            shapes = self._target_function.internal_state_shapes()
-            rnn_state_variables = create_variables(batch_size, shapes)
-            rnn_states[self._target_function.scope_name] = rnn_state_variables
-
-        training_variables.rnn_states.update(rnn_states)
-        return training_variables
```

## nnabla_rl/model_trainers/q_value/iqn_q_trainer.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021,2022 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -13,77 +13,47 @@
 # limitations under the License.
 
 from dataclasses import dataclass
 from typing import Dict, Sequence, Union
 
 import nnabla as nn
 from nnabla_rl.environments.environment_info import EnvironmentInfo
-from nnabla_rl.model_trainers.model_trainer import TrainingVariables, rnn_support
+from nnabla_rl.model_trainers.model_trainer import TrainingVariables
 from nnabla_rl.model_trainers.q_value.state_action_quantile_function_trainer import (
     StateActionQuantileFunctionTrainer, StateActionQuantileFunctionTrainerConfig)
 from nnabla_rl.models import StateActionQuantileFunction
-from nnabla_rl.utils.misc import create_variables
 
 
 @dataclass
 class IQNQTrainerConfig(StateActionQuantileFunctionTrainerConfig):
     pass
 
 
 class IQNQTrainer(StateActionQuantileFunctionTrainer):
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _target_function: StateActionQuantileFunction
-    _prev_a_star_rnn_states: Dict[str, Dict[str, nn.Variable]]
-    _prev_z_tau_j_rnn_states: Dict[str, Dict[str, nn.Variable]]
 
     def __init__(self,
                  train_functions: Union[StateActionQuantileFunction, Sequence[StateActionQuantileFunction]],
                  solvers: Dict[str, nn.solver.Solver],
                  target_function: StateActionQuantileFunction,
                  env_info: EnvironmentInfo,
                  config: IQNQTrainerConfig = IQNQTrainerConfig()):
         self._target_function = target_function
-        self._prev_a_star_rnn_states = {}
-        self._prev_z_tau_j_rnn_states = {}
         super(IQNQTrainer, self).__init__(train_functions, solvers, env_info, config)
 
-    def support_rnn(self) -> bool:
-        return True
-
     def _compute_target(self, training_variables: TrainingVariables) -> nn.Variable:
         gamma = training_variables.gamma
         reward = training_variables.reward
         non_terminal = training_variables.non_terminal
         s_next = training_variables.s_next
 
         batch_size = training_variables.batch_size
         N_prime = self._config.N_prime
 
         tau_j = self._target_function.sample_tau(shape=(batch_size, N_prime))
-
-        train_rnn_states = training_variables.rnn_states
-        prev_rnn_states = self._prev_a_star_rnn_states
-        with rnn_support(self._target_function, prev_rnn_states, train_rnn_states, training_variables, self._config):
-            a_star = self._target_function.as_q_function().argmax_q(s_next)
-
-        train_rnn_states = training_variables.rnn_states
-        prev_rnn_states = self._prev_z_tau_j_rnn_states
-        with rnn_support(self._target_function, prev_rnn_states, train_rnn_states, training_variables, self._config):
-            Z_tau_j = self._target_function.quantile_values(s_next, a_star, tau_j)
-
+        Z_tau_j = self._target_function.max_q_quantile_values(s_next, tau_j)
         assert Z_tau_j.shape == (batch_size, N_prime)
         target = reward + non_terminal * gamma * Z_tau_j
         return target
-
-    def _setup_training_variables(self, batch_size: int) -> TrainingVariables:
-        training_variables = super()._setup_training_variables(batch_size)
-
-        rnn_states = {}
-        if self._target_function.is_recurrent():
-            shapes = self._target_function.internal_state_shapes()
-            rnn_state_variables = create_variables(batch_size, shapes)
-            rnn_states[self._target_function.scope_name] = rnn_state_variables
-
-        training_variables.rnn_states.update(rnn_states)
-        return training_variables
```

## nnabla_rl/model_trainers/q_value/munchausen_rl_q_trainer.py

```diff
@@ -15,21 +15,20 @@
 from dataclasses import dataclass
 from typing import Dict, Sequence, Union
 
 import nnabla as nn
 import nnabla.functions as NF
 import nnabla_rl.functions as RF
 from nnabla_rl.environments.environment_info import EnvironmentInfo
-from nnabla_rl.model_trainers.model_trainer import TrainingVariables, rnn_support
+from nnabla_rl.model_trainers.model_trainer import TrainingVariables
 from nnabla_rl.model_trainers.q_value.squared_td_q_function_trainer import (SquaredTDQFunctionTrainer,
                                                                             SquaredTDQFunctionTrainerConfig)
 from nnabla_rl.model_trainers.q_value.state_action_quantile_function_trainer import (
     StateActionQuantileFunctionTrainer, StateActionQuantileFunctionTrainerConfig)
 from nnabla_rl.models import QFunction, StateActionQuantileFunction
-from nnabla_rl.utils.misc import create_variables
 
 
 def _pi(q_values: nn.Variable, max_q: nn.Variable, tau: float):
     return NF.softmax((q_values - max_q) / tau)
 
 
 def _all_tau_log_pi(q_values: nn.Variable, max_q: nn.Variable, tau: float):
@@ -54,89 +53,47 @@
 
 class MunchausenDQNQTrainer(SquaredTDQFunctionTrainer):
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _target_function: QFunction
     _config: MunchausenDQNQTrainerConfig
-    _prev_all_next_q_rnn_states: Dict[str, Dict[str, nn.Variable]]
-    _prev_max_next_q_rnn_states: Dict[str, Dict[str, nn.Variable]]
-    _prev_current_q_rnn_states: Dict[str, Dict[str, nn.Variable]]
-    _prev_all_current_q_rnn_states: Dict[str, Dict[str, nn.Variable]]
-    _prev_max_current_q_rnn_states: Dict[str, Dict[str, nn.Variable]]
 
     def __init__(self,
                  train_functions: Union[QFunction, Sequence[QFunction]],
                  solvers: Dict[str, nn.solver.Solver],
                  target_function: QFunction,
                  env_info: EnvironmentInfo,
                  config: MunchausenDQNQTrainerConfig = MunchausenDQNQTrainerConfig()):
         self._target_function = target_function
-        self._prev_all_next_q_rnn_states = {}
-        self._prev_max_next_q_rnn_states = {}
-        self._prev_current_q_rnn_states = {}
-        self._prev_all_current_q_rnn_states = {}
-        self._prev_max_current_q_rnn_states = {}
-
         super(MunchausenDQNQTrainer, self).__init__(train_functions, solvers, env_info, config)
 
-    def support_rnn(self) -> bool:
-        return True
-
     def _compute_target(self, training_variables: TrainingVariables) -> nn.Variable:
         s_current = training_variables.s_current
         a_current = training_variables.a_current
         gamma = training_variables.gamma
         reward = training_variables.reward
         non_terminal = training_variables.non_terminal
         s_next = training_variables.s_next
 
-        train_rnn_states = training_variables.rnn_states
-        prev_rnn_states = self._prev_all_next_q_rnn_states
-        with rnn_support(self._target_function, prev_rnn_states, train_rnn_states, training_variables, self._config):
-            all_next_q = self._target_function.all_q(s_next)
-
-        prev_rnn_states = self._prev_max_next_q_rnn_states
-        with rnn_support(self._target_function, prev_rnn_states, train_rnn_states, training_variables, self._config):
-            max_next_q = self._target_function.max_q(s_next)
-
+        all_next_q = self._target_function.all_q(s_next)
+        max_next_q = self._target_function.max_q(s_next)
         pi = _pi(all_next_q, max_next_q, tau=self._config.tau)
         all_tau_log_pi = _all_tau_log_pi(all_next_q, max_next_q, self._config.tau)
         assert pi.shape == all_next_q.shape
         assert pi.shape == all_tau_log_pi.shape
         soft_q_target = NF.sum(pi * (all_next_q - all_tau_log_pi), axis=(pi.ndim - 1),  keepdims=True)
 
-        prev_rnn_states = self._prev_current_q_rnn_states
-        with rnn_support(self._target_function, prev_rnn_states, train_rnn_states, training_variables, self._config):
-            current_q = self._target_function.q(s_current, a_current)
-
-        prev_rnn_states = self._prev_all_current_q_rnn_states
-        with rnn_support(self._target_function, prev_rnn_states, train_rnn_states, training_variables, self._config):
-            all_current_q = self._target_function.all_q(s_current)
-
-        prev_rnn_states = self._prev_max_current_q_rnn_states
-        with rnn_support(self._target_function, prev_rnn_states, train_rnn_states, training_variables, self._config):
-            max_current_q = self._target_function.max_q(s_current)
-
+        current_q = self._target_function.q(s_current, a_current)
+        all_current_q = self._target_function.all_q(s_current)
+        max_current_q = self._target_function.max_q(s_current)
         tau_log_pi = _tau_log_pi(current_q, all_current_q, max_current_q, self._config.tau)
         clipped_tau_log_pi = NF.clip_by_value(tau_log_pi, self._config.clip_min, self._config.clip_max)
         return reward + self._config.alpha * clipped_tau_log_pi + gamma * non_terminal * soft_q_target
 
-    def _setup_training_variables(self, batch_size: int) -> TrainingVariables:
-        training_variables = super()._setup_training_variables(batch_size)
-
-        rnn_states = {}
-        if self._target_function.is_recurrent():
-            shapes = self._target_function.internal_state_shapes()
-            rnn_state_variables = create_variables(batch_size, shapes)
-            rnn_states[self._target_function.scope_name] = rnn_state_variables
-
-        training_variables.rnn_states.update(rnn_states)
-        return training_variables
-
 
 @dataclass
 class MunchausenIQNQTrainerConfig(StateActionQuantileFunctionTrainerConfig):
     tau: float = 0.03
     alpha: float = 0.9
     clip_min: float = -1.0
     clip_max: float = 0.0
@@ -144,79 +101,53 @@
 
 class MunchausenIQNQTrainer(StateActionQuantileFunctionTrainer):
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _target_function: StateActionQuantileFunction
     _config: MunchausenIQNQTrainerConfig
-    _prev_next_quantile_rnn_states: Dict[str, Dict[str, nn.Variable]]
-    _prev_current_quantile_rnn_states: Dict[str, Dict[str, nn.Variable]]
 
     def __init__(self,
                  train_functions: Union[StateActionQuantileFunction, Sequence[StateActionQuantileFunction]],
                  solvers: Dict[str, nn.solver.Solver],
                  target_function: StateActionQuantileFunction,
                  env_info: EnvironmentInfo,
                  config: MunchausenIQNQTrainerConfig = MunchausenIQNQTrainerConfig()):
         self._target_function = target_function
-        self._prev_next_quantile_rnn_states = {}
-        self._prev_current_quantile_rnn_states = {}
-
         super(MunchausenIQNQTrainer, self).__init__(train_functions, solvers, env_info, config)
 
-    def support_rnn(self) -> bool:
-        return True
-
     def _compute_target(self, training_variables: TrainingVariables) -> nn.Variable:
         s_current = training_variables.s_current
         a_current = training_variables.a_current
         gamma = training_variables.gamma
         reward = training_variables.reward
         non_terminal = training_variables.non_terminal
         s_next = training_variables.s_next
 
-        batch_size = training_variables.batch_size
+        batch_size = s_next.shape[0]
 
         tau_j = self._target_function.sample_tau(shape=(batch_size, self._config.N_prime))
-        prev_rnn_states = self._prev_next_quantile_rnn_states
-        train_rnn_states = training_variables.rnn_states
-        with rnn_support(self._target_function, prev_rnn_states, train_rnn_states, training_variables, self._config):
-            target_return_samples = self._target_function.all_quantile_values(s_next, tau_j)
+        target_return_samples = self._target_function.all_quantile_values(s_next, tau_j)
         assert target_return_samples.shape[0:-1] == (batch_size, self._config.N_prime)
 
         all_next_q = NF.transpose(target_return_samples, axes=(0, 2, 1))
         all_next_q = NF.mean(all_next_q, axis=2)
         max_next_q = NF.max(all_next_q, axis=1, keepdims=True)
         pi = _pi(all_next_q, max_next_q, tau=self._config.tau)
         pi = RF.expand_dims(pi, axis=1)
         all_tau_log_pi = _all_tau_log_pi(all_next_q, max_next_q, self._config.tau)
         all_tau_log_pi = RF.expand_dims(all_tau_log_pi, axis=1)
         assert pi.shape[1] == 1
         assert pi.shape == all_tau_log_pi.shape
         soft_q_target = NF.sum(pi * (target_return_samples - all_tau_log_pi), axis=(pi.ndim - 1))
 
-        prev_rnn_states = self._prev_current_quantile_rnn_states
-        train_rnn_states = training_variables.rnn_states
-        with rnn_support(self._target_function, prev_rnn_states, train_rnn_states, training_variables, self._config):
-            current_return_samples = self._target_function.all_quantile_values(s_current, tau_j)
+        current_return_samples = self._target_function.all_quantile_values(s_current, tau_j)
         all_current_q = NF.transpose(current_return_samples, axes=(0, 2, 1))
         all_current_q = NF.mean(all_current_q, axis=2)
         max_current_q = NF.max(all_current_q, axis=1, keepdims=True)
         one_hot = NF.one_hot(NF.reshape(a_current, (-1, 1), inplace=False), (all_current_q.shape[1],))
         current_q = NF.sum(all_current_q * one_hot, axis=1, keepdims=True)  # get q value of a
 
         tau_log_pi = _tau_log_pi(current_q, all_current_q, max_current_q, self._config.tau)
         clipped_tau_log_pi = NF.clip_by_value(tau_log_pi, self._config.clip_min, self._config.clip_max)
 
         return reward + self._config.alpha * clipped_tau_log_pi + gamma * non_terminal * soft_q_target
-
-    def _setup_training_variables(self, batch_size: int) -> TrainingVariables:
-        training_variables = super()._setup_training_variables(batch_size)
-
-        rnn_states = {}
-        if self._target_function.is_recurrent():
-            shapes = self._target_function.internal_state_shapes()
-            rnn_state_variables = create_variables(batch_size, shapes)
-            rnn_states[self._target_function.scope_name] = rnn_state_variables
-
-        training_variables.rnn_states.update(rnn_states)
-        return training_variables
```

## nnabla_rl/model_trainers/q_value/qrdqn_q_trainer.py

```diff
@@ -13,63 +13,42 @@
 # limitations under the License.
 
 from dataclasses import dataclass
 from typing import Dict, Sequence, Union
 
 import nnabla as nn
 from nnabla_rl.environments.environment_info import EnvironmentInfo
-from nnabla_rl.model_trainers.model_trainer import TrainingVariables, rnn_support
+from nnabla_rl.model_trainers.model_trainer import TrainingVariables
 from nnabla_rl.model_trainers.q_value.quantile_distribution_function_trainer import (
     QuantileDistributionFunctionTrainer, QuantileDistributionFunctionTrainerConfig)
 from nnabla_rl.models import QuantileDistributionFunction
-from nnabla_rl.utils.misc import create_variables
 
 
 @dataclass
 class QRDQNQTrainerConfig(QuantileDistributionFunctionTrainerConfig):
     pass
 
 
 class QRDQNQTrainer(QuantileDistributionFunctionTrainer):
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _target_function: QuantileDistributionFunction
-    _prev_target_rnn_states: Dict[str, Dict[str, nn.Variable]]
 
     def __init__(self,
                  train_functions: Union[QuantileDistributionFunction, Sequence[QuantileDistributionFunction]],
                  solvers: Dict[str, nn.solver.Solver],
                  target_function: QuantileDistributionFunction,
                  env_info: EnvironmentInfo,
                  config: QRDQNQTrainerConfig = QRDQNQTrainerConfig()):
         self._target_function = target_function
-        self._prev_target_rnn_states = {}
         super(QRDQNQTrainer, self).__init__(train_functions, solvers, env_info, config)
 
-    def support_rnn(self) -> bool:
-        return True
-
     def _compute_target(self, training_variables: TrainingVariables, **kwargs) -> nn.Variable:
         gamma = training_variables.gamma
         reward = training_variables.reward
         non_terminal = training_variables.non_terminal
         s_next = training_variables.s_next
 
-        prev_rnn_states = self._prev_target_rnn_states
-        train_rnn_states = training_variables.rnn_states
-        with rnn_support(self._target_function, prev_rnn_states, train_rnn_states, training_variables, self._config):
-            theta_j = self._target_function.max_q_quantiles(s_next)
+        theta_j = self._target_function.max_q_quantiles(s_next)
         Ttheta_j = reward + non_terminal * gamma * theta_j
         return Ttheta_j
-
-    def _setup_training_variables(self, batch_size: int) -> TrainingVariables:
-        training_variables = super()._setup_training_variables(batch_size)
-
-        rnn_states = {}
-        if self._target_function.is_recurrent():
-            shapes = self._target_function.internal_state_shapes()
-            rnn_state_variables = create_variables(batch_size, shapes)
-            rnn_states[self._target_function.scope_name] = rnn_state_variables
-
-        training_variables.rnn_states.update(rnn_states)
-        return training_variables
```

## nnabla_rl/model_trainers/q_value/quantile_distribution_function_trainer.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021,2022 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -18,117 +18,84 @@
 import numpy as np
 
 import nnabla as nn
 import nnabla.functions as NF
 import nnabla_rl.functions as RF
 from nnabla_rl.environments.environment_info import EnvironmentInfo
 from nnabla_rl.logger import logger
-from nnabla_rl.model_trainers.model_trainer import LossIntegration, TrainingBatch, TrainingVariables, rnn_support
-from nnabla_rl.model_trainers.q_value.multi_step_trainer import MultiStepTrainer, MultiStepTrainerConfig
+from nnabla_rl.model_trainers.model_trainer import ModelTrainer, TrainerConfig, TrainingBatch, TrainingVariables
 from nnabla_rl.models import Model, QuantileDistributionFunction
-from nnabla_rl.utils.data import set_data_to_variable
-from nnabla_rl.utils.misc import create_variable, create_variables
 
 
 @dataclass
-class QuantileDistributionFunctionTrainerConfig(MultiStepTrainerConfig):
+class QuantileDistributionFunctionTrainerConfig(TrainerConfig):
     num_quantiles: int = 200
     kappa: float = 1.0
 
 
-class QuantileDistributionFunctionTrainer(MultiStepTrainer):
+class QuantileDistributionFunctionTrainer(ModelTrainer):
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _config: QuantileDistributionFunctionTrainerConfig
     _tau_hat_var: nn.Variable
     _quantile_huber_loss: nn.Variable
-    _prev_rnn_states: Dict[str, Dict[str, nn.Variable]]
 
     def __init__(self,
                  models: Union[QuantileDistributionFunction, Sequence[QuantileDistributionFunction]],
                  solvers: Dict[str, nn.solver.Solver],
                  env_info: EnvironmentInfo,
                  config: QuantileDistributionFunctionTrainerConfig = QuantileDistributionFunctionTrainerConfig()):
         if config.kappa == 0.0:
             logger.info("kappa is set to 0.0. Quantile regression loss will be used for training")
         else:
             logger.info("kappa is non 0.0. Quantile huber loss will be used for training")
 
         tau_hat = self._precompute_tau_hat(config.num_quantiles)
         self._tau_hat_var = nn.Variable.from_numpy_array(tau_hat)
-        self._prev_rnn_states = {}
         super(QuantileDistributionFunctionTrainer, self).__init__(models, solvers, env_info, config)
 
     def _update_model(self,
                       models: Sequence[Model],
                       solvers: Dict[str, nn.solver.Solver],
                       batch: TrainingBatch,
                       training_variables: TrainingVariables,
-                      **kwargs) -> Dict[str, np.ndarray]:
-        for t, b in zip(training_variables, batch):
-            set_data_to_variable(t.s_current, b.s_current)
-            set_data_to_variable(t.a_current, b.a_current)
-            set_data_to_variable(t.reward, b.reward)
-            set_data_to_variable(t.gamma, b.gamma)
-            set_data_to_variable(t.non_terminal, b.non_terminal)
-            set_data_to_variable(t.s_next, b.s_next)
-
-            for model in models:
-                if not model.is_recurrent():
-                    continue
-                # Check batch keys. Because it can be empty.
-                # If batch does not provide rnn states, train with zero initial state.
-                if model.scope_name not in batch.rnn_states.keys():
-                    continue
-                b_rnn_states = b.rnn_states[model.scope_name]
-                t_rnn_states = t.rnn_states[model.scope_name]
-
-                for state_name in t_rnn_states.keys():
-                    set_data_to_variable(t_rnn_states[state_name], b_rnn_states[state_name])
+                      **kwargs) -> Dict[str, np.array]:
+        training_variables.s_current.d = batch.s_current
+        training_variables.a_current.d = batch.a_current
+        training_variables.reward.d = batch.reward
+        training_variables.gamma.d = batch.gamma
+        training_variables.non_terminal.d = batch.non_terminal
+        training_variables.s_next.d = batch.s_next
 
         for solver in solvers.values():
             solver.zero_grad()
         self._quantile_huber_loss.forward()
         self._quantile_huber_loss.backward()
         for solver in solvers.values():
             solver.update()
 
         trainer_state = {}
-        trainer_state['q_loss'] = self._quantile_huber_loss.d.copy()
+        trainer_state['q_loss'] = float(self._quantile_huber_loss.d.copy())
         return trainer_state
 
     def _build_training_graph(self,
                               models: Sequence[Model],
                               training_variables: TrainingVariables):
-        self._quantile_huber_loss = 0
-        ignore_intermediate_loss = self._config.loss_integration is LossIntegration.LAST_TIMESTEP_ONLY
-        for step_index, variables in enumerate(training_variables):
-            is_burn_in_steps = step_index < self._config.burn_in_steps
-            is_intermediate_steps = step_index < self._config.burn_in_steps + self._config.unroll_steps - 1
-            ignore_loss = is_burn_in_steps or (is_intermediate_steps and ignore_intermediate_loss)
-            self._build_one_step_graph(models, variables, ignore_loss=ignore_loss)
-
-    def _build_one_step_graph(self,
-                              models: Sequence[Model],
-                              training_variables: TrainingVariables,
-                              ignore_loss: bool):
         models = cast(Sequence[QuantileDistributionFunction], models)
 
         # Ttheta_j is the target quantile distribution
         Ttheta_j = self._compute_target(training_variables)
         Ttheta_j = RF.expand_dims(Ttheta_j, axis=1)
         Ttheta_j.need_grad = False
 
-        prev_rnn_states = self._prev_rnn_states
-        train_rnn_states = training_variables.rnn_states
+        self._quantile_huber_loss = 0
         for model in models:
-            with rnn_support(model, prev_rnn_states, train_rnn_states, training_variables, self._config):
-                loss, _ = self._compute_loss(model, Ttheta_j, training_variables)
-            self._quantile_huber_loss += 0.0 if ignore_loss else loss
+            loss, _ = self._compute_loss(model, Ttheta_j, training_variables)
+            self._quantile_huber_loss += loss
 
     def _compute_target(self, training_variables: TrainingVariables) -> nn.Variable:
         raise NotImplementedError
 
     def _compute_loss(self,
                       model: QuantileDistributionFunction,
                       target: nn.Variable,
@@ -151,40 +118,29 @@
 
         quantile_huber_loss = NF.mean(quantile_huber_loss, axis=2)
         quantile_huber_loss = NF.sum(quantile_huber_loss, axis=1)
         return NF.mean(quantile_huber_loss), {}
 
     def _setup_training_variables(self, batch_size) -> TrainingVariables:
         # Training input variables
-        s_current_var = create_variable(batch_size, self._env_info.state_shape)
-        a_current_var = create_variable(batch_size, self._env_info.action_shape)
-        s_next_var = create_variable(batch_size, self._env_info.state_shape)
-        reward_var = create_variable(batch_size, 1)
-        gamma_var = create_variable(batch_size, 1)
-        non_terminal_var = create_variable(batch_size, 1)
-
-        rnn_states = {}
-        for model in self._models:
-            if model.is_recurrent():
-                rnn_state_variables = create_variables(batch_size, model.internal_state_shapes())
-                rnn_states[model.scope_name] = rnn_state_variables
+        s_current_var = nn.Variable((batch_size, *self._env_info.state_shape))
+        a_current_var = nn.Variable((batch_size, 1))
+        s_next_var = nn.Variable((batch_size, *self._env_info.state_shape))
+        reward_var = nn.Variable((batch_size, 1))
+        gamma_var = nn.Variable((1, 1))
+        non_terminal_var = nn.Variable((batch_size, 1))
+        s_next_var = nn.Variable((batch_size, *self._env_info.state_shape))
 
         training_variables = TrainingVariables(batch_size=batch_size,
                                                s_current=s_current_var,
                                                a_current=a_current_var,
                                                reward=reward_var,
                                                gamma=gamma_var,
                                                non_terminal=non_terminal_var,
-                                               s_next=s_next_var,
-                                               rnn_states=rnn_states)
-
+                                               s_next=s_next_var)
         return training_variables
 
     @staticmethod
     def _precompute_tau_hat(num_quantiles):
         tau_hat = [(tau_prev + tau_i) / num_quantiles / 2.0
                    for tau_prev, tau_i in zip(range(0, num_quantiles), range(1, num_quantiles+1))]
         return np.array(tau_hat, dtype=np.float32)
-
-    @property
-    def loss_variables(self) -> Dict[str, nn.Variable]:
-        return {"quantile_huber_loss": self._quantile_huber_loss}
```

## nnabla_rl/model_trainers/q_value/soft_q_trainer.py

```diff
@@ -14,20 +14,19 @@
 
 from dataclasses import dataclass
 from typing import Dict, Sequence, Union
 
 import nnabla as nn
 import nnabla_rl.functions as RF
 from nnabla_rl.environments.environment_info import EnvironmentInfo
-from nnabla_rl.model_trainers.model_trainer import TrainingVariables, rnn_support
+from nnabla_rl.model_trainers.model_trainer import TrainingVariables
 from nnabla_rl.model_trainers.q_value.squared_td_q_function_trainer import (SquaredTDQFunctionTrainer,
                                                                             SquaredTDQFunctionTrainerConfig)
 from nnabla_rl.models import QFunction, StochasticPolicy
 from nnabla_rl.utils.data import convert_to_list_if_not_list
-from nnabla_rl.utils.misc import create_variables
 
 
 @dataclass
 class SoftQTrainerConfig(SquaredTDQFunctionTrainerConfig):
     pass
 
 
@@ -35,67 +34,36 @@
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _target_functions: Sequence[QFunction]
     _target_policy: StochasticPolicy
     _config: SoftQTrainerConfig
     _temperature: nn.Variable
-    _prev_target_rnn_states: Dict[str, Dict[str, nn.Variable]]
-    _prev_q_rnn_states: Dict[str, Dict[str, nn.Variable]]
 
     def __init__(self,
                  train_functions: Union[QFunction, Sequence[QFunction]],
                  solvers: Dict[str, nn.solver.Solver],
                  target_functions: Union[QFunction, Sequence[QFunction]],
                  target_policy: StochasticPolicy,
                  temperature: nn.Variable,
                  env_info: EnvironmentInfo,
                  config: SoftQTrainerConfig = SoftQTrainerConfig()):
         self._target_functions = convert_to_list_if_not_list(target_functions)
-        self._assert_no_duplicate_model(self._target_functions)
         self._target_policy = target_policy
         self._temperature = temperature
-        self._prev_target_rnn_states = {}
-        self._prev_q_rnn_states = {}
         super(SoftQTrainer, self).__init__(train_functions, solvers, env_info, config)
 
-    def support_rnn(self) -> bool:
-        return True
-
     def _compute_target(self, training_variables: TrainingVariables, **kwargs) -> nn.Variable:
         gamma = training_variables.gamma
         reward = training_variables.reward
         non_terminal = training_variables.non_terminal
         s_next = training_variables.s_next
 
-        train_rnn_states = training_variables.rnn_states
-        prev_rnn_states = self._prev_target_rnn_states
-        with rnn_support(self._target_policy, prev_rnn_states, train_rnn_states, training_variables, self._config):
-            policy_distribution = self._target_policy.pi(s_next)
+        policy_distribution = self._target_policy.pi(s_next)
         a_next, log_pi = policy_distribution.sample_and_compute_log_prob()
 
         q_values = []
-        prev_rnn_states = self._prev_q_rnn_states
         for target_q_function in self._target_functions:
-            with rnn_support(target_q_function, prev_rnn_states, train_rnn_states, training_variables, self._config):
-                q_value = target_q_function.q(s_next, a_next)
-                q_values.append(q_value)
-
+            q_value = target_q_function.q(s_next, a_next)
+            q_values.append(q_value)
         target_q = RF.minimum_n(q_values)
         return reward + gamma * non_terminal * (target_q - self._temperature * log_pi)
-
-    def _setup_training_variables(self, batch_size: int) -> TrainingVariables:
-        training_variables = super()._setup_training_variables(batch_size)
-
-        rnn_states = {}
-        for target_function in self._target_functions:
-            if target_function.is_recurrent():
-                shapes = target_function.internal_state_shapes()
-                rnn_state_variables = create_variables(batch_size, shapes)
-                rnn_states[target_function.scope_name] = rnn_state_variables
-        if self._target_policy.is_recurrent():
-            shapes = self._target_policy.internal_state_shapes()
-            rnn_state_variables = create_variables(batch_size, shapes)
-            rnn_states[self._target_policy.scope_name] = rnn_state_variables
-
-        training_variables.rnn_states.update(rnn_states)
-        return training_variables
```

## nnabla_rl/model_trainers/q_value/squared_td_q_function_trainer.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -16,122 +16,84 @@
 from typing import Dict, Optional, Sequence, Tuple, Union, cast
 
 import numpy as np
 
 import nnabla as nn
 import nnabla.functions as NF
 from nnabla_rl.environments.environment_info import EnvironmentInfo
-from nnabla_rl.model_trainers.model_trainer import LossIntegration, TrainingBatch, TrainingVariables, rnn_support
-from nnabla_rl.model_trainers.q_value.multi_step_trainer import MultiStepTrainer, MultiStepTrainerConfig
+from nnabla_rl.model_trainers.model_trainer import ModelTrainer, TrainerConfig, TrainingBatch, TrainingVariables
 from nnabla_rl.models import Model, QFunction
-from nnabla_rl.utils.data import set_data_to_variable
-from nnabla_rl.utils.misc import create_variable, create_variables
 
 
 @dataclass
-class SquaredTDQFunctionTrainerConfig(MultiStepTrainerConfig):
-    loss_type: str = 'squared'
+class SquaredTDQFunctionTrainerConfig(TrainerConfig):
     reduction_method: str = 'mean'
     grad_clip: Optional[tuple] = None
     q_loss_scalar: float = 1.0
-    reward_dimension: int = 1
-    huber_delta: Optional[float] = None
 
     def __post_init__(self):
-        self._assert_one_of(self.loss_type, ['squared', 'huber'], 'loss_type')
         self._assert_one_of(self.reduction_method, ['sum', 'mean'], 'reduction_method')
         if self.grad_clip is not None:
             self._assert_ascending_order(self.grad_clip, 'grad_clip')
             self._assert_length(self.grad_clip, 2, 'grad_clip')
 
 
-class SquaredTDQFunctionTrainer(MultiStepTrainer):
+class SquaredTDQFunctionTrainer(ModelTrainer):
     _config: SquaredTDQFunctionTrainerConfig
     # Training loss/output
     _td_error: nn.Variable
     _q_loss: nn.Variable
-    _prev_rnn_states: Dict[str, Dict[str, nn.Variable]]
 
     def __init__(self,
                  models: Union[QFunction, Sequence[QFunction]],
                  solvers: Dict[str, nn.solver.Solver],
                  env_info: EnvironmentInfo,
                  config: SquaredTDQFunctionTrainerConfig = SquaredTDQFunctionTrainerConfig()):
-        self._prev_rnn_states = {}
         super(SquaredTDQFunctionTrainer, self).__init__(models, solvers, env_info, config)
 
     def _update_model(self,
                       models: Sequence[Model],
                       solvers: Dict[str, nn.solver.Solver],
                       batch: TrainingBatch,
                       training_variables: TrainingVariables,
-                      **kwargs) -> Dict[str, np.ndarray]:
-        for t, b in zip(training_variables, batch):
-            set_data_to_variable(t.s_current, b.s_current)
-            set_data_to_variable(t.a_current, b.a_current)
-            set_data_to_variable(t.reward, b.reward)
-            set_data_to_variable(t.gamma, b.gamma)
-            set_data_to_variable(t.non_terminal, b.non_terminal)
-            set_data_to_variable(t.s_next, b.s_next)
-            set_data_to_variable(t.weight, b.weight)
-
-            for model in models:
-                if not model.is_recurrent():
-                    continue
-                # Check batch keys. Because it can be empty.
-                # If batch does not provide rnn states, train with zero initial state.
-                if model.scope_name not in batch.rnn_states.keys():
-                    continue
-                b_rnn_states = b.rnn_states[model.scope_name]
-                t_rnn_states = t.rnn_states[model.scope_name]
-
-                for state_name in t_rnn_states.keys():
-                    set_data_to_variable(t_rnn_states[state_name], b_rnn_states[state_name])
+                      **kwargs) -> Dict[str, np.array]:
+        training_variables.s_current.d = batch.s_current
+        training_variables.a_current.d = batch.a_current
+        training_variables.reward.d = batch.reward
+        training_variables.gamma.d = batch.gamma
+        training_variables.non_terminal.d = batch.non_terminal
+        training_variables.s_next.d = batch.s_next
+        training_variables.weight.d = batch.weight
 
         # update model
         for q_solver in solvers.values():
             q_solver.zero_grad()
         self._q_loss.forward(clear_no_need_grad=True)
         self._q_loss.backward(clear_buffer=True)
         for q_solver in solvers.values():
             q_solver.update()
 
         trainer_state = {}
-        trainer_state['q_loss'] = self._q_loss.d.copy()
+        trainer_state['q_loss'] = float(self._q_loss.d.copy())
         trainer_state['td_errors'] = self._td_error.d.copy()
         return trainer_state
 
     def _build_training_graph(self,
                               models: Sequence[Model],
                               training_variables: TrainingVariables):
-        self._q_loss = 0
-        ignore_intermediate_loss = self._config.loss_integration is LossIntegration.LAST_TIMESTEP_ONLY
-        for step_index, variables in enumerate(training_variables):
-            is_burn_in_steps = step_index < self._config.burn_in_steps
-            is_intermediate_steps = step_index < self._config.burn_in_steps + self._config.unroll_steps - 1
-            ignore_loss = is_burn_in_steps or (is_intermediate_steps and ignore_intermediate_loss)
-            self._build_one_step_graph(models, variables, ignore_loss=ignore_loss)
-
-    def _build_one_step_graph(self,
-                              models: Sequence[Model],
-                              training_variables: TrainingVariables,
-                              ignore_loss: bool):
         models = cast(Sequence[QFunction], models)
 
         # NOTE: Target q value depends on underlying implementation
         target_q = self._compute_target(training_variables)
         target_q.need_grad = False
 
-        prev_rnn_states = self._prev_rnn_states
-        train_rnn_states = training_variables.rnn_states
+        self._q_loss = 0
         for model in models:
-            with rnn_support(model, prev_rnn_states, train_rnn_states, training_variables, self._config):
-                q_loss, extra = self._compute_loss(model, target_q, training_variables)
-            self._q_loss += 0.0 if ignore_loss else q_loss
-
+            q_loss, extra = self._compute_loss(model, target_q, training_variables)
+            self._q_loss += q_loss
         # FIXME: using the last q function's td error for prioritized replay. Is this fine?
         self._td_error = extra['td_error']
         self._td_error.persistent = True
 
     def _compute_target(self, training_variables: TrainingVariables) -> nn.Variable:
         raise NotImplementedError
 
@@ -148,54 +110,41 @@
         if self._config.grad_clip is not None:
             # NOTE: Gradient clipping is used in DQN and its variants.
             # This operation is same as using huber_loss if the grad_clip value is set to (-1, 1)
             clip_min, clip_max = self._config.grad_clip
             minimum = nn.Variable.from_numpy_array(np.full(td_error.shape, clip_min))
             maximum = nn.Variable.from_numpy_array(np.full(td_error.shape, clip_max))
             td_error = NF.clip_grad_by_value(td_error, minimum, maximum)
-        if self._config.loss_type == 'squared':
-            squared_td_error = training_variables.weight * NF.pow_scalar(td_error, 2.0)
-        elif self._config.loss_type == 'huber':
-            zero = nn.Variable.from_numpy_array(np.zeros(shape=td_error.shape))
-            squared_td_error = training_variables.weight * NF.huber_loss(td_error, zero, delta=self._config.huber_delta)
-        else:
-            raise RuntimeError
+        squared_td_error = training_variables.weight * NF.pow_scalar(td_error, 2.0)
         if self._config.reduction_method == 'mean':
             q_loss += self._config.q_loss_scalar * NF.mean(squared_td_error)
         elif self._config.reduction_method == 'sum':
             q_loss += self._config.q_loss_scalar * NF.sum(squared_td_error)
         else:
             raise RuntimeError
 
         extra = {'td_error': td_error}
         return q_loss, extra
 
     def _setup_training_variables(self, batch_size) -> TrainingVariables:
         # Training input variables
-        s_current_var = create_variable(batch_size, self._env_info.state_shape)
-        a_current_var = create_variable(batch_size, self._env_info.action_shape)
-        s_next_var = create_variable(batch_size, self._env_info.state_shape)
-        reward_var = create_variable(batch_size, self._config.reward_dimension)
-        gamma_var = create_variable(batch_size, 1)
-        non_terminal_var = create_variable(batch_size, 1)
-        weight_var = create_variable(batch_size, 1)
-
-        rnn_states = {}
-        for model in self._models:
-            if model.is_recurrent():
-                rnn_state_variables = create_variables(batch_size, model.internal_state_shapes())
-                rnn_states[model.scope_name] = rnn_state_variables
+        s_current_var = nn.Variable((batch_size, *self._env_info.state_shape))
+        if self._env_info.is_discrete_action_env():
+            a_current_var = nn.Variable((batch_size, 1))
+        else:
+            a_current_var = nn.Variable((batch_size, self._env_info.action_dim))
+        s_next_var = nn.Variable((batch_size, *self._env_info.state_shape))
+        reward_var = nn.Variable((batch_size, 1))
+        gamma_var = nn.Variable((1, 1))
+        non_terminal_var = nn.Variable((batch_size, 1))
+        s_next_var = nn.Variable((batch_size, *self._env_info.state_shape))
+        weight_var = nn.Variable((batch_size, 1))
 
         training_variables = TrainingVariables(batch_size=batch_size,
                                                s_current=s_current_var,
                                                a_current=a_current_var,
                                                reward=reward_var,
                                                gamma=gamma_var,
                                                non_terminal=non_terminal_var,
                                                s_next=s_next_var,
-                                               weight=weight_var,
-                                               rnn_states=rnn_states)
+                                               weight=weight_var)
         return training_variables
-
-    @property
-    def loss_variables(self) -> Dict[str, nn.Variable]:
-        return {"q_loss": self._q_loss}
```

## nnabla_rl/model_trainers/q_value/state_action_quantile_function_trainer.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021,2022 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -17,113 +17,79 @@
 
 import numpy as np
 
 import nnabla as nn
 import nnabla.functions as NF
 import nnabla_rl.functions as RF
 from nnabla_rl.environments.environment_info import EnvironmentInfo
-from nnabla_rl.model_trainers.model_trainer import LossIntegration, TrainingBatch, TrainingVariables, rnn_support
-from nnabla_rl.model_trainers.q_value.multi_step_trainer import MultiStepTrainer, MultiStepTrainerConfig
+from nnabla_rl.model_trainers.model_trainer import ModelTrainer, TrainerConfig, TrainingBatch, TrainingVariables
 from nnabla_rl.models import Model, StateActionQuantileFunction
-from nnabla_rl.utils.data import set_data_to_variable
-from nnabla_rl.utils.misc import create_variable, create_variables
 
 
 @dataclass
-class StateActionQuantileFunctionTrainerConfig(MultiStepTrainerConfig):
+class StateActionQuantileFunctionTrainerConfig(TrainerConfig):
     N: int = 64
     N_prime: int = 64
     K: int = 32
     kappa: float = 1.0
 
 
-class StateActionQuantileFunctionTrainer(MultiStepTrainer):
+class StateActionQuantileFunctionTrainer(ModelTrainer):
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _config: StateActionQuantileFunctionTrainerConfig
     _quantile_huber_loss: nn.Variable
-    _prev_rnn_states: Dict[str, Dict[str, nn.Variable]]
 
     def __init__(self,
                  models: Union[StateActionQuantileFunction, Sequence[StateActionQuantileFunction]],
                  solvers: Dict[str, nn.solver.Solver],
                  env_info: EnvironmentInfo,
                  config: StateActionQuantileFunctionTrainerConfig = StateActionQuantileFunctionTrainerConfig()):
-        self._prev_rnn_states = {}
         super(StateActionQuantileFunctionTrainer, self).__init__(models, solvers, env_info, config)
 
     def _update_model(self,
                       models: Sequence[Model],
                       solvers: Dict[str, nn.solver.Solver],
                       batch: TrainingBatch,
                       training_variables: TrainingVariables,
-                      **kwargs) -> Dict[str, np.ndarray]:
-        for t, b in zip(training_variables, batch):
-            set_data_to_variable(t.s_current, b.s_current)
-            set_data_to_variable(t.a_current, b.a_current)
-            set_data_to_variable(t.reward, b.reward)
-            set_data_to_variable(t.gamma, b.gamma)
-            set_data_to_variable(t.non_terminal, b.non_terminal)
-            set_data_to_variable(t.s_next, b.s_next)
-
-            for model in models:
-                if not model.is_recurrent():
-                    continue
-                # Check batch keys. Because it can be empty.
-                # If batch does not provide rnn states, train with zero initial state.
-                if model.scope_name not in batch.rnn_states.keys():
-                    continue
-                b_rnn_states = b.rnn_states[model.scope_name]
-                t_rnn_states = t.rnn_states[model.scope_name]
-
-                for state_name in t_rnn_states.keys():
-                    set_data_to_variable(t_rnn_states[state_name], b_rnn_states[state_name])
+                      **kwargs) -> Dict[str, np.array]:
+        training_variables.s_current.d = batch.s_current
+        training_variables.a_current.d = batch.a_current
+        training_variables.reward.d = batch.reward
+        training_variables.gamma.d = batch.gamma
+        training_variables.non_terminal.d = batch.non_terminal
+        training_variables.s_next.d = batch.s_next
 
         for solver in solvers.values():
             solver.zero_grad()
         self._quantile_huber_loss.forward()
         self._quantile_huber_loss.backward()
         for solver in solvers.values():
             solver.update()
 
         trainer_state = {}
-        trainer_state['q_loss'] = self._quantile_huber_loss.d.copy()
+        trainer_state['q_loss'] = float(self._quantile_huber_loss.d.copy())
         return trainer_state
 
     def _build_training_graph(self,
                               models: Sequence[Model],
                               training_variables: TrainingVariables):
-        self._quantile_huber_loss = 0
-        ignore_intermediate_loss = self._config.loss_integration is LossIntegration.LAST_TIMESTEP_ONLY
-        for step_index, variables in enumerate(training_variables):
-            is_burn_in_steps = step_index < self._config.burn_in_steps
-            is_intermediate_steps = step_index < self._config.burn_in_steps + self._config.unroll_steps - 1
-            ignore_loss = is_burn_in_steps or (is_intermediate_steps and ignore_intermediate_loss)
-            self._build_one_step_graph(models, variables, ignore_loss=ignore_loss)
-
-    def _build_one_step_graph(self,
-                              models: Sequence[Model],
-                              training_variables: TrainingVariables,
-                              ignore_loss: bool):
         models = cast(Sequence[StateActionQuantileFunction], models)
 
         batch_size = training_variables.batch_size
 
         target = self._compute_target(training_variables)
         target = RF.expand_dims(target, axis=1)
         target.need_grad = False
         assert target.shape == (batch_size, 1, self._config.N_prime)
 
-        prev_rnn_states = self._prev_rnn_states
-        train_rnn_states = training_variables.rnn_states
+        self._quantile_huber_loss = 0
         for model in models:
-            with rnn_support(model, prev_rnn_states, train_rnn_states, training_variables, self._config):
-                loss = self._compute_loss(model, target, training_variables)
-            self._quantile_huber_loss += 0.0 if ignore_loss else loss
+            self._quantile_huber_loss += self._compute_loss(model, target, training_variables)
 
     def _compute_target(self, training_variables: TrainingVariables):
         raise NotImplementedError
 
     def _compute_loss(self,
                       model: StateActionQuantileFunction,
                       target: nn.Variable,
@@ -142,35 +108,23 @@
         quantile_huber_loss = RF.quantile_huber_loss(target, Z_tau_i, self._config.kappa, tau_i)
         assert quantile_huber_loss.shape == (batch_size, self._config.N, self._config.N_prime)
         quantile_huber_loss = NF.mean(quantile_huber_loss, axis=2)
         quantile_huber_loss = NF.sum(quantile_huber_loss, axis=1)
         return NF.mean(quantile_huber_loss)
 
     def _setup_training_variables(self, batch_size) -> TrainingVariables:
-        # Training input variables
-        s_current_var = create_variable(batch_size, self._env_info.state_shape)
-        a_current_var = create_variable(batch_size, self._env_info.action_shape)
-        s_next_var = create_variable(batch_size, self._env_info.state_shape)
-        reward_var = create_variable(batch_size, 1)
-        gamma_var = create_variable(batch_size, 1)
-        non_terminal_var = create_variable(batch_size, 1)
-
-        rnn_states = {}
-        for model in self._models:
-            if model.is_recurrent():
-                rnn_state_variables = create_variables(batch_size, model.internal_state_shapes())
-                rnn_states[model.scope_name] = rnn_state_variables
+        s_current_var = nn.Variable((batch_size, *self._env_info.state_shape))
+        a_current_var = nn.Variable((batch_size, 1))
+        s_next_var = nn.Variable((batch_size, *self._env_info.state_shape))
+        reward_var = nn.Variable((batch_size, 1))
+        gamma_var = nn.Variable((1, 1))
+        non_terminal_var = nn.Variable((batch_size, 1))
+        s_next_var = nn.Variable((batch_size, *self._env_info.state_shape))
 
         training_variables = TrainingVariables(batch_size=batch_size,
                                                s_current=s_current_var,
                                                a_current=a_current_var,
                                                reward=reward_var,
                                                gamma=gamma_var,
                                                non_terminal=non_terminal_var,
-                                               s_next=s_next_var,
-                                               rnn_states=rnn_states)
-
+                                               s_next=s_next_var)
         return training_variables
-
-    @property
-    def loss_variables(self) -> Dict[str, nn.Variable]:
-        return {"quantile_huber_loss": self._quantile_huber_loss}
```

## nnabla_rl/model_trainers/q_value/td3_q_trainer.py

```diff
@@ -15,20 +15,19 @@
 from dataclasses import dataclass
 from typing import Dict, Sequence, Union
 
 import nnabla as nn
 import nnabla.functions as NF
 import nnabla_rl.functions as RF
 from nnabla_rl.environments.environment_info import EnvironmentInfo
-from nnabla_rl.model_trainers.model_trainer import TrainingVariables, rnn_support
+from nnabla_rl.model_trainers.model_trainer import TrainingVariables
 from nnabla_rl.model_trainers.q_value.squared_td_q_function_trainer import (SquaredTDQFunctionTrainer,
                                                                             SquaredTDQFunctionTrainerConfig)
 from nnabla_rl.models import DeterministicPolicy, QFunction
 from nnabla_rl.utils.data import convert_to_list_if_not_list
-from nnabla_rl.utils.misc import create_variables
 
 
 @dataclass
 class TD3QTrainerConfig(SquaredTDQFunctionTrainerConfig):
     train_action_noise_sigma: float = 0.2
     train_action_noise_abs: float = 0.5
 
@@ -36,74 +35,43 @@
 class TD3QTrainer(SquaredTDQFunctionTrainer):
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _target_functions: Sequence[QFunction]
     _target_policy: DeterministicPolicy
     _config: TD3QTrainerConfig
-    _prev_target_rnn_states: Dict[str, Dict[str, nn.Variable]]
-    _prev_q_rnn_states: Dict[str, Dict[str, nn.Variable]]
 
     def __init__(self,
                  train_functions: Union[QFunction, Sequence[QFunction]],
                  solvers: Dict[str, nn.solver.Solver],
                  target_functions: Union[QFunction, Sequence[QFunction]],
                  target_policy: DeterministicPolicy,
                  env_info: EnvironmentInfo,
                  config: TD3QTrainerConfig = TD3QTrainerConfig()):
         self._target_policy = target_policy
         self._target_functions = convert_to_list_if_not_list(target_functions)
-        self._assert_no_duplicate_model(self._target_functions)
-        self._prev_target_rnn_states = {}
-        self._prev_q_rnn_states = {}
         super(TD3QTrainer, self).__init__(train_functions, solvers, env_info, config)
 
-    def support_rnn(self) -> bool:
-        return True
-
     def _compute_target(self, training_variables: TrainingVariables, **kwargs) -> nn.Variable:
         gamma = training_variables.gamma
         reward = training_variables.reward
         non_terminal = training_variables.non_terminal
         s_next = training_variables.s_next
 
-        train_rnn_states = training_variables.rnn_states
-        prev_rnn_states = self._prev_target_rnn_states
-        with rnn_support(self._target_policy, prev_rnn_states, train_rnn_states, training_variables, self._config):
-            a_next = self._compute_noisy_action(s_next)
-            a_next.need_grad = False
-
         q_values = []
-        prev_rnn_states = self._prev_q_rnn_states
+        a_next = self._compute_noisy_action(s_next)
+        a_next.need_grad = False
         for target_q_function in self._target_functions:
-            with rnn_support(target_q_function, prev_rnn_states, train_rnn_states, training_variables, self._config):
-                q_value = target_q_function.q(s_next, a_next)
-                q_values.append(q_value)
+            q_value = target_q_function.q(s_next, a_next)
+            q_values.append(q_value)
         # Use the minimum among computed q_values by default
         target_q = RF.minimum_n(q_values)
         return reward + gamma * non_terminal * target_q
 
     def _compute_noisy_action(self, state):
         a_next_var = self._target_policy.pi(state)
         epsilon = NF.clip_by_value(NF.randn(sigma=self._config.train_action_noise_sigma,
                                             shape=a_next_var.shape),
                                    min=-self._config.train_action_noise_abs,
                                    max=self._config.train_action_noise_abs)
         a_tilde_var = a_next_var + epsilon
         return a_tilde_var
-
-    def _setup_training_variables(self, batch_size: int) -> TrainingVariables:
-        training_variables = super()._setup_training_variables(batch_size)
-
-        rnn_states = {}
-        for target_function in self._target_functions:
-            if target_function.is_recurrent():
-                shapes = target_function.internal_state_shapes()
-                rnn_state_variables = create_variables(batch_size, shapes)
-                rnn_states[target_function.scope_name] = rnn_state_variables
-        if self._target_policy.is_recurrent():
-            shapes = self._target_policy.internal_state_shapes()
-            rnn_state_variables = create_variables(batch_size, shapes)
-            rnn_states[self._target_policy.scope_name] = rnn_state_variables
-
-        training_variables.rnn_states.update(rnn_states)
-        return training_variables
```

## nnabla_rl/model_trainers/q_value/v_targeted_q_trainer.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -14,78 +14,45 @@
 
 from dataclasses import dataclass
 from typing import Dict, Sequence, Union
 
 import nnabla as nn
 import nnabla_rl.functions as RNF
 from nnabla_rl.environments.environment_info import EnvironmentInfo
-from nnabla_rl.model_trainers.model_trainer import TrainingVariables, rnn_support
+from nnabla_rl.model_trainers.model_trainer import TrainingVariables
 from nnabla_rl.model_trainers.q_value.squared_td_q_function_trainer import (SquaredTDQFunctionTrainer,
                                                                             SquaredTDQFunctionTrainerConfig)
 from nnabla_rl.models import QFunction, VFunction
 from nnabla_rl.utils.data import convert_to_list_if_not_list
-from nnabla_rl.utils.misc import create_variables
 
 
 @dataclass
 class VTargetedQTrainerConfig(SquaredTDQFunctionTrainerConfig):
-    """List of VTargetedQTrainer configuration.
-
-    Args:
-        pure_exploration (bool): If True, compute q-value target without adding reward. \
-            :math:`target=\\gamma\\times V(s_{t+1})`.\
-            Used in Disentangled MME.\
-    """
-    pure_exploration: bool = False
+    pass
 
 
 class VTargetedQTrainer(SquaredTDQFunctionTrainer):
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _target_functions: Sequence[VFunction]
-    _target_v_rnn_states: Dict[str, Dict[str, nn.Variable]]
-    _config: VTargetedQTrainerConfig
 
     def __init__(self,
                  train_functions: Union[QFunction, Sequence[QFunction]],
                  solvers: Dict[str, nn.solver.Solver],
                  target_functions: Union[VFunction, Sequence[VFunction]],
                  env_info: EnvironmentInfo,
                  config: VTargetedQTrainerConfig = VTargetedQTrainerConfig()):
         self._target_functions = convert_to_list_if_not_list(target_functions)
-        self._assert_no_duplicate_model(self._target_functions)
-        self._target_v_rnn_states = {}
         super(VTargetedQTrainer, self).__init__(train_functions, solvers, env_info, config)
 
-    def support_rnn(self) -> bool:
-        return True
-
     def _compute_target(self, training_variables: TrainingVariables, **kwargs) -> nn.Variable:
         gamma = training_variables.gamma
         reward = training_variables.reward
         non_terminal = training_variables.non_terminal
         s_next = training_variables.s_next
 
         target_vs = []
-        prev_rnn_states = self._target_v_rnn_states
-        train_rnn_states = training_variables.rnn_states
         for v_function in self._target_functions:
-            with rnn_support(v_function, prev_rnn_states, train_rnn_states, training_variables, self._config):
-                target_vs.append(v_function.v(s_next))
+            target_vs.append(v_function.v(s_next))
         target_v = RNF.minimum_n(target_vs)
-        if self._config.pure_exploration:
-            return gamma * non_terminal * target_v
-        else:
-            return reward + gamma * non_terminal * target_v
-
-    def _setup_training_variables(self, batch_size: int) -> TrainingVariables:
-        training_variables = super()._setup_training_variables(batch_size)
-
-        rnn_states = {}
-        for target_function in self._target_functions:
-            if target_function.is_recurrent():
-                shapes = target_function.internal_state_shapes()
-                rnn_state_variables = create_variables(batch_size, shapes)
-                rnn_states[target_function.scope_name] = rnn_state_variables
-        training_variables.rnn_states.update(rnn_states)
-        return training_variables
+        return reward + gamma * non_terminal * target_v
```

## nnabla_rl/model_trainers/q_value/value_distribution_function_trainer.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021,2022 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -16,79 +16,55 @@
 from typing import Dict, Sequence, Tuple, Union, cast
 
 import numpy as np
 
 import nnabla as nn
 import nnabla.functions as NF
 from nnabla_rl.environments.environment_info import EnvironmentInfo
-from nnabla_rl.model_trainers.model_trainer import LossIntegration, TrainingBatch, TrainingVariables, rnn_support
-from nnabla_rl.model_trainers.q_value.multi_step_trainer import MultiStepTrainer, MultiStepTrainerConfig
+from nnabla_rl.model_trainers.model_trainer import ModelTrainer, TrainerConfig, TrainingBatch, TrainingVariables
 from nnabla_rl.models import Model, ValueDistributionFunction
-from nnabla_rl.utils.data import set_data_to_variable
-from nnabla_rl.utils.misc import create_variable, create_variables
 
 
 @dataclass
-class ValueDistributionFunctionTrainerConfig(MultiStepTrainerConfig):
-    reduction_method: str = 'mean'
+class ValueDistributionFunctionTrainerConfig(TrainerConfig):
     v_min: float = -10.0
     v_max: float = 10.0
     num_atoms: int = 51
 
-    def __post_init__(self):
-        self._assert_one_of(self.reduction_method, ['sum', 'mean'], 'reduction_method')
-        return super().__post_init__()
 
-
-class ValueDistributionFunctionTrainer(MultiStepTrainer):
+class ValueDistributionFunctionTrainer(ModelTrainer):
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _config: ValueDistributionFunctionTrainerConfig
     _model: ValueDistributionFunction
     # Training loss/output
     _kl_loss: nn.Variable
     _cross_entropy_loss: nn.Variable
-    _prev_rnn_states: Dict[str, Dict[str, nn.Variable]]
 
     def __init__(self,
                  models: Union[ValueDistributionFunction, Sequence[ValueDistributionFunction]],
                  solvers: Dict[str, nn.solver.Solver],
                  env_info: EnvironmentInfo,
                  config: ValueDistributionFunctionTrainerConfig):
-        self._prev_rnn_states = {}
         super(ValueDistributionFunctionTrainer, self).__init__(models, solvers, env_info, config)
 
     def _update_model(self,
                       models: Sequence[Model],
                       solvers: Dict[str, nn.solver.Solver],
                       batch: TrainingBatch,
                       training_variables: TrainingVariables,
-                      **kwargs) -> Dict[str, np.ndarray]:
-        for t, b in zip(training_variables, batch):
-            set_data_to_variable(t.s_current, b.s_current)
-            set_data_to_variable(t.a_current, b.a_current)
-            set_data_to_variable(t.reward, b.reward)
-            set_data_to_variable(t.gamma, b.gamma)
-            set_data_to_variable(t.non_terminal, b.non_terminal)
-            set_data_to_variable(t.s_next, b.s_next)
-            set_data_to_variable(t.weight, b.weight)
-
-            for model in models:
-                if not model.is_recurrent():
-                    continue
-                # Check batch keys. Because it can be empty.
-                # If batch does not provide rnn states, train with zero initial state.
-                if model.scope_name not in batch.rnn_states.keys():
-                    continue
-                b_rnn_states = b.rnn_states[model.scope_name]
-                t_rnn_states = t.rnn_states[model.scope_name]
-
-                for state_name in t_rnn_states.keys():
-                    set_data_to_variable(t_rnn_states[state_name], b_rnn_states[state_name])
+                      **kwargs) -> Dict[str, np.array]:
+        training_variables.s_current.d = batch.s_current
+        training_variables.a_current.d = batch.a_current
+        training_variables.reward.d = batch.reward
+        training_variables.non_terminal.d = batch.non_terminal
+        training_variables.gamma.d = batch.gamma
+        training_variables.s_next.d = batch.s_next
+        training_variables.weight.d = batch.weight
 
         for solver in solvers.values():
             solver.zero_grad()
         self._cross_entropy_loss.forward()
         self._cross_entropy_loss.backward()
         for solver in solvers.values():
             solver.update()
@@ -99,39 +75,25 @@
         trainer_state['td_errors'] = self._kl_loss.d.copy()
         trainer_state['cross_entropy_loss'] = float(self._cross_entropy_loss.d.copy())
         return trainer_state
 
     def _build_training_graph(self,
                               models: Sequence[Model],
                               training_variables: TrainingVariables):
-        self._cross_entropy_loss = 0
-        ignore_intermediate_loss = self._config.loss_integration is LossIntegration.LAST_TIMESTEP_ONLY
-        for step_index, variables in enumerate(training_variables):
-            is_burn_in_steps = step_index < self._config.burn_in_steps
-            is_intermediate_steps = step_index < self._config.burn_in_steps + self._config.unroll_steps - 1
-            ignore_loss = is_burn_in_steps or (is_intermediate_steps and ignore_intermediate_loss)
-            self._build_one_step_graph(models, variables, ignore_loss=ignore_loss)
-
-    def _build_one_step_graph(self,
-                              models: Sequence[Model],
-                              training_variables: TrainingVariables,
-                              ignore_loss: bool):
         models = cast(Sequence[ValueDistributionFunction], models)
 
         # Computing the target probabilities
         mi = self._compute_target(training_variables)
         mi.need_grad = False
 
-        prev_rnn_states = self._prev_rnn_states
-        train_rnn_states = training_variables.rnn_states
+        self._cross_entropy_loss = 0
         for model in models:
-            with rnn_support(model, prev_rnn_states, train_rnn_states, training_variables, self._config):
-                loss, extra_info = self._compute_loss(model, mi, training_variables)
+            loss, extra_info = self._compute_loss(model, mi, training_variables)
             # Sum over models
-            self._cross_entropy_loss += 0.0 if ignore_loss else loss
+            self._cross_entropy_loss += loss
         # for prioritized experience replay
         # See: https://arxiv.org/pdf/1710.02298.pdf
         # keep kl_loss only for the last model for prioritized replay
         kl_loss = extra_info['kl_loss']
         self._kl_loss = kl_loss
         self._kl_loss.persistent = True
 
@@ -139,52 +101,37 @@
         raise NotImplementedError
 
     def _compute_loss(self,
                       model: ValueDistributionFunction,
                       target: nn.Variable,
                       training_variables: TrainingVariables) -> Tuple[nn.Variable, Dict[str, nn.Variable]]:
         batch_size = training_variables.batch_size
-        atom_probabilities = model.probs(training_variables.s_current, training_variables.a_current)
+        atom_probabilities = model.probs(self._training_variables.s_current, self._training_variables.a_current)
         atom_probabilities = NF.clip_by_value(atom_probabilities, 1e-10, 1.0)
         cross_entropy = target * NF.log(atom_probabilities)
         assert cross_entropy.shape == (batch_size, self._config.num_atoms)
 
         kl_loss = -NF.sum(cross_entropy, axis=1, keepdims=True)
-        if self._config.reduction_method == 'mean':
-            loss = NF.mean(kl_loss * training_variables.weight)
-        elif self._config.reduction_method == 'sum':
-            loss = NF.sum(kl_loss * training_variables.weight)
-        else:
-            raise RuntimeError
+        loss = NF.mean(kl_loss * training_variables.weight)
+
         extra = {'kl_loss': kl_loss}
         return loss, extra
 
     def _setup_training_variables(self, batch_size) -> TrainingVariables:
         # Training input variables
-        s_current_var = create_variable(batch_size, self._env_info.state_shape)
-        a_current_var = create_variable(batch_size, self._env_info.action_shape)
-        s_next_var = create_variable(batch_size, self._env_info.state_shape)
-        reward_var = create_variable(batch_size, 1)
-        gamma_var = create_variable(batch_size, 1)
-        non_terminal_var = create_variable(batch_size, 1)
-        weight_var = create_variable(batch_size, 1)
-
-        rnn_states = {}
-        for model in self._models:
-            if model.is_recurrent():
-                rnn_state_variables = create_variables(batch_size, model.internal_state_shapes())
-                rnn_states[model.scope_name] = rnn_state_variables
-
+        s_current_var = nn.Variable((batch_size, *self._env_info.state_shape))
+        a_current_var = nn.Variable((batch_size, 1))
+        s_next_var = nn.Variable((batch_size, *self._env_info.state_shape))
+        reward_var = nn.Variable((batch_size, 1))
+        gamma_var = nn.Variable((1, 1))
+        non_terminal_var = nn.Variable((batch_size, 1))
+        s_next_var = nn.Variable((batch_size, *self._env_info.state_shape))
+        weight_var = nn.Variable((batch_size, 1))
         training_variables = TrainingVariables(batch_size=batch_size,
                                                s_current=s_current_var,
                                                a_current=a_current_var,
                                                reward=reward_var,
                                                gamma=gamma_var,
                                                non_terminal=non_terminal_var,
                                                s_next=s_next_var,
-                                               weight=weight_var,
-                                               rnn_states=rnn_states)
+                                               weight=weight_var)
         return training_variables
-
-    @property
-    def loss_variables(self) -> Dict[str, nn.Variable]:
-        return {"cross_entropy_loss": self._cross_entropy_loss}
```

## nnabla_rl/model_trainers/reward/__init__.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021,2022,2023,2024 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -10,9 +10,7 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from nnabla_rl.model_trainers.reward.gail_reward_function_trainer import (  # noqa
     GAILRewardFunctionTrainer,  GAILRewardFunctionTrainerConfig)
-from nnabla_rl.model_trainers.reward.amp_reward_function_trainer import (  # noqa
-    AMPRewardFunctionTrainer,  AMPRewardFunctionTrainerConfig)
```

## nnabla_rl/model_trainers/reward/gail_reward_function_trainer.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021,2022 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -15,19 +15,17 @@
 from dataclasses import dataclass
 from typing import Dict, Iterable, Sequence, Union, cast
 
 import numpy as np
 
 import nnabla as nn
 import nnabla.functions as NF
-from nnabla_rl.model_trainers.model_trainer import (LossIntegration, ModelTrainer, TrainerConfig, TrainingBatch,
-                                                    TrainingVariables)
+from nnabla_rl.model_trainers.model_trainer import ModelTrainer, TrainerConfig, TrainingBatch, TrainingVariables
 from nnabla_rl.models import Model, RewardFunction
-from nnabla_rl.utils.data import convert_to_list_if_not_list, set_data_to_variable
-from nnabla_rl.utils.misc import create_variable
+from nnabla_rl.utils.data import convert_to_list_if_not_list
 
 
 @dataclass
 class GAILRewardFunctionTrainerConfig(TrainerConfig):
     batch_size: int = 1024
     learning_rate: float = 3e-4
     entropy_coef: float = 0.001
@@ -51,49 +49,47 @@
         super(GAILRewardFunctionTrainer, self).__init__(models, solvers, env_info, config)
 
     def _update_model(self,
                       models: Iterable[Model],
                       solvers: Dict[str, nn.solver.Solver],
                       batch: TrainingBatch,
                       training_variables: TrainingVariables,
-                      **kwargs) -> Dict[str, np.ndarray]:
-        for t, b in zip(training_variables, batch):
-            for key in batch.extra.keys():
-                set_data_to_variable(t.extra[key], b.extra[key])
+                      **kwargs) -> Dict[str, np.array]:
+        s_curr_agent = batch.extra['s_current_agent']
+        a_curr_agent = batch.extra['a_current_agent']
+        s_next_agent = batch.extra['s_next_agent']
+        s_curr_expert = batch.extra['s_current_expert']
+        a_curr_expert = batch.extra['a_current_expert']
+        s_next_expert = batch.extra['s_next_expert']
+
+        training_variables.extra['s_current_expert'].d = s_curr_expert
+        training_variables.extra['a_current_expert'].d = a_curr_expert
+        training_variables.extra['s_next_expert'].d = s_next_expert
+        training_variables.extra['s_current_agent'].d = s_curr_agent
+        training_variables.extra['a_current_agent'].d = a_curr_agent
+        training_variables.extra['s_next_agent'].d = s_next_agent
 
         # update model
         for solver in solvers.values():
             solver.zero_grad()
         self._binary_classification_loss.forward()
         self._binary_classification_loss.backward()
         for solver in solvers.values():
             solver.update()
 
-        trainer_state: Dict[str, np.ndarray] = {}
-        trainer_state['reward_loss'] = self._binary_classification_loss.d.copy()
+        trainer_state = {}
+        trainer_state['reward_loss'] = float(self._binary_classification_loss.d.copy())
         return trainer_state
 
     def _build_training_graph(self, models: Union[Model, Sequence[Model]],
                               training_variables: TrainingVariables):
         models = convert_to_list_if_not_list(models)
         models = cast(Sequence[RewardFunction], models)
 
         self._binary_classification_loss = 0
-        ignore_intermediate_loss = self._config.loss_integration is LossIntegration.LAST_TIMESTEP_ONLY
-        for step_index, variables in enumerate(training_variables):
-            is_burn_in_steps = step_index < self._config.burn_in_steps
-            is_intermediate_steps = step_index < self._config.burn_in_steps + self._config.unroll_steps - 1
-            ignore_loss = is_burn_in_steps or (is_intermediate_steps and ignore_intermediate_loss)
-            self._build_one_step_graph(models, variables, ignore_loss=ignore_loss)
-
-    def _build_one_step_graph(self,
-                              models: Sequence[Model],
-                              training_variables: TrainingVariables,
-                              ignore_loss: bool):
-        models = cast(Sequence[RewardFunction], models)
         for model in models:
             # fake path
             logits_fake = model.r(training_variables.extra['s_current_agent'],
                                   training_variables.extra['a_current_agent'],
                                   training_variables.extra['s_next_agent'])
             fake_loss = NF.mean(NF.sigmoid_cross_entropy(logits_fake, NF.constant(0, logits_fake.shape)))
             # real path
@@ -101,29 +97,31 @@
                                   training_variables.extra['a_current_expert'],
                                   training_variables.extra['s_next_expert'])
             real_loss = NF.mean(NF.sigmoid_cross_entropy(logits_real, NF.constant(1, logits_real.shape)))
             # entropy loss
             logits = NF.concatenate(logits_fake, logits_real, axis=0)
             entropy = NF.mean((1. - NF.sigmoid(logits)) * logits - NF.log_sigmoid(logits))
             entropy_loss = - self._config.entropy_coef * entropy  # maximize
-            self._binary_classification_loss += 0.0 if ignore_loss else fake_loss + real_loss + entropy_loss
+            self._binary_classification_loss += fake_loss + real_loss + entropy_loss
 
     def _setup_training_variables(self, batch_size):
-        s_current_agent_var = create_variable(batch_size, self._env_info.state_shape)
-        s_next_agent_var = create_variable(batch_size, self._env_info.state_shape)
-        s_current_expert_var = create_variable(batch_size, self._env_info.state_shape)
-        s_next_expert_var = create_variable(batch_size, self._env_info.state_shape)
-        a_current_agent_var = create_variable(batch_size, self._env_info.action_shape)
-        a_current_expert_var = create_variable(batch_size, self._env_info.action_shape)
+        s_current_agent_var = nn.Variable((batch_size, *self._env_info.state_shape))
+        s_next_agent_var = nn.Variable((batch_size, *self._env_info.state_shape))
+        s_current_expert_var = nn.Variable((batch_size, *self._env_info.state_shape))
+        s_next_expert_var = nn.Variable((batch_size, *self._env_info.state_shape))
+
+        if self._env_info.is_discrete_action_env():
+            a_current_agent_var = nn.Variable((batch_size, 1))
+            a_current_expert_var = nn.Variable((batch_size, 1))
+        else:
+            a_current_agent_var = nn.Variable((batch_size, self._env_info.action_dim))
+            a_current_expert_var = nn.Variable((batch_size, self._env_info.action_dim))
 
         variables = {'s_current_expert': s_current_expert_var,
                      'a_current_expert': a_current_expert_var,
                      's_next_expert': s_next_expert_var,
                      's_current_agent': s_current_agent_var,
                      'a_current_agent': a_current_agent_var,
                      's_next_agent': s_next_agent_var}
+        training_variables = TrainingVariables(batch_size, extra=variables)
 
-        return TrainingVariables(batch_size, extra=variables)
-
-    @property
-    def loss_variables(self) -> Dict[str, nn.Variable]:
-        return {"binary_classification_loss": self._binary_classification_loss}
+        return training_variables
```

## nnabla_rl/model_trainers/v_value/__init__.py

```diff
@@ -1,24 +1,18 @@
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from nnabla_rl.model_trainers.v_value.demme_v_trainer import (  # noqa
-    DEMMEVTrainer, DEMMEVTrainerConfig)
-from nnabla_rl.model_trainers.v_value.xql_v_trainer import (  # noqa
-    XQLVTrainer, XQLVTrainerConfig)
-from nnabla_rl.model_trainers.v_value.mme_v_trainer import (  # noqa
-    MMEVTrainer, MMEVTrainerConfig)
 from nnabla_rl.model_trainers.v_value.monte_carlo_v_trainer import (  # noqa
     MonteCarloVTrainer, MonteCarloVTrainerConfig)
 from nnabla_rl.model_trainers.v_value.soft_v_trainer import (  # noqa
     SoftVTrainer, SoftVTrainerConfig)
```

## nnabla_rl/model_trainers/v_value/monte_carlo_v_trainer.py

```diff
@@ -11,54 +11,46 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from dataclasses import dataclass
 from typing import Dict, Sequence, Union
 
-import numpy as np
-
 import nnabla as nn
 from nnabla_rl.environments.environment_info import EnvironmentInfo
 from nnabla_rl.model_trainers.model_trainer import TrainingBatch, TrainingVariables
 from nnabla_rl.model_trainers.v_value.squared_td_v_function_trainer import (SquaredTDVFunctionTrainer,
                                                                             SquaredTDVFunctionTrainerConfig)
 from nnabla_rl.models import VFunction
-from nnabla_rl.models.model import Model
-from nnabla_rl.utils.data import set_data_to_variable
-from nnabla_rl.utils.misc import create_variable
 
 
 @dataclass
 class MonteCarloVTrainerConfig(SquaredTDVFunctionTrainerConfig):
     pass
 
 
 class MonteCarloVTrainer(SquaredTDVFunctionTrainer):
+    # type declarations to type check with mypy
+    # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
+    # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
+    _v_target: nn.Variable
+
     def __init__(self,
                  train_functions: Union[VFunction, Sequence[VFunction]],
                  solvers: Dict[str, nn.solver.Solver],
                  env_info: EnvironmentInfo,
                  config: MonteCarloVTrainerConfig = MonteCarloVTrainerConfig()):
         super(MonteCarloVTrainer, self).__init__(train_functions, solvers, env_info, config)
 
-    def _update_model(self,
-                      models: Sequence[Model],
-                      solvers: Dict[str, nn.solver.Solver],
-                      batch: TrainingBatch,
-                      training_variables: TrainingVariables,
-                      **kwargs) -> Dict[str, np.ndarray]:
-        for t, b in zip(training_variables, batch):
-            set_data_to_variable(t.extra['v_target'], b.extra['v_target'])
-        return super()._update_model(models, solvers, batch, training_variables, **kwargs)
+    def _setup_batch(self, batch: TrainingBatch):
+        batch_size = batch.batch_size
+        v_target = batch.extra['v_target']
+        if self._v_target is None or self._v_target.shape[0] != batch_size:
+            self._v_target = nn.Variable((batch_size, 1))
+        self._v_target.d = v_target
+        return batch
 
     def _compute_target(self, training_variables: TrainingVariables, **kwargs) -> nn.Variable:
-        return training_variables.extra['v_target']
-
-    def _setup_training_variables(self, batch_size) -> TrainingVariables:
-        training_variables = super()._setup_training_variables(batch_size)
-
-        extra = {}
-        extra['v_target'] = create_variable(batch_size, 1)
-        training_variables.extra.update(extra)
-
-        return training_variables
+        batch_size = training_variables.batch_size
+        if not hasattr(self, '_v_target') or self._v_target.shape[0] != batch_size:
+            self._v_target = nn.Variable((batch_size, 1))
+        return self._v_target
```

## nnabla_rl/model_trainers/v_value/soft_v_trainer.py

```diff
@@ -1,8 +1,8 @@
-# Copyright 2021,2022 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -14,79 +14,49 @@
 
 from dataclasses import dataclass
 from typing import Dict, Sequence, Union
 
 import nnabla as nn
 import nnabla_rl.functions as RNF
 from nnabla_rl.environments.environment_info import EnvironmentInfo
-from nnabla_rl.model_trainers.model_trainer import TrainingVariables, rnn_support
+from nnabla_rl.model_trainers.model_trainer import TrainingVariables
 from nnabla_rl.model_trainers.v_value.squared_td_v_function_trainer import (SquaredTDVFunctionTrainer,
                                                                             SquaredTDVFunctionTrainerConfig)
 from nnabla_rl.models import QFunction, StochasticPolicy, VFunction
 from nnabla_rl.utils.data import convert_to_list_if_not_list
-from nnabla_rl.utils.misc import create_variables
 
 
 @dataclass
 class SoftVTrainerConfig(SquaredTDVFunctionTrainerConfig):
     pass
 
 
 class SoftVTrainer(SquaredTDVFunctionTrainer):
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _target_functions: Sequence[QFunction]
     _target_policy: StochasticPolicy
-    _prev_target_rnn_states: Dict[str, Dict[str, nn.Variable]]
-    _prev_q_rnn_states: Dict[str, Dict[str, nn.Variable]]
 
     def __init__(self,
                  train_functions: Union[VFunction, Sequence[VFunction]],
                  solvers: Dict[str, nn.solver.Solver],
                  target_functions: Union[QFunction, Sequence[QFunction]],
                  target_policy: StochasticPolicy,
                  env_info: EnvironmentInfo,
                  config: SoftVTrainerConfig = SoftVTrainerConfig()):
         self._target_functions = convert_to_list_if_not_list(target_functions)
         self._target_policy = target_policy
-        self._prev_target_rnn_states = {}
-        self._prev_q_rnn_states = {}
         super(SoftVTrainer, self).__init__(train_functions, solvers, env_info, config)
 
-    def support_rnn(self) -> bool:
-        return True
-
     def _compute_target(self, training_variables: TrainingVariables, **kwargs):
         s_current = training_variables.s_current
 
-        train_rnn_states = training_variables.rnn_states
-        prev_rnn_states = self._prev_target_rnn_states
-        with rnn_support(self._target_policy, prev_rnn_states, train_rnn_states, training_variables, self._config):
-            policy_distribution = self._target_policy.pi(s_current)
+        policy_distribution = self._target_policy.pi(s_current)
         sampled_action, log_pi = policy_distribution.sample_and_compute_log_prob()
 
         q_values = []
-        prev_rnn_states = self._prev_q_rnn_states
         for q_function in self._target_functions:
-            with rnn_support(q_function, prev_rnn_states, train_rnn_states, training_variables, self._config):
-                q_values.append(q_function.q(s_current, sampled_action))
+            q_values.append(q_function.q(s_current, sampled_action))
         target_q = RNF.minimum_n(q_values)
 
         return target_q - log_pi
-
-    def _setup_training_variables(self, batch_size: int) -> TrainingVariables:
-        training_variables = super()._setup_training_variables(batch_size)
-
-        rnn_states = {}
-        for target_function in self._target_functions:
-            if target_function.is_recurrent():
-                shapes = target_function.internal_state_shapes()
-                rnn_state_variables = create_variables(batch_size, shapes)
-                rnn_states[target_function.scope_name] = rnn_state_variables
-        if self._target_policy.is_recurrent():
-            shapes = self._target_policy.internal_state_shapes()
-            rnn_state_variables = create_variables(batch_size, shapes)
-            rnn_states[self._target_policy.scope_name] = rnn_state_variables
-
-        training_variables.rnn_states.update(rnn_states)
-        return training_variables
```

## nnabla_rl/model_trainers/v_value/squared_td_v_function_trainer.py

```diff
@@ -1,55 +1,108 @@
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from dataclasses import dataclass
-from typing import Dict, Sequence, Union
+from typing import Dict, Optional, Sequence, Union, cast
+
+import numpy as np
 
 import nnabla as nn
 import nnabla.functions as NF
 from nnabla_rl.environments.environment_info import EnvironmentInfo
-from nnabla_rl.model_trainers.model_trainer import TrainingVariables, rnn_support
-from nnabla_rl.model_trainers.v_value.v_function_trainer import VFunctionTrainer, VFunctionTrainerConfig
-from nnabla_rl.models import VFunction
+from nnabla_rl.model_trainers.model_trainer import ModelTrainer, TrainerConfig, TrainingBatch, TrainingVariables
+from nnabla_rl.models import Model, VFunction
+from nnabla_rl.utils.misc import clip_grad_by_global_norm
 
 
 @dataclass
-class SquaredTDVFunctionTrainerConfig(VFunctionTrainerConfig):
-    pass
+class SquaredTDVFunctionTrainerConfig(TrainerConfig):
+    reduction_method: str = 'mean'
+    v_loss_scalar: float = 1.0
+    max_grad_norm: Optional[float] = None
+
+    def __post_init__(self):
+        super(SquaredTDVFunctionTrainerConfig, self).__post_init__()
+        self._assert_one_of(self.reduction_method, ['sum', 'mean'], 'reduction_method')
 
 
-class SquaredTDVFunctionTrainer(VFunctionTrainer):
+class SquaredTDVFunctionTrainer(ModelTrainer):
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _config: SquaredTDVFunctionTrainerConfig
-    _prev_rnn_states: Dict[str, Dict[str, nn.Variable]]
+    _v_loss: nn.Variable  # Training loss/output
 
     def __init__(self,
                  models: Union[VFunction, Sequence[VFunction]],
                  solvers: Dict[str, nn.solver.Solver],
                  env_info: EnvironmentInfo,
                  config: SquaredTDVFunctionTrainerConfig = SquaredTDVFunctionTrainerConfig()):
-        self._prev_rnn_states = {}
         super(SquaredTDVFunctionTrainer, self).__init__(models, solvers, env_info, config)
 
+    def _update_model(self,
+                      models: Sequence[Model],
+                      solvers: Dict[str, nn.solver.Solver],
+                      batch: TrainingBatch,
+                      training_variables: TrainingVariables,
+                      **kwargs) -> Dict[str, np.array]:
+        training_variables.s_current.d = batch.s_current
+
+        # update model
+        for solver in solvers.values():
+            solver.zero_grad()
+        self._v_loss.forward(clear_no_need_grad=True)
+        self._v_loss.backward(clear_buffer=True)
+        for solver in solvers.values():
+            if self._config.max_grad_norm is not None:
+                clip_grad_by_global_norm(solver, self._config.max_grad_norm)
+            solver.update()
+
+        trainer_state = {}
+        trainer_state['v_loss'] = float(self._v_loss.d.copy())
+        return trainer_state
+
+    def _build_training_graph(self,
+                              models: Sequence[Model],
+                              training_variables: TrainingVariables):
+        models = cast(Sequence[VFunction], models)
+
+        # value function learning
+        target_v = self._compute_target(training_variables)
+
+        self._v_loss = 0
+        for v_function in models:
+            self._v_loss += self._compute_loss(v_function, target_v, training_variables)
+
     def _compute_loss(self,
                       model: VFunction,
                       target_value: nn.Variable,
                       training_variables: TrainingVariables) -> nn.Variable:
-        prev_rnn_states = self._prev_rnn_states
-        train_rnn_states = training_variables.rnn_states
-        with rnn_support(model, prev_rnn_states, train_rnn_states, training_variables, self._config):
-            v_value = model.v(training_variables.s_current)
-        td_error = target_value - v_value
-        return NF.pow_scalar(td_error, 2.0)
+        td_error = target_value - model.v(training_variables.s_current)
+        squared_td_error = NF.pow_scalar(td_error, 2.0)
+        if self._config.reduction_method == 'mean':
+            v_loss = self._config.v_loss_scalar * NF.mean(squared_td_error)
+        elif self._config.reduction_method == 'sum':
+            v_loss = self._config.v_loss_scalar * NF.sum(squared_td_error)
+        else:
+            raise RuntimeError
+        return v_loss
+
+    def _compute_target(self, training_variables: TrainingVariables) -> nn.Variable:
+        raise NotImplementedError
+
+    def _setup_training_variables(self, batch_size) -> TrainingVariables:
+        # Training input variables
+        s_current_var = nn.Variable((batch_size, *self._env_info.state_shape))
+        training_variables = TrainingVariables(batch_size, s_current_var)
+        return training_variables
```

## nnabla_rl/models/__init__.py

```diff
@@ -1,79 +1,55 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023,2024 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from nnabla_rl.models.decision_transformer import (DecisionTransformer,  # noqa
-                                                   DeterministicDecisionTransformer,
-                                                   StochasticDecisionTransformer)
 from nnabla_rl.models.distributional_function import (ValueDistributionFunction,  # noqa
                                                       DiscreteValueDistributionFunction,
                                                       ContinuousValueDistributionFunction)
 from nnabla_rl.models.distributional_function import (QuantileDistributionFunction,  # noqa
                                                       DiscreteQuantileDistributionFunction,
                                                       ContinuousQuantileDistributionFunction)
 from nnabla_rl.models.distributional_function import (StateActionQuantileFunction,  # noqa
                                                       DiscreteStateActionQuantileFunction,
                                                       ContinuousStateActionQuantileFunction)
-from nnabla_rl.models.dynamics import Dynamics, DeterministicDynamics, StochasticDynamics  # noqa
 from nnabla_rl.models.model import Model  # noqa
 from nnabla_rl.models.perturbator import Perturbator  # noqa
 from nnabla_rl.models.policy import Policy, DeterministicPolicy, StochasticPolicy  # noqa
-from nnabla_rl.models.q_function import QFunction, DiscreteQFunction, ContinuousQFunction, FactoredContinuousQFunction  # noqa
+from nnabla_rl.models.q_function import QFunction, DiscreteQFunction, ContinuousQFunction  # noqa
 from nnabla_rl.models.v_function import VFunction  # noqa
 from nnabla_rl.models.reward_function import RewardFunction  # noqa
 from nnabla_rl.models.encoder import Encoder, VariationalAutoEncoder  # noqa
 
 from nnabla_rl.models.mujoco.policies import TD3Policy, SACPolicy, BEARPolicy, TRPOPolicy  # noqa
-from nnabla_rl.models.mujoco.q_functions import TD3QFunction, SACQFunction, SACDQFunction, HERQFunction, XQLQFunction  # noqa
-from nnabla_rl.models.mujoco.decision_transformers import MujocoDecisionTransformer  # noqa
-from nnabla_rl.models.mujoco.distributional_functions import QRSACQuantileDistributionFunction  # noqa
-from nnabla_rl.models.mujoco.v_functions import SACVFunction, TRPOVFunction, ATRPOVFunction  # noqa
+from nnabla_rl.models.mujoco.q_functions import TD3QFunction, SACQFunction  # noqa
+from nnabla_rl.models.mujoco.v_functions import SACVFunction, TRPOVFunction  # noqa
 from nnabla_rl.models.mujoco.v_functions import PPOVFunction as PPOMujocoVFunction  # noqa
-from nnabla_rl.models.mujoco.v_functions import GAILVFunction  # noqa
-from nnabla_rl.models.mujoco.v_functions import XQLVFunction  # noqa
 from nnabla_rl.models.mujoco.encoders import UnsquashedVariationalAutoEncoder, BCQVariationalAutoEncoder  # noqa
 from nnabla_rl.models.mujoco.perturbators import BCQPerturbator  # noqa
 from nnabla_rl.models.mujoco.policies import ICML2015TRPOPolicy as ICML2015TRPOMujocoPolicy  # noqa
 from nnabla_rl.models.mujoco.policies import PPOPolicy as PPOMujocoPolicy  # noqa
 from nnabla_rl.models.mujoco.policies import GAILPolicy  # noqa
-from nnabla_rl.models.mujoco.policies import HERPolicy  # noqa
-from nnabla_rl.models.mujoco.policies import ATRPOPolicy  # noqa
-from nnabla_rl.models.mujoco.policies import XQLPolicy  # noqa
+from nnabla_rl.models.mujoco.v_functions import GAILVFunction  # noqa
 from nnabla_rl.models.mujoco.reward_functions import GAILDiscriminator  # noqa
-from nnabla_rl.models.atari.decision_transformers import AtariDecisionTransformer  # noqa
 from nnabla_rl.models.atari.policies import PPOPolicy as PPOAtariPolicy  # noqa
 from nnabla_rl.models.atari.policies import A3CPolicy  # noqa
-from nnabla_rl.models.atari.q_functions import DQNQFunction, DRQNQFunction  # noqa
+from nnabla_rl.models.atari.q_functions import DQNQFunction  # noqa
 from nnabla_rl.models.atari.v_functions import PPOVFunction as PPOAtariVFunction  # noqa
 from nnabla_rl.models.atari.v_functions import A3CVFunction  # noqa
 from nnabla_rl.models.atari.shared_functions import PPOSharedFunctionHead, A3CSharedFunctionHead  # noqa
+from nnabla_rl.models.atari.distributional_functions import IQNQuantileFunction  # noqa
 from nnabla_rl.models.atari.distributional_functions import (C51ValueDistributionFunction,  # noqa
-                                                             RainbowValueDistributionFunction,
-                                                             RainbowNoDuelValueDistributionFunction,
-                                                             RainbowNoNoisyValueDistributionFunction,
-                                                             QRDQNQuantileDistributionFunction,
-                                                             IQNQuantileFunction)
+                                                             QRDQNQuantileDistributionFunction)
 from nnabla_rl.models.atari.policies import ICML2015TRPOPolicy as ICML2015TRPOAtariPolicy  # noqa
 
-from nnabla_rl.models.pybullet.q_functions import ICRA2018QtOptQFunction  # noqa
-from nnabla_rl.models.pybullet.reward_functions import AMPDiscriminator  # noqa
-from nnabla_rl.models.pybullet.policy import AMPGatedPolicy, AMPPolicy  # noqa
-from nnabla_rl.models.pybullet.v_functions import AMPGatedVFunction, AMPVFunction  # noqa
-
 from nnabla_rl.models.classic_control.policies import REINFORCEContinousPolicy, REINFORCEDiscretePolicy  # noqa
-from nnabla_rl.models.classic_control.dynamics import MPPIDeterministicDynamics  # noqa
-
-from nnabla_rl.models.hybrid_env.encoders import HyARVAE  # noqa
-from nnabla_rl.models.hybrid_env.policies import HyARPolicy  # noqa
-from nnabla_rl.models.hybrid_env.q_functions import HyARQFunction  # noqa
```

## nnabla_rl/models/distributional_function.py

```diff
@@ -1,47 +1,47 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from abc import ABCMeta, abstractmethod
-from typing import Any, Callable, Dict, Iterable, Optional, Tuple
+from typing import Any, Callable, Dict, Iterable, Optional
 
 import nnabla as nn
 import nnabla.functions as NF
 import nnabla_rl.functions as RF
 from nnabla_rl.models.model import Model
 from nnabla_rl.models.q_function import QFunction
 
 
 class ValueDistributionFunction(Model, metaclass=ABCMeta):
-    """Base value distribution class.
+    '''Base value distribution class.
 
     Computes the probabilities of q-value for each action.
     Value distribution function models the probabilities of q value for each action by dividing
     the values between the maximum q value and minimum q value into 'n_atom' number of bins and
     assigning the probability to each bin.
 
     Args:
         scope_name (str): scope name of the model
         n_action (int): Number of actions which used in target environment.
         n_atom (int): Number of bins.
         v_min (int): Minimum value of the distribution.
         v_max (int): Maximum value of the distribution.
-    """
+    '''
 
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _n_action: int
     _n_atom: int
     _v_min: float
@@ -60,65 +60,64 @@
 
     def __deepcopy__(self, memodict: Dict[Any, Any] = {}):
         # nn.Variable cannot be deepcopied directly
         return self.__class__(self._scope_name, self._n_action, self._n_atom, self._v_min, self._v_max)
 
     @abstractmethod
     def probs(self, s: nn.Variable, a: nn.Variable) -> nn.Variable:
-        """Compute probabilities of atoms for given state and action.
+        """Compute probabilities of atoms for given state and action
 
         Args:
             s (nn.Variable): state variable
             a (nn.Variable): action variable
 
         Returns:
             nn.Variable: probabilities of atoms for given state and action
         """
         raise NotImplementedError
 
     def all_probs(self, s: nn.Variable) -> nn.Variable:
-        """Compute probabilities of atoms for all posible actions for given
-        state.
+        """Compute probabilities of atoms for all posible actions for given state
 
         Args:
             s (nn.Variable): state variable
 
         Returns:
             nn.Variable: probabilities of atoms for all posible actions for given state
         """
         raise NotImplementedError
 
     def max_q_probs(self, s: nn.Variable) -> nn.Variable:
-        """Compute probabilities of atoms for given state that maximizes the
-        q_value.
+        """Compute probabilities of atoms for given state that maximizes the q_value
 
         Args:
             s (nn.Variable): state variable
 
         Returns:
             nn.Variable: probabilities of atoms for given state that maximizes the q_value
         """
         raise NotImplementedError
 
     def as_q_function(self) -> QFunction:
-        """Convert the value distribution function to QFunction.
+        '''Convert the value distribution function to QFunction.
 
         Returns:
             nnabla_rl.models.q_function.QFunction:
                 QFunction instance which computes the q-values based on the probabilities.
-        """
+        '''
         raise NotImplementedError
 
     def _compute_z(self, n_atom: int, v_min: float, v_max: float) -> nn.Variable:
         delta_z = (v_max - v_min) / (n_atom - 1)
         return v_min + delta_z * NF.arange(0, n_atom)
 
 
 class DiscreteValueDistributionFunction(ValueDistributionFunction):
-    """Base value distribution class for discrete action envs."""
+    '''Base value distribution class for discrete action envs.
+    '''
     @abstractmethod
     def all_probs(self, s: nn.Variable) -> nn.Variable:
         raise NotImplementedError
 
     def probs(self, s: nn.Variable, a: nn.Variable) -> nn.Variable:
         probs = self.all_probs(s)
         return self._probabilities_of(probs, a)
@@ -126,14 +125,15 @@
     def max_q_probs(self, s: nn.Variable) -> nn.Variable:
         probs = self.all_probs(s)
         a_star = self._argmax_q_from_probabilities(probs)
         return self._probabilities_of(probs, a_star)
 
     def as_q_function(self) -> QFunction:
         class Wrapper(QFunction):
+
             _value_distribution_function: 'DiscreteValueDistributionFunction'
 
             def __init__(self, value_distribution_function: 'DiscreteValueDistributionFunction'):
                 super(Wrapper, self).__init__(value_distribution_function.scope_name)
                 self._value_distribution_function = value_distribution_function
 
             def q(self, s: nn.Variable, a: nn.Variable) -> nn.Variable:
@@ -147,26 +147,14 @@
                 return NF.max(q_values, axis=1, keepdims=True)
 
             def argmax_q(self, s: nn.Variable) -> nn.Variable:
                 probabilities = self._value_distribution_function.all_probs(s)
                 greedy_action = self._value_distribution_function._argmax_q_from_probabilities(probabilities)
                 return greedy_action
 
-            def is_recurrent(self) -> bool:
-                return self._value_distribution_function.is_recurrent()
-
-            def internal_state_shapes(self) -> Dict[str, Tuple[int, ...]]:
-                return self._value_distribution_function.internal_state_shapes()
-
-            def set_internal_states(self, states: Optional[Dict[str, nn.Variable]] = None):
-                return self._value_distribution_function.set_internal_states(states)
-
-            def get_internal_states(self) -> Dict[str, nn.Variable]:
-                return self._value_distribution_function.get_internal_states()
-
         return Wrapper(self)
 
     def _argmax_q_from_probabilities(self, atom_probabilities: nn.Variable) -> nn.Variable:
         q_values = self._probabilities_to_q_values(atom_probabilities)
         return RF.argmax(q_values, axis=1, keepdims=True)
 
     def _state_to_q_values(self, s: nn.Variable) -> nn.Variable:
@@ -198,98 +186,91 @@
         one_hot = NF.one_hot(a, (self._n_action,))
         one_hot = RF.expand_dims(one_hot, axis=1)
         one_hot = NF.broadcast(one_hot, shape=(batch_size, self._n_atom, self._n_action))
         return one_hot
 
 
 class ContinuousValueDistributionFunction(ValueDistributionFunction):
-    """Base value distribution class for continuous action envs."""
+    '''Base value distribution class for continuous action envs.
+    '''
     pass
 
 
 class QuantileDistributionFunction(Model, metaclass=ABCMeta):
-    """Base quantile distribution class.
+    '''Base quantile distribution class.
 
     Computes the quantiles of q-value for each action.
     Quantile distribution function models the quantiles of q value for each action by dividing
     the probability (which is between 0.0 and 1.0) into 'n_quantile' number of bins and
     assigning the n-quantile to n-th bin.
 
     Args:
         scope_name (str): scope name of the model
+        n_action (int): Number of actions which used in target environment.
         n_quantile (int): Number of bins.
-    """
+    '''
 
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
+    _n_action: int
     _n_quantile: int
     _qj: float
 
-    def __init__(self, scope_name: str, n_quantile: int):
+    def __init__(self, scope_name: str, n_action: int, n_quantile: int):
         super(QuantileDistributionFunction, self).__init__(scope_name)
+        self._n_action = n_action
         self._n_quantile = n_quantile
         self._qj = 1 / n_quantile
 
     def all_quantiles(self, s: nn.Variable) -> nn.Variable:
-        """Computes the quantiles of q-value for each action for the given
-        state.
+        '''Computes the quantiles of q-value for each action for the given state.
 
         Args:
             s (nn.Variable): state variable
 
         Returns:
             nn.Variable: quantiles of q-value for each action for the given state
-        """
+        '''
         raise NotImplementedError
 
     def quantiles(self, s: nn.Variable, a: nn.Variable) -> nn.Variable:
-        """Computes the quantiles of q-value for given state and action.
+        '''Computes the quantiles of q-value for given state and action.
 
         Args:
             s (nn.Variable): state variable
             a (nn.Variable): action variable
 
         Returns:
             nn.Variable: quantiles of q-value for given state and action.
-        """
+        '''
         raise NotImplementedError
 
     def max_q_quantiles(self, s: nn.Variable) -> nn.Variable:
-        """Compute the quantiles of q-value for given state that maximizes the
-        q_value.
+        """Compute the quantiles of q-value for given state that maximizes the q_value
 
         Args:
             s (nn.Variable): state variable
 
         Returns:
             nn.Variable: quantiles of q-value for given state that maximizes the q_value
         """
         raise NotImplementedError
 
     def as_q_function(self) -> QFunction:
-        """Convert the quantile distribution function to QFunction.
+        '''Convert the quantile distribution function to QFunction.
 
         Returns:
             nnabla_rl.models.q_function.QFunction:
                 QFunction instance which computes the q-values based on the quantiles.
-        """
+        '''
         raise NotImplementedError
 
 
 class DiscreteQuantileDistributionFunction(QuantileDistributionFunction):
-    # type declarations to type check with mypy
-    # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
-    # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
-    _n_action: int
-
-    def __init__(self, scope_name: str, n_action: int, n_quantile: int):
-        super().__init__(scope_name, n_quantile)
-        self._n_action = n_action
-
     @abstractmethod
     def all_quantiles(self, s: nn.Variable) -> nn.Variable:
         raise NotImplementedError
 
     def quantiles(self, s: nn.Variable, a: nn.Variable) -> nn.Variable:
         quantiles = self.all_quantiles(s)
         return self._quantiles_of(quantiles, a)
@@ -297,14 +278,15 @@
     def max_q_quantiles(self, s: nn.Variable) -> nn.Variable:
         probs = self.all_quantiles(s)
         a_star = self._argmax_q_from_quantiles(probs)
         return self._quantiles_of(probs, a_star)
 
     def as_q_function(self) -> QFunction:
         class Wrapper(QFunction):
+
             _quantile_distribution_function: 'DiscreteQuantileDistributionFunction'
 
             def __init__(self, quantile_distribution_function: 'DiscreteQuantileDistributionFunction'):
                 super(Wrapper, self).__init__(quantile_distribution_function.scope_name)
                 self._quantile_distribution_function = quantile_distribution_function
 
             def q(self, s: nn.Variable, a: nn.Variable) -> nn.Variable:
@@ -318,26 +300,14 @@
                 return NF.max(q_values, axis=1, keepdims=True)
 
             def argmax_q(self, s: nn.Variable) -> nn.Variable:
                 quantiles = self._quantile_distribution_function.all_quantiles(s)
                 greedy_action = self._quantile_distribution_function._argmax_q_from_quantiles(quantiles)
                 return greedy_action
 
-            def is_recurrent(self) -> bool:
-                return self._quantile_distribution_function.is_recurrent()
-
-            def internal_state_shapes(self) -> Dict[str, Tuple[int, ...]]:
-                return self._quantile_distribution_function.internal_state_shapes()
-
-            def set_internal_states(self, states: Optional[Dict[str, nn.Variable]] = None):
-                return self._quantile_distribution_function.set_internal_states(states)
-
-            def get_internal_states(self) -> Dict[str, nn.Variable]:
-                return self._quantile_distribution_function.get_internal_states()
-
         return Wrapper(self)
 
     def _argmax_q_from_quantiles(self, quantiles: nn.Variable) -> nn.Variable:
         q_values = self._quantiles_to_q_values(quantiles)
         return RF.argmax(q_values, axis=1, keepdims=True)
 
     def _quantiles_to_q_values(self, quantiles: nn.Variable) -> nn.Variable:
@@ -364,65 +334,35 @@
         one_hot = NF.one_hot(a, (self._n_action,))
         one_hot = RF.expand_dims(one_hot, axis=1)
         one_hot = NF.broadcast(one_hot, shape=(batch_size, self._n_quantile, self._n_action))
         return one_hot
 
 
 class ContinuousQuantileDistributionFunction(QuantileDistributionFunction):
-    def as_q_function(self) -> QFunction:
-        class Wrapper(QFunction):
-            _quantile_distribution_function: 'ContinuousQuantileDistributionFunction'
-
-            def __init__(self, quantile_distribution_function: 'ContinuousQuantileDistributionFunction'):
-                super(Wrapper, self).__init__(quantile_distribution_function.scope_name)
-                self._quantile_distribution_function = quantile_distribution_function
-
-            def q(self, s: nn.Variable, a: nn.Variable) -> nn.Variable:
-                quantiles = self._quantile_distribution_function.quantiles(s, a)
-                return NF.mean(quantiles, axis=len(quantiles.shape) - 1, keepdims=True)
-
-            def max_q(self, s: nn.Variable) -> nn.Variable:
-                raise NotImplementedError
-
-            def argmax_q(self, s: nn.Variable) -> nn.Variable:
-                raise NotImplementedError
-
-            def is_recurrent(self) -> bool:
-                return self._quantile_distribution_function.is_recurrent()
-
-            def internal_state_shapes(self) -> Dict[str, Tuple[int, ...]]:
-                return self._quantile_distribution_function.internal_state_shapes()
-
-            def set_internal_states(self, states: Optional[Dict[str, nn.Variable]] = None):
-                return self._quantile_distribution_function.set_internal_states(states)
-
-            def get_internal_states(self) -> Dict[str, nn.Variable]:
-                return self._quantile_distribution_function.get_internal_states()
-
-        return Wrapper(self)
+    pass
 
 
 def risk_neutral_measure(tau: nn.Variable) -> nn.Variable:
     return tau
 
 
 class StateActionQuantileFunction(Model, metaclass=ABCMeta):
-    """State-action quantile function class.
+    '''state-action quantile function class.
 
     Computes the return samples of q-value for each action.
     State-action quantile function computes the return samples of q value for each action
     using sampled quantile threshold (e.g. :math:`\\tau\\sim U([0,1])`) for given state.
 
     Args:
         scope_name (str): scope name of the model
         n_action (int): Number of actions which used in target environment.
         K (int): Number of samples for quantile threshold :math:`\\tau`.
         risk_measure_function (Callable[[nn.Variable], nn.Variable]): Risk measure funciton which
             modifies the weightings of tau. Defaults to risk neutral measure which does not do any change to the taus.
-    """
+    '''
 
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _n_action: int
     _K: int
     # _risk_measure_funciton: Callable[[nn.Variable], nn.Variable]
@@ -434,72 +374,70 @@
                  risk_measure_function: Callable[[nn.Variable], nn.Variable] = risk_neutral_measure):
         super(StateActionQuantileFunction, self).__init__(scope_name)
         self._n_action = n_action
         self._K = K
         self._risk_measure_function = risk_measure_function
 
     def all_quantile_values(self, s: nn.Variable, tau: nn.Variable) -> nn.Variable:
-        """Compute the return samples for all action for given state and
-        quantile threshold.
+        '''Compute the return samples for all action for given state and quantile threshold.
 
         Args:
             s (nn.Variable): state variable.
             tau (nn.Variable): quantile threshold.
 
         Returns:
             nn.Variable: return samples from implicit return distribution for given state using tau.
-        """
+        '''
         pass
 
     def quantile_values(self, s: nn.Variable, a: nn.Variable, tau: nn.Variable) -> nn.Variable:
-        """Compute the return samples for given state and action.
+        '''Compute the return samples for given state and action.
 
         Args:
             s (nn.Variable): state variable.
             a (nn.Variable): action variable.
             tau (nn.Variable): quantile threshold.
 
         Returns:
             nn.Variable: return samples from implicit return distribution for given state and action using tau.
-        """
+        '''
         pass
 
     def max_q_quantile_values(self, s: nn.Variable, tau: nn.Variable) -> nn.Variable:
-        """Compute the return samples from distribution that maximizes q value
-        for given state using quantile threshold.
+        '''Compute the return samples from distribution that maximizes q value for given state using quantile threshold.
 
         Args:
             s (nn.Variable): state variable.
             tau (nn.Variable): quantile threshold.
 
         Returns:
             nn.Variable: return samples from implicit return distribution that maximizes q for given state using tau.
-        """
+        '''
         pass
 
     def sample_tau(self, shape: Optional[Iterable] = None) -> nn.Variable:
-        """Sample quantile thresholds from uniform distribution.
+        '''Sample quantile thresholds from uniform distribution
 
         Args:
             shape (Tuple[int] or None): shape of the quantile threshold to sample. If None the shape will be (1, K).
 
         Returns:
             nn.Variable: quantile thresholds
-        """
+        '''
         if shape is None:
             shape = (1, self._K)
         return NF.rand(low=0.0, high=1.0, shape=shape)
 
     def as_q_function(self) -> QFunction:
-        """Convert the state action quantile function to QFunction.
+        '''Convert the state action quantile function to QFunction.
 
         Returns:
             nnabla_rl.models.q_function.QFunction:
                 QFunction instance which computes the q-values based on return samples.
-        """
+        '''
         raise NotImplementedError
 
     def _sample_risk_measured_tau(self, shape: Optional[Iterable]) -> nn.Variable:
         tau = self.sample_tau(shape)
         return self._risk_measure_function(tau)
 
 
@@ -509,35 +447,29 @@
         raise NotImplementedError
 
     def quantile_values(self, s: nn.Variable, a: nn.Variable, tau: nn.Variable) -> nn.Variable:
         return_samples = self.all_quantile_values(s, tau)
         return self._return_samples_of(return_samples, a)
 
     def max_q_quantile_values(self, s: nn.Variable, tau: nn.Variable) -> nn.Variable:
-        if self.is_recurrent():
-            raise RuntimeError('max_q_quantile_values should be reimplemented in inherited class to support RNN layers')
-
         batch_size = s.shape[0]
         tau_k = self._sample_risk_measured_tau(shape=(batch_size, self._K))
-        # This implementation does not support RNNs because internal state will be overwritten by
-        # the second call of all_quantile_values()
         _return_samples = self.all_quantile_values(s, tau_k)
         a_star = self._argmax_q_from_return_samples(_return_samples)
 
-        # This will overwrite the internal state. So may not properly trained if this is called during training.
         return_samples = self.all_quantile_values(s, tau)
         return self._return_samples_of(return_samples, a_star)
 
     def as_q_function(self) -> QFunction:
-        """Convert the state action quantile function to QFunction.
+        '''Convert the state action quantile function to QFunction.
 
         Returns:
             nnabla_rl.models.q_function.QFunction:
                 QFunction instance which computes the q-values based on the return_samples.
-        """
+        '''
         class Wrapper(QFunction):
             _quantile_function: 'DiscreteStateActionQuantileFunction'
 
             def __init__(self, quantile_function: 'DiscreteStateActionQuantileFunction'):
                 super(Wrapper, self).__init__(quantile_function.scope_name)
                 self._quantile_function = quantile_function
 
@@ -557,51 +489,38 @@
             def argmax_q(self, s: nn.Variable) -> nn.Variable:
                 batch_size = s.shape[0]
                 tau = self._quantile_function._sample_risk_measured_tau(shape=(batch_size, self._quantile_function._K))
                 samples = self._quantile_function.all_quantile_values(s, tau)
                 greedy_action = self._quantile_function._argmax_q_from_return_samples(samples)
                 return greedy_action
 
-            def is_recurrent(self) -> bool:
-                return self._quantile_function.is_recurrent()
-
-            def internal_state_shapes(self) -> Dict[str, Tuple[int, ...]]:
-                return self._quantile_function.internal_state_shapes()
-
-            def set_internal_states(self, states: Optional[Dict[str, nn.Variable]] = None):
-                return self._quantile_function.set_internal_states(states)
-
-            def get_internal_states(self) -> Dict[str, nn.Variable]:
-                return self._quantile_function.get_internal_states()
-
         return Wrapper(self)
 
     def _return_samples_to_q_values(self, return_samples: nn.Variable) -> nn.Variable:
-        """Compute the q values for each action for given return samples.
+        '''Compute the q values for each action for given return samples.
 
         Args:
             return_samples (nn.Variable): return samples.
 
         Returns:
             nn.Variable: q values for each action for given return samples.
-        """
+        '''
         samples = NF.transpose(return_samples, axes=(0, 2, 1))
         q_values = NF.mean(samples, axis=2)
         return q_values
 
     def _argmax_q_from_return_samples(self, return_samples: nn.Variable) -> nn.Variable:
-        """Compute the action which maximizes the q value computed from given
-        return samples.
+        '''Compute the action which maximizes the q value computed from given return samples.
 
         Args:
             return_samples (nn.Variable): return samples.
 
         Returns:
             nn.Variable: action which maximizes the q value for given return samples.
-        """
+        '''
         q_values = self._return_samples_to_q_values(return_samples)
         return RF.argmax(q_values, axis=1, keepdims=True)
 
     def _state_to_q_values(self, s: nn.Variable) -> nn.Variable:
         tau = self._sample_risk_measured_tau(shape=(1, self._K))
         samples = self.all_quantile_values(s, tau)
         return self._return_samples_to_q_values(samples)
```

## nnabla_rl/models/encoder.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -20,66 +20,71 @@
 from nnabla_rl.distributions import Distribution
 from nnabla_rl.models.model import Model
 
 
 class Encoder(Model, metaclass=ABCMeta):
     @abstractmethod
     def encode(self, x: nn.Variable, **kwargs) -> nn.Variable:
-        """Encode the input variable to latent representation.
+        '''
+        Encode the input variable to latent representation.
 
         Args:
             x (nn.Variable): encoder input.
 
         Returns:
             nn.Variable: latent variable
-        """
+        '''
         raise NotImplementedError
 
 
 class VariationalAutoEncoder(Encoder):
     @abstractmethod
     def encode_and_decode(self, x: nn.Variable, **kwargs) -> Tuple[Distribution, nn.Variable]:
-        """Encode the input variable and reconstruct.
+        '''
+        Encode the input variable and reconstruct.
 
         Args:
             x (nn.Variable): encoder input.
 
         Returns:
             Tuple[Distribution, nn.Variable]: latent distribution and reconstructed input
-        """
+        '''
         raise NotImplementedError
 
     @abstractmethod
     def decode(self, z: Optional[nn.Variable], **kwargs) -> nn.Variable:
-        """Reconstruct the latent representation.
+        '''
+        Reconstruct the latent representation.
 
         Args:
             z (nn.Variable, optional): latent variable. If the input is None, random sample will be used instead.
 
         Returns:
             nn.Variable: reconstructed variable
-        """
+        '''
         raise NotImplementedError
 
     @abstractmethod
     def decode_multiple(self, z: Optional[nn.Variable], decode_num: int, **kwargs):
-        """Reconstruct multiple latent representations.
+        '''
+        Reconstruct multiple latent representations.
 
         Args:
             z (nn.Variable, optional): encoder input. If the input is None, random sample will be used instead.
 
         Returns:
             nn.Variable: Reconstructed input and latent distribution
-        """
+        '''
         raise NotImplementedError
 
     @abstractmethod
     def latent_distribution(self, x: nn.Variable, **kwargs) -> Distribution:
-        """Compute the latent distribution :math:`p(z|x)`.
+        '''
+        Compute the latent distribution :math:`p(z|x)`.
 
         Args:
             x (nn.Variable): encoder input.
 
         Returns:
             Distribution: latent distribution
-        """
+        '''
         raise NotImplementedError
```

## nnabla_rl/models/model.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023,2024 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -11,24 +11,22 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import copy
 import pathlib
-from typing import Dict, Optional, Tuple, TypeVar, Union
+from typing import Dict, Union
 
 import nnabla as nn
 from nnabla_rl.logger import logger
 
-T = TypeVar('T', bound='Model')
-
 
 class Model(object):
-    """Model Class.
+    """Model Class
 
     Args:
         scope_name (str): the scope name of model
     """
 
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
@@ -36,146 +34,79 @@
     _scope_name: str
 
     def __init__(self, scope_name: str):
         self._scope_name = scope_name
 
     @property
     def scope_name(self) -> str:
-        """scope_name Get scope name of this model.
+        '''scope_name
+        Get scope name of this model.
 
         Returns:
             scope_name (str): scope name of the model
-        """
+        '''
         return self._scope_name
 
     def get_parameters(self, grad_only: bool = True) -> Dict[str, nn.Variable]:
-        """Get_parameters Retrive parameters associated with this model.
+        '''get_parameters
+        Retrive parameters associated with this model
 
         Args:
             grad_only (bool): Retrive parameters only with need_grad = True. Defaults to True.
 
         Returns:
             parameters (OrderedDict): Parameter map.
-        """
+        '''
         with nn.parameter_scope(self.scope_name):
             parameters: Dict[str, nn.Variable] = nn.get_parameters(grad_only=grad_only)
             return parameters
 
-    def clear_parameters(self):
-        """clear_parameters Clear all parameters associated with this model."""
-        with nn.parameter_scope(self.scope_name):
-            parameters: Dict[str, nn.Variable] = nn.clear_parameters()
-            return parameters
-
-    def is_recurrent(self) -> bool:
-        """is_recurrent Check whether the model uses recurrent network
-        component or not.
-
-        Model which use LSTM, GRU and/or any other recurrent network component must return True.
-        Returns:
-            bool: True if the model uses recurrent network component. Otherwise False.
-        """
-        return False
-
-    def internal_state_shapes(self) -> Dict[str, Tuple[int, ...]]:
-        """Internal_state_shapes Return internal state shape as tuple of ints
-        for each internal state (excluding the batch_size). This method will be
-        called by (:py:class:`RNNModelTrainer.
-
-        <nnabla_rl.model_trainers.model_trainer.RNNModelTrainer>`) and its
-        subclasses to setup training variables. Model which use LSTM, GRU
-        and/or any other recurrent network component must implement this
-        method.
-
-        Returns:
-            Dict[str, Tuple[int, ...]]: internal state shapes. key is the name of each internal state.
-        """
-        raise NotImplementedError
-
-    def set_internal_states(self, states: Optional[Dict[str, nn.Variable]] = None):
-        """set_internal states Set the internal state variable of rnn cell to
-        given state.
-
-        Model which use LSTM, GRU and/or any other recurrent network component must implement this method.
-        Args:
-            states (None or Dict[str, nn.Variable]): If None, reset all internal state to zero.
-            If state is provided, set the provided state as internal state.
-        """
-        raise NotImplementedError
-
-    def reset_internal_states(self):
-        """reset_internal states Set the internal state variable of rnn cell to
-        given zero."""
-        self.set_internal_states(None)
-
-    def get_internal_states(self) -> Dict[str, nn.Variable]:
-        """get_internal states Get the internal state variable of rnn cell.
-        Model which use LSTM, GRU and/or any other recurrent network component
-        must implement this method.
-
-        Returns:
-            Dict[str, nn.Variable]: Value of each internal state. key is the name of each internal state.
-        """
-        raise NotImplementedError
-
     def save_parameters(self, filepath: Union[str, pathlib.Path]) -> None:
-        """save_parameters Save model parameters to given filepath.
+        '''save_parameters
+        Save model parameters to given filepath.
 
         Args:
             filepath (str or pathlib.Path): paramter file path
-        """
+        '''
         if isinstance(filepath, pathlib.Path):
             filepath = str(filepath)
         with nn.parameter_scope(self.scope_name):
             nn.save_parameters(path=filepath)
 
     def load_parameters(self, filepath: Union[str, pathlib.Path]) -> None:
-        """load_parameters Load model parameters from given filepath.
+        '''load_parameters
+        Load model parameters from given filepath.
 
         Args:
             filepath (str or pathlib.Path): paramter file path
-        """
+        '''
         if isinstance(filepath, pathlib.Path):
             filepath = str(filepath)
         with nn.parameter_scope(self.scope_name):
             nn.load_parameters(path=filepath)
 
-    def deepcopy(self: T, new_scope_name: str) -> T:
-        """Deepcopy Create a (deep) copy of the model. All the model
-        parameter's (if exist) associated with will be copied and
-        new_scope_name will be assigned.
+    def deepcopy(self, new_scope_name: str) -> 'Model':
+        '''deepcopy
+        Create a copy of the model. All the model parameter's (if exist) associated with will be copied.
 
         Args:
             new_scope_name (str): scope_name of parameters for newly created model
 
         Returns:
             Model: copied model
 
         Raises:
             ValueError: Given scope name is same as the model or already exists.
-        """
+        '''
         assert new_scope_name != self._scope_name, 'Can not use same scope_name!'
         copied = copy.deepcopy(self)
         copied._scope_name = new_scope_name
         # copy current parameter if is already created
         params = self.get_parameters(grad_only=False)
         with nn.parameter_scope(new_scope_name):
             for param_name, param in params.items():
                 if nn.parameter.get_parameter(param_name) is not None:
                     raise RuntimeError(f'Model with scope_name: {new_scope_name} already exists!!')
                 logger.info(
                     f'copying param with name: {self.scope_name}/{param_name} ---> {new_scope_name}/{param_name}')
                 nn.parameter.get_parameter_or_create(param_name, shape=param.shape, initializer=param.d)
         return copied
-
-    def shallowcopy(self: T) -> T:
-        """Shallowcopy Create a (shallow) copy of the model. Unlike deepcopy,
-        shallowcopy will KEEP sharing the original network parameter by using
-        same scope_name as original model. However, all the class members will
-        be (deep) copied to the new instance. Do NOT use this method unless you
-        understand what this method does.
-
-        Returns:
-            Model: (shallow) copied model
-        """
-        copied = copy.deepcopy(self)
-        return copied
```

## nnabla_rl/models/perturbator.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -15,18 +15,19 @@
 
 from abc import ABCMeta, abstractmethod
 
 from nnabla_rl.models.model import Model
 
 
 class Perturbator(Model, metaclass=ABCMeta):
-    """DeterministicPolicy Abstract class for perturbator.
+    ''' DeterministicPolicy
+    Abstract class for perturbator
 
     Perturbator generates noise to append to current state's action
-    """
+    '''
 
     def __init__(self, scope_name):
         super(Perturbator, self).__init__(scope_name)
 
     @abstractmethod
     def generate_noise(self, s, a, phi):
         raise NotImplementedError
```

## nnabla_rl/models/policy.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -22,37 +22,38 @@
 
 class Policy(Model, metaclass=ABCMeta):
     def __init__(self, scope_name: str):
         super(Policy, self).__init__(scope_name)
 
 
 class DeterministicPolicy(Policy, metaclass=ABCMeta):
-    """DeterministicPolicy Abstract class for deterministic policy.
+    ''' DeterministicPolicy
+    Abstract class for deterministic policy
 
     This policy returns an action for the given state.
-    """
+    '''
     @abstractmethod
     def pi(self, s: nn.Variable) -> nn.Variable:
         '''pi
 
         Args:
             state (nnabla.Variable): State variable
 
         Returns:
             nnabla.Variable : Action for the given state
         '''
         raise NotImplementedError
 
 
 class StochasticPolicy(Policy, metaclass=ABCMeta):
-    """StochasticPolicy Abstract class for stochastic policy.
+    ''' StochasticPolicy
+    Abstract class for stochastic policy
 
-    This policy returns a probability distribution of action for the
-    given state.
-    """
+    This policy returns a probability distribution of action for the given state.
+    '''
     @abstractmethod
     def pi(self, s: nn.Variable) -> Distribution:
         '''pi
 
         Args:
             state (nnabla.Variable): State variable
```

## nnabla_rl/models/q_function.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -18,64 +18,66 @@
 import nnabla as nn
 import nnabla.functions as NF
 import nnabla_rl.functions as RF
 from nnabla_rl.models.model import Model
 
 
 class QFunction(Model, metaclass=ABCMeta):
-    """Base QFunction Class."""
+    """Base QFunction Class
+    """
     @abstractmethod
     def q(self, s: nn.Variable, a: nn.Variable) -> nn.Variable:
-        """Compute Q-value for given state and action.
+        """Compute Q-value for given state and action
 
         Args:
             s (nn.Variable): state variable
             a (nn.Variable): action variable
 
         Returns:
             nn.Variable: Q-value for given state and action
         """
         raise NotImplementedError
 
     def all_q(self, s: nn.Variable) -> nn.Variable:
-        """Compute Q-values for each action for given state.
+        """Compute Q-values for each action for given state
 
         Args:
             s (nn.Variable): state variable
 
         Returns:
             nn.Variable: Q-values for each action for given state
         """
         raise NotImplementedError
 
     def max_q(self, s: nn.Variable) -> nn.Variable:
-        """Compute maximum Q-value for given state.
+        """Compute maximum Q-value for given state
 
         Args:
             s (nn.Variable): state variable
 
         Returns:
             nn.Variable: maximum Q-value value for given state
         """
         raise NotImplementedError
 
     def argmax_q(self, s: nn.Variable) -> nn.Variable:
-        """Compute the action which maximizes the Q-value for given state.
+        """Compute the action which maximizes the Q-value for given state
 
         Args:
             s (nn.Variable): state variable
 
         Returns:
             nn.Variable: action which maximizes the Q-value for given state
         """
         raise NotImplementedError
 
 
 class DiscreteQFunction(QFunction):
-    """Base QFunction Class for discrete action environment."""
+    """Base QFunction Class for discrete action environment
+    """
     @abstractmethod
     def all_q(self, s: nn.Variable) -> nn.Variable:
         raise NotImplementedError
 
     def q(self, s: nn.Variable, a: nn.Variable) -> nn.Variable:
         q_values = self.all_q(s)
         q_value = NF.sum(q_values * NF.one_hot(NF.reshape(a, (-1, 1), inplace=False), (q_values.shape[1],)),
@@ -90,36 +92,10 @@
 
     def argmax_q(self, s: nn.Variable) -> nn.Variable:
         q_values = self.all_q(s)
         return RF.argmax(q_values, axis=1, keepdims=True)
 
 
 class ContinuousQFunction(QFunction):
-    """Base QFunction Class for continuous action environment."""
+    """Base QFunction Class for continuous action environment
+    """
     pass
-
-
-class FactoredContinuousQFunction(ContinuousQFunction):
-    """Base FactoredContinuousQFunction Class for continuous action
-    environment."""
-    @abstractmethod
-    def factored_q(self, s: nn.Variable, a: nn.Variable) -> nn.Variable:
-        """Compute factored Q-value for given state.
-
-        Args:
-            s (nn.Variable): state variable
-            a (nn.Variable): action variable
-
-        Returns:
-            nn.Variable: factored Q-value value for given state
-        """
-        raise NotImplementedError
-
-    @property
-    @abstractmethod
-    def num_factors(self) -> int:
-        """Return the number of output dimensions.
-
-        Returns:
-            nn.Variable: output dimensions
-        """
-        raise NotImplementedError
```

## nnabla_rl/models/reward_function.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -16,27 +16,28 @@
 from abc import ABCMeta, abstractmethod
 
 import nnabla as nn
 from nnabla_rl.models.model import Model
 
 
 class RewardFunction(Model, metaclass=ABCMeta):
-    """Base reward function class."""
+    '''Base reward function class
+    '''
 
     def __init__(self, scope_name: str):
         super(RewardFunction, self).__init__(scope_name)
 
     @abstractmethod
     def r(self, s_current: nn.Variable, a_current: nn.Variable, s_next: nn.Variable) -> nn.Variable:
-        """R Computes the reward for the given state, action and next state.
-        One (or more than one) of the input variables may not be used in the
-        actual computation.
+        '''r
+        Computes the reward for the given state, action and next state.
+        One (or more than one) of the input variables may not be used in the actual computation.
 
         Args:
             s_current (nnabla.Variable): State variable
             a_current (nnabla.Variable): Action variable
             s_next (nnabla.Variable): Next state variable
 
         Returns:
             nnabla.Variable : Reward for the given state, action and next state.
-        """
+        '''
         raise NotImplementedError
```

## nnabla_rl/models/v_function.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -16,23 +16,24 @@
 from abc import ABCMeta, abstractmethod
 
 import nnabla as nn
 from nnabla_rl.models.model import Model
 
 
 class VFunction(Model, metaclass=ABCMeta):
-    """Base Value function class."""
+    '''Base Value function class
+    '''
 
     def __init__(self, scope_name: str):
         super(VFunction, self).__init__(scope_name)
 
     @abstractmethod
     def v(self, s: nn.Variable) -> nn.Variable:
-        """Compute the state value (V) for given state.
+        '''Compute the state value (V) for given state
 
         Args:
             s (nn.Variable): state variable
 
         Returns:
             nn.Variable: State value for given state
-        """
+        '''
         raise NotImplementedError
```

## nnabla_rl/models/atari/distributional_functions.py

```diff
@@ -17,15 +17,14 @@
 
 import numpy as np
 
 import nnabla as nn
 import nnabla.functions as NF
 import nnabla.parametric_functions as NPF
 import nnabla_rl.functions as RF
-import nnabla_rl.parametric_functions as RPF
 from nnabla_rl.models import (DiscreteQuantileDistributionFunction, DiscreteStateActionQuantileFunction,
                               DiscreteValueDistributionFunction)
 
 
 class C51ValueDistributionFunction(DiscreteValueDistributionFunction):
     def all_probs(self, s: nn.Variable) -> nn.Variable:
         batch_size = s.shape[0]
@@ -47,112 +46,14 @@
                 h = NPF.affine(
                     h, n_outmaps=self._n_action * self._n_atom)
             h = NF.reshape(h, (-1, self._n_action, self._n_atom))
         assert h.shape == (batch_size, self._n_action, self._n_atom)
         return NF.softmax(h, axis=2)
 
 
-class RainbowValueDistributionFunction(DiscreteValueDistributionFunction):
-    def all_probs(self, s: nn.Variable) -> nn.Variable:
-        batch_size = s.shape[0]
-        with nn.parameter_scope(self.scope_name):
-            with nn.parameter_scope("conv1"):
-                h = NPF.convolution(s, outmaps=32, kernel=(8, 8), stride=(4, 4))
-            h = NF.relu(x=h)
-            with nn.parameter_scope("conv2"):
-                h = NPF.convolution(h, outmaps=64, kernel=(4, 4), stride=(2, 2))
-            h = NF.relu(x=h)
-            with nn.parameter_scope("conv3"):
-                h = NPF.convolution(h, outmaps=64, kernel=(3, 3), stride=(1, 1))
-            h = NF.relu(x=h)
-            h = NF.reshape(h, shape=(batch_size, -1))
-            with nn.parameter_scope("affine1"):
-                h = RPF.noisy_net(h, n_outmap=512 * 2)
-            h = NF.relu(x=h)
-
-            h = NF.reshape(h, shape=(batch_size, 2, -1))
-            value, advantage = NF.split(h, axis=1)
-
-            with nn.parameter_scope("affine_v"):
-                value = RPF.noisy_net(value, n_outmap=self._n_atom)
-                value = NF.reshape(value, (-1, 1, self._n_atom))
-
-            with nn.parameter_scope("affine_a"):
-                advantage = RPF.noisy_net(advantage, n_outmap=self._n_action * self._n_atom)
-                advantage = NF.reshape(advantage, (-1, self._n_action, self._n_atom))
-
-            average = NF.mean(advantage, axis=1, keepdims=True)
-            assert average.shape == (batch_size, 1, self._n_atom)
-
-            out = value + advantage - average
-        assert out.shape == (batch_size, self._n_action, self._n_atom)
-        return NF.softmax(out, axis=2)
-
-
-class RainbowNoDuelValueDistributionFunction(DiscreteValueDistributionFunction):
-    def all_probs(self, s: nn.Variable) -> nn.Variable:
-        batch_size = s.shape[0]
-        with nn.parameter_scope(self.scope_name):
-            with nn.parameter_scope("conv1"):
-                h = NPF.convolution(s, outmaps=32, kernel=(8, 8), stride=(4, 4))
-            h = NF.relu(x=h)
-            with nn.parameter_scope("conv2"):
-                h = NPF.convolution(h, outmaps=64, kernel=(4, 4), stride=(2, 2))
-            h = NF.relu(x=h)
-            with nn.parameter_scope("conv3"):
-                h = NPF.convolution(h, outmaps=64, kernel=(3, 3), stride=(1, 1))
-            h = NF.relu(x=h)
-            h = NF.reshape(h, shape=(batch_size, -1))
-            with nn.parameter_scope("affine1"):
-                h = RPF.noisy_net(h, n_outmap=512)
-            h = NF.relu(x=h)
-            with nn.parameter_scope("affine2"):
-                h = RPF.noisy_net(h, n_outmap=self._n_action * self._n_atom)
-            h = NF.reshape(h, (-1, self._n_action, self._n_atom))
-        assert h.shape == (batch_size, self._n_action, self._n_atom)
-        return NF.softmax(h, axis=2)
-
-
-class RainbowNoNoisyValueDistributionFunction(DiscreteValueDistributionFunction):
-    def all_probs(self, s: nn.Variable) -> nn.Variable:
-        batch_size = s.shape[0]
-        with nn.parameter_scope(self.scope_name):
-            with nn.parameter_scope("conv1"):
-                h = NPF.convolution(s, outmaps=32, kernel=(8, 8), stride=(4, 4))
-            h = NF.relu(x=h)
-            with nn.parameter_scope("conv2"):
-                h = NPF.convolution(h, outmaps=64, kernel=(4, 4), stride=(2, 2))
-            h = NF.relu(x=h)
-            with nn.parameter_scope("conv3"):
-                h = NPF.convolution(h, outmaps=64, kernel=(3, 3), stride=(1, 1))
-            h = NF.relu(x=h)
-            h = NF.reshape(h, shape=(batch_size, -1))
-            with nn.parameter_scope("affine1"):
-                h = NPF.affine(h, n_outmaps=512 * 2)
-            h = NF.relu(x=h)
-
-            h = NF.reshape(h, shape=(batch_size, 2, -1))
-            value, advantage = NF.split(h, axis=1)
-
-            with nn.parameter_scope("affine_v"):
-                value = NPF.affine(value, n_outmaps=self._n_atom)
-                value = NF.reshape(value, (-1, 1, self._n_atom))
-
-            with nn.parameter_scope("affine_a"):
-                advantage = NPF.affine(advantage, n_outmaps=self._n_action * self._n_atom)
-                advantage = NF.reshape(advantage, (-1, self._n_action, self._n_atom))
-
-            average = NF.mean(advantage, axis=1, keepdims=True)
-            assert average.shape == (batch_size, 1, self._n_atom)
-
-            out = value + advantage - average
-        assert out.shape == (batch_size, self._n_action, self._n_atom)
-        return NF.softmax(out, axis=2)
-
-
 class QRDQNQuantileDistributionFunction(DiscreteQuantileDistributionFunction):
     def all_quantiles(self, s: nn.Variable) -> nn.Variable:
         batch_size = s.shape[0]
         with nn.parameter_scope(self.scope_name):
             with nn.parameter_scope("conv1"):
                 h = NPF.convolution(s, outmaps=32, stride=(4, 4), kernel=(8, 8))
             h = NF.relu(x=h)
```

## nnabla_rl/models/atari/policies.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -20,20 +20,19 @@
 import nnabla_rl.initializers as RI
 from nnabla_rl.distributions.distribution import Distribution
 from nnabla_rl.models.atari.shared_functions import PPOSharedFunctionHead
 from nnabla_rl.models.policy import StochasticPolicy
 
 
 class PPOPolicy(StochasticPolicy):
-    """Shared parameter function proposed used in PPO paper for atari
-    environment.
-
+    '''
+    Shared parameter function proposed used in PPO paper for atari environment.
     This network outputs the policy distribution.
     See: https://arxiv.org/pdf/1707.06347.pdf
-    """
+    '''
 
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _head: PPOSharedFunctionHead
     _action_dim: int
 
@@ -51,19 +50,19 @@
         return D.Softmax(z=z)
 
     def _hidden(self, s: nn.Variable) -> nn.Variable:
         return self._head(s)
 
 
 class ICML2015TRPOPolicy(StochasticPolicy):
-    """Policy network proposed in TRPO original paper for atari environment.
-
+    '''
+    Policy network proposed in TRPO original paper for atari environment.
     This network outputs the value and policy distribution.
     See: https://arxiv.org/pdf/1502.05477.pdf
-    """
+    '''
 
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _action_dim: int
 
     def __init__(self, scope_name: str, action_dim: int):
@@ -85,18 +84,18 @@
             with nn.parameter_scope("affine2"):
                 z = NPF.affine(h, self._action_dim)
 
         return D.Softmax(z=z)
 
 
 class A3CPolicy(StochasticPolicy):
-    """Shared parameter function used in A3C paper for atari environment.
-
+    '''
+    Shared parameter function used in A3C paper for atari environment.
     See: https://arxiv.org/pdf/1602.01783.pdf
-    """
+    '''
 
     def __init__(self, head, scope_name, state_shape, action_dim):
         super(A3CPolicy, self).__init__(scope_name=scope_name)
         self._state_shape = state_shape
         self._action_dim = action_dim
         self._head = head
```

## nnabla_rl/models/atari/q_functions.py

```diff
@@ -1,24 +1,22 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from typing import Any, Dict, Optional, Tuple, Union
-
 import nnabla as nn
 import nnabla.functions as NF
 import nnabla.parametric_functions as NPF
 import nnabla_rl.initializers as RI
 from nnabla_rl.models.q_function import DiscreteQFunction
 
 
@@ -38,15 +36,16 @@
     _n_action: int
 
     def __init__(self, scope_name: str, n_action: int):
         super(DQNQFunction, self).__init__(scope_name)
         self._n_action = n_action
 
     def all_q(self, s: nn.Variable) -> nn.Variable:
-        """Predict all q values of the given state."""
+        ''' Predict all q values of the given state
+        '''
         with nn.parameter_scope(self.scope_name):
 
             with nn.parameter_scope("conv1"):
                 h = NF.relu(NPF.convolution(s, 32, (8, 8), stride=(4, 4),
                                             w_init=RI.HeNormal(s.shape[1],
                                                                32,
                                                                kernel=(8, 8))
@@ -74,114 +73,7 @@
                                        ))
 
             with nn.parameter_scope("affine2"):
                 h = NPF.affine(h, self._n_action,
                                w_init=RI.HeNormal(h.shape[1], self._n_action)
                                )
         return h
-
-
-class DRQNQFunction(DiscreteQFunction):
-    '''
-    Q function with LSTM layer proposed by M. Hausknecht et al. used in DRQN paper for atari environment.
-    See: https://arxiv.org/pdf/1507.06527.pdf
-
-    Args:
-        scope_name (str): the scope name
-        n_action (int): the number of discrete action
-    '''
-
-    # type declarations to type check with mypy
-    # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
-    # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
-    _n_action: int
-    _lstm_cell: NPF.LSTMCell
-    _h: Union[nn.Variable, None]
-    _c: Union[nn.Variable, None]
-
-    def __init__(self, scope_name: str, n_action: int):
-        super(DRQNQFunction, self).__init__(scope_name)
-        self._n_action = n_action
-        self._h = None
-        self._c = None
-
-        self._lstm_state_size = 512
-
-    def __deepcopy__(self, memodict: Dict[Any, Any] = {}):
-        # nn.Variable cannot be deepcopied directly
-        return self.__class__(self._scope_name, self._n_action)
-
-    def all_q(self, s: nn.Variable) -> nn.Variable:
-        """Predict all q values of the given state."""
-        with nn.parameter_scope(self.scope_name):
-
-            with nn.parameter_scope("conv1"):
-                h = NF.relu(NPF.convolution(s, 32, (8, 8), stride=(4, 4),
-                                            w_init=RI.HeNormal(s.shape[1],
-                                                               32,
-                                                               kernel=(8, 8))
-                                            ))
-
-            with nn.parameter_scope("conv2"):
-                h = NF.relu(NPF.convolution(h, 64, (4, 4), stride=(2, 2),
-                                            w_init=RI.HeNormal(h.shape[1],
-                                                               64,
-                                                               kernel=(4, 4))
-                                            ))
-
-            with nn.parameter_scope("conv3"):
-                h = NF.relu(NPF.convolution(h, 64, (3, 3), stride=(1, 1),
-                                            w_init=RI.HeNormal(h.shape[1],
-                                                               64,
-                                                               kernel=(3, 3))
-                                            ))
-
-            h = NF.reshape(h, (-1, 3136))
-
-            with nn.parameter_scope("lstm1"):
-                if not self._is_internal_state_created():
-                    # automaatically create internal states if not provided
-                    batch_size = h.shape[0]
-                    self._create_internal_states(batch_size)
-                w_init = RI.HeNormal(h.shape[1], self._lstm_state_size)
-                self._h, self._c = NPF.lstm_cell(h, self._h, self._c, self._lstm_state_size, w_init=w_init)
-                h = self._h
-
-            with nn.parameter_scope("affine2"):
-                h = NPF.affine(h, self._n_action,
-                               w_init=RI.HeNormal(h.shape[1], self._n_action))
-        return h
-
-    def is_recurrent(self) -> bool:
-        return True
-
-    def internal_state_shapes(self) -> Dict[str, Tuple[int, ...]]:
-        shapes: Dict[str, nn.Variable] = {}
-        shapes['lstm_hidden'] = (self._lstm_state_size, )
-        shapes['lstm_cell'] = (self._lstm_state_size, )
-        return shapes
-
-    def get_internal_states(self) -> Dict[str, nn.Variable]:
-        states: Dict[str, nn.Variable] = {}
-        states['lstm_hidden'] = self._h
-        states['lstm_cell'] = self._c
-        return states
-
-    def set_internal_states(self, states: Optional[Dict[str, nn.Variable]] = None):
-        if states is None:
-            if self._h is not None:
-                self._h.data.zero()
-            if self._c is not None:
-                self._c.data.zero()
-        else:
-            self._h = states['lstm_hidden']
-            self._c = states['lstm_cell']
-
-    def _create_internal_states(self, batch_size):
-        self._h = nn.Variable((batch_size, self._lstm_state_size))
-        self._c = nn.Variable((batch_size, self._lstm_state_size))
-
-        self._h.data.zero()
-        self._c.data.zero()
-
-    def _is_internal_state_created(self) -> bool:
-        return self._h is not None and self._c is not None
```

## nnabla_rl/models/atari/v_functions.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -17,20 +17,19 @@
 import nnabla.parametric_functions as NPF
 import nnabla_rl.initializers as RI
 from nnabla_rl.models.atari.shared_functions import PPOSharedFunctionHead
 from nnabla_rl.models.v_function import VFunction
 
 
 class PPOVFunction(VFunction):
-    """Shared parameter function proposed used in PPO paper for atari
-    environment.
-
+    '''
+    Shared parameter function proposed used in PPO paper for atari environment.
     This network outputs the value
     See: https://arxiv.org/pdf/1707.06347.pdf
-    """
+    '''
 
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _head: PPOSharedFunctionHead
 
     def __init__(self, head: PPOSharedFunctionHead, scope_name: str):
@@ -46,19 +45,18 @@
         return v
 
     def _hidden(self, s: nn.Variable) -> nn.Variable:
         return self._head(s)
 
 
 class A3CVFunction(VFunction):
-    """Shared parameter function proposed and used in A3C paper for atari
-    environment.
-
+    '''
+    Shared parameter function proposed and used in A3C paper for atari environment.
     See: https://arxiv.org/pdf/1602.01783.pdf
-    """
+    '''
 
     def __init__(self, head, scope_name, state_shape):
         super(A3CVFunction, self).__init__(scope_name=scope_name)
         self._state_shape = state_shape
         self._head = head
 
     def v(self, s):
```

## nnabla_rl/models/classic_control/policies.py

```diff
@@ -1,40 +1,38 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from typing import Union, cast
-
 import numpy as np
 
 import nnabla as nn
 import nnabla.functions as NF
 import nnabla.parametric_functions as NPF
 import nnabla_rl.distributions as D
 import nnabla_rl.initializers as RI
 from nnabla_rl.models.policy import StochasticPolicy
 
 
 class REINFORCEDiscretePolicy(StochasticPolicy):
-    """REINFORCE policy for classic control discrete environment.
-
+    '''
+    REINFORCE policy for classic control discrete environment.
     This network outputs the policy distribution.
     See: http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-5.pdf
-    """
+    '''
 
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _action_dim: int
 
     def __init__(self, scope_name: str, action_dim: int):
@@ -52,41 +50,40 @@
             z = NPF.affine(h, n_outmaps=self._action_dim,
                            name="linear3", w_init=RI.LeCunNormal(s.shape[1], 200))
 
         return D.Softmax(z=z)
 
 
 class REINFORCEContinousPolicy(StochasticPolicy):
-    """REINFORCE policy for classic control continous environment.
-
+    '''
+    REINFORCE policy for classic control continous environment.
     This network outputs the policy distribution.
     See: http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-5.pdf
-    """
+    '''
 
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _action_dim: int
     _fixed_ln_var: np.ndarray
 
-    def __init__(self, scope_name: str, action_dim: int, fixed_ln_var: Union[np.ndarray, float]):
+    def __init__(self, scope_name: str, action_dim: int, fixed_ln_var: np.ndarray):
         super(REINFORCEContinousPolicy, self).__init__(scope_name=scope_name)
         self._action_dim = action_dim
         if np.isscalar(fixed_ln_var):
             self._fixed_ln_var = np.full(self._action_dim, fixed_ln_var)
         else:
-            self._fixed_ln_var = cast(np.ndarray, fixed_ln_var)
+            self._fixed_ln_var = fixed_ln_var
 
     def pi(self, s: nn.Variable) -> nn.Variable:
         batch_size = s.shape[0]
         with nn.parameter_scope(self.scope_name):
             h = NPF.affine(s, n_outmaps=200, name="linear1",
                            w_init=RI.HeNormal(s.shape[1], 200))
             h = NF.leaky_relu(h)
             h = NPF.affine(h, n_outmaps=200, name="linear2",
                            w_init=RI.HeNormal(s.shape[1], 200))
             h = NF.leaky_relu(h)
             z = NPF.affine(h, n_outmaps=self._action_dim,
                            name="linear3", w_init=RI.HeNormal(s.shape[1], 200))
 
-        ln_var = nn.Variable.from_numpy_array(np.tile(self._fixed_ln_var, (batch_size, 1)))
-        return D.Gaussian(z, ln_var)
+        return D.Gaussian(z, np.tile(self._fixed_ln_var, (batch_size, 1)))
```

## nnabla_rl/models/mujoco/encoders.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -21,20 +21,19 @@
 import nnabla_rl.distributions as D
 import nnabla_rl.functions as RF
 from nnabla_rl.distributions import Distribution
 from nnabla_rl.models.encoder import VariationalAutoEncoder
 
 
 class UnsquashedVariationalAutoEncoder(VariationalAutoEncoder):
-    """Almost identical to BCQ style variational auto encoder proposed by S.
-
-    Fujimoto in BCQ paper for mujoco environment.
+    '''
+    Almost identical to BCQ style variational auto encoder proposed by S. Fujimoto in BCQ paper for mujoco environment.
     See: https://arxiv.org/pdf/1812.02900.pdf
     The main difference is that the output action is not squashed with tanh for computational convenience.
-    """
+    '''
 
     def __init__(self, scope_name, state_dim, action_dim, latent_dim):
         super(UnsquashedVariationalAutoEncoder, self).__init__(scope_name)
         self._state_dim = state_dim
         self._action_dim = action_dim
         self._latent_dim = latent_dim
 
@@ -105,19 +104,18 @@
             mean, ln_var = NF.split(reshaped, axis=1)
             # Clip for numerical stability
             ln_var = NF.clip_by_value(ln_var, min=-8, max=30)
         return D.Gaussian(mean, ln_var)
 
 
 class BCQVariationalAutoEncoder(UnsquashedVariationalAutoEncoder):
-    """BCQ style variational auto encoder proposed by S.
-
-    Fujimoto in BCQ paper for mujoco environment.
+    '''
+    BCQ style variational auto encoder proposed by S. Fujimoto in BCQ paper for mujoco environment.
     See: https://arxiv.org/pdf/1812.02900.pdf
-    """
+    '''
 
     def __init__(self, scope_name, state_dim, action_dim, latent_dim, max_action_value):
         super(BCQVariationalAutoEncoder, self).__init__(scope_name, state_dim, action_dim, latent_dim)
         self._max_action_value = max_action_value
 
     def decode(self, z: nn.Variable, **kwargs) -> nn.Variable:
         unsquashed = super(BCQVariationalAutoEncoder, self).decode(z, **kwargs)
```

## nnabla_rl/models/mujoco/perturbators.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -16,19 +16,18 @@
 import nnabla as nn
 import nnabla.functions as NF
 import nnabla.parametric_functions as NPF
 from nnabla_rl.models.perturbator import Perturbator
 
 
 class BCQPerturbator(Perturbator):
-    """Perturbator model proposed by S.
-
-    Fujimoto in BCQ paper for mujoco environment.
+    '''
+    Perturbator model proposed by S. Fujimoto in BCQ paper for mujoco environment.
     See: https://arxiv.org/abs/1812.02900
-    """
+    '''
 
     def __init__(self, scope_name, state_dim, action_dim, max_action_value):
         super(BCQPerturbator, self).__init__(scope_name)
         self._state_dim = state_dim
         self._action_dim = action_dim
         self._max_action_value = max_action_value
```

## nnabla_rl/models/mujoco/policies.py

```diff
@@ -1,43 +1,40 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from typing import Tuple
-
 import numpy as np
 
 import nnabla as nn
 import nnabla.functions as NF
 import nnabla.initializer as NI
 import nnabla.parametric_functions as NPF
 import nnabla_rl.distributions as D
 import nnabla_rl.initializers as RI
 from nnabla.parameter import get_parameter_or_create
 from nnabla_rl.distributions.distribution import Distribution
 from nnabla_rl.models.policy import DeterministicPolicy, StochasticPolicy
 
 
 class TD3Policy(DeterministicPolicy):
-    """Actor model proposed by S.
-
-    Fujimoto in TD3 paper for mujoco environment.
+    '''
+    Actor model proposed by S. Fujimoto in TD3 paper for mujoco environment.
     See: https://arxiv.org/abs/1802.09477
-    """
+    '''
 
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _action_dim: int
     _max_action_value: float
 
@@ -62,19 +59,18 @@
                 inmaps=300, outmaps=self._action_dim, factor=1/3)
             h = NPF.affine(h, n_outmaps=self._action_dim, name="linear3",
                            w_init=linear3_init, b_init=linear3_init)
         return NF.tanh(h) * self._max_action_value
 
 
 class SACPolicy(StochasticPolicy):
-    """Actor model proposed by T.
-
-    Haarnoja in SAC paper for mujoco environment.
+    '''
+    Actor model proposed by T. Haarnoja in SAC paper for mujoco environment.
     See: https://arxiv.org/pdf/1801.01290.pdf
-    """
+    '''
 
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _action_dim: int
     _clip_log_sigma: bool
     _min_log_sigma: float
@@ -107,19 +103,18 @@
                 ln_sigma = NF.clip_by_value(
                     ln_sigma, min=self._min_log_sigma, max=self._max_log_sigma)
             ln_var = ln_sigma * 2.0
         return D.SquashedGaussian(mean=mean, ln_var=ln_var)
 
 
 class BEARPolicy(StochasticPolicy):
-    """Actor model proposed by A.
-
-    Kumar, et al. in BEAR paper for mujoco environment.
+    '''
+    Actor model proposed by A. Kumar, et al. in BEAR paper for mujoco environment.
     See: https://arxiv.org/pdf/1906.00949.pdf
-    """
+    '''
 
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _action_dim: int
 
     def __init__(self, scope_name: str, action_dim: int):
@@ -146,20 +141,19 @@
             mean, ln_var = NF.split(reshaped, axis=1)
             assert mean.shape == ln_var.shape
             assert mean.shape == (s.shape[0], self._action_dim)
         return D.Gaussian(mean=mean, ln_var=ln_var)
 
 
 class PPOPolicy(StochasticPolicy):
-    """Actor model proposed by John Schulman, et al.
-
-    in PPO paper for mujoco environment.
+    '''
+    Actor model proposed by John Schulman, et al. in PPO paper for mujoco environment.
     This network outputs the policy distribution
     See: https://arxiv.org/pdf/1707.06347.pdf
-    """
+    '''
 
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _action_dim: int
 
     def __init__(self, scope_name: str, action_dim: int):
@@ -182,19 +176,18 @@
                 ln_sigma, (s.shape[0], self._action_dim)) * 2.0
             assert mean.shape == ln_var.shape
             assert mean.shape == (s.shape[0], self._action_dim)
         return D.Gaussian(mean=mean, ln_var=ln_var)
 
 
 class ICML2015TRPOPolicy(StochasticPolicy):
-    """Actor model proposed by John Schulman, et al.
-
-    in TRPO paper for mujoco environment.
+    '''
+    Actor model proposed by John Schulman, et al. in TRPO paper for mujoco environment.
     See: https://arxiv.org/pdf/1502.05477.pdf (Original paper)
-    """
+    '''
 
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _action_dim: int
 
     def __init__(self, scope_name: str, action_dim: int):
@@ -212,19 +205,19 @@
             assert mean.shape == ln_sigma.shape
             assert mean.shape == (s.shape[0], self._action_dim)
             ln_var = ln_sigma * 2.0
         return D.Gaussian(mean=mean, ln_var=ln_var)
 
 
 class TRPOPolicy(StochasticPolicy):
-    """Actor model proposed by Peter Henderson, et al.
-
+    '''
+    Actor model proposed by Peter Henderson, et al.
     in Deep Reinforcement Learning that Matters paper for mujoco environment.
     See: https://arxiv.org/abs/1709.06560.pdf
-    """
+    '''
 
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _action_dim: int
 
     def __init__(self, scope_name: str, action_dim: int):
@@ -246,57 +239,19 @@
             ln_sigma = get_parameter_or_create(
                 "ln_sigma", shape=(1, self._action_dim), initializer=NI.ConstantInitializer(0.))
             ln_var = NF.broadcast(
                 ln_sigma, (s.shape[0], self._action_dim)) * 2.0
         return D.Gaussian(mean, ln_var)
 
 
-class ATRPOPolicy(StochasticPolicy):
-    """Actor model proposed by Yiming Zhang, et al.
-
-    in On-Policy Deep Reinforcement Learning for the Average-Reward Criterion
-    See: https://arxiv.org/pdf/2106.07329.pdf
-    """
-
-    # type declarations to type check with mypy
-    # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
-    # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
-    _action_dim: int
-
-    def __init__(self, scope_name: str, action_dim: int):
-        super(ATRPOPolicy, self).__init__(scope_name)
-        self._action_dim = action_dim
-
-    def pi(self, s: nn.Variable) -> Distribution:
-        with nn.parameter_scope(self.scope_name):
-            h = NPF.affine(s, n_outmaps=64, name="linear1",
-                           w_init=RI.HeUniform(inmaps=64, outmaps=64, factor=1./3.),
-                           b_init=RI.HeUniform(inmaps=64, outmaps=64, factor=1./3.))
-            h = NF.tanh(x=h)
-            h = NPF.affine(h, n_outmaps=64, name="linear2",
-                           w_init=RI.HeUniform(inmaps=64, outmaps=64, factor=1./3.),
-                           b_init=RI.HeUniform(inmaps=64, outmaps=64, factor=1./3.))
-            h = NF.tanh(x=h)
-            mean = NPF.affine(h, n_outmaps=self._action_dim, name="linear3",
-                              w_init=RI.HeUniform(inmaps=64, outmaps=self._action_dim, factor=0.01/3.),
-                              b_init=NI.ConstantInitializer(0.))
-            assert mean.shape == (s.shape[0], self._action_dim)
-
-            ln_sigma = get_parameter_or_create(
-                "ln_sigma", shape=(1, self._action_dim), initializer=NI.ConstantInitializer(-0.5))
-            ln_var = NF.broadcast(
-                ln_sigma, (s.shape[0], self._action_dim)) * 2.0
-        return D.Gaussian(mean, ln_var)
-
-
 class GAILPolicy(StochasticPolicy):
-    """Actor model proposed by Jonathan Ho, et al.
-
+    '''
+    Actor model proposed by Jonathan Ho, et al.
     See: https://arxiv.org/pdf/1606.03476.pdf
-    """
+    '''
 
     def __init__(self, scope_name: str, action_dim: str):
         super(GAILPolicy, self).__init__(scope_name)
         self._action_dim = action_dim
 
     def pi(self, s: nn.Variable) -> Distribution:
         with nn.parameter_scope(self.scope_name):
@@ -311,81 +266,7 @@
             assert mean.shape == (s.shape[0], self._action_dim)
 
             ln_sigma = get_parameter_or_create(
                 "ln_sigma", shape=(1, self._action_dim), initializer=NI.ConstantInitializer(0.))
             ln_var = NF.broadcast(
                 ln_sigma, (s.shape[0], self._action_dim)) * 2.0
         return D.Gaussian(mean, ln_var)
-
-
-class HERPolicy(DeterministicPolicy):
-    def __init__(self, scope_name: str, action_dim: int, max_action_value: float):
-        super(HERPolicy, self).__init__(scope_name)
-        self._action_dim = action_dim
-        self._max_action_value = max_action_value
-
-    def pi(self, s: Tuple[nn.Variable, nn.Variable, nn.Variable]) -> nn.Variable:
-        # s = (observation, goal, achieved_goal)
-        obs, goal, _ = s
-        with nn.parameter_scope(self.scope_name):
-            h = NF.concatenate(obs, goal, axis=1)
-            linear1_init = RI.GlorotUniform(inmaps=h.shape[1], outmaps=64)
-            h = NPF.affine(h, n_outmaps=64, name='linear1', w_init=linear1_init)
-            h = NF.relu(h)
-            linear2_init = RI.GlorotUniform(inmaps=h.shape[1], outmaps=64)
-            h = NPF.affine(h, n_outmaps=64, name='linear2', w_init=linear2_init)
-            h = NF.relu(h)
-            linear3_init = RI.GlorotUniform(inmaps=h.shape[1], outmaps=64)
-            h = NPF.affine(h, n_outmaps=64, name='linear3', w_init=linear3_init)
-            h = NF.relu(h)
-            action_init = RI.GlorotUniform(inmaps=h.shape[1], outmaps=self._action_dim)
-            h = NPF.affine(h, n_outmaps=self._action_dim, name='action', w_init=action_init)
-        return NF.tanh(h) * self._max_action_value
-
-
-class XQLPolicy(StochasticPolicy):
-    """Actor model proposed by D.
-
-    Garg in XQL paper for offline mujoco environment.
-    See: https://arxiv.org/pdf/2301.02328.pdf
-    """
-
-    # type declarations to type check with mypy
-    # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
-    # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
-    _action_dim: int
-    _clip_log_sigma: bool
-    _min_log_sigma: float
-    _max_log_sigma: float
-
-    def __init__(self,
-                 scope_name: str,
-                 action_dim: int,
-                 clip_log_sigma: bool = True,
-                 min_log_sigma: float = -5.0,
-                 max_log_sigma: float = 2.0):
-        super(XQLPolicy, self).__init__(scope_name)
-        self._action_dim = action_dim
-        self._clip_log_sigma = clip_log_sigma
-        self._min_log_sigma = min_log_sigma
-        self._max_log_sigma = max_log_sigma
-
-    def pi(self, s: nn.Variable) -> Distribution:
-        w_init = NI.OrthogonalInitializer(np.sqrt(2.0))
-        with nn.parameter_scope(self.scope_name):
-            h = NPF.affine(s, n_outmaps=256, name="linear1", w_init=w_init)
-            h = NF.relu(x=h)
-            h = NPF.affine(h, n_outmaps=256, name="linear2", w_init=w_init)
-            h = NF.relu(x=h)
-            h = NPF.affine(h, n_outmaps=self._action_dim, name="linear3", w_init=w_init)
-            mean = NF.tanh(h)
-            # create parameter with shape (1, self._action_dim)
-            # because these parameters should be independent from states (and from batches also)
-            ln_sigma = get_parameter_or_create("ln_sigma", shape=(1, self._action_dim),
-                                               initializer=NI.ConstantInitializer(0.))
-            ln_sigma = NF.broadcast(ln_sigma, (s.shape[0], self._action_dim))
-            assert mean.shape == ln_sigma.shape
-            assert mean.shape == (s.shape[0], self._action_dim)
-            if self._clip_log_sigma:
-                ln_sigma = NF.clip_by_value(ln_sigma, min=self._min_log_sigma, max=self._max_log_sigma)
-            ln_var = ln_sigma * 2.0
-        return D.Gaussian(mean=mean, ln_var=ln_var)
```

## nnabla_rl/models/mujoco/q_functions.py

```diff
@@ -1,41 +1,37 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from typing import Optional, Tuple
-
-import numpy as np
+from typing import Optional
 
 import nnabla as nn
 import nnabla.functions as NF
-import nnabla.initializer as NI
 import nnabla.parametric_functions as NPF
 import nnabla_rl.initializers as RI
 from nnabla_rl.models.policy import DeterministicPolicy
-from nnabla_rl.models.q_function import ContinuousQFunction, FactoredContinuousQFunction
+from nnabla_rl.models.q_function import ContinuousQFunction
 
 
 class TD3QFunction(ContinuousQFunction):
-    """Critic model proposed by S.
-
-    Fujimoto in TD3 paper for mujoco environment.
+    '''
+    Critic model proposed by S. Fujimoto in TD3 paper for mujoco environment.
     See: https://arxiv.org/abs/1802.09477
-    """
+    '''
 
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _optimal_policy: Optional[DeterministicPolicy]
 
     def __init__(self, scope_name: str, optimal_policy: Optional[DeterministicPolicy] = None):
@@ -64,19 +60,18 @@
     def max_q(self, s: nn.Variable) -> nn.Variable:
         assert self._optimal_policy, 'Optimal policy is not set!'
         optimal_action = self._optimal_policy.pi(s)
         return self.q(s, optimal_action)
 
 
 class SACQFunction(ContinuousQFunction):
-    """QFunciton model proposed by T.
-
-    Haarnoja in SAC paper for mujoco environment.
+    '''
+    QFunciton model proposed by T. Haarnoja in SAC paper for mujoco environment.
     See: https://arxiv.org/pdf/1801.01290.pdf
-    """
+    '''
 
     # type declarations to type check with mypy
     # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
     # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
     _optimal_policy: Optional[DeterministicPolicy]
 
     def __init__(self, scope_name, optimal_policy: Optional[DeterministicPolicy] = None):
@@ -93,110 +88,7 @@
             h = NPF.affine(h, n_outmaps=1, name="linear3")
         return h
 
     def max_q(self, s: nn.Variable) -> nn.Variable:
         assert self._optimal_policy, 'Optimal policy is not set!'
         optimal_action = self._optimal_policy.pi(s)
         return self.q(s, optimal_action)
-
-
-class SACDQFunction(FactoredContinuousQFunction):
-    """Factored QFunciton model proposed by J.
-
-    MacGlashan in SAC-D paper for mujoco environment.
-    See: https://arxiv.org/abs/2206.13901
-    """
-
-    # type declarations to type check with mypy
-    # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
-    # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
-    _num_factors: int
-    _optimal_policy: Optional[DeterministicPolicy]
-
-    def __init__(self, scope_name, num_factors: int, optimal_policy: Optional[DeterministicPolicy] = None):
-        super(SACDQFunction, self).__init__(scope_name)
-        self._num_factors = num_factors
-        self._optimal_policy = optimal_policy
-
-    def q(self, s: nn.Variable, a: nn.Variable) -> nn.Variable:
-        return NF.sum(self.factored_q(s, a), axis=1, keepdims=True)
-
-    def factored_q(self, s: nn.Variable, a: nn.Variable) -> nn.Variable:
-        with nn.parameter_scope(self.scope_name):
-            h = NF.concatenate(s, a)
-            h = NPF.affine(h, n_outmaps=256, name="linear1")
-            h = NF.relu(x=h)
-            h = NPF.affine(h, n_outmaps=256, name="linear2")
-            h = NF.relu(x=h)
-            h = NPF.affine(h, n_outmaps=self._num_factors, name="linear3")
-        return h
-
-    def max_q(self, s: nn.Variable) -> nn.Variable:
-        assert self._optimal_policy, 'Optimal policy is not set!'
-        optimal_action = self._optimal_policy.pi(s)
-        return self.q(s, optimal_action)
-
-    @property
-    def num_factors(self) -> int:
-        return self._num_factors
-
-
-class HERQFunction(ContinuousQFunction):
-    def __init__(self, scope_name: str, optimal_policy: Optional[DeterministicPolicy] = None):
-        super(HERQFunction, self).__init__(scope_name)
-        self._optimal_policy = optimal_policy
-
-    def q(self, s: Tuple[nn.Variable, nn.Variable, nn.Variable], a: nn.Variable) -> nn.Variable:
-        obs, goal, _ = s
-        with nn.parameter_scope(self.scope_name):
-            h = NF.concatenate(obs, goal, a, axis=1)
-            linear1_init = RI.GlorotUniform(inmaps=h.shape[1], outmaps=64)
-            h = NPF.affine(h, n_outmaps=64, name='linear1', w_init=linear1_init)
-            h = NF.relu(h)
-            linear2_init = RI.GlorotUniform(inmaps=h.shape[1], outmaps=64)
-            h = NPF.affine(h, n_outmaps=64, name='linear2', w_init=linear2_init)
-            h = NF.relu(h)
-            linear3_init = RI.GlorotUniform(inmaps=h.shape[1], outmaps=64)
-            h = NPF.affine(h, n_outmaps=64, name='linear3', w_init=linear3_init)
-            h = NF.relu(h)
-            pred_q_init = RI.GlorotUniform(inmaps=h.shape[1], outmaps=1)
-            q = NPF.affine(h, n_outmaps=1, name='pred_q', w_init=pred_q_init)
-        return q
-
-    def max_q(self, s: nn.Variable) -> nn.Variable:
-        assert self._optimal_policy, 'Optimal policy is not set!'
-        optimal_action = self._optimal_policy.pi(s)
-        return self.q(s, optimal_action)
-
-
-class XQLQFunction(ContinuousQFunction):
-    """QFunction model used in the training of XQL.
-
-    Used by D. Garg et al. for experiments in mujoco environment.
-    Same as the SACQFunction except for the initializer.
-    See: https://github.com/Div99/XQL
-    """
-
-    # type declarations to type check with mypy
-    # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
-    # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
-    _optimal_policy: Optional[DeterministicPolicy]
-
-    def __init__(self, scope_name, optimal_policy: Optional[DeterministicPolicy] = None):
-        super(XQLQFunction, self).__init__(scope_name)
-        self._optimal_policy = optimal_policy
-
-    def q(self, s: nn.Variable, a: nn.Variable) -> nn.Variable:
-        w_init = NI.OrthogonalInitializer(np.sqrt(2.0))
-        with nn.parameter_scope(self.scope_name):
-            h = NF.concatenate(s, a)
-            h = NPF.affine(h, n_outmaps=256, name="linear1", w_init=w_init)
-            h = NF.relu(x=h)
-            h = NPF.affine(h, n_outmaps=256, name="linear2", w_init=w_init)
-            h = NF.relu(x=h)
-            h = NPF.affine(h, n_outmaps=1, name="linear3", w_init=w_init)
-        return h
-
-    def max_q(self, s: nn.Variable) -> nn.Variable:
-        assert self._optimal_policy, 'Optimal policy is not set!'
-        optimal_action = self._optimal_policy.pi(s)
-        return self.q(s, optimal_action)
```

## nnabla_rl/models/mujoco/reward_functions.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -17,19 +17,18 @@
 import nnabla.functions as NF
 import nnabla.parametric_functions as NPF
 import nnabla_rl.initializers as RI
 from nnabla_rl.models.reward_function import RewardFunction
 
 
 class GAILDiscriminator(RewardFunction):
-    """Discriminator model used as reward function proposed by Jonathan Ho, et
-    al.
-
+    '''
+    discriminator model used as reward function proposed by Jonathan Ho, et al.
     See: https://arxiv.org/pdf/1606.03476.pdf
-    """
+    '''
 
     def __init__(self, scope_name: str):
         super(GAILDiscriminator, self).__init__(scope_name)
 
     def r(self, s_current: nn.Variable, a_current: nn.Variable, s_next: nn.Variable) -> nn.Variable:
         '''
         Notes:
```

## nnabla_rl/models/mujoco/v_functions.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -20,36 +20,35 @@
 import nnabla.initializer as NI
 import nnabla.parametric_functions as NPF
 import nnabla_rl.initializers as RI
 from nnabla_rl.models.v_function import VFunction
 
 
 class SACVFunction(VFunction):
-    """VFunciton model proposed by T.
-
-    Haarnoja in SAC paper for mujoco environment.
+    '''
+    VFunciton model proposed by T. Haarnoja in SAC paper for mujoco environment.
     See: https://arxiv.org/pdf/1801.01290.pdf
-    """
+    '''
 
     def v(self, s: nn.Variable) -> nn.Variable:
         with nn.parameter_scope(self.scope_name):
             h = NPF.affine(s, n_outmaps=256, name="linear1")
             h = NF.relu(x=h)
             h = NPF.affine(h, n_outmaps=256, name="linear2")
             h = NF.relu(x=h)
             h = NPF.affine(h, n_outmaps=1, name="linear3")
         return h
 
 
 class TRPOVFunction(VFunction):
-    """Vfunction proposed by Peter Henderson, et al.
-
+    '''
+    Vfunction proposed by Peter Henderson, et al.
     in Deep Reinforcement Learning that Matters paper for mujoco environment.
     See: https://arxiv.org/abs/1709.06560.pdf
-    """
+    '''
 
     def v(self, s: nn.Variable) -> nn.Variable:
         with nn.parameter_scope(self.scope_name):
             h = NPF.affine(s, n_outmaps=64, name="linear1",
                            w_init=NI.OrthogonalInitializer(np.sqrt(2.)))
             h = NF.tanh(x=h)
             h = NPF.affine(h, n_outmaps=64, name="linear2",
@@ -57,20 +56,19 @@
             h = NF.tanh(x=h)
             h = NPF.affine(h, n_outmaps=1, name="linear3",
                            w_init=NI.OrthogonalInitializer(np.sqrt(2.)))
         return h
 
 
 class PPOVFunction(VFunction):
-    """Shared parameter function proposed used in PPO paper for mujoco
-    environment.
-
+    '''
+    Shared parameter function proposed used in PPO paper for mujoco environment.
     This network outputs the state value
     See: https://arxiv.org/pdf/1707.06347.pdf
-    """
+    '''
 
     def v(self, s: nn.Variable) -> nn.Variable:
         with nn.parameter_scope(self.scope_name):
             with nn.parameter_scope("linear1"):
                 h = NPF.affine(s, n_outmaps=64,
                                w_init=RI.NormcInitializer(std=1.0))
             h = NF.tanh(x=h)
@@ -81,18 +79,18 @@
             with nn.parameter_scope("linear_v"):
                 v = NPF.affine(h, n_outmaps=1,
                                w_init=RI.NormcInitializer(std=1.0))
         return v
 
 
 class GAILVFunction(VFunction):
-    """Value function proposed by Jonathan Ho, et al.
-
+    '''
+    Value function proposed by Jonathan Ho, et al.
     See: https://arxiv.org/pdf/1606.03476.pdf
-    """
+    '''
 
     def __init__(self, scope_name: str):
         super(GAILVFunction, self).__init__(scope_name)
 
     def v(self, s: nn.Variable) -> nn.Variable:
         with nn.parameter_scope(self.scope_name):
             h = NPF.affine(s, n_outmaps=100, name="linear1",
@@ -100,55 +98,7 @@
             h = NF.tanh(x=h)
             h = NPF.affine(h, n_outmaps=100, name="linear2",
                            w_init=RI.NormcInitializer(std=1.0))
             h = NF.tanh(x=h)
             h = NPF.affine(h, n_outmaps=1, name="linear3",
                            w_init=RI.NormcInitializer(std=1.0))
         return h
-
-
-class ATRPOVFunction(VFunction):
-    """Actor model proposed by Yiming Zhang, et al.
-
-    in On-Policy Deep Reinforcement Learning for the Average-Reward Criterion
-    See: https://arxiv.org/pdf/2106.07329.pdf
-    """
-
-    def v(self, s: nn.Variable) -> nn.Variable:
-        with nn.parameter_scope(self.scope_name):
-            h = NPF.affine(s, n_outmaps=64, name="linear1",
-                           w_init=RI.HeUniform(inmaps=64, outmaps=64, factor=1./3.),
-                           b_init=RI.HeUniform(inmaps=64, outmaps=64, factor=1./3.))
-            h = NF.tanh(x=h)
-            h = NPF.affine(h, n_outmaps=64, name="linear2",
-                           w_init=RI.HeUniform(inmaps=64, outmaps=64, factor=1./3.),
-                           b_init=RI.HeUniform(inmaps=64, outmaps=64, factor=1./3.))
-            h = NF.tanh(x=h)
-            h = NPF.affine(h, n_outmaps=1, name="linear3",
-                           w_init=RI.HeUniform(inmaps=64, outmaps=1, factor=0.01/3.),
-                           b_init=NI.ConstantInitializer(0.))
-        return h
-
-
-class XQLVFunction(VFunction):
-    """VFunciton model used in the training of XQL.
-
-    Used by D. Garg et al. for experiments in mujoco environment.
-    See: https://github.com/Div99/XQL
-    """
-
-    def v(self, s: nn.Variable) -> nn.Variable:
-        w_init = NI.OrthogonalInitializer(np.sqrt(2.0))
-        with nn.parameter_scope(self.scope_name):
-            with nn.parameter_scope('linear1'):
-                h = NPF.affine(s, n_outmaps=256, w_init=w_init)
-            with nn.parameter_scope('layer_norm1'):
-                h = NPF.layer_normalization(h, eps=1e-6)
-            h = NF.relu(x=h)
-            with nn.parameter_scope('linear2'):
-                h = NPF.affine(h, n_outmaps=256, w_init=w_init)
-            with nn.parameter_scope('layer_norm2'):
-                h = NPF.layer_normalization(h, eps=1e-6)
-            h = NF.relu(x=h)
-            with nn.parameter_scope('linear3'):
-                h = NPF.affine(h, n_outmaps=1, w_init=w_init)
-        return h
```

## nnabla_rl/preprocessors/__init__.py

```diff
@@ -11,8 +11,7 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from nnabla_rl.preprocessors.preprocessor import Preprocessor  # noqa
 from nnabla_rl.preprocessors.running_mean_normalizer import RunningMeanNormalizer  # noqa
-from nnabla_rl.preprocessors.her_preprocessor import HERPreprocessor  # noqa
```

## nnabla_rl/preprocessors/running_mean_normalizer.py

```diff
@@ -1,101 +1,49 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023,2024 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from typing import Optional, Tuple, Union
-
 import numpy as np
 
 import nnabla as nn
+import nnabla.functions as NF
 import nnabla.initializer as NI
-from nnabla_rl.functions import compute_std, normalize
 from nnabla_rl.models.model import Model
 from nnabla_rl.preprocessors.preprocessor import Preprocessor
-from nnabla_rl.typing import Shape
 
 
 class RunningMeanNormalizer(Preprocessor, Model):
-    # type declarations to type check with mypy
-    # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
-    # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
-    _shape: Shape
-    """Running mean normalizer. This normalizer computes a running mean and \
-    variance from the data and use them to process the given varible.
-
-    Args:
-        scope_name (str): scope name of running mean normalizer's parameters
-        shape (Shape): shape of the variable which is normalized.
-        epsilon (float): value to improve numerical stability for computing the standard deviation. Defaults to 1e-8.
-        value_clip (Optional[Tuple[float, float]]): value clip. This clipping is applied after the normalization.
-        mode_for_floating_point_error (str): mode for avoiding a floating point error \
-            when computing the standard deviation from the variance to normalize the data. \
-            Must be one of:
-
-            - `add`: Use the square root of the sum of var and epsilon as the standard deviation.
-            - `max`: Use the epsilon if the square root of var is smaller than epsilon, \
-                otherwise it returns the square root of var as the standard deviation.
-
-            Defaults to add.
-        mean_initializer (Union[NI.BaseInitializer, np.ndarray]): initializer for normalizer's mean. \
-            The computation of a running mean is started from this value. Defaults to NI.ConstantInitializer(0.0).
-        var_initializer (Union[NI.BaseInitializer, np.ndarray]): initializer for normalizer's variance. \
-            The computation of a running variance is started from this value. Defaults to NI.ConstantInitializer(1.0).
-    """
-
-    def __init__(self,
-                 scope_name: str,
-                 shape: Shape,
-                 epsilon: float = 1e-8,
-                 value_clip: Optional[Tuple[float, float]] = None,
-                 mode_for_floating_point_error: str = "add",
-                 mean_initializer: Union[NI.BaseInitializer, np.ndarray] = NI.ConstantInitializer(0.0),
-                 var_initializer: Union[NI.BaseInitializer, np.ndarray] = NI.ConstantInitializer(1.0)):
+    def __init__(self, scope_name, shape, epsilon=1e-8, value_clip=None):
         super(RunningMeanNormalizer, self).__init__(scope_name)
 
         if value_clip is not None and value_clip[0] > value_clip[1]:
             raise ValueError(
-                f"Unexpected clipping value range: {value_clip[0]} > {value_clip[1]}")
+                f'Unexpected clipping value range: {value_clip[0]} > {value_clip[1]}')
         self._value_clip = value_clip
-
-        if isinstance(shape, int):
-            self._shape = (shape,)
-        elif isinstance(shape, tuple):
-            self._shape = shape
-        else:
-            raise ValueError
-
         self._epsilon = epsilon
-        self._mode_for_floating_point_error = mode_for_floating_point_error
-
-        if isinstance(mean_initializer, np.ndarray):
-            assert mean_initializer.shape == shape
-            mean_initializer = mean_initializer[np.newaxis, :]
-        self._mean_initializer = mean_initializer
-
-        if isinstance(var_initializer, np.ndarray):
-            assert var_initializer.shape == shape
-            var_initializer = var_initializer[np.newaxis, :]
-        self._var_initializer = var_initializer
+        self._shape = shape
 
     def process(self, x):
         assert 0 < self._count.d
-        std = compute_std(self._var, self._epsilon, self._mode_for_floating_point_error)
-        normalized = normalize(x, self._mean, std, self._value_clip)
+        std = (self._var + self._epsilon) ** 0.5
+        normalized = (x - self._mean) / std
+        if self._value_clip is not None:
+            normalized = NF.clip_by_value(
+                normalized, min=self._value_clip[0], max=self._value_clip[1])
         normalized.need_grad = False
         return normalized
 
     def update(self, data):
         avg_a = self._mean.d
         var_a = self._var.d
         n_a = self._count.d
@@ -114,20 +62,20 @@
         self._var.d = M2 / n_ab
         self._count.d = n_ab
 
     @property
     def _mean(self):
         with nn.parameter_scope(self.scope_name):
             return nn.parameter.get_parameter_or_create(name='mean', shape=(1, *self._shape),
-                                                        initializer=self._mean_initializer)
+                                                        initializer=NI.ConstantInitializer(0.0))
 
     @property
     def _var(self):
         with nn.parameter_scope(self.scope_name):
             return nn.parameter.get_parameter_or_create(name='var', shape=(1, *self._shape),
-                                                        initializer=self._var_initializer)
+                                                             initializer=NI.ConstantInitializer(1.0))
 
     @property
     def _count(self):
         with nn.parameter_scope(self.scope_name):
             return nn.parameter.get_parameter_or_create(name='count', shape=(1, 1),
                                                         initializer=NI.ConstantInitializer(1e-4))
```

## nnabla_rl/replay_buffers/__init__.py

```diff
@@ -1,27 +1,20 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from nnabla_rl.replay_buffers.buffer_iterator import BufferIterator  # noqa
-from nnabla_rl.replay_buffers.hindsight_replay_buffer import HindsightReplayBuffer  # noqa
-from nnabla_rl.replay_buffers.memory_efficient_atari_buffer import (MemoryEfficientAtariBuffer,  # noqa
-                                                                    MemoryEfficientAtariTrajectoryBuffer,
-                                                                    ProportionalPrioritizedAtariBuffer,
-                                                                    RankBasedPrioritizedAtariBuffer)
+from nnabla_rl.replay_buffers.memory_efficient_atari_buffer import MemoryEfficientAtariBuffer  # noqa
 from nnabla_rl.replay_buffers.decorable_replay_buffer import DecorableReplayBuffer  # noqa
 from nnabla_rl.replay_buffers.replacement_sampling_replay_buffer import ReplacementSamplingReplayBuffer  # noqa
-from nnabla_rl.replay_buffers.prioritized_replay_buffer import (PrioritizedReplayBuffer,  # noqa
-                                                                ProportionalPrioritizedReplayBuffer,
-                                                                RankBasedPrioritizedReplayBuffer)
-from nnabla_rl.replay_buffers.trajectory_replay_buffer import TrajectoryReplayBuffer  # noqa
+from nnabla_rl.replay_buffers.prioritized_replay_buffer import PrioritizedReplayBuffer  # noqa
```

## nnabla_rl/replay_buffers/buffer_iterator.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -13,18 +13,18 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import numpy as np
 
 
 class BufferIterator(object):
-    """Simple iterator for iterating through the replay buffer.
-
+    '''
+    Simple iterator for iterating through the replay buffer.
     Replay buffer must support indexing.
-    """
+    '''
 
     def __init__(self, buffer, batch_size, shuffle=True, repeat=True):
         super(BufferIterator, self).__init__()
         self._replay_buffer = buffer
         self._new_epoch = False
         self._batch_size = batch_size
         self._shuffle = shuffle
@@ -52,22 +52,19 @@
                 self._index += rest
             else:
                 self._index = len(self._replay_buffer)
             self._new_epoch = True
         else:
             self._index += self._batch_size
             self._new_epoch = (len(self._replay_buffer) <= self._index)
-        return self._sample(indices)
+        return self._replay_buffer.sample_indices(indices)
 
     __next__ = next
 
     def is_new_epoch(self):
         return self._new_epoch
 
     def reset(self):
         self._indices = np.arange(len(self._replay_buffer))
         if self._shuffle:
             np.random.shuffle(self._indices)
         self._index = 0
-
-    def _sample(self, indices):
-        return self._replay_buffer.sample_indices(indices)
```

## nnabla_rl/replay_buffers/decorable_replay_buffer.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -13,22 +13,20 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from nnabla_rl.replay_buffer import ReplayBuffer
 
 
 class DecorableReplayBuffer(ReplayBuffer):
-    """Buffer which can decorate the experience with external decoration
-    function.
+    '''Buffer which can decorate the experience with external decoration function
 
-    This buffer enables decorating the experience before the item is
-    used for building the batch. Decoration function will be called when
-    __getitem__ is called. You can use this buffer to augment the data
-    or add noise to the experience.
-    """
+    This buffer enables decorating the experience before the item is used for building the batch.
+    Decoration function will be called when __getitem__ is called.
+    You can use this buffer to augment the data or add noise to the experience.
+    '''
 
     def __init__(self, capacity, decor_fun):
         super(DecorableReplayBuffer, self).__init__(capacity=capacity)
         self._decor_fun = decor_fun
 
     def __getitem__(self, item):
         experience = self._buffer[item]
```

## nnabla_rl/replay_buffers/memory_efficient_atari_buffer.py

```diff
@@ -1,258 +1,101 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from collections import deque
-from typing import Optional, Sequence, Tuple, cast
 
 import numpy as np
 
 from nnabla_rl.replay_buffer import ReplayBuffer
-from nnabla_rl.replay_buffers.prioritized_replay_buffer import (ProportionalPrioritizedReplayBuffer,
-                                                                RankBasedPrioritizedReplayBuffer)
-from nnabla_rl.replay_buffers.trajectory_replay_buffer import TrajectoryReplayBuffer
-from nnabla_rl.typing import Trajectory
 from nnabla_rl.utils.data import RingBuffer
 
 
 class MemoryEfficientAtariBuffer(ReplayBuffer):
-    """Buffer designed to compactly save experiences of Atari environments used
-    in DQN.
+    '''Buffer designed to compactly save experiences of Atari environments used in DQN.
 
-    DQN (and other training algorithms) requires large replay buffer
-    when training on Atari games. If you naively save the experiences,
-    you'll need more than 100GB to save them (assuming 1M experiences).
-    Which usually does not fit in the machine's memory (unless you have
-    money:). This replay buffer reduces the size of experience by
-    casting the images to uint8 and removing old frames concatenated to
-    the observation. By using this buffer, you can hold 1M experiences
-    using only 20GB(approx.) of memory. Note that this class is designed
-    only for DQN style training on atari environment. (i.e. State
-    consists of "stacked_frames" number of concatenated grayscaled
-    frames and its values are normalized between 0 and 1)
-    """
-    # type declarations to type check with mypy
-    # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
-    # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
-    _buffer: RingBuffer
-    _sub_buffer: deque
+    DQN (and other training algorithms) requires large replay buffer when training on Atari games.
+    If you naively save the experiences, you'll need more than 100GB to save them (assuming 1M experiences).
+    Which usually does not fit in the machine's memory (unless you have money:).
+    This replay buffer reduces the size of experience by casting the images to uint8 and
+    removing old frames concatenated to the observation.
+    By using this buffer, you can hold 1M experiences using only 20GB(approx.) of memory.
+
+    Note that this class is designed only for DQN style training on atari environment.
+    (i.e. State consists of 4 concatenated grayscaled frames and its values are normalized between 0 and 1)
+    '''
 
-    def __init__(self, capacity: int, stacked_frames: int = 4):
+    def __init__(self, capacity):
         super(MemoryEfficientAtariBuffer, self).__init__(capacity=capacity)
+        self._scale = 255.0
         self._reset = True
         self._buffer = RingBuffer(maxlen=capacity)
-        self._sub_buffer = deque(maxlen=stacked_frames-1)
-        self._stacked_frames = stacked_frames
+        self._sub_buffer = deque(maxlen=3)
 
     def append(self, experience):
-        self._reset = _append_to_buffer(experience, self._buffer, self._sub_buffer, self._reset)
-
-    def __getitem__(self, index: int):
-        return _getitem_from_buffer(index, self._buffer, self._sub_buffer, self._stacked_frames)
-
-
-class _LazyAtariTrajectory(object):
-    def __init__(self, buffer: MemoryEfficientAtariBuffer):
-        self._buffer = buffer
-
-    def __len__(self):
-        return len(self._buffer)
-
-    def __getitem__(self, key):
-        if isinstance(key, slice):
-            return [self._buffer[i] for i in range(key.stop)[key]]
-        elif isinstance(key, int):
-            return self._buffer[key]
-        else:
-            raise TypeError('Invalid key type')
-
-
-class MemoryEfficientAtariTrajectoryBuffer(TrajectoryReplayBuffer):
-    def __init__(self, num_trajectories=None):
-        super(MemoryEfficientAtariTrajectoryBuffer, self).__init__(num_trajectories)
-
-    def append_trajectory(self, trajectory: Trajectory):
-        # Use memory efficient atari buffer to save the trajectory efficiently
-        atari_buffer = MemoryEfficientAtariBuffer(capacity=len(trajectory))
-        atari_buffer.append_all(trajectory)
-
-        self._buffer.append(atari_buffer)
-
-        # Below is the same as super class' code
-        self._samples_per_trajectory.append(len(trajectory))
-        num_experiences = 0
-        cumsum_experiences = []
-        for i in range(self.trajectory_num):
-            num_experiences += self._samples_per_trajectory[i]
-            cumsum_experiences.append(num_experiences)
-        self._num_experiences = num_experiences
-        self._cumsum_experiences = cumsum_experiences
-
-    def get_trajectory(self, trajectory_index: int) -> Trajectory:
-        return self._buffer_to_trajectory(self._get_atari_buffer(trajectory_index))
-
-    def sample(self, num_samples: int = 1, num_steps: int = 1):
-        raise NotImplementedError
-
-    def sample_indices(self, indices: Sequence[int], num_steps: int = 1):
-        raise NotImplementedError
-
-    def _buffer_to_trajectory(self, buffer: MemoryEfficientAtariBuffer) -> Trajectory:
-        return cast(Trajectory, _LazyAtariTrajectory(buffer))
-
-    def _get_atari_buffer(self, trajectory_index: int) -> MemoryEfficientAtariBuffer:
-        return cast(MemoryEfficientAtariBuffer, self._buffer[trajectory_index])
-
-
-class ProportionalPrioritizedAtariBuffer(ProportionalPrioritizedReplayBuffer):
-    """Prioritized buffer designed to compactly save experiences of Atari
-    environments used in DQN.
-
-    Proportional Prioritized version of efficient Atari buffer. Note
-    that this class is designed only for DQN style training on atari
-    environment. (i.e. State consists of "stacked_frames" number of
-    concatenated grayscaled frames and its values are normalized between
-    0 and 1)
-    """
-    # type declarations to type check with mypy
-    # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
-    # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
-    _sub_buffer: deque
-
-    def __init__(self,
-                 capacity: int,
-                 alpha: float = 0.6,
-                 beta: float = 0.4,
-                 betasteps: int = 50000000,
-                 error_clip: Optional[Tuple[float, float]] = (-1, 1),
-                 epsilon: float = 1e-8,
-                 normalization_method: str = "buffer_max",
-                 stacked_frames: int = 4):
-        super(ProportionalPrioritizedAtariBuffer, self).__init__(capacity=capacity,
-                                                                 alpha=alpha,
-                                                                 beta=beta,
-                                                                 betasteps=betasteps,
-                                                                 error_clip=error_clip,
-                                                                 epsilon=epsilon,
-                                                                 normalization_method=normalization_method)
-        self._reset = True
-        self._sub_buffer = deque(maxlen=stacked_frames-1)
-        self._stacked_frames = stacked_frames
-
-    def append(self, experience):
-        self._reset = _append_to_buffer(experience, self._buffer, self._sub_buffer, self._reset)
-
-    def __getitem__(self, index: int):
-        return _getitem_from_buffer(index, self._buffer, self._sub_buffer, self._stacked_frames)
-
-
-class RankBasedPrioritizedAtariBuffer(RankBasedPrioritizedReplayBuffer):
-    """Prioritized buffer designed to compactly save experiences of Atari
-    environments used in DQN.
-
-    RankBased Prioritized version of efficient Atari buffer. Note that
-    this class is designed only for DQN style training on atari
-    environment. (i.e. State consists of "stacked_frames" number of
-    concatenated grayscaled frames and its values are normalized between
-    0 and 1)
-    """
-    # type declarations to type check with mypy
-    # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
-    # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
-    _sub_buffer: deque
-
-    def __init__(self,
-                 capacity: int,
-                 alpha: float = 0.7,
-                 beta: float = 0.5,
-                 betasteps: int = 50000000,
-                 error_clip: Optional[Tuple[float, float]] = (-1, 1),
-                 reset_segment_interval: int = 1000,
-                 sort_interval: int = 1000000,
-                 stacked_frames: int = 4):
-        super(RankBasedPrioritizedAtariBuffer, self).__init__(capacity=capacity,
-                                                              alpha=alpha,
-                                                              beta=beta,
-                                                              betasteps=betasteps,
-                                                              error_clip=error_clip,
-                                                              reset_segment_interval=reset_segment_interval,
-                                                              sort_interval=sort_interval)
-        self._reset = True
-        self._sub_buffer = deque(maxlen=stacked_frames-1)
-        self._stacked_frames = stacked_frames
-
-    def append(self, experience):
-        self._reset = _append_to_buffer(experience, self._buffer, self._sub_buffer, self._reset)
-
-    def __getitem__(self, index: int):
-        return _getitem_from_buffer(index, self._buffer, self._sub_buffer, self._stacked_frames)
-
-
-def _denormalize_state(state, scalar=255.0):
-    return (state * scalar).astype(np.uint8)
-
-
-def _normalize_state(state, scalar=255.0):
-    return state.astype(np.float32) / scalar
-
-
-def _is_float(state):
-    return np.issubdtype(state.dtype, np.floating)
-
+        s, a, r, non_terminal, s_next, *_ = experience
+        if not self._is_float(s):
+            raise ValueError('dtype {} is not supported'.format(s.dtype))
+        if not self._is_float(s_next):
+            raise ValueError('dtype {} is not supported'.format(s_next.dtype))
+
+        # cast to uint8 and use only the last image to reduce memory
+        s = self._denormalize_state(s[-1])
+        s_next = self._denormalize_state(s_next[-1])
+        s = np.array(s, copy=True, dtype=np.uint8)
+        s_next = np.array(s_next, copy=True, dtype=np.uint8)
+        assert s.shape == (84, 84)
+        assert s.shape == s_next.shape
+        experience = (s, a, r, non_terminal, s_next, self._reset)
+        removed = self._buffer.append_with_removed_item_check(experience)
+        if removed is not None:
+            self._sub_buffer.append(removed)
+        self._reset = (0 == non_terminal)
+
+    def append_all(self, experiences):
+        for experience in experiences:
+            self.append(experience)
+
+    def __getitem__(self, index):
+        (_, a, r, non_terminal, s_next, _) = self._buffer[index]
+        states = np.empty(shape=(4, 84, 84), dtype=np.uint8)
+        for i in range(0, 4):
+            buffer_index = index - i
+            if 0 <= buffer_index:
+                (s, _, _, _, _, reset) = self._buffer[buffer_index]
+            else:
+                (s, _, _, _, _, reset) = self._sub_buffer[buffer_index]
+            assert s.shape == (84, 84)
+            tail_index = 4-i
+            if reset:
+                states[0:tail_index] = s
+                break
+            else:
+                states[tail_index-1] = s
+        s = self._normalize_state(states)
+        assert s.shape == (4, 84, 84)
 
-def _append_to_buffer(experience, buffer, sub_buffer, reset_flag):
-    s, a, r, non_terminal, s_next, info, *_ = experience
-    if s.shape != (84, 84):
-        # Use only the last image to reduce memory
-        s = s[-1]
-        s_next = s_next[-1]
-    if _is_float(s):
-        s = _denormalize_state(s)
-    if _is_float(s_next):
-        s_next = _denormalize_state(s_next)
-    assert s.shape == (84, 84)
-    assert s.shape == s_next.shape
+        s_next = np.expand_dims(s_next, axis=0)
+        s_next = self._normalize_state(s_next)
+        s_next = np.concatenate((s[1:], s_next), axis=0)
+        assert s.shape == s_next.shape
+        return (s, a, r, non_terminal, s_next)
 
-    experience = (s, a, r, non_terminal, s_next, info, reset_flag)
-    removed = buffer.append_with_removed_item_check(experience)
-    if removed is not None:
-        sub_buffer.append(removed)
-    return (0 == non_terminal)
+    def _denormalize_state(self, state):
+        return (state * self._scale).astype(np.uint8)
 
+    def _normalize_state(self, state):
+        return state.astype(np.float32) / self._scale
 
-def _getitem_from_buffer(index, buffer, sub_buffer, stacked_frames):
-    (_, a, r, non_terminal, s_next, info, _) = buffer[index]
-    states = np.zeros(shape=(stacked_frames, 84, 84), dtype=np.uint8)
-    for i in range(0, stacked_frames):
-        buffer_index = index - i
-        if 0 <= buffer_index:
-            (s, _, _, _, _, _, reset) = buffer[buffer_index]
-        else:
-            (s, _, _, _, _, _, reset) = sub_buffer[buffer_index]
-        assert s.shape == (84, 84)
-        tail_index = stacked_frames-i
-        if reset:
-            states[0:tail_index] = s
-            break
-        else:
-            states[tail_index-1] = s
-    s = _normalize_state(states)
-    assert s.shape == (stacked_frames, 84, 84)
-
-    s_next = np.expand_dims(s_next, axis=0)
-    s_next = _normalize_state(s_next)
-    if 1 < stacked_frames:
-        s_next = np.concatenate((s[1:], s_next), axis=0)
-    assert s.shape == s_next.shape
-    return (s, a, r, non_terminal, s_next, info)
+    def _is_float(self, state):
+        return np.issubdtype(state.dtype, np.floating)
```

## nnabla_rl/replay_buffers/prioritized_replay_buffer.py

```diff
@@ -1,660 +1,176 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import math
-import sys
-from abc import abstractmethod
 from dataclasses import dataclass
-from typing import Any, ClassVar, Generic, List, Optional, Sequence, Tuple, TypeVar, Union, cast
 
 import numpy as np
 
-import nnabla_rl as rl
 from nnabla_rl.replay_buffer import ReplayBuffer
-from nnabla_rl.typing import Experience
-from nnabla_rl.utils.data import DataHolder, RingBuffer
 
-T = TypeVar('T')
-
-
-# NOTE: index naming convention used in this module
-# relative index: 0: oldest item's index. capacity - 1: newest item's index.
-# absolute index: actual data index in list. 0: list's head. capacity - 1: list's tail.
-# tree index: 0: root of the tree. 2 * capacity - 1: right most leaf of the tree.
-# heap index: 0: head of the heap. If max heap, maximum value is saved in this index. capacity - 1: tail of the heap.
 
 @dataclass
-class Node(Generic[T]):
-    value: T
+class Node:
     parent: int = -1
     left: int = 1
     right: int = 2
+    value: float = 0.0
 
 
-class BinaryTree(Generic[T]):
-    """Common Binary Tree Class SumTree and MinTree is derived from this class.
-
-    Args:
-        capacity (int): the maximum number of saved data.
-        init_node_value (T): the initial value of node.
-    """
-
-    # type declarations to type check with mypy
-    # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
-    # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
-    _tree: List[Node[T]]
-    _tail_index: int
-
-    def __init__(self, capacity: int, init_node_value: T):
+class SumTree(object):
+    def __init__(self, capacity, init_max_p=1.0):
         self._capacity = capacity
-        self._init_node_value = init_node_value
-        self._tail_index = 0
-        self._length = 0
-        self._tree = [self._make_init_node(i) for i in range(2*capacity-1)]
-
-    def __len__(self):
-        return self._length
-
-    def __getitem__(self, tree_index: int):
-        return self._tree[tree_index].value
 
-    def append(self, value: T):
-        self.update(self._tail_index, value)
-        self._tail_index = (self._tail_index + 1) % self._capacity
-        if self._length < self._capacity:
-            self._length += 1
-
-    def update(self, absolute_index: int, value: T):
-        tree_index = self.absolute_to_tree_index(absolute_index)
-        self._tree[tree_index].value = value
-        self._update_parent(tree_index)
-
-    @abstractmethod
-    def _update_parent(self, tree_index: int):
-        raise NotImplementedError
-
-    def tree_to_absolute_index(self, tree_index: int):
-        return tree_index - (self._capacity - 1)
-
-    def absolute_to_tree_index(self, absolute_index: int):
-        return absolute_index + self._capacity - 1
+        self._data = np.zeros(capacity, dtype=object)
+        self._tree = [self._make_init_node(i) for i in range(2*capacity-1)]
+        self._index = 0
+        self._data_num = 0
+        self._min_p = math.inf
+        self._max_p = init_max_p
+        self._latest_indices = None
 
-    def _make_init_node(self, index: int):
+    def _make_init_node(self, index):
         parent = (index - 1) // 2
         left = 2 * index + 1 if index < self._capacity - 1 else -1
         right = left + 1 if index < self._capacity - 1 else -1
-        value = self._init_node_value
-        return Node(value=value, parent=parent, left=left, right=right)
+        value = 0.
+        return Node(parent, left, right, value)
 
+    def append(self, data):
+        self._data[self._index] = data
+        self.update(self._index, self._max_p)
 
-class MinTree(BinaryTree[float]):
-    def __init__(self, capacity: int):
-        super(MinTree, self).__init__(capacity, init_node_value=math.inf)
+        self._index = (self._index + 1) % self._capacity
+        if self._data_num < self._capacity:
+            self._data_num += 1
+
+    def update(self, index, p):
+        tree_index = index + self._capacity - 1
+        change_p = p - self._tree[tree_index].value
+        self._tree[tree_index].value = float(p)
+        self._update_parent(tree_index, change_p)
+
+        self._min_p = min(self._min_p, p)
+        self._max_p = max(self._max_p, p)
+
+    def _update_parent(self, index, change_p):
+        if index > 0:
+            parent = self._tree[index].parent
+            self._tree[parent].value += change_p
+            self._update_parent(parent, change_p)
+
+    def sample(self, num_samples=1, beta=0.6):
+        random_values = np.random.uniform(0.0, self.total, size=num_samples)
+        indices = [self._get_data_index_from_query(v) for v in random_values]
+        return self.sample_indices(indices, beta)
 
-    def min(self):
-        return self._tree[0].value
+    def sample_indices(self, indices, beta=0.6):
+        if self._latest_indices is not None:
+            raise RuntimeError('Trying to sample data from buffer without updating priority. '
+                               'Check that the algorithm supports prioritized replay buffer.')
+        data = [self._data[i] for i in indices]
+        priorities = np.array([self._get_priority(i)
+                               for i in indices])[:, np.newaxis]
+        weights = self._weights_from_priorities(priorities, beta)
+        self._latest_indices = indices
+        return data, weights
 
-    def _update_parent(self, tree_index: int):
-        if tree_index > 0:
-            parent_index = self._tree[tree_index].parent
-            left_index = self._tree[parent_index].left
-            left_value = self._tree[left_index].value
-            right_index = self._tree[parent_index].right
-            right_value = self._tree[right_index].value
-            self._tree[parent_index].value = min(left_value, right_value)
-            self._update_parent(parent_index)
-
-
-class SumTree(BinaryTree[float]):
-    def __init__(self, capacity: int):
-        super(SumTree, self).__init__(capacity, init_node_value=0.0)
-
-    def get_absolute_index_from_query(self, query: float):
-        """Sample absolute index from query value."""
-        if query < 0 or query > self.sum():
-            raise ValueError(f"You must use value between [0, {self.sum()}] as query")
+    def _get_data_index_from_query(self, query):
         node = self._tree[0]
         while node.left >= 0:
             left_value = self._tree[node.left].value
             if query < left_value:
-                tree_index = node.left
+                index = node.left
             else:
-                tree_index = node.right
+                index = node.right
                 query -= left_value
-            node = self._tree[tree_index]
-        return self.tree_to_absolute_index(tree_index)
-
-    def sum(self):
-        return self._tree[0].value
-
-    def _update_parent(self, tree_index: int):
-        if tree_index > 0:
-            parent_index = self._tree[tree_index].parent
-            left_index = self._tree[parent_index].left
-            left_value = self._tree[left_index].value
-            right_index = self._tree[parent_index].right
-            right_value = self._tree[right_index].value
-            self._tree[parent_index].value = left_value + right_value
-            self._update_parent(parent_index)
-
-
-class MaxHeap(object):
-    def __init__(self, capacity):
-        self._capacity = capacity
-        self._heap = [None for _ in range(capacity)]
-        self._heap_to_absolute_index_map = [None for _ in range(capacity)]
-        self._absolute_to_heap_index_map = [None for _ in range(capacity)]
-
-        self._tail_index = 0
-        self._oldest_index = 0
-        self._length = 0
-
-    def __len__(self):
-        return self._length
-
-    def __getitem__(self, heap_index: int):
-        return self._heap[heap_index]
-
-    def append(self, value: float):
-        if len(self) == self._capacity:
-            # remove the oldest and replace with new data
-            # Reset the priority of oldest_index data to maximum
-            # We know that new data will be inserted there
-            self.update(self._oldest_index, value)
-            self._oldest_index = (self._oldest_index + 1) % self._capacity
-        else:
-            self._heappush(self._tail_index, value)
-            if self._tail_index < self._capacity - 1:
-                self._tail_index += 1
-            self._length += 1
-
-    def sort_data(self):
-        # Decreasing order
-        self._heap = sorted(self._heap, key=lambda item: -math.inf if item is None else item[1], reverse=True)
-
-        # Reset index map
-        for index, item in enumerate(self._heap):
-            if item is not None:
-                self._heap_to_absolute_index_map[index] = item[0]
-                self._absolute_to_heap_index_map[item[0]] = index
-            else:
-                self._heap_to_absolute_index_map[index] = None
-
-    def get_absolute_index_from_heap_index(self, heap_index: int):
-        return self.heap_to_absolute_index(heap_index)
-
-    def update(self, absolute_index: int, value: float):
-        heap_index = self.absolute_to_heap_index(absolute_index)
-        (absolute_index, _) = self._heap[heap_index]
-        self._heap[heap_index] = (absolute_index, value)
-        self._heapup(heap_index)
-        self._heapdown(heap_index)
-
-    def _parent_index(self, child_index):
-        return (child_index - 1) // 2
-
-    def _heappush(self, absolute_index, error):
-        heap_index = self._tail_index
-        self._heap_to_absolute_index_map[heap_index] = absolute_index
-        self._absolute_to_heap_index_map[absolute_index] = heap_index
-        self._heap[heap_index] = (absolute_index, error)
-        self._heapup(heap_index)
-
-    def _heapup(self, heap_index):
-        if heap_index == 0:
-            return
-        heap_data = self._heap[heap_index]
-        parent_index = self._parent_index(heap_index)
-        parent_data = self._heap[parent_index]
-        if parent_data[1] < heap_data[1]:
-            self._swap_item(heap_index, parent_index)
-            self._heapup(parent_index)
-
-    def _heapdown(self, heap_index):
-        heap_length = len(self)
-        if heap_length <= heap_index:
-            return
-        heap_data = self._heap[heap_index]
-        child_l_index = heap_index * 2 + 1
-        child_r_index = heap_index * 2 + 2
-        child_l_data = self._heap[child_l_index] if child_l_index < self._capacity else None
-        child_r_data = self._heap[child_r_index] if child_r_index < self._capacity else None
-
-        largest_data_index = heap_index
-        if child_l_data is not None:
-            if (child_l_index < heap_length) and (child_l_data[1] > heap_data[1]):
-                largest_data_index = child_l_index
-        if child_r_data is not None:
-            if (child_r_index < heap_length) and (child_r_data[1] > self._heap[largest_data_index][1]):
-                largest_data_index = child_r_index
-        if largest_data_index != heap_index:
-            self._swap_item(heap_index, largest_data_index)
-            self._heapdown(largest_data_index)
-
-    def _swap_item(self, heap_index1, heap_index2):
-        heap_index1_data = self._heap[heap_index1]
-        heap_index2_data = self._heap[heap_index2]
-        self._heap[heap_index1], self._heap[heap_index2] = heap_index2_data, heap_index1_data
-        self._heap_to_absolute_index_map[heap_index1] = heap_index2_data[0]
-        self._absolute_to_heap_index_map[heap_index2_data[0]] = heap_index1
-        self._heap_to_absolute_index_map[heap_index2] = heap_index1_data[0]
-        self._absolute_to_heap_index_map[heap_index1_data[0]] = heap_index2
-
-    def absolute_to_heap_index(self, absolute_index):
-        return self._absolute_to_heap_index_map[absolute_index]
-
-    def heap_to_absolute_index(self, heap_index):
-        return self._heap_to_absolute_index_map[heap_index]
+            node = self._tree[index]
+        data_index = index - (self._capacity - 1)
+        return data_index
 
+    def _get_priority(self, index):
+        tree_index = index + self._capacity - 1
+        return self._tree[tree_index].value
 
-class PrioritizedDataHolder(DataHolder[Any]):
-    def __init__(self, capacity: int):
-        self._capacity = capacity
-        self._data = RingBuffer(maxlen=capacity)
+    def _weights_from_priorities(self, priorities, beta):
+        weights = (priorities / self._min_p) ** (-beta)
+        return weights
+
+    def update_latest_priorities(self, priorities):
+        for index, priority in zip(self._latest_indices, priorities):
+            self.update(index, priority)
+        self._latest_indices = None
 
     def __len__(self):
-        return len(self._data)
-
-    def __getitem__(self, relative_index: int):
-        return self._data[relative_index]
+        return self._data_num
 
-    def append(self, data):
-        # ignore returned value
-        self.append_with_removed_item_check(data)
-
-    def append_with_removed_item_check(self, data):
-        raise NotImplementedError
+    def __getitem__(self, index):
+        return self._data[index]
 
-    def update_priority(self, relative_index: int, priority: int):
-        raise NotImplementedError
+    @property
+    def total(self):
+        return self._tree[0].value
 
-    def get_priority(self, relative_index: int):
-        raise NotImplementedError
-
-    def _relative_to_absolute_index(self, relative_index):
-        return (relative_index + self._data._head) % self._capacity
-
-    def _absolute_to_relative_index(self, absolute_index):
-        return (absolute_index - self._data._head) % self._capacity
-
-
-class SumTreeDataHolder(PrioritizedDataHolder):
-    def __init__(self, capacity, initial_max_priority, keep_min=True):
-        super().__init__(capacity=capacity)
-        self._sum_tree = SumTree(capacity=capacity)
-        self._keep_min = keep_min
-        if self._keep_min:
-            self._min_tree = MinTree(capacity=capacity)
-        self._max_priority = initial_max_priority
-
-    def append_with_removed_item_check(self, data):
-        removed = self._data.append_with_removed_item_check(data)
-        self._sum_tree.append(self._max_priority)
-        if self._keep_min:
-            self._min_tree.append(self._max_priority)
-        return removed
-
-    def get_priority(self, relative_index: int):
-        absolute_index = self._relative_to_absolute_index(relative_index)
-        tree_index = self._sum_tree.absolute_to_tree_index(absolute_index)
-        return self._sum_tree[tree_index]
-
-    def sum_priority(self):
-        return self._sum_tree.sum()
-
-    def min_priority(self):
-        return self._min_tree.min()
-
-    def update_priority(self, relative_index: int, priority: float):
-        absolute_index = self._relative_to_absolute_index(relative_index)
-        self._sum_tree.update(absolute_index, priority)
-        if self._keep_min:
-            self._min_tree.update(absolute_index, priority)
-        self._max_priority = max(self._max_priority, priority)
-
-    def get_index_from_query(self, query: float):
-        absolute_index = self._sum_tree.get_absolute_index_from_query(query)
-        return self._absolute_to_relative_index(absolute_index)
-
-
-class MaxHeapDataHolder(PrioritizedDataHolder):
-    def __init__(self, capacity: int, alpha: float):
-        super().__init__(capacity=capacity)
-        self._max_heap = MaxHeap(capacity)
-        self._alpha = alpha
 
-    def append_with_removed_item_check(self, data):
-        removed = self._data.append_with_removed_item_check(data)
-        self._max_heap.append(math.inf)
-        return removed
-
-    def get_priority(self, relative_index: int):
-        absolute_index = self._relative_to_absolute_index(relative_index)
-        heap_index = self._max_heap.absolute_to_heap_index(absolute_index)
-        rank = (heap_index + 1)
-        return self._compute_priority(rank)
-
-    def get_relative_index_from_heap_index(self, heap_index: int):
-        absolute_index = self._max_heap.get_absolute_index_from_heap_index(heap_index)
-        return self._absolute_to_relative_index(absolute_index)
-
-    def update_priority(self, relative_index: int, priority: float):
-        absolute_index = self._relative_to_absolute_index(relative_index)
-        self._max_heap.update(absolute_index, priority)
-
-    def sort_data(self):
-        self._max_heap.sort_data()
-
-    def _compute_priority(self, rank: int):
-        priority = (1 / rank) ** self._alpha
-
-        # We do not normalize priority here to reduce computation.
-        # Normalization term will be compensated when dividing with maximum weight
-        return priority
-
-
-class _PrioritizedReplayBuffer(ReplayBuffer):
-    def __init__(self,
-                 capacity: int,
-                 alpha: float,
-                 beta: float,
-                 betasteps: int,
-                 error_clip: Optional[Tuple[float, float]]):
-        # Do not call super class' constructor
+class PrioritizedReplayBuffer(ReplayBuffer):
+    def __init__(
+        self, capacity,
+        alpha=0.6, beta=0.4, betasteps=10000, epsilon=1e-8
+    ):
+        # No need to call super class contructor
         self._capacity_check(capacity)
         self._capacity = capacity
-
+        self._buffer = SumTree(capacity)
         self._alpha = alpha
         self._beta = beta
         self._beta_diff = (1.0 - beta) / betasteps
+        self._epsilon = epsilon
 
-        self._error_clip = error_clip
-
-        # last absolute indices of experiences sampled from buffer
-        self._last_sampled_indices: Union[Sequence[int], None] = None
-
-    def __getitem__(self, relative_index: int):
-        # NOTE: relative index 0 means the oldest entry and len(self) - 1 the latest entry
-        return self._buffer[relative_index]
-
-    def __len__(self):
-        return len(self._buffer)
-
-    def sample(self, num_samples: int = 1, num_steps: int = 1):
-        raise NotImplementedError
-
-    def sample_indices(self, indices: Sequence[int], num_steps: int = 1):
-        if len(indices) == 0:
-            raise ValueError('Indices are empty')
-        if self._last_sampled_indices is not None:
-            raise RuntimeError('Trying to sample data from buffer without updating priority. '
-                               'Check that the algorithm supports prioritized replay buffer.')
-        experiences: Union[Sequence[Experience], Tuple[Sequence[Experience], ...]]
-        if num_steps == 1:
-            experiences = [self.__getitem__(index) for index in indices]
-        else:
-            experiences = tuple([self.__getitem__(index+i) for index in indices] for i in range(num_steps))
-
-        weights = self._get_weights(indices, self._alpha, self._beta)
-        info = dict(weights=weights)
-
-        self._beta = min(self._beta + self._beta_diff, 1.0)
-        self._last_sampled_indices = indices
-        return experiences, info
-
-    def update_priorities(self, errors: np.ndarray):
-        raise NotImplementedError
-
-    def _preprocess_errors(self, errors: np.ndarray):
-        if self._error_clip is not None:
-            errors = np.clip(errors, self._error_clip[0], self._error_clip[1])
-        return np.abs(errors)
-
-    def _get_weights(self, indices: Sequence[int], alpha: float, beta: float):
-        raise NotImplementedError
-
-    def _capacity_check(self, capacity: int):
+    def _capacity_check(self, capacity):
         if capacity is None or capacity <= 0:
             error_msg = 'buffer size must be greater than 0'
             raise ValueError(error_msg)
 
-
-class ProportionalPrioritizedReplayBuffer(_PrioritizedReplayBuffer):
-    # type declarations to type check with mypy
-    # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
-    # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
-    _buffer: SumTreeDataHolder
-    _epsilon: float
-
-    def __init__(self, capacity: int,
-                 alpha: float = 0.6,
-                 beta: float = 0.4,
-                 betasteps: int = 10000,
-                 error_clip: Optional[Tuple[float, float]] = (-1, 1),
-                 epsilon: float = 1e-8,
-                 init_max_error: float = 1.0,
-                 normalization_method: str = "buffer_max"):
-        super(ProportionalPrioritizedReplayBuffer, self).__init__(capacity, alpha, beta, betasteps, error_clip)
-        assert normalization_method in ("batch_max", "buffer_max")
-        self._normalization_method = normalization_method
-        keep_min = (self._normalization_method == "buffer_max")
-        self._buffer = SumTreeDataHolder(capacity=capacity, initial_max_priority=init_max_error, keep_min=keep_min)
-        self._epsilon = epsilon
-
-    def append(self, experience):
-        self._buffer.append(experience)
-
-    def sample(self, num_samples: int = 1, num_steps: int = 1):
-        buffer_length = len(self)
-        if num_samples > buffer_length:
-            error_msg = f'num_samples: {num_samples} is greater than the size of buffer: {buffer_length}'
-            raise ValueError(error_msg)
-        if buffer_length - num_steps < 0:
-            raise RuntimeError(f'Insufficient buffer length. buffer: {buffer_length} < steps: {num_steps}')
-
-        # In paper,
-        # "To sample a minibatch of size k, the range [0, ptotal] is divided equally into k ranges.
-        # Next, a value is uniformly sampled from each range"
-        indices = []
-        interval = self._buffer.sum_priority() / num_samples
-        for i in range(num_samples):
-            index = sys.maxsize
-            while index >= buffer_length - num_steps + 1:
-                random_value = rl.random.drng.uniform(interval * i, interval * (i + 1))
-                index = self._buffer.get_index_from_query(random_value)
-            indices.append(index)
-        return self.sample_indices(indices, num_steps)
-
-    def update_priorities(self, errors: np.ndarray):
-        errors = self._preprocess_errors(errors)
-        errors = ((errors + self._epsilon) ** self._alpha).flatten()
-        indices = cast(Sequence[int], self._last_sampled_indices)
-        for index, error in zip(indices, errors):
-            self._buffer.update_priority(index, error)
-        self._last_sampled_indices = None
-
-    def _get_weights(self, indices: Sequence[int], alpha: float, beta: float):
-        priorities = np.asarray([self._buffer.get_priority(i) for i in indices])[:, np.newaxis]
-        if self._normalization_method == "batch_max":
-            # Use min priority. This is same as max of weight.
-            min_priority = priorities.min()
-        elif self._normalization_method == "buffer_max":
-            # Use min priority. This is same as max of weight.
-            min_priority = self._buffer.min_priority()
-        else:
-            raise RuntimeError(f"Unknown normalization method {self._normalization_method}")
-        return (priorities / min_priority) ** (-beta)
-
-
-class RankBasedPrioritizedReplayBuffer(_PrioritizedReplayBuffer):
-    # type declarations to type check with mypy
-    # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
-    # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
-    _buffer: MaxHeapDataHolder
-    _reset_segment_interval: int
-    _sort_interval: int
-    _boundaries: List[int]
-    _prev_num_samples: int
-    _prev_num_steps: int
-    _appends_since_prev_start: int
-
-    def __init__(self, capacity: int,
-                 alpha: float = 0.7,
-                 beta: float = 0.5,
-                 betasteps: int = 10000,
-                 error_clip: Optional[Tuple[float, float]] = (-1, 1),
-                 reset_segment_interval: int = 1000,
-                 sort_interval: int = 1000000):
-        super(RankBasedPrioritizedReplayBuffer, self).__init__(capacity, alpha, beta, betasteps, error_clip)
-        self._buffer = MaxHeapDataHolder(capacity, alpha)
-
-        self._reset_segment_interval = reset_segment_interval
-        self._sort_interval = sort_interval
-
-        self._boundaries = []
-        self._prev_num_samples = 0
-        self._prev_num_steps = 0
-        self._appends_since_prev_sort = 0
-        self._ps_cumsum = np.cumsum(np.asarray([(1 / (i + 1)) ** alpha for i in range(capacity)]))
-
     def append(self, experience):
         self._buffer.append(experience)
 
-        self._appends_since_prev_sort += 1
-        if self._appends_since_prev_sort % self._sort_interval == 0:
-            self._buffer.sort_data()
-            self._appends_since_prev_sort = 0
-
-    def sample(self, num_samples: int = 1, num_steps: int = 1):
+    def sample(self, num_samples=1):
         buffer_length = len(self)
         if num_samples > buffer_length:
-            error_msg = f'num_samples: {num_samples} is greater than the size of buffer: {buffer_length}'
+            error_msg = 'num_samples: {} is greater than the size of buffer: {}'.format(
+                num_samples, buffer_length)
             raise ValueError(error_msg)
-        if buffer_length - num_steps < 0:
-            raise RuntimeError(
-                f'Insufficient buffer length. buffer: {buffer_length} < steps: {num_steps}')
-        if (num_samples != self._prev_num_samples) or \
-           (num_steps != self._prev_num_steps) or \
-           (buffer_length % self._reset_segment_interval == 0 and buffer_length != self._capacity) or \
-           (len(self._boundaries) == 0):
-            self._boundaries = self._compute_segment_boundaries(N=buffer_length, k=num_samples)
-            self._prev_num_samples = num_samples
-            self._prev_num_steps = num_steps
-
-        indices = []
-        prev_boundary = 0
-        for boundary in self._boundaries:
-            heap_index = rl.random.drng.integers(low=prev_boundary, high=boundary)
-            index = self._buffer.get_relative_index_from_heap_index(heap_index)
-            prev_boundary = boundary
-            if index < buffer_length - num_steps + 1:
-                indices.append(index)
-        while len(indices) < num_samples:
-            # Enters here only when 1 < num_steps and (one or more than one) sampled indices exceeded buffer length
-            boundary_index = rl.random.drng.choice(len(self._boundaries))
-            if boundary_index != 0:
-                boundary_low = self._boundaries[boundary_index - 1]
-            else:
-                boundary_low = 0
-            boundary_high = self._boundaries[boundary_index]
-            heap_index = rl.random.drng.integers(low=boundary_low, high=boundary_high)
-            index = self._buffer.get_relative_index_from_heap_index(heap_index)
-            if index < buffer_length - num_steps + 1:
-                indices.append(index)
-        return self.sample_indices(indices, num_steps)
-
-    def update_priorities(self, errors: np.ndarray):
-        errors = self._preprocess_errors(errors)
-        indices = cast(Sequence[int], self._last_sampled_indices)
-        for index, error in zip(indices, errors):
-            self._buffer.update_priority(index, error)
-        self._last_sampled_indices = None
-
-    def _compute_segment_boundaries(self, N: int, k: int):
-        if N < k:
-            raise ValueError(f"Batch size {k} is greater than buffer size {N}")
-        boundaries: List[int] = []
-        denominator = self._ps_cumsum[N-1]
-        for i in range(N):
-            if (len(boundaries) + 1) / k <= self._ps_cumsum[i] / denominator:
-                boundaries.append(i + 1)
-        assert len(boundaries) == k
-        return boundaries
-
-    def _get_weights(self, indices: Sequence[int], alpha: float, beta: float):
-        priorities = np.asarray([self._buffer.get_priority(i) for i in indices])[:, np.newaxis]
-        worst_rank = len(self._buffer)
-        min_priority = (1 / worst_rank) ** alpha
-        return (priorities / min_priority) ** (-beta)
-
-
-class PrioritizedReplayBuffer(ReplayBuffer):
-    _variants: ClassVar[Sequence[str]] = ['proportional', 'rank_based']
-    _buffer_impl: _PrioritizedReplayBuffer
-
-    def __init__(self,
-                 capacity: int,
-                 alpha: float = 0.6,
-                 beta: float = 0.4,
-                 betasteps: int = 10000,
-                 error_clip: Optional[Tuple[float, float]] = (-1, 1),
-                 epsilon: float = 1e-8,
-                 reset_segment_interval: int = 1000,
-                 sort_interval: int = 1000000,
-                 variant: str = 'proportional'):
-        if variant not in PrioritizedReplayBuffer._variants:
-            raise ValueError(f'Unknown prioritized replay buffer variant: {variant}')
-        if variant == 'proportional':
-            self._buffer_impl = ProportionalPrioritizedReplayBuffer(capacity=capacity,
-                                                                    alpha=alpha,
-                                                                    beta=beta,
-                                                                    betasteps=betasteps,
-                                                                    error_clip=error_clip,
-                                                                    epsilon=epsilon)
-        elif variant == 'rank_based':
-            self._buffer_impl = RankBasedPrioritizedReplayBuffer(capacity=capacity,
-                                                                 alpha=alpha,
-                                                                 beta=beta,
-                                                                 betasteps=betasteps,
-                                                                 error_clip=error_clip,
-                                                                 reset_segment_interval=reset_segment_interval,
-                                                                 sort_interval=sort_interval)
-        else:
-            raise NotImplementedError
-
-    @property
-    def capacity(self):
-        return self._buffer_impl.capacity
-
-    def append(self, experience):
-        self._buffer_impl.append(experience)
-
-    def append_all(self, experiences):
-        self._buffer_impl.append_all(experiences)
-
-    def sample(self, num_samples: int = 1, num_steps: int = 1):
-        return self._buffer_impl.sample(num_samples, num_steps)
+        experiences, weights = self._buffer.sample(num_samples, self._beta)
+        info = dict(weights=weights)
+        self._beta = min(self._beta + self._beta_diff, 1.0)
+        return experiences, info
 
-    def sample_indices(self, indices: Sequence[int], num_steps: int = 1):
-        return self._buffer_impl.sample_indices(indices, num_steps)
+    def sample_indices(self, indices):
+        if len(indices) == 0:
+            raise ValueError('Indices are empty')
+        experiences, weights = self._buffer.sample_indices(indices, self._beta)
+        info = dict(weights=weights)
+        self._beta = min(self._beta + self._beta_diff, 1.0)
+        return experiences, info
 
-    def update_priorities(self, errors: np.ndarray):
-        self._buffer_impl.update_priorities(errors)
+    def update_priorities(self, errors):
+        priorities = ((errors + self._epsilon) ** self._alpha).flatten()
+        self._buffer.update_latest_priorities(priorities)
 
     def __len__(self):
-        return len(self._buffer_impl)
+        return len(self._buffer)
 
-    def __getitem__(self, item: int) -> Experience:
-        return cast(Experience, self._buffer_impl[item])
+    def __getitem__(self, index):
+        return self._buffer[index]
```

## nnabla_rl/replay_buffers/replacement_sampling_replay_buffer.py

```diff
@@ -1,39 +1,37 @@
 # Copyright 2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import nnabla_rl as rl
+import random
+
 from nnabla_rl.replay_buffer import ReplayBuffer
 
 
 class ReplacementSamplingReplayBuffer(ReplayBuffer):
-    """ReplacementSamplingReplayBuffer.
-
-    From all experiences in the buffer, this buffer samples the
-    experiences with replacement. Therefore, sampled batch may contain
-    duplicate (=same experience) entries. Unlike the default
-    ReplayBuffer, you can sample larger number of data than total size
-    of the buffer.
-    """
+    '''ReplacementSamplingReplayBuffer.
+    From all experiences in the buffer, this buffer samples the experiences with replacement.
+    Therefore, sampled batch may contain duplicate (=same experience) entries.
+    Unlike the default ReplayBuffer, you can sample larger number of data than total size of the buffer.
+    '''
 
     def __init__(self, capacity=None):
         super(ReplacementSamplingReplayBuffer, self).__init__(capacity)
 
-    def sample(self, num_samples=1, num_steps=1):
-        max_index = len(self) - num_steps + 1
-        indices = self._random_indices(num_samples=num_samples, max_index=max_index)
-        return self.sample_indices(indices, num_steps=num_steps)
-
-    def _random_indices(self, num_samples, max_index):
-        return rl.random.drng.choice(max_index, size=num_samples, replace=True)
+    def sample(self, num_samples=1):
+        indices = self._random_indices(num_samples=num_samples)
+        return self.sample_indices(indices)
+
+    def _random_indices(self, num_samples):
+        buffer_length = len(self)
+        return random.choices(range(buffer_length), k=num_samples)
```

## nnabla_rl/utils/data.py

```diff
@@ -1,280 +1,80 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023,2024 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from typing import Any, Dict, Generic, Iterable, List, Optional, Sequence, Tuple, TypeVar, Union, cast
+from typing import Iterable, List, TypeVar, Union
 
 import numpy as np
 
-import nnabla as nn
-from nnabla_rl.logger import logger
-from nnabla_rl.typing import TupledData
-
 T = TypeVar('T')
 
 
 def add_axis_if_single_dim(data):
     if data.ndim == 1:
         return np.expand_dims(data, axis=-1)
     else:
         return data
 
 
-def marshal_experiences(experiences: Iterable[TupledData]) -> TupledData:
-    """Marshall experiences. This function concatenates each of the elements in
-    the given experience and return it as a tuple. Note that if the given
-    experience has a tuple, this function be applied recursively.
-
-    Example:
-        >>> import numpy as np
-        >>> from nnabla_rl.utils.data import marshall_experiences
-        >>> experiences = tuple((np.random.randn(1, ), np.random.randn(2, )) for _ in range(10))
-        >>> marshaled_experience = marshall_experiences(experiences)
-        >>> marshaled_experience[0].shape
-        (10, 1)
-        >>> marshaled_experience[1].shape
-        (10, 2)
-        >>> tupled_experiences = tuple(((np.random.randn(1, ), np.random.randn(2, )), np.random.randn(3, )) \
-            for _ in range(10))
-        >>> marshaled_tupled_experience = marshall_experiences(tupled_experiences)
-        >>> type(tupled_experiences[0])
-        <class 'tuple'>
-        >>> marshaled_tupled_experience[0][0].shape
-        (10, 1)
-        >>> marshaled_tupled_experience[0][1].shape
-        (10, 2)
-        >>> marshaled_tupled_experience[1].shape
-        (10, 3)
-
-    Args:
-        experiences (Iterable[TupledData]): iterable object of experience
-    Returns:
-        TupledData: marshaled experiences
-    """
+def marshal_experiences(experiences):
     unzipped_experiences = unzip(experiences)
-    marshaled_experiences: List = []
-    for data in unzipped_experiences:
-        if isinstance(data[0], tuple):
-            marshaled_experiences.append(marshal_experiences(data))
-        elif isinstance(data[0], dict):
-            marshaled_experiences.append(marshal_dict_experiences(data))
-        else:
-            marshaled_experiences.append(add_axis_if_single_dim(np.asarray(data)))
-    return tuple(marshaled_experiences)
-
-
-def marshal_dict_experiences(dict_experiences: Sequence[Dict[str, Any]]) -> Dict:
-    dict_of_list = list_of_dict_to_dict_of_list(dict_experiences)
-    marshaled_experiences = {}
-    for key, data in dict_of_list.items():
-        try:
-            if isinstance(data[0], Dict):
-                marshaled_experiences.update({key: marshal_dict_experiences(data)})
-            else:
-                marshaled_experiences.update({key: add_axis_if_single_dim(np.asarray(data))})
-        except ValueError as e:
-            # do nothing
-            logger.warn(f'key: {key} contains inconsistent elements!. Details: {e}')
-    return marshaled_experiences
-
-
-def list_of_dict_to_dict_of_list(list_of_dict: Sequence[Dict[Any, Any]]):
-    return {key: [d.get(key, None) for d in list_of_dict] for key in list_of_dict[0]}
+    return tuple(add_axis_if_single_dim(np.asarray(data)) for data in unzipped_experiences)
 
 
-def unzip(zipped_data) -> List[Tuple]:
+def unzip(zipped_data):
     return list(zip(*zipped_data))
 
 
 def is_array_like(x):
     return hasattr(x, "__len__")
 
 
 def convert_to_list_if_not_list(value: Union[Iterable[T], T]) -> List[T]:
     if isinstance(value, Iterable):
         return list(value)
     else:
         return [value]
 
 
-def set_data_to_variable(variable: Union[nn.Variable, Tuple[nn.Variable, ...]],
-                         data: Union[float, np.ndarray, Tuple[np.ndarray, ...]]) -> None:
-    """Set data to variable.
-
-    Args:
-        variable (Union[nn.Variable, Tuple[nn.Variable, ...]]): variable
-        data (Union[float, np.ndarray, Tuple[np.ndarray, ...]]): data set to variable.d
-    """
-    if isinstance(data, tuple) and isinstance(variable, tuple):
-        assert len(variable) == len(data)
-        for v, d in zip(variable, data):
-            set_data_to_variable(v, d)
-    elif isinstance(data, np.ndarray) and isinstance(variable, nn.Variable):
-        variable.d = data
-    elif isinstance(data, float) and isinstance(variable, nn.Variable):
-        variable.d = data
-    else:
-        raise ValueError
-
-
-def add_batch_dimension(data: Union[np.ndarray, Tuple[np.ndarray, ...]]) -> Union[np.ndarray, Tuple[np.ndarray, ...]]:
-    """Apply np.expand_dims to data. If the data is tuple, this function apply
-    np.expand_dims to each elements of data.
-
-    Args:
-        data (Union[np.ndarray, Tuple[np.ndarray, ...]]): data
-    Returns:
-        Union[np.ndarray, Tuple[np.ndarray, ...]]: expanded data
-    """
-    if isinstance(data, np.ndarray):
-        return cast(np.ndarray, np.expand_dims(data, axis=0))
-    else:
-        return tuple(np.expand_dims(d, axis=0) for d in data)
-
-
-class DataHolder(Generic[T]):
-    """DataHolder.
-
-    FIFO (First Input First Out) data container.
-    """
-
-    def __len__(self):
-        raise NotImplementedError
-
-    def __getitem__(self, index: int):
-        raise NotImplementedError
-
-    def append(self, data: T):
-        """Append new data. If the holder's capacity exceeds by appending new
-        data, oldest data will be removed and new data will be appended to the
-        tail.
-
-        Args:
-            data T: data to append.
-        """
-        raise NotImplementedError
-
-    def append_with_removed_item_check(self, data: T) -> Union[T, None]:
-        """Append new data. If the holder's capacity exceeds by appending new
-        data, oldest data will be removed and new data will be appended to the
-        tail. If oldest data is removed from the holder, will return removed
-        data otherwise None.
-
-        Args:
-            data T: data to append.
-
-        Returns:
-            Union[T, None]: Removed item. If no data is removed, None will be returned.
-        """
-        raise NotImplementedError
-
-
-class RingBuffer(DataHolder[Any]):
-    def __init__(self, maxlen: int):
+class RingBuffer(object):
+    def __init__(self, maxlen):
         # Do NOT replace this list with collections.deque.
         # deque is too slow when randomly accessed to sample data for creating batch
         self._buffer = [None for _ in range(maxlen)]
         self._maxlen = maxlen
         self._head = 0
         self._length = 0
 
     def __len__(self):
         return self._length
 
-    def __getitem__(self, index: int):
+    def __getitem__(self, index):
         if index < 0 or index >= len(self):
             raise KeyError
         return self._buffer[(self._head + index) % self._maxlen]
 
-    def append(self, data: T):
+    def append(self, data):
         self.append_with_removed_item_check(data)
 
     def append_with_removed_item_check(self, data):
         if self._length < self._maxlen:
             self._length += 1
         elif self._length == self._maxlen:
             self._head = (self._head + 1) % self._maxlen
         else:
             raise IndexError
         index = (self._head + self._length - 1) % self._maxlen
         removed = self._buffer[index]
         self._buffer[index] = data
         return removed
-
-
-def normalize_ndarray(ndarray: np.ndarray,
-                      mean: np.ndarray,
-                      std: np.ndarray,
-                      value_clip: Optional[Tuple[float, float]] = None) -> np.ndarray:
-    """Normalize the given ndarray.
-
-    Args:
-        ndarray (np.ndarray): variable to be normalized.
-        mean (np.ndarray): mean.
-        std (np.ndarray): standard deviation.
-        value_clip (Optional[Tuple[float, float]]): clipping value. defaults to None.
-
-    Returns:
-        np.ndarray: normalized value.
-    """
-    normalized: np.ndarray = (ndarray - mean) / std
-    if value_clip is not None:
-        normalized = np.clip(normalized, a_min=value_clip[0], a_max=value_clip[1])
-    return normalized
-
-
-def unnormalize_ndarray(ndarray: np.ndarray,
-                        mean: np.ndarray,
-                        std: np.ndarray,
-                        value_clip: Optional[Tuple[float, float]] = None) -> np.ndarray:
-    """Unnormalize the given ndarray.
-
-    Args:
-        ndarray (np.ndarray): variable to be unnormalized.
-        mean (np.ndarray): mean.
-        std (np.ndarray): standard deviation.
-        value_clip (Optional[Tuple[float, float]]): clipping value. defaults to None.
-
-    Returns:
-        np.ndarray: unnormalized value.
-    """
-    unnormalized: np.ndarray = ndarray * std + mean
-    if value_clip is not None:
-        unnormalized = np.clip(unnormalized, a_min=value_clip[0], a_max=value_clip[1])
-    return unnormalized
-
-
-def compute_std_ndarray(var: np.ndarray, epsilon: float, mode_for_floating_point_error: str) -> np.ndarray:
-    """Compute standard deviation.
-
-    Args:
-        var (nn.Variable): variance
-        epsilon (float): value to improve numerical stability for computing the standard deviation.
-        mode_for_floating_point_error (str): mode for avoiding a floating point error
-            when computing the standard deviation. Must be one of:
-
-            - `add`: It returns the square root of the sum of var and epsilon.
-            - `max`: It returns epsilon if the square root of var is smaller than epsilon, \
-                otherwise it returns the square root of var.
-
-    Returns:
-        nn.Variable: standard deviation
-    """
-    if mode_for_floating_point_error == "add":
-        std = (var + epsilon) ** 0.5
-    elif mode_for_floating_point_error == "max":
-        std = np.maximum(var**0.5, epsilon)
-    else:
-        raise ValueError
-    return std
```

## nnabla_rl/utils/debugging.py

```diff
@@ -1,29 +1,23 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import pathlib
-from typing import Optional, Union
-
-import nnabla as nn
 import nnabla.experimental.viewers as V
-import nnabla.solvers as S
-from nnabla.utils.profiler import GraphProfiler, GraphProfilerCsvWriter
 from nnabla_rl.hook import Hook
 from nnabla_rl.logger import logger
 
 
 def print_network(x):
     def accept_nnabla_func(nnabla_func):
         print("==========")
@@ -53,52 +47,14 @@
     '''
     parameter_number = 0
     for parameter in parameters.values():
         parameter_number += parameter.size
     return parameter_number
 
 
-def profile_graph(
-    output_variable: nn.Variable,
-    csv_file_path: Union[str, pathlib.Path],
-    solver: Optional[S.Solver] = None,
-    ext_name: str = 'cudnn',
-    device_id: int = 0,
-    n_run: int = 1000,
-) -> None:
-    """Profile computational graph. Print the profile result to console and
-    save it to the csv_file_path.
-
-    Args:
-        output_variable (nn.Variable): output variable of the graph.
-        csv_file_path (Union[str, pathlib.Path]): csv file path of the profile results.
-        solver (Optional[S.Solver]): nnabla solver, if this parameter is not None,
-            nnabla Profiler also measures updating the parameter, defaults to None.
-        ext_name (str): extention name, defaults to cudnn.
-        device_id (int): device id, defaults to 0.
-        n_run: (int): number of runs, defaults to 1000.
-
-    Examples:
-        >>> import nnabla as nn
-        >>> import nnabla.functions as NF
-        >>> from nnabla_rl.utils.debugging import profile_graph
-        >>> x = nn.Variable([1, 1])
-        >>> y = NF.relu(x)
-        >>> output_file_path = "sample.csv"
-        >>> profile_graph(y, output_file_path)
-        # The profile result is shown in the console and the result is saved as sample.csv.
-    """
-    B = GraphProfiler(output_variable, solver=solver, device_id=device_id, ext_name=ext_name, n_run=n_run)
-    B.run()
-    B.print_result()
-    with open(csv_file_path, "w") as f:
-        writer = GraphProfilerCsvWriter(B, file=f)
-        writer.write()
-
-
 try:
     from pympler import muppy, summary
 
     class PrintMemoryDumpHook(Hook):
         def __init__(self, timing):
             super(PrintMemoryDumpHook, self).__init__(timing=timing)
```

## nnabla_rl/utils/evaluator.py

```diff
@@ -1,38 +1,35 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from typing import Callable, List, Union
-
 import numpy as np
 
 from nnabla_rl.logger import logger
-from nnabla_rl.typing import Experience
 
 
 class EpisodicEvaluator():
     def __init__(self, run_per_evaluation=10):
         self._num_episodes = run_per_evaluation
 
     def __call__(self, algorithm, env):
         returns = []
         for num in range(1, self._num_episodes + 1):
-            reward_sum, *_ = run_one_episode(algorithm, env)
+            reward_sum, _ = run_one_episode(algorithm, env)
             returns.append(reward_sum)
             logger.info(
                 'Finished evaluation run: #{} out of {}. Total reward: {}'
                 .format(num, self._num_episodes, reward_sum))
         return returns
 
 
@@ -44,15 +41,16 @@
         returns = []
         timesteps = 0
 
         def limit_checker(t):
             return t + timesteps > self._num_timesteps
 
         while True:
-            reward_sum, episode_timesteps, *_ = run_one_episode(algorithm, env, timestep_limit=limit_checker)
+            reward_sum, episode_timesteps = run_one_episode(
+                algorithm, env, timestep_limit=limit_checker)
             timesteps += episode_timesteps
 
             if timesteps > self._num_timesteps:
                 break
 
             returns.append(reward_sum)
             logger.info(
@@ -60,50 +58,22 @@
                 .format(timesteps, self._num_timesteps, len(returns), reward_sum))
         if len(returns) == 0:
             # In case the time limit reaches on first episode, save the return received up to that time
             returns.append(reward_sum)
         return returns
 
 
-class EpisodicSuccessEvaluator():
-    def __init__(self, check_success: Callable[[List[Experience]], Union[bool, float]],
-                 run_per_evaluation=10):
-        self._num_episodes = run_per_evaluation
-        self._compute_success_func = check_success
-
-    def __call__(self, algorithm, env):
-        results = []
-        for num in range(1, self._num_episodes + 1):
-            _, _, experiences = run_one_episode(algorithm, env)
-            success = self._compute_success_func(experiences)
-            results.append(success)
-            success_tag = 'Success' if success else 'Failed'
-            logger.info(
-                'Finished evaluation run: #{} out of {}. {}'
-                .format(num, self._num_episodes, success_tag)
-            )
-
-        return results
-
-
 def run_one_episode(algorithm, env, timestep_limit=lambda t: False):
-    experiences = []
     rewards = []
     timesteps = 0
     state = env.reset()
-    extra_info = {'reward': 0}
-    action = algorithm.compute_eval_action(state, begin_of_episode=True, extra_info=extra_info)
     while True:
-        next_state, reward, done, info = env.step(action)
-        non_terminal = 0.0 if done else 1.0
-        experience = (state, action, reward, non_terminal, next_state, info)
-        experiences.append(experience)
+        action = algorithm.compute_eval_action(state)
+        next_state, reward, done, _ = env.step(action)
 
         rewards.append(reward)
         timesteps += 1
         if done or timestep_limit(timesteps):
             break
         else:
             state = next_state
-            extra_info['reward'] = reward
-            action = algorithm.compute_eval_action(state, begin_of_episode=False, extra_info=extra_info)
-    return np.sum(rewards), timesteps, experiences
+    return np.sum(rewards), timesteps
```

## nnabla_rl/utils/files.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -13,54 +13,58 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import os
 
 
 def file_exists(path):
-    """Check file existence on given path.
+    '''
+    Check file existence on given path
 
     Args:
         path (str or pathlib.Path): Path of the file to check existence
     Returns:
         file_existence (bool): True if file exists otherwise False
-    """
+    '''
     return os.path.exists(path)
 
 
 def create_dir_if_not_exist(outdir):
-    """Check directory existence and creates new directory if not exist.
+    '''
+    Check directory existence and creates new directory if not exist
 
     Args:
         outdir (str or pathlib.Path): Path of the file to create directory
     Raises:
         RuntimeError: File exists in outdir but it is not a directory
-    """
+    '''
     if file_exists(outdir):
         if not os.path.isdir(outdir):
             raise RuntimeError('{} is not a directory'.format(outdir))
         else:
             return
     os.makedirs(outdir)
 
 
 def read_text_from_file(file_path):
-    """Read given file as text.
+    '''
+    Read given file as text
 
     Args:
         file_path (str or pathlib.Path): Path of the file to read data
     Returns:
         data (str): Text read from the file
-    """
+    '''
     with open(file_path, 'r') as f:
         return f.read()
 
 
 def write_text_to_file(file_path, data):
-    """Write given text data to file.
+    '''
+    Write given text data to file
 
     Args:
         file_path (str or pathlib.Path): Path of the file to write data
         data (str): Text to write to the file
-    """
+    '''
     with open(file_path, 'w') as f:
         f.write(data)
```

## nnabla_rl/utils/matrices.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -16,22 +16,22 @@
 import numpy as np
 
 import nnabla as nn
 import nnabla.functions as NF
 
 
 def compute_hessian(y, x):
-    """Compute hessian (= dy^2 / dx^2) in Naive way,
+    ''' Compute hessian (= dy^2 / dx^2) in Naive way,
 
     Args:
         y (nn.Variable): Outputs of the differentiable function.
         x (list[nn.Variable]): List of parameters
     Returns:
         hessian (numpy.ndarray): Hessian of outputs with respect to the parameters
-    """
+    '''
     for param in x:
         param.grad.zero()
     grads = nn.grad([y], x)
     if len(grads) > 1:
         flat_grads = NF.concatenate(
             *[NF.reshape(grad, (-1,), inplace=False) for grad in grads])
     else:
```

## nnabla_rl/utils/misc.py

```diff
@@ -1,34 +1,26 @@
 # Copyright 2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from typing import TYPE_CHECKING, Dict, Tuple, Union, cast
-
 import numpy as np
 
-import nnabla as nn
 from nnabla.solver import Solver
-
-if TYPE_CHECKING:
-    from nnabla_rl.model_trainers.model_trainer import TrainingVariables
-
 from nnabla_rl.models import Model
-from nnabla_rl.typing import Shape
 
 
 def sync_model(src: Model, dst: Model, tau: float = 1.0):
     copy_network_parameters(origin_params=src.get_parameters(), target_params=dst.get_parameters(), tau=tau)
 
 
 def copy_network_parameters(origin_params, target_params, tau=1.0):
@@ -41,65 +33,7 @@
 
 def clip_grad_by_global_norm(solver: Solver, max_grad_norm: float):
     parameters = solver.get_parameters()
     global_norm = np.linalg.norm([np.linalg.norm(param.g) for param in parameters.values()])
     scalar = max_grad_norm / global_norm
     if scalar < 1.0:
         solver.scale_grad(scalar)
-
-
-def create_variable(batch_size: int, shape: Shape) -> Union[nn.Variable, Tuple[nn.Variable, ...]]:
-    if isinstance(shape, int):
-        return nn.Variable((batch_size, shape))
-    elif isinstance(shape[0], int):
-        return nn.Variable((batch_size, *shape))
-    else:
-        shape = cast(Tuple[Tuple[int, ...], ...], shape)
-        return tuple(nn.Variable((batch_size, *_shape)) for _shape in shape)
-
-
-def create_variables(batch_size: int, shapes: Dict[str, Tuple[int, ...]]) -> Dict[str, nn.Variable]:
-    variables: Dict[str, nn.Variale] = {}
-    for name, shape in shapes.items():
-        state: nn.Variable = create_variable(batch_size, shape)
-        state.data.zero()
-        variables[name] = state
-    return variables
-
-
-def retrieve_internal_states(scope_name: str,
-                             prev_rnn_states: Dict[str, Dict[str, nn.Variable]],
-                             train_rnn_states: Dict[str, Dict[str, nn.Variable]],
-                             training_variables: 'TrainingVariables',
-                             reset_on_terminal: bool) -> Dict[str, nn.Variable]:
-    internal_states: Dict[str, nn.Variable] = {}
-    if training_variables.is_initial_step():
-        internal_states = train_rnn_states[scope_name]
-    else:
-        prev_states = prev_rnn_states[scope_name]
-        train_states = train_rnn_states[scope_name]
-        for state_name in train_states.keys():
-            prev_state = prev_states[state_name]
-            train_state = train_states[state_name]
-            if reset_on_terminal:
-                assert training_variables.prev_step_variables
-                prev_non_terminal = training_variables.prev_step_variables.non_terminal
-                internal_states[state_name] = prev_non_terminal * prev_state + (1.0 - prev_non_terminal) * train_state
-            else:
-                internal_states[state_name] = prev_state
-    return internal_states
-
-
-def create_attention_mask(num_query: int, num_key: int) -> nn.Variable:
-    """Return attention mask for transformers.
-
-    Args:
-        num_query (int): number of query vectors
-        num_key (int): number of key vectors
-
-    Return:
-        mask (nn.Variable): additive mask to be used before softmax operation
-    """
-    mask = np.ones(shape=(num_query, num_key))
-    mask = np.triu(mask, k=1) * np.finfo(np.float32).min
-    mask = np.reshape(mask, newshape=(1, 1, num_query, num_key))
-    return nn.Variable.from_numpy_array(mask)
```

## nnabla_rl/utils/multiprocess.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -36,55 +36,55 @@
     ctype = np.ctypeslib.as_ctypes_type(dtype)
     mp_array = np.frombuffer(mp_array, dtype=ctype)
     np_array = np.ctypeslib.as_array(mp_array).reshape(np_shape)
     return np_array
 
 
 def new_mp_arrays_from_params(params):
-    """Converts nnabla's parameters to dictionary of multiprocessable arrays.
+    '''Converts nnabla's parameters to dictionary of multiprocessable arrays
 
     Args:
       params(OrderedDict): dictionary of parameters to convert to multiprocess arrays
 
     Returns: dict
       dictionary of multiprocess arrays with size same as corresponding parameter's size
-    """
+    '''
     arrays = {}
     for key in params.keys():
         param_np_array = params[key].d
         # FIXME: cast to float32.
         # This is a workaround for compensating nnabla's parameter initialization with float64
         param_np_array = param_np_array.astype(np.float32)
         arrays[key] = mp_array_from_np_array(param_np_array)
     return arrays
 
 
 def copy_params_to_mp_arrays(params, mp_arrays):
-    """Copy nnabla's parameters to multiprocessable arrays.
+    '''Copy nnabla's parameters to multiprocessable arrays
 
     Args:
       params(OrderedDict): dictionary of parameters to convert to multiprocess arrays
       mp_arrays(dict): dictionary of multiprocess arrays with keys same as corresponding parameter's key
-    """
+    '''
     for key in params.keys():
         np_array = params[key].d
         # FIXME: cast to float32.
         # This is a workaround for compensating nnabla's parameter initialization with float64
         np_array = np_array.astype(np.float32)
         mp_array = mp_arrays[key]
         mp_array = np_to_mp_array(np_array, mp_array, dtype=np_array.dtype)
 
 
 def copy_mp_arrays_to_params(mp_arrays, params):
-    """Copy nnabla's parameters from multiprocessable arrays.
+    '''Copy nnabla's parameters from multiprocessable arrays
 
     Args:
       mp_arrays(dict): dictionary of multiprocess arrays with keys same as corresponding parameter's key
       params(OrderedDict): dictionary of parameters to convert to multiprocess arrays
-    """
+    '''
     for key, mp_array in mp_arrays.items():
         param_shape = params[key].shape
         # FIXME: force using float32.
         # This is a workaround for compensating nnabla's parameter initialization with float64
         np_array = mp_to_np_array(
             mp_array, np_shape=param_shape, dtype=np.float32)
         params[key].d = np_array
```

## nnabla_rl/utils/optimization.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -13,28 +13,27 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import numpy as np
 
 
 def conjugate_gradient(compute_Ax, b, max_iterations=10, residual_tol=1e-10):
-    """Conjugate gradient method to solve x = A^-1b.
-
+    ''' Conjugate gradient method to solve x = A^-1b
         We implemented that iteration version of conjugate gradient method
         This function minimize f(x) = 0.5 * x^TAx + bx
 
     Args:
         compute_Ax (callable funtion): function of computing Ax
         b (numpy.ndarray): vector, shape like as x
         max_iterations (int): number of maximum iteration, default is 10.
             If given None, iteration lasts until residual value is enough small
         residual_tol (float): residual value of tolerance.
     Returns:
         x (numpy.ndarray): optimization results, solving x = A^-1b
-    """
+    '''
     x = np.zeros_like(b)
     r = b - compute_Ax(x)
     p = r.copy()
     square_r = np.dot(r, r)
     iteration_number = 0
 
     while True:
```

## nnabla_rl/utils/reproductions.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023,2024 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -12,79 +12,47 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import random as py_random
 
 import gym
-import gymnasium
 import numpy as np
 
 import nnabla as nn
-import nnabla_rl as rl
-from nnabla_rl.environments.environment_info import EnvironmentInfo
-from nnabla_rl.environments.wrappers import (Gymnasium2GymWrapper, NumpyFloat32Env, ScreenRenderEnv, make_atari,
-                                             wrap_deepmind)
+from nnabla_rl.environments.wrappers import NumpyFloat32Env, ScreenRenderEnv, make_atari, wrap_deepmind
 from nnabla_rl.logger import logger
 
+import nnabla_rl.environments  # noqa
+
 
 def set_global_seed(seed: int):
     np.random.seed(seed=seed)
     py_random.seed(seed)
     nn.seed(seed)
-    rl.seed(seed)
 
 
-def build_classic_control_env(id_or_env, seed=None, render=False):
+def build_atari_env(id_or_env, test=False, seed=None, render=False):
     if isinstance(id_or_env, gym.Env):
         env = id_or_env
     else:
-        env = gym.make(id_or_env)
-    env = NumpyFloat32Env(env)
-
-    if render:
-        env = ScreenRenderEnv(env)
-
-    env.seed(seed)
-    return env
+        env = make_atari(id_or_env)
+    print_env_info(env)
 
-
-def build_atari_env(id_or_env,
-                    test=False,
-                    seed=None,
-                    render=False,
-                    print_info=True,
-                    max_frames_per_episode=None,
-                    frame_stack=True,
-                    flicker_probability=0.0,
-                    use_gymnasium=False):
-    if isinstance(id_or_env, gym.Env):
-        env = id_or_env
-    elif isinstance(id_or_env, gymnasium.Env):
-        env = Gymnasium2GymWrapper(id_or_env)
-    else:
-        env = make_atari(id_or_env, max_frames_per_episode=max_frames_per_episode, use_gymnasium=use_gymnasium)
-    if print_info:
-        print_env_info(env)
-
-    env = wrap_deepmind(env,
-                        episode_life=not test,
-                        clip_rewards=not test,
-                        frame_stack=frame_stack,
-                        flicker_probability=flicker_probability)
+    env = wrap_deepmind(env, episode_life=not test, clip_rewards=not test)
     env = NumpyFloat32Env(env)
 
     if render:
         env = ScreenRenderEnv(env)
 
     env.seed(seed)
     return env
 
 
-def build_mujoco_env(id_or_env, test=False, seed=None, render=False, print_info=True, use_gymnasium=False):
+def build_mujoco_env(id_or_env, test=False, seed=None, render=False):
     try:
         # Add pybullet env
         import pybullet_envs  # noqa
     except ModuleNotFoundError:
         # Ignore if pybullet is not installed
         pass
     try:
@@ -92,52 +60,18 @@
         import d4rl  # noqa
     except ModuleNotFoundError:
         # Ignore if d4rl is not installed
         pass
 
     if isinstance(id_or_env, gym.Env):
         env = id_or_env
-    elif isinstance(id_or_env, gymnasium.Env):
-        env = Gymnasium2GymWrapper(id_or_env)
     else:
-        if use_gymnasium:
-            env = gymnasium.make(id_or_env)
-            env = Gymnasium2GymWrapper(env)
-        else:
-            env = gym.make(id_or_env)
-
-    if print_info:
-        print_env_info(env)
-
-    env = NumpyFloat32Env(env)
-
-    if render:
-        env = ScreenRenderEnv(env)
-
-    env.seed(seed)
-    return env
-
-
-def build_dmc_env(id_or_env, test=False, seed=None, render=False, print_info=True):
-    from nnabla_rl.external.dmc_env import DMCEnv
-
-    if isinstance(id_or_env, gym.Env):
-        env = id_or_env
-    elif id_or_env.startswith("FakeDMControl"):
         env = gym.make(id_or_env)
-    else:
-        domain_name, task_name = id_or_env.split('-')
-        env = DMCEnv(domain_name,
-                     task_name=task_name,
-                     task_kwargs={'random': seed})
-        env = gym.wrappers.FlattenObservation(env)
-        env = gym.wrappers.RescaleAction(env, min_action=-1., max_action=1.)
+    print_env_info(env)
 
-    if print_info:
-        print_env_info(env)
     env = NumpyFloat32Env(env)
 
     if render:
         env = ScreenRenderEnv(env)
 
     env.seed(seed)
     return env
@@ -162,20 +96,34 @@
 
 def print_env_info(env):
     if env.unwrapped.spec is not None:
         env_name = env.unwrapped.spec.id
     else:
         env_name = 'Unknown'
 
-    env_info = EnvironmentInfo.from_env(env)
+    if isinstance(env.observation_space, gym.spaces.Discrete):
+        state_dim = env.observation_space.n
+        state_high = 'N/A'
+        state_low = 'N/A'
+    elif isinstance(env.observation_space, gym.spaces.Box):
+        state_dim = env.observation_space.shape
+        state_high = env.observation_space.high
+        state_low = env.observation_space.low
+
+    if isinstance(env.action_space, gym.spaces.Discrete):
+        action_dim = env.action_space.n
+        action_high = 'N/A'
+        action_low = 'N/A'
+    elif isinstance(env.action_space, gym.spaces.Box):
+        action_dim = env.action_space.shape
+        action_high = env.action_space.high
+        action_low = env.action_space.low
 
     info = f'''env: {env_name},
-               state_dim: {env_info.state_dim},
-               state_shape: {env_info.state_shape},
-               state_high: {env_info.state_high},
-               state_low: {env_info.state_low},
-               action_dim: {env_info.action_dim},
-               action_shape: {env_info.action_shape},
-               action_high: {env_info.action_high},
-               action_low: {env_info.action_low},
+               space_dim/classes: {state_dim},
+               space_high: {state_high},
+               space_low: {state_low},
+               action_dim/classes: {action_dim},
+               action_high: {action_high},
+               action_low: {action_low},
                max_episode_steps: {env.spec.max_episode_steps}'''
     logger.info(info)
```

## nnabla_rl/utils/serializers.py

```diff
@@ -1,105 +1,97 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import dataclasses
 import json
 import pathlib
+import pickle
 import warnings
 
-import gym
-
 import nnabla_rl.algorithms as A
 import nnabla_rl.utils.files as files
 from nnabla_rl.algorithm import Algorithm
-from nnabla_rl.environments.environment_info import EnvironmentInfo
 
 _TRAINING_INFO_FILENAME = 'training_info.json'
+_ENV_INFO_FILENAME = 'env_info.pickle'
 
 _KEY_ALGORITHM_NAME = 'algorithm_name'
 _KEY_ALGORITHM_CLASS_NAME = 'algorithm_class_name'
 _KEY_ALGORITHM_CONFIG = 'algorithm_config'
 _KEY_ITERATION_NUM = 'iteration_num'
 _KEY_MODELS = 'models'
 _KEY_SOLVERS = 'solvers'
 
 
 def save_snapshot(path, algorithm):
-    """Save training snapshot to file.
+    '''Save training snapshot to file
 
     Args:
       path(str or pathlib.Path): Path to the snapshot saved dir
       algorithm(nnabla_rl.Algorithm): Algorithm object to save the snapshot
 
     Returns: pathlib.Path
       File path where the snapshot is saved to
-    """
+    '''
     assert isinstance(algorithm, Algorithm)
     if isinstance(path, str):
         path = pathlib.Path(path)
     dirname = 'iteration-' + str(algorithm.iteration_num)
     outdir = path / dirname
     files.create_dir_if_not_exist(outdir=outdir)
 
     training_info = _create_training_info(algorithm)
     _save_training_info(outdir, training_info)
+    _save_env_info(outdir, algorithm)
     _save_network_parameters(outdir, algorithm)
     _save_solver_states(outdir, algorithm)
 
     return outdir
 
 
 def load_snapshot(path,
-                  env_or_env_info,
                   algorithm_kwargs={}):
-    """Load training snapshot from file.
+    '''Load training snapshot from file
 
     Args:
       path(str or pathlib.Path): Path to the snapshot saved dir
       algorithm_kwargs(dictionary): parameters passed to the constructor of loaded algorithm class
 
     Returns: nnabla_rl.Algorithm
       Algorithm with parameters and settings loaded from file
-    """
+    '''
     if isinstance(path, str):
         path = pathlib.Path(path)
-    if not isinstance(env_or_env_info, (gym.Env, EnvironmentInfo)):
-        raise RuntimeError(
-            'load_snapshot requires training gym.Env or EnvironmentInfo. '
-            'Automatic loading of env_info is no longer supported since v0.10.0')
     training_info = _load_training_info(path)
+    env_info = _load_env_info(path)
     algorithm = _instantiate_algorithm_from_training_info(
-        training_info, env_or_env_info, **algorithm_kwargs)
+        training_info, env_info, **algorithm_kwargs)
     _load_network_parameters(path, algorithm)
     _load_solver_states(path, algorithm)
     return algorithm
 
 
 def _instantiate_algorithm_from_training_info(training_info, env_info, **kwargs):
     algorithm_name = training_info[_KEY_ALGORITHM_CLASS_NAME]
     (algorithm_klass, config_klass) = A.get_class_of(algorithm_name)
 
-    config = kwargs.get('config', None)
-    if not isinstance(config, config_klass):
+    if kwargs.get('config', None) is None:
         saved_config = training_info[_KEY_ALGORITHM_CONFIG]
-        saved_config = config_klass(**saved_config)
-        config = dataclasses.replace(saved_config, **config) if isinstance(config, dict) else saved_config
-    kwargs['config'] = config
+        kwargs['config'] = config_klass(**saved_config)
     algorithm = algorithm_klass(env_info, **kwargs)
     algorithm._iteration_num = training_info[_KEY_ITERATION_NUM]
     return algorithm
 
 
 def _create_training_info(algorithm):
     training_info = {}
@@ -109,14 +101,26 @@
     training_info[_KEY_ITERATION_NUM] = algorithm.iteration_num
     training_info[_KEY_MODELS] = list(algorithm._models().keys())
     training_info[_KEY_SOLVERS] = list(algorithm._solvers().keys())
 
     return training_info
 
 
+def _save_env_info(path, algorithm):
+    filepath = path / _ENV_INFO_FILENAME
+    with open(filepath, 'wb+') as outfile:
+        pickle.dump(algorithm._env_info, outfile)
+
+
+def _load_env_info(path):
+    filepath = path / _ENV_INFO_FILENAME
+    with open(filepath, 'rb') as infile:
+        return pickle.load(infile)
+
+
 def _save_training_info(path, training_info):
     filepath = path / _TRAINING_INFO_FILENAME
     with open(filepath, 'w+') as outfile:
         json.dump(training_info, outfile)
 
 
 def _load_training_info(path):
```

## nnabla_rl/writers/__init__.py

```diff
@@ -1,17 +1,16 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from nnabla_rl.writers.file_writer import FileWriter  # noqa
-from nnabla_rl.writers.monitor_writer import MonitorWriter  # noqa
```

## nnabla_rl/writers/file_writer.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023,2024 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -18,22 +18,22 @@
 import numpy as np
 
 import nnabla_rl.utils.files as files
 from nnabla_rl.writer import Writer
 
 
 class FileWriter(Writer):
-    def __init__(self, outdir, file_prefix, fmt="%.3f"):
+    def __init__(self, outdir, file_prefix):
         super(FileWriter, self).__init__()
         if isinstance(outdir, str):
             outdir = pathlib.Path(outdir)
         self._outdir = outdir
         files.create_dir_if_not_exist(outdir=outdir)
         self._file_prefix = file_prefix
-        self._fmt = fmt
+        self._fmt = '%.3f'
 
     def write_scalar(self, iteration_num, scalar):
         outfile = self._outdir / (self._file_prefix + '_scalar.tsv')
 
         len_scalar = len(scalar.values())
         out_scalar = {}
         out_scalar['iteration'] = iteration_num
```

## tests/algorithms/test_a2c.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -26,35 +26,42 @@
 
     def test_algorithm_name(self):
         dummy_env = E.DummyDiscreteImg()
         a2c = A.A2C(dummy_env)
 
         assert a2c.__name__ == 'A2C'
 
-    def test_continuous_action_env_unsupported(self):
-        """Check that error occurs when training on continuous action env."""
+    def test_continuous_env_unsupported(self):
+        '''
+        Check that error occurs when training on continuous env
+        '''
+
         dummy_env = E.DummyContinuous()
         config = A.A2CConfig()
         with pytest.raises(Exception):
             A.A2C(dummy_env, config=config)
 
     def test_run_online_discrete_env_training(self):
-        """Check that no error occurs when calling online training (discrete
-        env)"""
+        '''
+        Check that no error occurs when calling online training (discrete env)
+        '''
 
         dummy_env = E.DummyDiscreteImg()
         n_steps = 4
         actor_num = 2
         config = A.A2CConfig(n_steps=n_steps, actor_num=actor_num)
         a2c = A.A2C(dummy_env, config=config)
 
         a2c.train_online(dummy_env, total_iterations=n_steps*actor_num)
 
     def test_run_offline_training(self):
-        """Check that no error occurs when calling offline training."""
+        '''
+        Check that no error occurs when calling offline training
+        '''
+
         dummy_env = E.DummyDiscreteImg()
         a2c = A.A2C(dummy_env)
 
         with pytest.raises(ValueError):
             a2c.train_offline([], total_iterations=10)
 
     def test_parameter_range(self):
@@ -70,16 +77,17 @@
             A.A2CConfig(n_steps=-1)
         with pytest.raises(ValueError):
             A.A2CConfig(actor_num=-1)
         with pytest.raises(ValueError):
             A.A2CConfig(learning_rate=-1)
 
     def test_latest_iteration_state(self):
-        """Check that latest iteration state has the keys and values we
-        expected."""
+        '''
+        Check that latest iteration state has the keys and values we expected
+        '''
 
         dummy_env = E.DummyDiscreteImg()
         a2c = A.A2C(dummy_env)
 
         a2c._policy_trainer_state = {'pi_loss': 0.}
         a2c._v_function_trainer_state = {'v_loss': 1.}
```

## tests/algorithms/test_bcq.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -29,31 +29,40 @@
     def test_algorithm_name(self):
         dummy_env = E.DummyContinuous()
         bcq = A.BCQ(dummy_env)
 
         assert bcq.__name__ == 'BCQ'
 
     def test_run_online_training(self):
-        """Check that error occurs when calling online training."""
+        '''
+        Check that error occurs when calling online training
+        '''
+
         dummy_env = E.DummyContinuous()
         config = A.BCQConfig()
         bcq = A.BCQ(dummy_env, config=config)
 
         with pytest.raises(NotImplementedError):
             bcq.train_online(dummy_env, total_iterations=10)
 
-    def test_discrete_action_env_unsupported(self):
-        """Check that error occurs when training on discrete action env."""
+    def test_discrete_env_unsupported(self):
+        '''
+        Check that error occurs when training on discrete env
+        '''
+
         dummy_env = E.DummyDiscrete()
         config = A.BCQConfig()
         with pytest.raises(Exception):
             A.BCQ(dummy_env, config=config)
 
     def test_run_offline_training(self):
-        """Check that no error occurs when calling offline training."""
+        '''
+        Check that no error occurs when calling offline training
+        '''
+
         batch_size = 5
         dummy_env = E.DummyContinuous()
         config = A.BCQConfig(batch_size=batch_size)
         bcq = A.BCQ(dummy_env, config=config)
 
         experiences = generate_dummy_experiences(dummy_env, batch_size)
         buffer = ReplayBuffer()
@@ -84,16 +93,17 @@
             A.BCQConfig(phi=-0.1)
         with pytest.raises(ValueError):
             A.BCQConfig(num_q_ensembles=-100)
         with pytest.raises(ValueError):
             A.BCQConfig(num_action_samples=-100)
 
     def test_latest_iteration_state(self):
-        """Check that latest iteration state has the keys and values we
-        expected."""
+        '''
+        Check that latest iteration state has the keys and values we expected
+        '''
 
         dummy_env = E.DummyContinuous()
         bcq = A.BCQ(dummy_env)
 
         bcq._encoder_trainer_state = {'encoder_loss': 0.}
         bcq._q_function_trainer_state = {'q_loss': 1., 'td_errors': np.array([0., 1.])}
         bcq._perturbator_trainer_state = {'perturbator_loss': 2.}
@@ -106,11 +116,13 @@
         assert latest_iteration_state['scalar']['encoder_loss'] == 0.
         assert latest_iteration_state['scalar']['q_loss'] == 1.
         assert latest_iteration_state['scalar']['perturbator_loss'] == 2.
         assert np.allclose(latest_iteration_state['histogram']['td_errors'], np.array([0., 1.]))
 
 
 if __name__ == "__main__":
+    import sys
+    sys.path.insert(0, "./")
     from testing_utils import generate_dummy_experiences
     pytest.main()
 else:
-    from ..testing_utils import generate_dummy_experiences
+    from .testing_utils import generate_dummy_experiences
```

## tests/algorithms/test_bear.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -28,31 +28,40 @@
 
     def test_algorithm_name(self):
         dummy_env = E.DummyContinuous()
         bear = A.BEAR(dummy_env)
 
         assert bear.__name__ == 'BEAR'
 
-    def test_discrete_action_env_unsupported(self):
-        """Check that error occurs when training on discrete action env."""
+    def test_discrete_env_unsupported(self):
+        '''
+        Check that error occurs when training on discrete env
+        '''
+
         dummy_env = E.DummyDiscrete()
         config = A.BEARConfig()
         with pytest.raises(Exception):
             A.BEAR(dummy_env, config=config)
 
     def test_run_online_training(self):
-        """Check that error occurs when calling online training."""
+        '''
+        Check that error occurs when calling online training
+        '''
+
         dummy_env = E.DummyContinuous()
         bear = A.BEAR(dummy_env)
 
         with pytest.raises(NotImplementedError):
             bear.train_online(dummy_env, total_iterations=10)
 
     def test_run_offline_training(self):
-        """Check that no error occurs when calling offline training."""
+        '''
+        Check that no error occurs when calling offline training
+        '''
+
         batch_size = 5
         dummy_env = E.DummyContinuous()
         config = A.BEARConfig(batch_size=batch_size)
         bear = A.BEAR(dummy_env, config=config)
 
         experiences = generate_dummy_experiences(dummy_env, batch_size)
         buffer = ReplayBuffer()
@@ -84,16 +93,17 @@
             A.BEARConfig(num_mmd_actions=-100)
         with pytest.raises(ValueError):
             A.BEARConfig(num_action_samples=-100)
         with pytest.raises(ValueError):
             A.BEARConfig(warmup_iterations=-100)
 
     def test_latest_iteration_state(self):
-        """Check that latest iteration state has the keys and values we
-        expected."""
+        '''
+        Check that latest iteration state has the keys and values we expected
+        '''
 
         dummy_env = E.DummyContinuous()
         bear = A.BEAR(dummy_env)
 
         bear._encoder_trainer_state = {'encoder_loss': 0.}
         bear._q_function_trainer_state = {'q_loss': 1., 'td_errors': np.array([0., 1.])}
         bear._policy_trainer_state = {'pi_loss': 2.}
@@ -106,11 +116,13 @@
         assert latest_iteration_state['scalar']['encoder_loss'] == 0.
         assert latest_iteration_state['scalar']['q_loss'] == 1.
         assert latest_iteration_state['scalar']['pi_loss'] == 2.
         assert np.allclose(latest_iteration_state['histogram']['td_errors'], np.array([0., 1.]))
 
 
 if __name__ == "__main__":
+    import sys
+    sys.path.insert(0, "./")
     from testing_utils import generate_dummy_experiences
     pytest.main()
 else:
-    from ..testing_utils import generate_dummy_experiences
+    from .testing_utils import generate_dummy_experiences
```

## tests/algorithms/test_categorical_dqn.py

```diff
@@ -1,176 +1,71 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from typing import Dict, Optional, Tuple
-
 import numpy as np
 import pytest
 
 import nnabla as nn
-import nnabla.functions as NF
-import nnabla.parametric_functions as NPF
 import nnabla_rl.algorithms as A
 import nnabla_rl.environments as E
-from nnabla_rl.builders.model_builder import ModelBuilder
-from nnabla_rl.models import DiscreteValueDistributionFunction, ValueDistributionFunction
 from nnabla_rl.replay_buffer import ReplayBuffer
 
 
-class RNNValueDistributionFunction(DiscreteValueDistributionFunction):
-    def __init__(self, scope_name: str, n_action: int, n_atom: int, v_min: float, v_max: float):
-        super().__init__(scope_name, n_action, n_atom, v_min, v_max)
-        self._h = None
-        self._c = None
-
-        self._lstm_state_size = 512
-
-    def all_probs(self, s: nn.Variable) -> nn.Variable:
-        batch_size = s.shape[0]
-        with nn.parameter_scope(self.scope_name):
-            with nn.parameter_scope("conv1"):
-                h = NPF.convolution(s, outmaps=32, stride=(4, 4), kernel=(8, 8))
-            h = NF.relu(x=h)
-            with nn.parameter_scope("conv2"):
-                h = NPF.convolution(h, outmaps=64, kernel=(4, 4), stride=(2, 2))
-            h = NF.relu(x=h)
-            with nn.parameter_scope("conv3"):
-                h = NPF.convolution(h, outmaps=64, kernel=(3, 3), stride=(1, 1))
-            h = NF.relu(x=h)
-            h = NF.reshape(h, shape=(batch_size, -1))
-            with nn.parameter_scope("affine1"):
-                if not self._is_internal_state_created():
-                    # automatically create internal states if not provided
-                    batch_size = h.shape[0]
-                    self._create_internal_states(batch_size)
-                self._h, self._c = NPF.lstm_cell(h, self._h, self._c, self._lstm_state_size)
-                h = self._h
-            h = NF.relu(x=h)
-            with nn.parameter_scope("affine2"):
-                h = NPF.affine(
-                    h, n_outmaps=self._n_action * self._n_atom)
-            h = NF.reshape(h, (-1, self._n_action, self._n_atom))
-        assert h.shape == (batch_size, self._n_action, self._n_atom)
-        return NF.softmax(h, axis=2)
-
-    def is_recurrent(self) -> bool:
-        return True
-
-    def internal_state_shapes(self) -> Dict[str, Tuple[int, ...]]:
-        shapes: Dict[str, nn.Variable] = {}
-        shapes['lstm_hidden'] = (self._lstm_state_size, )
-        shapes['lstm_cell'] = (self._lstm_state_size, )
-        return shapes
-
-    def get_internal_states(self) -> Dict[str, nn.Variable]:
-        states: Dict[str, nn.Variable] = {}
-        states['lstm_hidden'] = self._h
-        states['lstm_cell'] = self._c
-        return states
-
-    def set_internal_states(self, states: Optional[Dict[str, nn.Variable]] = None):
-        if states is None:
-            if self._h is not None:
-                self._h.data.zero()
-            if self._c is not None:
-                self._c.data.zero()
-        else:
-            self._h = states['lstm_hidden']
-            self._c = states['lstm_cell']
-
-    def _create_internal_states(self, batch_size):
-        self._h = nn.Variable((batch_size, self._lstm_state_size))
-        self._c = nn.Variable((batch_size, self._lstm_state_size))
-
-        self._h.data.zero()
-        self._c.data.zero()
-
-    def _is_internal_state_created(self) -> bool:
-        return self._h is not None and self._c is not None
-
-
 class TestCategoricalDQN(object):
     def setup_method(self, method):
         nn.clear_parameters()
 
     def test_algorithm_name(self):
         dummy_env = E.DummyDiscreteImg()
         categorical_dqn = A.CategoricalDQN(dummy_env)
 
         assert categorical_dqn.__name__ == 'CategoricalDQN'
 
-    def test_continuous_action_env_unsupported(self):
-        """Check that error occurs when training on continuous action env."""
+    def test_continuous_env_unsupported(self):
+        '''
+        Check that error occurs when training on continuous env
+        '''
+
         dummy_env = E.DummyContinuous()
         config = A.CategoricalDQNConfig()
         with pytest.raises(Exception):
             A.CategoricalDQN(dummy_env, config=config)
 
     def test_run_online_training(self):
-        """Check that no error occurs when calling online training."""
-        dummy_env = E.DummyDiscreteImg()
-        config = A.CategoricalDQNConfig()
-        config.start_timesteps = 5
-        config.batch_size = 5
-        config.learner_update_frequency = 1
-        config.target_update_frequency = 1
-        categorical_dqn = A.CategoricalDQN(dummy_env, config=config)
-
-        categorical_dqn.train_online(dummy_env, total_iterations=10)
+        '''
+        Check that no error occurs when calling online training
+        '''
 
-    def test_run_online_training_multistep(self):
-        """Check that no error occurs when calling online training."""
         dummy_env = E.DummyDiscreteImg()
         config = A.CategoricalDQNConfig()
-        config.num_steps = 2
         config.start_timesteps = 5
         config.batch_size = 5
         config.learner_update_frequency = 1
         config.target_update_frequency = 1
         categorical_dqn = A.CategoricalDQN(dummy_env, config=config)
 
         categorical_dqn.train_online(dummy_env, total_iterations=10)
 
-    def test_run_online_rnn_training(self):
-        """Check that no error occurs when calling online training with RNN
-        model."""
-        class RNNModelBuilder(ModelBuilder[ValueDistributionFunction]):
-            def build_model(self, scope_name: str, env_info, algorithm_config, **kwargs):
-                n_action = env_info.action_dim
-                n_atom = algorithm_config.num_atoms
-                v_min = algorithm_config.v_min
-                v_max = algorithm_config.v_max
-                return RNNValueDistributionFunction(scope_name, n_action, n_atom, v_min, v_max)
-        dummy_env = E.DummyDiscreteImg()
-        config = A.CategoricalDQNConfig()
-        config.num_steps = 2
-        config.unroll_steps = 2
-        config.burn_in_steps = 2
-        config.start_timesteps = 7
-        config.batch_size = 2
-        config.learner_update_frequency = 1
-        config.target_update_frequency = 1
-        categorical_dqn = A.CategoricalDQN(dummy_env, config=config, value_distribution_builder=RNNModelBuilder())
-
-        categorical_dqn.train_online(dummy_env, total_iterations=10)
-
     def test_run_offline_training(self):
-        """Check that no error occurs when calling offline training."""
+        '''
+        Check that no error occurs when calling offline training
+        '''
+
         batch_size = 5
         dummy_env = E.DummyDiscreteImg()
         config = A.CategoricalDQNConfig(batch_size=batch_size)
         categorical_dqn = A.CategoricalDQN(dummy_env, config=config)
 
         experiences = generate_dummy_experiences(dummy_env, batch_size)
         buffer = ReplayBuffer()
@@ -184,27 +79,30 @@
         state = dummy_env.reset()
         state = np.float32(state)
         action = categorical_dqn.compute_eval_action(state)
 
         assert action.shape == (1, )
 
     def test_latest_iteration_state(self):
-        """Check that latest iteration state has the keys and values we
-        expected."""
+        '''
+        Check that latest iteration state has the keys and values we expected
+        '''
 
         dummy_env = E.DummyDiscreteImg()
         categorical_dqn = A.CategoricalDQN(dummy_env)
 
         categorical_dqn._model_trainer_state = {'cross_entropy_loss': 0., 'td_errors': np.array([0., 1.])}
 
         latest_iteration_state = categorical_dqn.latest_iteration_state
         assert 'cross_entropy_loss' in latest_iteration_state['scalar']
         assert 'td_errors' in latest_iteration_state['histogram']
         assert latest_iteration_state['scalar']['cross_entropy_loss'] == 0.
         assert np.allclose(latest_iteration_state['histogram']['td_errors'], np.array([0., 1.]))
 
 
 if __name__ == "__main__":
+    import sys
+    sys.path.insert(0, "./")
     from testing_utils import generate_dummy_experiences
     pytest.main()
 else:
-    from ..testing_utils import generate_dummy_experiences
+    from .testing_utils import generate_dummy_experiences
```

## tests/algorithms/test_common_utils.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -13,138 +13,62 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import numpy as np
 import pytest
 
 import nnabla as nn
-from nnabla_rl.algorithms.common_utils import (_DeterministicPolicyActionSelector, _InfluenceMetricsEvaluator,
-                                               _StatePreprocessedDeterministicPolicy,
-                                               _StatePreprocessedStochasticPolicy, _StatePreprocessedVFunction,
-                                               compute_average_v_target_and_advantage, compute_v_target_and_advantage,
-                                               has_batch_dimension)
-from nnabla_rl.environments.dummy import (DummyContinuous, DummyContinuousActionGoalEnv, DummyDiscrete,
-                                          DummyFactoredContinuous, DummyTupleContinuous, DummyTupleDiscrete)
-from nnabla_rl.environments.environment_info import EnvironmentInfo
+from nnabla_rl.algorithms.common_utils import (_StatePreprocessedPolicy, _StatePreprocessedVFunction,
+                                               compute_v_target_and_advantage)
 from nnabla_rl.models import VFunction
 
 
 class DummyVFunction(VFunction):
     def __init__(self):
         super(DummyVFunction, self).__init__("test_v_function")
+        self._state_dim = 1
 
     def v(self, s):
         with nn.parameter_scope(self.scope_name):
-            if isinstance(s, tuple):
-                h = s[0] * 2. + s[1] * 2.
-            else:
-                h = s * 2.
+            h = s * 2.
         return h
 
 
 class TestCommonUtils():
     def setup_method(self, method):
         nn.clear_parameters()
 
-    def _collect_dummy_experience(self, num_episodes=1, episode_length=3, tupled_state=False):
+    def _collect_dummy_experince(self, num_episodes=1, episode_length=3):
         experience = []
         for _ in range(num_episodes):
             for i in range(episode_length):
-                s_current = (np.ones(1, ), np.ones(1, )) if tupled_state else np.ones(1, )
+                s_current = np.ones(1, )
                 a = np.ones(1, )
-                s_next = (np.ones(1, ), np.ones(1, )) if tupled_state else np.ones(1, )
+                s_next = np.ones(1, )
                 r = np.ones(1, )
                 non_terminal = np.ones(1, )
                 if i == episode_length-1:
-                    non_terminal = np.zeros(1, )
+                    non_terminal = 0
                 experience.append((s_current, a, r, non_terminal, s_next))
         return experience
 
-    def test_has_batch_dimension_tupled_continuous_state(self):
-        env = DummyTupleContinuous()
-        env_info = EnvironmentInfo.from_env(env)
-
-        batch_size = 5
-        state_shapes = env_info.state_shape
-        batched_state = tuple(np.empty(shape=(batch_size, *state_shape)) for state_shape in state_shapes)
-        non_batched_state = tuple(np.empty(shape=state_shape) for state_shape in state_shapes)
-
-        assert has_batch_dimension(batched_state, env_info)
-        assert not has_batch_dimension(non_batched_state, env_info)
-
-    def test_has_batch_dimension_tupled_discrete_state(self):
-        env = DummyTupleDiscrete()
-        env_info = EnvironmentInfo.from_env(env)
-
-        batch_size = 5
-        state_shapes = env_info.state_shape
-        batched_state = tuple(np.empty(shape=(batch_size, *state_shape)) for state_shape in state_shapes)
-        non_batched_state = tuple(np.empty(shape=state_shape) for state_shape in state_shapes)
-
-        assert has_batch_dimension(batched_state, env_info)
-        assert not has_batch_dimension(non_batched_state, env_info)
-
-    def test_has_batch_dimension_non_tupled_continuous_state(self):
-        env = DummyContinuous()
-        env_info = EnvironmentInfo.from_env(env)
-
-        batch_size = 5
-        state_shape = env_info.state_shape
-        batched_state = np.empty(shape=(batch_size, *state_shape))
-        non_batched_state = np.empty(shape=state_shape)
-
-        assert has_batch_dimension(batched_state, env_info)
-        assert not has_batch_dimension(non_batched_state, env_info)
-
-    def test_has_batch_dimension_non_tupled_discrete_state(self):
-        env = DummyDiscrete()
-        env_info = EnvironmentInfo.from_env(env)
-
-        batch_size = 5
-        state_shape = env_info.state_shape
-        batched_state = np.empty(shape=(batch_size, *state_shape))
-        non_batched_state = np.empty(shape=state_shape)
-
-        assert has_batch_dimension(batched_state, env_info)
-        assert not has_batch_dimension(non_batched_state, env_info)
-
-    @pytest.mark.parametrize("gamma, lmb, expected_adv, expected_vtarg, tupled_state",
-                             [[1., 0., np.array([[1.], [1.], [-1.]]), np.array([[3.], [3.], [1.]]), False],
-                              [1., 1., np.array([[1.], [0.], [-1.]]), np.array([[3.], [2.], [1.]]), False],
-                              [0.9, 0.7, np.array([[0.9071], [0.17], [-1.]]),
-                               np.array([[2.9071], [2.17], [1.]]), False],
-                              [1., 0., np.array([[1.], [1.], [-3.]]), np.array([[5.], [5.], [1.]]), True],
-                              [1., 1., np.array([[-1.], [-2.], [-3.]]), np.array([[3.], [2.], [1.]]), True],
+    @pytest.mark.parametrize("gamma, lmb, expected",
+                             [[1., 0., np.array([[1.], [1.], [-1.]])],
+                              [1., 1., np.array([[1.], [0.], [-1.]])],
+                              [0.9, 0.7, np.array([[0.9071], [0.17], [-1.]])],
                               ])
-    def test_compute_v_target_and_advantage(self, gamma, lmb, expected_adv, expected_vtarg, tupled_state):
+    def test_compute(self, gamma, lmb, expected):
         dummy_v_function = DummyVFunction()
-        dummy_experience = self._collect_dummy_experience(tupled_state=tupled_state)
+        dummy_experince = self._collect_dummy_experince()
 
         actual_vtarg, actual_adv = compute_v_target_and_advantage(
-            dummy_v_function, dummy_experience, gamma, lmb)
+            dummy_v_function, dummy_experince, gamma, lmb)
 
-        assert np.allclose(actual_adv, expected_adv)
-        assert np.allclose(actual_vtarg, expected_vtarg)
-
-    @pytest.mark.parametrize("lmb, expected_adv, expected_vtarg, tupled_state",
-                             [[0., np.array([[0.], [0.], [-2.]]), np.array([[2.], [2.], [0.]]), False],
-                              [1., np.array([[-2.], [-2.], [-2.]]), np.array([[0.], [0.], [0.]]), False],
-                              [0.7, np.array([[-0.98], [-1.4], [-2.]]), np.array([[1.02], [0.6], [0.]]), False],
-                              [0., np.array([[0.], [0.], [-4.]]), np.array([[4.], [4.], [0.]]), True],
-                              [1., np.array([[-4.], [-4.], [-4.]]), np.array([[0.], [0.], [0.]]), True],
-                              ])
-    def test_compute_average_v_target_and_advantage(self, lmb, expected_adv, expected_vtarg, tupled_state):
-        dummy_v_function = DummyVFunction()
-        dummy_experience = self._collect_dummy_experience(tupled_state=tupled_state)
-
-        actual_vtarg, actual_adv = compute_average_v_target_and_advantage(
-            dummy_v_function, dummy_experience, lmb)
-
-        assert np.allclose(actual_adv, expected_adv)
-        assert np.allclose(actual_vtarg, expected_vtarg)
+        assert np.allclose(actual_adv, expected)
+        assert np.allclose(actual_vtarg, expected + 2.)
 
     def test_state_preprocessed_v_function(self):
         state_shape = (5, )
 
         from nnabla_rl.models import TRPOVFunction
         v_scope_name = 'old_v'
         v_function = TRPOVFunction(v_scope_name)
@@ -160,134 +84,35 @@
 
         v_new_scope_name = 'new_v'
         v_function_new = v_function_old.deepcopy(v_new_scope_name)
 
         assert v_function_old.scope_name != v_function_new.scope_name
         assert v_function_old._preprocessor.scope_name == v_function_new._preprocessor.scope_name
 
-    def test_state_preprocessed_stochastic_policy(self):
+    def test_state_preprocessed_policy(self):
         state_shape = (5, )
         action_dim = 10
 
         from nnabla_rl.models import TRPOPolicy
         pi_scope_name = 'old_pi'
         pi = TRPOPolicy(pi_scope_name, action_dim=action_dim)
 
         import nnabla_rl.preprocessors as RP
         preprocessor_scope_name = 'test_preprocessor'
         preprocessor = RP.RunningMeanNormalizer(preprocessor_scope_name, shape=state_shape)
 
-        pi_old = _StatePreprocessedStochasticPolicy(policy=pi, preprocessor=preprocessor)
-
-        s = nn.Variable.from_numpy_array(np.empty(shape=(1, *state_shape)))
-        _ = pi_old.pi(s)
-
-        pi_new_scope_name = 'new_pi'
-        pi_new = pi_old.deepcopy(pi_new_scope_name)
-
-        assert pi_old.scope_name != pi_new.scope_name
-        assert pi_old._preprocessor.scope_name == pi_new._preprocessor.scope_name
-
-    def test_state_preprocessed_deterministic_policy(self):
-        state_shape = (5, )
-        action_dim = 10
-
-        from nnabla_rl.models import TD3Policy
-        pi_scope_name = 'old_pi'
-        pi = TD3Policy(pi_scope_name, action_dim=action_dim, max_action_value=1.0)
-
-        import nnabla_rl.preprocessors as RP
-        preprocessor_scope_name = 'test_preprocessor'
-        preprocessor = RP.RunningMeanNormalizer(preprocessor_scope_name, shape=state_shape)
-
-        pi_old = _StatePreprocessedDeterministicPolicy(policy=pi, preprocessor=preprocessor)
+        pi_old = _StatePreprocessedPolicy(policy=pi, preprocessor=preprocessor)
 
         s = nn.Variable.from_numpy_array(np.empty(shape=(1, *state_shape)))
         _ = pi_old.pi(s)
 
-        pi_new_scope_name = 'new_pi'
+        pi_new_scope_name = 'new_v'
         pi_new = pi_old.deepcopy(pi_new_scope_name)
 
         assert pi_old.scope_name != pi_new.scope_name
         assert pi_old._preprocessor.scope_name == pi_new._preprocessor.scope_name
 
-    def test_action_selector_tupled_state(self):
-        from nnabla_rl.environments.wrappers.goal_conditioned import GoalConditionedTupleObservationEnv
-
-        env = DummyContinuousActionGoalEnv()
-        env = GoalConditionedTupleObservationEnv(env)
-        env_info = EnvironmentInfo.from_env(env)
-
-        action_dim = env_info.action_dim
-
-        from nnabla_rl.models import HERPolicy
-        pi_scope_name = 'pi'
-        pi = HERPolicy(pi_scope_name, action_dim=action_dim, max_action_value=1.0)
-
-        selector = _DeterministicPolicyActionSelector(env_info, pi)
-
-        batch_size = 5
-        state_shapes = env_info.state_shape
-        batched_state = tuple(np.empty(shape=(batch_size, *state_shape)) for state_shape in state_shapes)
-
-        action, *_ = selector(batched_state)
-        assert len(action) == batch_size
-        assert action.shape[1:] == env_info.action_shape
-
-        non_batched_state = tuple(np.empty(shape=state_shape) for state_shape in state_shapes)
-        action, *_ = selector(non_batched_state)
-
-        assert action.shape == env_info.action_shape
-
-    def test_action_selector_non_tupled_state(self):
-        env = DummyContinuous()
-        env_info = EnvironmentInfo.from_env(env)
-
-        action_dim = env_info.action_dim
-
-        from nnabla_rl.models import TD3Policy
-        pi_scope_name = 'pi'
-        pi = TD3Policy(pi_scope_name, action_dim=action_dim, max_action_value=1.0)
-
-        selector = _DeterministicPolicyActionSelector(env_info, pi)
-
-        batch_size = 5
-        state_shape = env_info.state_shape
-        batched_state = np.empty(shape=(batch_size, *state_shape))
-
-        action, *_ = selector(batched_state)
-        assert len(action) == batch_size
-        assert action.shape[1:] == env_info.action_shape
-
-        non_batched_state = np.empty(shape=state_shape)
-        action, *_ = selector(non_batched_state)
-
-        assert action.shape == env_info.action_shape
-
-    def test_influence_metrics_evaluator(self):
-        num_factors = 2
-
-        env = DummyFactoredContinuous(reward_dimension=num_factors)
-        env_info = EnvironmentInfo.from_env(env)
-
-        from nnabla_rl.models import SACDQFunction
-        q_scope_name = "q"
-        q_function = SACDQFunction(q_scope_name, num_factors=num_factors)
-
-        evaluator = _InfluenceMetricsEvaluator(env_info, q_function)
-
-        batch_size = 5
-        state_shape = env_info.state_shape
-        action_shape = env_info.action_shape
-        batched_state = np.empty(shape=(batch_size, *state_shape))
-        batched_action = np.empty(shape=(batch_size, *action_shape))
-        influence, _ = evaluator(batched_state, batched_action)
-        assert influence.shape == (batch_size, num_factors)
-
-        non_batched_state = np.empty(shape=state_shape)
-        non_batched_action = np.empty(shape=action_shape)
-        influence, _ = evaluator(non_batched_state, non_batched_action)
-        assert influence.shape == (num_factors,)
-
 
 if __name__ == "__main__":
+    import sys
+    sys.path.insert(0, "./")
     pytest.main()
```

## tests/algorithms/test_ddpg.py

```diff
@@ -1,220 +1,68 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from typing import Dict, Optional, Tuple
-
 import numpy as np
 import pytest
 
 import nnabla as nn
-import nnabla.functions as NF
-import nnabla.parametric_functions as NPF
 import nnabla_rl.algorithms as A
 import nnabla_rl.environments as E
-from nnabla_rl.builders import ModelBuilder
-from nnabla_rl.models import DeterministicPolicy, QFunction
 from nnabla_rl.replay_buffer import ReplayBuffer
 
 
-class RNNActorFunction(DeterministicPolicy):
-    # type declarations to type check with mypy
-    # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
-    # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
-    _action_dim: int
-    _max_action_value: float
-
-    def __init__(self, scope_name: str, action_dim: int):
-        super(RNNActorFunction, self).__init__(scope_name)
-        self._action_dim = action_dim
-        self._lstm_state_size = action_dim
-        self._h = None
-        self._c = None
-
-    def pi(self, s: nn.Variable) -> nn.Variable:
-        with nn.parameter_scope(self.scope_name):
-            with nn.parameter_scope("linear1"):
-                h = NPF.affine(s, n_outmaps=400)
-            h = NF.relu(x=h)
-            with nn.parameter_scope("linear2"):
-                h = NPF.affine(h, n_outmaps=300)
-            h = NF.relu(x=h)
-            if not self._is_internal_state_created():
-                # automatically create internal states if not provided
-                batch_size = h.shape[0]
-                self._create_internal_states(batch_size)
-            with nn.parameter_scope("linear3"):
-                self._h, self._c = NPF.lstm_cell(h, self._h, self._c, self._lstm_state_size)
-            h = self._h
-        return NF.tanh(h)
-
-    def is_recurrent(self) -> bool:
-        return True
-
-    def internal_state_shapes(self) -> Dict[str, Tuple[int, ...]]:
-        shapes: Dict[str, nn.Variable] = {}
-        shapes['lstm_hidden'] = (self._lstm_state_size, )
-        shapes['lstm_cell'] = (self._lstm_state_size, )
-        return shapes
-
-    def get_internal_states(self) -> Dict[str, nn.Variable]:
-        states: Dict[str, nn.Variable] = {}
-        states['lstm_hidden'] = self._h
-        states['lstm_cell'] = self._c
-        return states
-
-    def set_internal_states(self, states: Optional[Dict[str, nn.Variable]] = None):
-        if states is None:
-            if self._h is not None:
-                self._h.data.zero()
-            if self._c is not None:
-                self._c.data.zero()
-        else:
-            self._h = states['lstm_hidden']
-            self._c = states['lstm_cell']
-
-    def _create_internal_states(self, batch_size):
-        self._h = nn.Variable((batch_size, self._lstm_state_size))
-        self._c = nn.Variable((batch_size, self._lstm_state_size))
-
-        self._h.data.zero()
-        self._c.data.zero()
-
-    def _is_internal_state_created(self) -> bool:
-        return self._h is not None and self._c is not None
-
-
-class RNNCriticFunction(QFunction):
-    # type declarations to type check with mypy
-    # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
-    # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
-
-    def __init__(self, scope_name: str):
-        super(RNNCriticFunction, self).__init__(scope_name)
-        self._lstm_state_size = 1
-        self._h = None
-        self._c = None
-
-    def q(self, s: nn.Variable, a: nn.Variable) -> nn.Variable:
-        with nn.parameter_scope(self.scope_name):
-            h = NF.concatenate(s, a)
-            with nn.parameter_scope("linear1"):
-                h = NPF.affine(h, n_outmaps=400)
-            h = NF.relu(x=h)
-            with nn.parameter_scope("linear2"):
-                h = NPF.affine(h, n_outmaps=300)
-            h = NF.relu(x=h)
-            if not self._is_internal_state_created():
-                # automatically create internal states if not provided
-                batch_size = h.shape[0]
-                self._create_internal_states(batch_size)
-            with nn.parameter_scope("linear3"):
-                self._h, self._c = NPF.lstm_cell(h, self._h, self._c, self._lstm_state_size)
-                h = self._h
-        return h
-
-    def is_recurrent(self) -> bool:
-        return True
-
-    def internal_state_shapes(self) -> Dict[str, Tuple[int, ...]]:
-        shapes: Dict[str, nn.Variable] = {}
-        shapes['lstm_hidden'] = (self._lstm_state_size, )
-        shapes['lstm_cell'] = (self._lstm_state_size, )
-        return shapes
-
-    def get_internal_states(self) -> Dict[str, nn.Variable]:
-        states: Dict[str, nn.Variable] = {}
-        states['lstm_hidden'] = self._h
-        states['lstm_cell'] = self._c
-        return states
-
-    def set_internal_states(self, states: Optional[Dict[str, nn.Variable]] = None):
-        if states is None:
-            if self._h is not None:
-                self._h.data.zero()
-            if self._c is not None:
-                self._c.data.zero()
-        else:
-            self._h = states['lstm_hidden']
-            self._c = states['lstm_cell']
-
-    def _create_internal_states(self, batch_size):
-        self._h = nn.Variable((batch_size, self._lstm_state_size))
-        self._c = nn.Variable((batch_size, self._lstm_state_size))
-
-        self._h.data.zero()
-        self._c.data.zero()
-
-    def _is_internal_state_created(self) -> bool:
-        return self._h is not None and self._c is not None
-
-
 class TestDDPG(object):
     def setup_method(self, method):
         nn.clear_parameters()
 
     def test_algorithm_name(self):
         dummy_env = E.DummyContinuous()
         ddpg = A.DDPG(dummy_env)
 
         assert ddpg.__name__ == 'DDPG'
 
-    def test_discrete_action_env_unsupported(self):
-        """Check that error occurs when training on discrete action env."""
+    def test_discrete_env_unsupported(self):
+        '''
+        Check that error occurs when training on discrete env
+        '''
+
         dummy_env = E.DummyDiscrete()
         config = A.DDPGConfig()
         with pytest.raises(Exception):
             A.DDPG(dummy_env, config=config)
 
     def test_run_online_training(self):
-        """Check that no error occurs when calling online training."""
+        '''
+        Check that no error occurs when calling online training
+        '''
+
         dummy_env = E.DummyContinuous()
         batch_size = 5
         config = A.DDPGConfig(batch_size=batch_size, start_timesteps=5)
         ddpg = A.DDPG(dummy_env, config=config)
 
         ddpg.train_online(dummy_env, total_iterations=10)
 
-    def test_run_online_rnn_training(self):
-        """Check that no error occurs when calling online training with RNN
-        model."""
-        class RNNActorBuilder(ModelBuilder[DeterministicPolicy]):
-            def build_model(self, scope_name: str, env_info, algorithm_config, **kwargs):
-                return RNNActorFunction(scope_name, action_dim=env_info.action_dim)
-
-        class RNNCriticBuilder(ModelBuilder[QFunction]):
-            def build_model(self, scope_name: str, env_info, algorithm_config, **kwargs):
-                return RNNCriticFunction(scope_name)
-
-        dummy_env = E.DummyContinuous()
-        config = A.DDPGConfig()
-        config.actor_unroll_steps = 2
-        config.actor_burn_in_steps = 2
-        config.critic_unroll_steps = 2
-        config.critic_burn_in_steps = 2
-        config.start_timesteps = 7
-        config.batch_size = 2
-        ddpg = A.DDPG(dummy_env, config=config, critic_builder=RNNCriticBuilder(), actor_builder=RNNActorBuilder())
-
-        ddpg.train_online(dummy_env, total_iterations=10)
-
     def test_run_offline_training(self):
-        """Check that no error occurs when calling offline training."""
+        '''
+        Check that no error occurs when calling offline training
+        '''
+
         batch_size = 5
         dummy_env = E.DummyContinuous()
         config = A.DDPGConfig(batch_size=batch_size)
         ddpg = A.DDPG(dummy_env, config=config)
 
         experiences = generate_dummy_experiences(dummy_env, batch_size)
         buffer = ReplayBuffer()
@@ -228,16 +76,17 @@
         state = dummy_env.reset()
         state = np.float32(state)
         action = ddpg.compute_eval_action(state)
 
         assert action.shape == dummy_env.action_space.shape
 
     def test_latest_iteration_state(self):
-        """Check that latest iteration state has the keys and values we
-        expected."""
+        '''
+        Check that latest iteration state has the keys and values we expected
+        '''
 
         dummy_env = E.DummyContinuous()
         ddpg = A.DDPG(dummy_env)
 
         ddpg._q_function_trainer_state = {'q_loss': 0., 'td_errors': np.array([0., 1.])}
         ddpg._policy_trainer_state = {'pi_loss': 1.}
 
@@ -247,11 +96,13 @@
         assert 'td_errors' in latest_iteration_state['histogram']
         assert latest_iteration_state['scalar']['q_loss'] == 0.
         assert latest_iteration_state['scalar']['pi_loss'] == 1.
         assert np.allclose(latest_iteration_state['histogram']['td_errors'], np.array([0., 1.]))
 
 
 if __name__ == "__main__":
+    import sys
+    sys.path.insert(0, "./")
     from testing_utils import generate_dummy_experiences
     pytest.main()
 else:
-    from ..testing_utils import generate_dummy_experiences
+    from .testing_utils import generate_dummy_experiences
```

## tests/algorithms/test_dqn.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -15,83 +15,55 @@
 
 import numpy as np
 import pytest
 
 import nnabla as nn
 import nnabla_rl.algorithms as A
 import nnabla_rl.environments as E
-from nnabla_rl.builders.model_builder import ModelBuilder
-from nnabla_rl.models import DRQNQFunction
-from nnabla_rl.models.q_function import QFunction
 from nnabla_rl.replay_buffer import ReplayBuffer
 
 
 class TestDQN(object):
     def setup_method(self, method):
         nn.clear_parameters()
 
     def test_algorithm_name(self):
         dummy_env = E.DummyDiscreteImg()
         dqn = A.DQN(dummy_env)
 
         assert dqn.__name__ == 'DQN'
 
-    def test_continuous_action_env_unsupported(self):
-        """Check that error occurs when training on continuous action env."""
+    def test_continuous_env_unsupported(self):
+        '''
+        Check that error occurs when training on continuous env
+        '''
+
         dummy_env = E.DummyContinuous()
         config = A.DQNConfig()
         with pytest.raises(Exception):
             A.DQN(dummy_env, config=config)
 
     def test_run_online_training(self):
-        """Check that no error occurs when calling online training."""
+        '''
+        Check that no error occurs when calling online training
+        '''
         dummy_env = E.DummyDiscreteImg()
         config = A.DQNConfig()
         config.start_timesteps = 5
         config.batch_size = 5
         config.learner_update_frequency = 1
         config.target_update_frequency = 1
         dqn = A.DQN(dummy_env, config=config)
 
         dqn.train_online(dummy_env, total_iterations=10)
 
-    def test_run_online_training_multistep(self):
-        """Check that no error occurs when calling online training."""
-        dummy_env = E.DummyDiscreteImg()
-        config = A.DQNConfig()
-        config.num_steps = 2
-        config.start_timesteps = 5
-        config.batch_size = 2
-        config.learner_update_frequency = 1
-        config.target_update_frequency = 1
-        dqn = A.DQN(dummy_env, config=config)
-
-        dqn.train_online(dummy_env, total_iterations=10)
-
-    def test_run_online_rnn_training(self):
-        """Check that no error occurs when calling online training with RNN
-        model."""
-        class RNNModelBuilder(ModelBuilder[QFunction]):
-            def build_model(self, scope_name: str, env_info, algorithm_config, **kwargs):
-                return DRQNQFunction(scope_name, env_info.action_dim)
-        dummy_env = E.DummyDiscreteImg()
-        config = A.DQNConfig()
-        config.num_steps = 2
-        config.unroll_steps = 2
-        config.burn_in_steps = 2
-        config.start_timesteps = 7
-        config.batch_size = 2
-        config.learner_update_frequency = 1
-        config.target_update_frequency = 1
-        dqn = A.DQN(dummy_env, config=config, q_func_builder=RNNModelBuilder())
-
-        dqn.train_online(dummy_env, total_iterations=10)
-
     def test_run_offline_training(self):
-        """Check that no error occurs when calling offline training."""
+        '''
+        Check that no error occurs when calling offline training
+        '''
         dummy_env = E.DummyDiscreteImg()
         batch_size = 5
         config = A.DQNConfig()
         config.batch_size = batch_size
         config.learner_update_frequency = 1
         config.target_update_frequency = 1
 
@@ -133,27 +105,30 @@
             A.DQNConfig(final_epsilon=1.1)
         with pytest.raises(ValueError):
             A.DQNConfig(test_epsilon=-1000)
         with pytest.raises(ValueError):
             A.DQNConfig(max_explore_steps=-100)
 
     def test_latest_iteration_state(self):
-        """Check that latest iteration state has the keys and values we
-        expected."""
+        '''
+        Check that latest iteration state has the keys and values we expected
+        '''
 
         dummy_env = E.DummyDiscreteImg()
         dqn = A.DQN(dummy_env)
 
         dqn._q_function_trainer_state = {'q_loss': 0., 'td_errors': np.array([0., 1.])}
 
         latest_iteration_state = dqn.latest_iteration_state
         assert 'q_loss' in latest_iteration_state['scalar']
         assert 'td_errors' in latest_iteration_state['histogram']
         assert latest_iteration_state['scalar']['q_loss'] == 0.
         assert np.allclose(latest_iteration_state['histogram']['td_errors'], np.array([0., 1.]))
 
 
 if __name__ == "__main__":
+    import sys
+    sys.path.insert(0, "./")
     from testing_utils import generate_dummy_experiences
     pytest.main()
 else:
-    from ..testing_utils import generate_dummy_experiences
+    from .testing_utils import generate_dummy_experiences
```

## tests/algorithms/test_dummy.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -28,22 +28,28 @@
     def test_algorithm_name(self):
         dummy_env = E.DummyDiscrete()
         dummy = A.Dummy(dummy_env)
 
         assert dummy.__name__ == 'Dummy'
 
     def test_run_online_training(self):
-        """Check that no error occurs when calling online training."""
+        '''
+        Check that no error occurs when calling online training
+        '''
+
         dummy_env = E.DummyContinuous()
         dummy = A.Dummy(dummy_env)
 
         dummy.train_online(dummy_env, total_iterations=10)
 
     def test_run_offline_training(self):
-        """Check that no error occurs when calling offline training."""
+        '''
+        Check that no error occurs when calling offline training
+        '''
+
         dummy_env = E.DummyContinuous()
         dummy = A.Dummy(dummy_env)
 
         experience_num = 100
         fake_states = np.empty(shape=(experience_num, ) +
                                dummy_env.observation_space.shape)
         fake_actions = np.empty(shape=(experience_num, ) +
```

## tests/algorithms/test_gail.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -35,40 +35,35 @@
     def test_algorithm_name(self):
         dummy_env = E.DummyContinuous()
         dummy_buffer = self._create_dummy_buffer(dummy_env)
         gail = A.GAIL(dummy_env, dummy_buffer)
 
         assert gail.__name__ == 'GAIL'
 
-    def test_discrete_action_env_unsupported(self):
-        """Check that error occurs when training on discrete action env."""
-        dummy_env = E.DummyDiscrete()
-        dummy_env = EpisodicEnv(dummy_env, min_episode_length=3)
-        dummy_buffer = self._create_dummy_buffer(dummy_env, batch_size=15)
-        config = A.GAILConfig()
-        with pytest.raises(Exception):
-            A.GAIL(dummy_env, dummy_buffer, config=config)
-
     def test_run_online_training(self):
-        """Check that no error occurs when calling online training."""
+        '''
+        Check that no error occurs when calling online training
+        '''
         dummy_env = E.DummyContinuous()
         dummy_env = EpisodicEnv(dummy_env, min_episode_length=3)
         dummy_buffer = self._create_dummy_buffer(dummy_env, batch_size=15)
 
         config = A.GAILConfig(num_steps_per_iteration=5,
                               pi_batch_size=5,
                               vf_batch_size=2,
                               discriminator_batch_size=2,
                               sigma_kl_divergence_constraint=10.0,
                               maximum_backtrack_numbers=50)
         gail = A.GAIL(dummy_env, dummy_buffer, config=config)
         gail.train_online(dummy_env, total_iterations=5)
 
     def test_run_offline_training(self):
-        """Check that raising error when calling offline training."""
+        '''
+        Check that raising error when calling offline training
+        '''
         dummy_env = E.DummyContinuous()
         dummy_buffer = self._create_dummy_buffer(dummy_env)
         gail = A.GAIL(dummy_env, dummy_buffer)
 
         with pytest.raises(NotImplementedError):
             gail.train_offline([], total_iterations=10)
 
@@ -110,16 +105,17 @@
             A.GAILConfig(policy_update_frequency=-5)
         with pytest.raises(ValueError):
             A.GAILConfig(discriminator_update_frequency=-5)
         with pytest.raises(ValueError):
             A.GAILConfig(adversary_entropy_coef=-0.5)
 
     def test_latest_iteration_state(self):
-        """Check that latest iteration state has the keys and values we
-        expected."""
+        '''
+        Check that latest iteration state has the keys and values we expected
+        '''
 
         dummy_env = E.DummyContinuous()
         dummy_buffer = self._create_dummy_buffer(dummy_env)
         gail = A.GAIL(dummy_env, dummy_buffer)
 
         gail._v_function_trainer_state = {'v_loss': 0.}
         gail._discriminator_trainer_state = {'reward_loss': 1.}
@@ -128,11 +124,13 @@
         assert 'v_loss' in latest_iteration_state['scalar']
         assert 'reward_loss' in latest_iteration_state['scalar']
         assert latest_iteration_state['scalar']['v_loss'] == 0.
         assert latest_iteration_state['scalar']['reward_loss'] == 1.
 
 
 if __name__ == "__main__":
+    import sys
+    sys.path.insert(0, "./")
     from testing_utils import EpisodicEnv, generate_dummy_experiences
     pytest.main()
 else:
-    from ..testing_utils import EpisodicEnv, generate_dummy_experiences
+    from .testing_utils import EpisodicEnv, generate_dummy_experiences
```

## tests/algorithms/test_icml2015_trpo.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -28,28 +28,32 @@
     def test_algorithm_name(self):
         dummy_env = E.DummyDiscreteImg()
         trpo = A.ICML2015TRPO(dummy_env)
 
         assert trpo.__name__ == 'ICML2015TRPO'
 
     def test_run_online_training(self):
-        """Check that no error occurs when calling online training."""
+        '''
+        Check that no error occurs when calling online training
+        '''
         dummy_env = E.DummyDiscreteImg()
         dummy_env = EpisodicEnv(dummy_env, min_episode_length=3)
         config = A.ICML2015TRPOConfig(batch_size=5,
                                       gpu_batch_size=2,
                                       num_steps_per_iteration=5,
                                       sigma_kl_divergence_constraint=10.0,
                                       maximum_backtrack_numbers=2)
         trpo = A.ICML2015TRPO(dummy_env, config=config)
 
         trpo.train_online(dummy_env, total_iterations=1)
 
     def test_run_offline_training(self):
-        """Check that no error occurs when calling offline training."""
+        '''
+        Check that no error occurs when calling offline training
+        '''
         dummy_env = E.DummyDiscreteImg()
         trpo = A.ICML2015TRPO(dummy_env)
 
         with pytest.raises(NotImplementedError):
             trpo.train_offline([], total_iterations=10)
 
     def test_compute_eval_action(self):
@@ -105,11 +109,13 @@
         icml2015_trpo = A.ICML2015TRPO(dummy_envinfo)
 
         with pytest.raises(ValueError):
             icml2015_trpo._compute_accumulated_reward(reward_sequence, gamma)
 
 
 if __name__ == "__main__":
+    import sys
+    sys.path.insert(0, "./")
     from testing_utils import EpisodicEnv
     pytest.main()
 else:
-    from ..testing_utils import EpisodicEnv
+    from .testing_utils import EpisodicEnv
```

## tests/algorithms/test_icml2018_sac.py

```diff
@@ -1,298 +1,65 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from typing import Dict, Optional, Tuple
-
 import numpy as np
 import pytest
 
 import nnabla as nn
-import nnabla.functions as NF
-import nnabla.parametric_functions as NPF
 import nnabla_rl.algorithms as A
-import nnabla_rl.distributions as D
 import nnabla_rl.environments as E
-from nnabla_rl.builders import ModelBuilder
-from nnabla_rl.models import QFunction, StochasticPolicy, VFunction
 from nnabla_rl.replay_buffer import ReplayBuffer
 
 
-class RNNPolicyFunction(StochasticPolicy):
-    # type declarations to type check with mypy
-    # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
-    # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
-    _action_dim: int
-    _max_action_value: float
-
-    def __init__(self, scope_name: str, action_dim: int):
-        super(RNNPolicyFunction, self).__init__(scope_name)
-        self._action_dim = action_dim
-        self._lstm_state_size = action_dim*2
-        self._h = None
-        self._c = None
-
-    def pi(self, s: nn.Variable) -> nn.Variable:
-        with nn.parameter_scope(self.scope_name):
-            with nn.parameter_scope("linear1"):
-                h = NPF.affine(s, n_outmaps=400)
-            h = NF.relu(x=h)
-            with nn.parameter_scope("linear2"):
-                h = NPF.affine(h, n_outmaps=300)
-            h = NF.relu(x=h)
-            if not self._is_internal_state_created():
-                # automatically create internal states if not provided
-                batch_size = h.shape[0]
-                self._create_internal_states(batch_size)
-            with nn.parameter_scope("linear3"):
-                self._h, self._c = NPF.lstm_cell(h, self._h, self._c, self._lstm_state_size)
-            h = self._h
-
-            reshaped = NF.reshape(h, shape=(-1, 2, self._action_dim))
-            mean, ln_sigma = NF.split(reshaped, axis=1)
-            assert mean.shape == ln_sigma.shape
-            assert mean.shape == (s.shape[0], self._action_dim)
-            ln_var = ln_sigma * 2.0
-        return D.SquashedGaussian(mean=mean, ln_var=ln_var)
-
-    def is_recurrent(self) -> bool:
-        return True
-
-    def internal_state_shapes(self) -> Dict[str, Tuple[int, ...]]:
-        shapes: Dict[str, nn.Variable] = {}
-        shapes['lstm_hidden'] = (self._lstm_state_size, )
-        shapes['lstm_cell'] = (self._lstm_state_size, )
-        return shapes
-
-    def get_internal_states(self) -> Dict[str, nn.Variable]:
-        states: Dict[str, nn.Variable] = {}
-        states['lstm_hidden'] = self._h
-        states['lstm_cell'] = self._c
-        return states
-
-    def set_internal_states(self, states: Optional[Dict[str, nn.Variable]] = None):
-        if states is None:
-            if self._h is not None:
-                self._h.data.zero()
-            if self._c is not None:
-                self._c.data.zero()
-        else:
-            self._h = states['lstm_hidden']
-            self._c = states['lstm_cell']
-
-    def _create_internal_states(self, batch_size):
-        self._h = nn.Variable((batch_size, self._lstm_state_size))
-        self._c = nn.Variable((batch_size, self._lstm_state_size))
-
-        self._h.data.zero()
-        self._c.data.zero()
-
-    def _is_internal_state_created(self) -> bool:
-        return self._h is not None and self._c is not None
-
-
-class RNNQFunction(QFunction):
-    # type declarations to type check with mypy
-    # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
-    # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
-
-    def __init__(self, scope_name: str):
-        super(RNNQFunction, self).__init__(scope_name)
-        self._lstm_state_size = 1
-        self._h = None
-        self._c = None
-
-    def q(self, s: nn.Variable, a: nn.Variable) -> nn.Variable:
-        with nn.parameter_scope(self.scope_name):
-            h = NF.concatenate(s, a)
-            with nn.parameter_scope("linear1"):
-                h = NPF.affine(h, n_outmaps=400)
-            h = NF.relu(x=h)
-            with nn.parameter_scope("linear2"):
-                h = NPF.affine(h, n_outmaps=300)
-            h = NF.relu(x=h)
-            if not self._is_internal_state_created():
-                # automatically create internal states if not provided
-                batch_size = h.shape[0]
-                self._create_internal_states(batch_size)
-            with nn.parameter_scope("linear3"):
-                self._h, self._c = NPF.lstm_cell(h, self._h, self._c, self._lstm_state_size)
-                h = self._h
-        return h
-
-    def is_recurrent(self) -> bool:
-        return True
-
-    def internal_state_shapes(self) -> Dict[str, Tuple[int, ...]]:
-        shapes: Dict[str, nn.Variable] = {}
-        shapes['lstm_hidden'] = (self._lstm_state_size, )
-        shapes['lstm_cell'] = (self._lstm_state_size, )
-        return shapes
-
-    def get_internal_states(self) -> Dict[str, nn.Variable]:
-        states: Dict[str, nn.Variable] = {}
-        states['lstm_hidden'] = self._h
-        states['lstm_cell'] = self._c
-        return states
-
-    def set_internal_states(self, states: Optional[Dict[str, nn.Variable]] = None):
-        if states is None:
-            if self._h is not None:
-                self._h.data.zero()
-            if self._c is not None:
-                self._c.data.zero()
-        else:
-            self._h = states['lstm_hidden']
-            self._c = states['lstm_cell']
-
-    def _create_internal_states(self, batch_size):
-        self._h = nn.Variable((batch_size, self._lstm_state_size))
-        self._c = nn.Variable((batch_size, self._lstm_state_size))
-
-        self._h.data.zero()
-        self._c.data.zero()
-
-    def _is_internal_state_created(self) -> bool:
-        return self._h is not None and self._c is not None
-
-
-class RNNVFunction(VFunction):
-    # type declarations to type check with mypy
-    # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
-    # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
-
-    def __init__(self, scope_name: str):
-        super(RNNVFunction, self).__init__(scope_name)
-        self._lstm_state_size = 1
-        self._h = None
-        self._c = None
-
-    def v(self, s: nn.Variable) -> nn.Variable:
-        with nn.parameter_scope(self.scope_name):
-            with nn.parameter_scope("linear1"):
-                h = NPF.affine(s, n_outmaps=400)
-            h = NF.relu(x=h)
-            with nn.parameter_scope("linear2"):
-                h = NPF.affine(h, n_outmaps=300)
-            h = NF.relu(x=h)
-            if not self._is_internal_state_created():
-                # automatically create internal states if not provided
-                batch_size = h.shape[0]
-                self._create_internal_states(batch_size)
-            with nn.parameter_scope("linear3"):
-                self._h, self._c = NPF.lstm_cell(h, self._h, self._c, self._lstm_state_size)
-                h = self._h
-        return h
-
-    def is_recurrent(self) -> bool:
-        return True
-
-    def internal_state_shapes(self) -> Dict[str, Tuple[int, ...]]:
-        shapes: Dict[str, nn.Variable] = {}
-        shapes['lstm_hidden'] = (self._lstm_state_size, )
-        shapes['lstm_cell'] = (self._lstm_state_size, )
-        return shapes
-
-    def get_internal_states(self) -> Dict[str, nn.Variable]:
-        states: Dict[str, nn.Variable] = {}
-        states['lstm_hidden'] = self._h
-        states['lstm_cell'] = self._c
-        return states
-
-    def set_internal_states(self, states: Optional[Dict[str, nn.Variable]] = None):
-        if states is None:
-            if self._h is not None:
-                self._h.data.zero()
-            if self._c is not None:
-                self._c.data.zero()
-        else:
-            self._h = states['lstm_hidden']
-            self._c = states['lstm_cell']
-
-    def _create_internal_states(self, batch_size):
-        self._h = nn.Variable((batch_size, self._lstm_state_size))
-        self._c = nn.Variable((batch_size, self._lstm_state_size))
-
-        self._h.data.zero()
-        self._c.data.zero()
-
-    def _is_internal_state_created(self) -> bool:
-        return self._h is not None and self._c is not None
-
-
 class TestICML2018SAC(object):
     def setup_method(self, method):
         nn.clear_parameters()
 
     def test_algorithm_name(self):
         dummy_env = E.DummyContinuous()
         sac = A.ICML2018SAC(dummy_env)
 
         assert sac.__name__ == 'ICML2018SAC'
 
-    def test_discrete_action_env_unsupported(self):
-        """Check that error occurs when training on discrete action env."""
+    def test_discrete_env_unsupported(self):
+        '''
+        Check that error occurs when training on discrete env
+        '''
+
         dummy_env = E.DummyDiscrete()
         with pytest.raises(Exception):
             A.ICML2018SAC(dummy_env)
 
     def test_run_online_training(self):
-        """Check that no error occurs when calling online training."""
-        dummy_env = E.DummyContinuous()
-        sac = A.ICML2018SAC(dummy_env)
-
-        sac.train_online(dummy_env, total_iterations=10)
-
-    def test_run_online_rnn_training(self):
-        """Check that no error occurs when calling online training with RNN
-        model."""
-        class RNNPolicyBuilder(ModelBuilder[StochasticPolicy]):
-            def build_model(self, scope_name: str, env_info, algorithm_config, **kwargs):
-                return RNNPolicyFunction(scope_name, action_dim=env_info.action_dim)
-
-        class RNNQFunctionBuilder(ModelBuilder[QFunction]):
-            def build_model(self, scope_name: str, env_info, algorithm_config, **kwargs):
-                return RNNQFunction(scope_name)
-
-        class RNNVFunctionBuilder(ModelBuilder[QFunction]):
-            def build_model(self, scope_name: str, env_info, algorithm_config, **kwargs):
-                return RNNVFunction(scope_name)
+        '''
+        Check that no error occurs when calling online training
+        '''
 
         dummy_env = E.DummyContinuous()
-        config = A.ICML2018SACConfig()
-        config.pi_unroll_steps = 2
-        config.pi_burn_in_steps = 2
-        config.q_unroll_steps = 2
-        config.q_burn_in_steps = 2
-        config.v_unroll_steps = 2
-        config.v_burn_in_steps = 2
-        config.num_steps = 2
-        config.start_timesteps = 7
-        config.batch_size = 2
-        sac = A.ICML2018SAC(dummy_env, config=config,
-                            policy_builder=RNNPolicyBuilder(),
-                            q_function_builder=RNNQFunctionBuilder(),
-                            v_function_builder=RNNVFunctionBuilder())
+        sac = A.ICML2018SAC(dummy_env)
 
         sac.train_online(dummy_env, total_iterations=10)
 
     def test_run_offline_training(self):
-        """Check that no error occurs when calling offline training."""
+        '''
+        Check that no error occurs when calling offline training
+        '''
+
         batch_size = 5
         dummy_env = E.DummyContinuous()
         config = A.ICML2018SACConfig(batch_size=batch_size)
         sac = A.ICML2018SAC(dummy_env, config=config)
 
         experiences = generate_dummy_experiences(dummy_env, batch_size)
         buffer = ReplayBuffer()
@@ -338,16 +105,17 @@
     def _has_same_parameters(self, params1, params2):
         for key in params1.keys():
             if not np.allclose(params1[key].data.data, params2[key].data.data):
                 return False
         return True
 
     def test_latest_iteration_state(self):
-        """Check that latest iteration state has the keys and values we
-        expected."""
+        '''
+        Check that latest iteration state has the keys and values we expected
+        '''
 
         dummy_env = E.DummyContinuous()
         sac = A.ICML2018SAC(dummy_env)
 
         sac._q_function_trainer_state = {'q_loss': 0., 'td_errors': np.array([0., 1.])}
         sac._policy_trainer_state = {'pi_loss': 2.}
         sac._v_function_trainer_state = {'v_loss': 1.}
@@ -360,11 +128,13 @@
         assert latest_iteration_state['scalar']['q_loss'] == 0.
         assert latest_iteration_state['scalar']['v_loss'] == 1.
         assert latest_iteration_state['scalar']['pi_loss'] == 2.
         assert np.allclose(latest_iteration_state['histogram']['td_errors'], np.array([0., 1.]))
 
 
 if __name__ == "__main__":
-    from tests.testing_utils import generate_dummy_experiences
+    import sys
+    sys.path.insert(0, "./")
+    from testing_utils import generate_dummy_experiences
     pytest.main()
 else:
-    from ..testing_utils import generate_dummy_experiences
+    from .testing_utils import generate_dummy_experiences
```

## tests/algorithms/test_iqn.py

```diff
@@ -1,184 +1,66 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from typing import Callable, Dict, Optional, Tuple
-
 import numpy as np
 import pytest
 
 import nnabla as nn
-import nnabla.functions as NF
-import nnabla.parametric_functions as NPF
 import nnabla_rl.algorithms as A
 import nnabla_rl.environments as E
-import nnabla_rl.functions as RF
-import nnabla_rl.parametric_functions as RPF
-from nnabla_rl.builders.model_builder import ModelBuilder
-from nnabla_rl.models import IQNQuantileFunction, StateActionQuantileFunction
 from nnabla_rl.replay_buffer import ReplayBuffer
 
 
-class RNNStateActionQuantileFunction(IQNQuantileFunction):
-    def __init__(self,
-                 scope_name: str,
-                 n_action: int,
-                 embedding_dim: int,
-                 K: int,
-                 risk_measure_function: Callable[[nn.Variable], nn.Variable]):
-        super().__init__(scope_name, n_action, embedding_dim, K, risk_measure_function)
-        self._h = None
-        self._c = None
-
-        self._lstm_state_size = 512
-
-    def all_quantile_values(self, s: nn.Variable, tau: nn.Variable) -> nn.Variable:
-        encoded = self._encode(s, tau.shape[-1])
-        embedding = self._compute_embedding(tau, encoded.shape[-1])
-
-        assert embedding.shape == encoded.shape
-
-        with nn.parameter_scope(self.scope_name):
-            h = encoded * embedding
-            with nn.parameter_scope("affine1"):
-                if not self._is_internal_state_created():
-                    # automatically create internal states if not provided
-                    batch_size = h.shape[0]
-                    self._create_internal_states(batch_size)
-                # This is just a workaround to enable using lstm state
-                hidden_state = RF.expand_dims(self._h, axis=1)
-                hidden_state = NF.broadcast(hidden_state, shape=(self._h.shape[0], tau.shape[-1], self._h.shape[-1]))
-                cell_state = RF.expand_dims(self._c, axis=1)
-                cell_state = NF.broadcast(cell_state, shape=(self._c.shape[0], tau.shape[-1], self._c.shape[-1]))
-                hidden_state, cell_state = RPF.lstm_cell(
-                    h, hidden_state, cell_state, self._lstm_state_size, base_axis=2)
-                h = hidden_state
-                # Save only the state of first sample for the next timestep
-                self._h, *_ = NF.split(hidden_state, axis=1)
-                self._c, *_ = NF.split(cell_state, axis=1)
-            h = NF.relu(x=h)
-            with nn.parameter_scope("affine2"):
-                return_samples = NPF.affine(h, n_outmaps=self._n_action, base_axis=2)
-        assert return_samples.shape == (s.shape[0], tau.shape[-1], self._n_action)
-        return return_samples
-
-    def is_recurrent(self) -> bool:
-        return True
-
-    def internal_state_shapes(self) -> Dict[str, Tuple[int, ...]]:
-        shapes: Dict[str, nn.Variable] = {}
-        shapes['lstm_hidden'] = (self._lstm_state_size, )
-        shapes['lstm_cell'] = (self._lstm_state_size, )
-        return shapes
-
-    def get_internal_states(self) -> Dict[str, nn.Variable]:
-        states: Dict[str, nn.Variable] = {}
-        states['lstm_hidden'] = self._h
-        states['lstm_cell'] = self._c
-        return states
-
-    def set_internal_states(self, states: Optional[Dict[str, nn.Variable]] = None):
-        if states is None:
-            if self._h is not None:
-                self._h.data.zero()
-            if self._c is not None:
-                self._c.data.zero()
-        else:
-            self._h = states['lstm_hidden']
-            self._c = states['lstm_cell']
-
-    def _create_internal_states(self, batch_size):
-        self._h = nn.Variable((batch_size, self._lstm_state_size))
-        self._c = nn.Variable((batch_size, self._lstm_state_size))
-
-        self._h.data.zero()
-        self._c.data.zero()
-
-    def _is_internal_state_created(self) -> bool:
-        return self._h is not None and self._c is not None
-
-
 class TestIQN(object):
     def setup_method(self, method):
         nn.clear_parameters()
 
     def test_algorithm_name(self):
         dummy_env = E.DummyDiscreteImg()
         iqn = A.IQN(dummy_env)
 
         assert iqn.__name__ == 'IQN'
 
-    def test_continuous_action_env_unsupported(self):
-        """Check that error occurs when training on continuous action env."""
+    def test_continuous_env_unsupported(self):
+        '''
+        Check that error occurs when training on continuous env
+        '''
+
         dummy_env = E.DummyContinuous()
         config = A.IQNConfig()
         with pytest.raises(Exception):
             A.IQN(dummy_env, config=config)
 
     def test_run_online_training(self):
-        """Check that no error occurs when calling online training."""
-        dummy_env = E.DummyDiscreteImg()
-        config = A.IQNConfig()
-        config.start_timesteps = 5
-        config.batch_size = 5
-        config.learner_update_frequency = 1
-        config.target_update_frequency = 1
-        iqn = A.IQN(dummy_env, config=config)
+        '''
+        Check that no error occurs when calling online training
+        '''
 
-        iqn.train_online(dummy_env, total_iterations=5)
-
-    def test_run_online_training_multistep(self):
-        """Check that no error occurs when calling online training."""
         dummy_env = E.DummyDiscreteImg()
         config = A.IQNConfig()
-        config.num_steps = 2
         config.start_timesteps = 5
         config.batch_size = 5
         config.learner_update_frequency = 1
         config.target_update_frequency = 1
         iqn = A.IQN(dummy_env, config=config)
 
         iqn.train_online(dummy_env, total_iterations=5)
 
-    def test_run_online_rnn_training(self):
-        """Check that no error occurs when calling online training with RNN
-        model."""
-        class RNNModelBuilder(ModelBuilder[StateActionQuantileFunction]):
-            def build_model(self, scope_name: str, env_info, algorithm_config, **kwargs):
-                risk_measure_function = kwargs['risk_measure_function']
-                return RNNStateActionQuantileFunction(scope_name,
-                                                      env_info.action_dim,
-                                                      algorithm_config.embedding_dim,
-                                                      K=algorithm_config.K,
-                                                      risk_measure_function=risk_measure_function)
-        dummy_env = E.DummyDiscreteImg()
-        config = A.IQNConfig()
-        config.num_steps = 2
-        config.unroll_steps = 2
-        config.burn_in_steps = 2
-        config.start_timesteps = 7
-        config.batch_size = 2
-        config.learner_update_frequency = 1
-        config.target_update_frequency = 1
-        qrdqn = A.IQN(dummy_env, config=config, quantile_function_builder=RNNModelBuilder())
-
-        qrdqn.train_online(dummy_env, total_iterations=10)
-
     def test_run_offline_training(self):
         dummy_env = E.DummyDiscreteImg()
         batch_size = 5
         config = A.IQNConfig()
         config.batch_size = batch_size
         config.learner_update_frequency = 1
         config.target_update_frequency = 1
@@ -229,25 +111,28 @@
             A.IQNConfig(N_prime=-1)
         with pytest.raises(ValueError):
             A.IQNConfig(kappa=-1)
         with pytest.raises(ValueError):
             A.IQNConfig(embedding_dim=-1)
 
     def test_latest_iteration_state(self):
-        """Check that latest iteration state has the keys and values we
-        expected."""
+        '''
+        Check that latest iteration state has the keys and values we expected
+        '''
 
         dummy_env = E.DummyDiscreteImg()
         iqn = A.IQN(dummy_env)
 
         iqn._quantile_function_trainer_state = {'q_loss': 0.}
 
         latest_iteration_state = iqn.latest_iteration_state
         assert 'q_loss' in latest_iteration_state['scalar']
         assert latest_iteration_state['scalar']['q_loss'] == 0.
 
 
 if __name__ == "__main__":
+    import sys
+    sys.path.insert(0, "./")
     from testing_utils import generate_dummy_experiences
     pytest.main()
 else:
-    from ..testing_utils import generate_dummy_experiences
+    from .testing_utils import generate_dummy_experiences
```

## tests/algorithms/test_munchausen_dqn.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -28,48 +28,42 @@
 
     def test_algorithm_name(self):
         dummy_env = E.DummyDiscreteImg()
         dqn = A.MunchausenDQN(dummy_env)
 
         assert dqn.__name__ == 'MunchausenDQN'
 
-    def test_continuous_action_env_unsupported(self):
-        """Check that error occurs when training on continuous action env."""
+    def test_continuous_env_unsupported(self):
+        '''
+        Check that error occurs when training on continuous env
+        '''
+
         dummy_env = E.DummyContinuous()
         config = A.MunchausenDQNConfig()
         with pytest.raises(Exception):
             A.MunchausenDQN(dummy_env, config=config)
 
     def test_run_online_training(self):
-        """Check that no error occurs when calling online training."""
-        dummy_env = E.DummyDiscreteImg()
-        config = A.MunchausenDQNConfig()
-        config.start_timesteps = 5
-        config.batch_size = 5
-        config.learner_update_frequency = 1
-        config.target_update_frequency = 1
-        dqn = A.MunchausenDQN(dummy_env, config=config)
-
-        dqn.train_online(dummy_env, total_iterations=10)
-
-    def test_run_online_training_multistep(self):
-        """Check that no error occurs when calling online training."""
+        '''
+        Check that no error occurs when calling online training
+        '''
         dummy_env = E.DummyDiscreteImg()
         config = A.MunchausenDQNConfig()
-        config.num_steps = 2
         config.start_timesteps = 5
         config.batch_size = 5
         config.learner_update_frequency = 1
         config.target_update_frequency = 1
         dqn = A.MunchausenDQN(dummy_env, config=config)
 
         dqn.train_online(dummy_env, total_iterations=10)
 
     def test_run_offline_training(self):
-        """Check that no error occurs when calling offline training."""
+        '''
+        Check that no error occurs when calling offline training
+        '''
         dummy_env = E.DummyDiscreteImg()
         batch_size = 5
         config = A.MunchausenDQNConfig()
         config.batch_size = batch_size
         config.learner_update_frequency = 1
         config.target_update_frequency = 1
 
@@ -111,27 +105,30 @@
             A.MunchausenDQNConfig(final_epsilon=1.1)
         with pytest.raises(ValueError):
             A.MunchausenDQNConfig(test_epsilon=-1000)
         with pytest.raises(ValueError):
             A.MunchausenDQNConfig(max_explore_steps=-100)
 
     def test_latest_iteration_state(self):
-        """Check that latest iteration state has the keys and values we
-        expected."""
+        '''
+        Check that latest iteration state has the keys and values we expected
+        '''
 
         dummy_env = E.DummyDiscreteImg()
         m_dqn = A.MunchausenDQN(dummy_env)
 
         m_dqn._q_function_trainer_state = {'q_loss': 0., 'td_errors': np.array([0., 1.])}
 
         latest_iteration_state = m_dqn.latest_iteration_state
         assert 'q_loss' in latest_iteration_state['scalar']
         assert 'td_errors' in latest_iteration_state['histogram']
         assert latest_iteration_state['scalar']['q_loss'] == 0.
         assert np.allclose(latest_iteration_state['histogram']['td_errors'], np.array([0., 1.]))
 
 
 if __name__ == "__main__":
+    import sys
+    sys.path.insert(0, "./")
     from testing_utils import generate_dummy_experiences
     pytest.main()
 else:
-    from ..testing_utils import generate_dummy_experiences
+    from .testing_utils import generate_dummy_experiences
```

## tests/algorithms/test_munchausen_iqn.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -28,38 +28,31 @@
 
     def test_algorithm_name(self):
         dummy_env = E.DummyDiscreteImg()
         m_iqn = A.MunchausenIQN(dummy_env)
 
         assert m_iqn.__name__ == 'MunchausenIQN'
 
-    def test_continuous_action_env_unsupported(self):
-        """Check that error occurs when training on continuous action env."""
+    def test_continuous_env_unsupported(self):
+        '''
+        Check that error occurs when training on continuous env
+        '''
+
         dummy_env = E.DummyContinuous()
         config = A.MunchausenIQNConfig()
         with pytest.raises(Exception):
             A.MunchausenIQN(dummy_env, config=config)
 
     def test_run_online_training(self):
-        """Check that no error occurs when calling online training."""
-        dummy_env = E.DummyDiscreteImg()
-        config = A.MunchausenIQNConfig()
-        config.start_timesteps = 5
-        config.batch_size = 5
-        config.learner_update_frequency = 1
-        config.target_update_frequency = 1
-        m_iqn = A.MunchausenIQN(dummy_env, config=config)
-
-        m_iqn.train_online(dummy_env, total_iterations=5)
+        '''
+        Check that no error occurs when calling online training
+        '''
 
-    def test_run_online_training_multistep(self):
-        """Check that no error occurs when calling online training."""
         dummy_env = E.DummyDiscreteImg()
         config = A.MunchausenIQNConfig()
-        config.num_steps = 2
         config.start_timesteps = 5
         config.batch_size = 5
         config.learner_update_frequency = 1
         config.target_update_frequency = 1
         m_iqn = A.MunchausenIQN(dummy_env, config=config)
 
         m_iqn.train_online(dummy_env, total_iterations=5)
@@ -120,25 +113,28 @@
             A.MunchausenIQNConfig(kappa=-1)
         with pytest.raises(ValueError):
             A.MunchausenIQNConfig(embedding_dim=-1)
         with pytest.raises(ValueError):
             A.MunchausenIQNConfig(clipping_value=100)
 
     def test_latest_iteration_state(self):
-        """Check that latest iteration state has the keys and values we
-        expected."""
+        '''
+        Check that latest iteration state has the keys and values we expected
+        '''
 
         dummy_env = E.DummyDiscreteImg()
         m_iqn = A.MunchausenIQN(dummy_env)
 
         m_iqn._quantile_function_trainer_state = {'q_loss': 0.}
 
         latest_iteration_state = m_iqn.latest_iteration_state
         assert 'q_loss' in latest_iteration_state['scalar']
         assert latest_iteration_state['scalar']['q_loss'] == 0.
 
 
 if __name__ == "__main__":
+    import sys
+    sys.path.insert(0, "./")
     from testing_utils import generate_dummy_experiences
     pytest.main()
 else:
-    from ..testing_utils import generate_dummy_experiences
+    from .testing_utils import generate_dummy_experiences
```

## tests/algorithms/test_ppo.py

```diff
@@ -1,143 +1,70 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023,2024 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import numpy as np
 import pytest
 
 import nnabla as nn
-import nnabla.functions as NF
 import nnabla_rl.algorithms as A
 import nnabla_rl.environments as E
-from nnabla_rl.algorithms.ppo import _copy_np_array_to_mp_array
-from nnabla_rl.builders import ModelBuilder
-from nnabla_rl.distributions import Gaussian
-from nnabla_rl.models import StochasticPolicy, VFunction
-from nnabla_rl.utils.multiprocess import mp_array_from_np_array, mp_to_np_array
-
-
-class TupleStateActor(StochasticPolicy):
-    _action_dim: int
-
-    def __init__(self, scope_name: str, action_dim: int):
-        super(TupleStateActor, self).__init__(scope_name)
-        self._action_dim = action_dim
-
-    def pi(self, s: nn.Variable):
-        s, *_ = s
-        return Gaussian(mean=nn.Variable.from_numpy_array(np.zeros(shape=(s.shape[0], self._action_dim))),
-                        ln_var=nn.Variable.from_numpy_array(np.zeros(shape=(s.shape[0], self._action_dim))))
-
-
-class TupleStateActorBuilder(ModelBuilder[VFunction]):
-    def build_model(self, scope_name: str, env_info, algorithm_config, **kwargs):
-        return TupleStateActor(scope_name, env_info.action_dim)
-
-
-class TupleStateVFunction(VFunction):
-    def __init__(self, scope_name: str):
-        super(TupleStateVFunction, self).__init__(scope_name)
-
-    def v(self, s: nn.Variable):
-        s, *_ = s
-        v = nn.Variable.from_numpy_array(np.ones(shape=(s.shape[0], 1)))
-        return NF.relu(v)
-
-
-class TupleStateVFunctionBuilder(ModelBuilder[VFunction]):
-    def build_model(self, scope_name: str, env_info, algorithm_config, **kwargs):
-        return TupleStateVFunction(scope_name)
 
 
 class TestPPO(object):
     def setup_method(self, method):
         nn.clear_parameters()
 
     def test_algorithm_name(self):
         dummy_env = E.DummyDiscreteImg()
         ppo = A.PPO(dummy_env)
 
         assert ppo.__name__ == 'PPO'
 
     def test_run_online_discrete_env_training(self):
-        """Check that no error occurs when calling online training (discrete
-        env)"""
+        '''
+        Check that no error occurs when calling online training (discrete env)
+        '''
 
         dummy_env = E.DummyDiscreteImg()
         actor_timesteps = 10
         actor_num = 2
         config = A.PPOConfig(batch_size=5, actor_timesteps=actor_timesteps, actor_num=actor_num)
         ppo = A.PPO(dummy_env, config=config)
 
         ppo.train_online(dummy_env, total_iterations=actor_timesteps*actor_num)
 
     def test_run_online_continuous_env_training(self):
-        """Check that no error occurs when calling online training (continuous
-        env)"""
+        '''
+        Check that no error occurs when calling online training (continuous env)
+        '''
 
         dummy_env = E.DummyContinuous()
         actor_timesteps = 10
         actor_num = 2
         config = A.PPOConfig(batch_size=5, actor_timesteps=actor_timesteps, actor_num=actor_num)
         ppo = A.PPO(dummy_env, config=config)
 
         ppo.train_online(dummy_env, total_iterations=actor_timesteps * actor_num)
 
-    def test_run_online_tuple_state_env_training(self):
-        """Check that no error occurs when calling online training (tuple state
-        env)"""
-
-        dummy_env = E.DummyTupleStateContinuous()
-        actor_timesteps = 10
-        actor_num = 2
-        config = A.PPOConfig(batch_size=5, actor_timesteps=actor_timesteps, actor_num=actor_num, preprocess_state=False)
-        ppo = A.PPO(dummy_env, config=config,
-                    v_function_builder=TupleStateVFunctionBuilder(),
-                    policy_builder=TupleStateActorBuilder())
-
-        ppo.train_online(dummy_env, total_iterations=actor_timesteps*actor_num)
-
-    def test_run_online_discrete_single_actor(self):
-        """Check that no error occurs when calling online training (discrete
-        env)"""
-
-        dummy_env = E.DummyDiscreteImg()
-        actor_timesteps = 10
-        actor_num = 1
-        config = A.PPOConfig(batch_size=5, actor_timesteps=actor_timesteps, actor_num=actor_num)
-        ppo = A.PPO(dummy_env, config=config)
-
-        ppo.train_online(dummy_env, total_iterations=actor_timesteps*actor_num)
-
-    def test_run_online_continuous_single_actor(self):
-        """Check that no error occurs when calling online training (continuous
-        env)"""
-
-        dummy_env = E.DummyContinuous()
-        actor_timesteps = 10
-        actor_num = 1
-        config = A.PPOConfig(batch_size=5, actor_timesteps=actor_timesteps, actor_num=actor_num)
-        ppo = A.PPO(dummy_env, config=config)
-
-        ppo.train_online(dummy_env, total_iterations=actor_timesteps * actor_num)
-
     def test_run_offline_training(self):
-        """Check that no error occurs when calling offline training."""
+        '''
+        Check that no error occurs when calling offline training
+        '''
+
         dummy_env = E.DummyDiscreteImg()
         ppo = A.PPO(dummy_env)
 
         with pytest.raises(ValueError):
             ppo.train_offline([], total_iterations=10)
 
     def test_parameter_range(self):
@@ -151,73 +78,26 @@
             A.PPOConfig(batch_size=-1)
         with pytest.raises(ValueError):
             A.PPOConfig(actor_timesteps=-1)
         with pytest.raises(ValueError):
             A.PPOConfig(total_timesteps=-1)
 
     def test_latest_iteration_state(self):
-        """Check that latest iteration state has the keys and values we
-        expected."""
+        '''
+        Check that latest iteration state has the keys and values we expected
+        '''
 
         dummy_env = E.DummyContinuous()
         ppo = A.PPO(dummy_env)
 
         ppo._policy_trainer_state = {'pi_loss': 0.}
         ppo._v_function_trainer_state = {'v_loss': 1.}
 
         latest_iteration_state = ppo.latest_iteration_state
         assert 'pi_loss' in latest_iteration_state['scalar']
         assert 'v_loss' in latest_iteration_state['scalar']
         assert latest_iteration_state['scalar']['pi_loss'] == 0.
         assert latest_iteration_state['scalar']['v_loss'] == 1.
 
-    def test_copy_np_array_to_mp_array(self):
-        shape = (10, 9, 8, 7)
-        mp_array_shape_type = (mp_array_from_np_array(np.random.uniform(size=shape)), shape, np.float64)
-
-        test_array = np.random.uniform(size=shape)
-        before_copying = mp_to_np_array(mp_array_shape_type[0], shape, dtype=mp_array_shape_type[2])
-        assert not np.allclose(before_copying, test_array)
-
-        _copy_np_array_to_mp_array(test_array, mp_array_shape_type)
-
-        after_copying = mp_to_np_array(mp_array_shape_type[0], shape, dtype=mp_array_shape_type[2])
-        assert np.allclose(after_copying, test_array)
-
-    def test_copy_tuple_np_array_to_tuple_mp_array_shape_type(self):
-        shape = ((10, 9, 8, 7), (6, 5, 4, 3))
-        tuple_mp_array_shape_type = tuple(
-            [(mp_array_from_np_array(np.random.uniform(size=s)), shape, np.float64) for s in shape]
-        )
-        tuple_test_array = tuple([np.random.uniform(size=s) for s in shape])
-
-        for mp_ary_shape_type, s, test_ary in zip(tuple_mp_array_shape_type, shape, tuple_test_array):
-            before_copying = mp_to_np_array(mp_ary_shape_type[0], s, dtype=mp_ary_shape_type[2])
-            assert not np.allclose(before_copying, test_ary)
-
-        _copy_np_array_to_mp_array(tuple_test_array, tuple_mp_array_shape_type)
-
-        for mp_ary_shape_type, s, test_ary in zip(tuple_mp_array_shape_type, shape, tuple_test_array):
-            after_copying = mp_to_np_array(mp_ary_shape_type[0], s, dtype=mp_ary_shape_type[2])
-            assert np.allclose(after_copying, test_ary)
-
-    def test_copy_np_array_to_tuple_mp_array_shape_type(self):
-        shape = ((10, 9, 8, 7), (6, 5, 4, 3))
-        tuple_mp_array_shape_type = tuple(
-            [(mp_array_from_np_array(np.random.uniform(size=s)), shape, np.float64) for s in shape]
-        )
-        test_array = np.random.uniform(size=shape[0])
-
-        with pytest.raises(ValueError):
-            _copy_np_array_to_mp_array(test_array, tuple_mp_array_shape_type)
-
-    def test_copy_tuple_np_array_to_mp_array_shape_type(self):
-        shape = ((10, 9, 8, 7), (6, 5, 4, 3))
-        mp_array_shape_type = (mp_array_from_np_array(np.random.uniform(size=shape[0])), shape, np.float64)
-        tuple_test_array = tuple([np.random.uniform(size=s) for s in shape])
-
-        with pytest.raises(ValueError):
-            _copy_np_array_to_mp_array(tuple_test_array, mp_array_shape_type)
-
 
 if __name__ == "__main__":
     pytest.main()
```

## tests/algorithms/test_qrdqn.py

```diff
@@ -1,172 +1,66 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from typing import Dict, Optional, Tuple
-
 import numpy as np
 import pytest
 
 import nnabla as nn
-import nnabla.functions as NF
-import nnabla.parametric_functions as NPF
 import nnabla_rl.algorithms as A
 import nnabla_rl.environments as E
-from nnabla_rl.builders.model_builder import ModelBuilder
-from nnabla_rl.models import DiscreteQuantileDistributionFunction, QuantileDistributionFunction
 from nnabla_rl.replay_buffer import ReplayBuffer
 
 
-class RNNQuantileDistributionFunction(DiscreteQuantileDistributionFunction):
-    def __init__(self, scope_name: str, n_action: int, n_quantile: int):
-        super().__init__(scope_name, n_action, n_quantile)
-        self._h = None
-        self._c = None
-
-        self._lstm_state_size = 512
-
-    def all_quantiles(self, s: nn.Variable) -> nn.Variable:
-        batch_size = s.shape[0]
-        with nn.parameter_scope(self.scope_name):
-            with nn.parameter_scope("conv1"):
-                h = NPF.convolution(s, outmaps=32, stride=(4, 4), kernel=(8, 8))
-            h = NF.relu(x=h)
-            with nn.parameter_scope("conv2"):
-                h = NPF.convolution(h, outmaps=64, kernel=(4, 4), stride=(2, 2))
-            h = NF.relu(x=h)
-            with nn.parameter_scope("conv3"):
-                h = NPF.convolution(h, outmaps=64, kernel=(3, 3), stride=(1, 1))
-            h = NF.relu(x=h)
-            h = NF.reshape(h, shape=(batch_size, -1))
-            with nn.parameter_scope("affine1"):
-                if not self._is_internal_state_created():
-                    # automatically create internal states if not provided
-                    batch_size = h.shape[0]
-                    self._create_internal_states(batch_size)
-                self._h, self._c = NPF.lstm_cell(h, self._h, self._c, self._lstm_state_size)
-                h = self._h
-            h = NF.relu(x=h)
-            with nn.parameter_scope("affine2"):
-                h = NPF.affine(h, n_outmaps=self._n_action * self._n_quantile)
-            quantiles = NF.reshape(
-                h, (-1, self._n_action, self._n_quantile))
-        assert quantiles.shape == (batch_size, self._n_action, self._n_quantile)
-        return quantiles
-
-    def is_recurrent(self) -> bool:
-        return True
-
-    def internal_state_shapes(self) -> Dict[str, Tuple[int, ...]]:
-        shapes: Dict[str, nn.Variable] = {}
-        shapes['lstm_hidden'] = (self._lstm_state_size, )
-        shapes['lstm_cell'] = (self._lstm_state_size, )
-        return shapes
-
-    def get_internal_states(self) -> Dict[str, nn.Variable]:
-        states: Dict[str, nn.Variable] = {}
-        states['lstm_hidden'] = self._h
-        states['lstm_cell'] = self._c
-        return states
-
-    def set_internal_states(self, states: Optional[Dict[str, nn.Variable]] = None):
-        if states is None:
-            if self._h is not None:
-                self._h.data.zero()
-            if self._c is not None:
-                self._c.data.zero()
-        else:
-            self._h = states['lstm_hidden']
-            self._c = states['lstm_cell']
-
-    def _create_internal_states(self, batch_size):
-        self._h = nn.Variable((batch_size, self._lstm_state_size))
-        self._c = nn.Variable((batch_size, self._lstm_state_size))
-
-        self._h.data.zero()
-        self._c.data.zero()
-
-    def _is_internal_state_created(self) -> bool:
-        return self._h is not None and self._c is not None
-
-
 class TestQRDQN(object):
     def setup_method(self, method):
         nn.clear_parameters()
 
     def test_algorithm_name(self):
         dummy_env = E.DummyDiscreteImg()
         qrdqn = A.QRDQN(dummy_env)
 
         assert qrdqn.__name__ == 'QRDQN'
 
-    def test_continuous_action_env_unsupported(self):
-        """Check that error occurs when training on continuous action env."""
+    def test_continuous_env_unsupported(self):
+        '''
+        Check that error occurs when training on continuous env
+        '''
+
         dummy_env = E.DummyContinuous()
         config = A.QRDQNConfig()
         with pytest.raises(Exception):
             A.QRDQN(dummy_env, config=config)
 
     def test_run_online_training(self):
-        """Check that no error occurs when calling online training."""
-        dummy_env = E.DummyDiscreteImg()
-        config = A.QRDQNConfig()
-        config.start_timesteps = 5
-        config.batch_size = 5
-        config.learner_update_frequency = 1
-        config.target_update_frequency = 1
-        qrdqn = A.QRDQN(dummy_env, config=config)
-
-        qrdqn.train_online(dummy_env, total_iterations=10)
+        '''
+        Check that no error occurs when calling online training
+        '''
 
-    def test_run_online_training_multistep(self):
-        """Check that no error occurs when calling online training."""
         dummy_env = E.DummyDiscreteImg()
         config = A.QRDQNConfig()
-        config.num_steps = 2
         config.start_timesteps = 5
         config.batch_size = 5
         config.learner_update_frequency = 1
         config.target_update_frequency = 1
         qrdqn = A.QRDQN(dummy_env, config=config)
 
         qrdqn.train_online(dummy_env, total_iterations=10)
 
-    def test_run_online_rnn_training(self):
-        """Check that no error occurs when calling online training with RNN
-        model."""
-        class RNNModelBuilder(ModelBuilder[QuantileDistributionFunction]):
-            def build_model(self, scope_name: str, env_info, algorithm_config, **kwargs):
-                n_action = env_info.action_dim
-                n_quantile = algorithm_config.num_quantiles
-                return RNNQuantileDistributionFunction(scope_name, n_action, n_quantile)
-        dummy_env = E.DummyDiscreteImg()
-        config = A.QRDQNConfig()
-        config.num_steps = 2
-        config.unroll_steps = 2
-        config.burn_in_steps = 2
-        config.start_timesteps = 7
-        config.batch_size = 2
-        config.learner_update_frequency = 1
-        config.target_update_frequency = 1
-        qrdqn = A.QRDQN(dummy_env, config=config, quantile_dist_function_builder=RNNModelBuilder())
-
-        qrdqn.train_online(dummy_env, total_iterations=10)
-
     def test_run_offline_training(self):
         dummy_env = E.DummyDiscreteImg()
         batch_size = 5
         config = A.QRDQNConfig()
         config.batch_size = batch_size
         config.learner_update_frequency = 1
         config.target_update_frequency = 1
@@ -212,25 +106,28 @@
             A.QRDQNConfig(test_epsilon=-1)
         with pytest.raises(ValueError):
             A.QRDQNConfig(num_quantiles=-1)
         with pytest.raises(ValueError):
             A.QRDQNConfig(kappa=-1)
 
     def test_latest_iteration_state(self):
-        """Check that latest iteration state has the keys and values we
-        expected."""
+        '''
+        Check that latest iteration state has the keys and values we expected
+        '''
 
         dummy_env = E.DummyDiscreteImg()
         qrdqn = A.QRDQN(dummy_env)
 
         qrdqn._quantile_dist_trainer_state = {'q_loss': 0.}
 
         latest_iteration_state = qrdqn.latest_iteration_state
         assert 'q_loss' in latest_iteration_state['scalar']
         assert latest_iteration_state['scalar']['q_loss'] == 0.
 
 
 if __name__ == "__main__":
+    import sys
+    sys.path.insert(0, "./")
     from testing_utils import generate_dummy_experiences
     pytest.main()
 else:
-    from ..testing_utils import generate_dummy_experiences
+    from .testing_utils import generate_dummy_experiences
```

## tests/algorithms/test_reinforce.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -27,22 +27,28 @@
     def test_algorithm_name(self):
         dummy_env = E.DummyDiscrete()
         reinforce = A.REINFORCE(dummy_env)
 
         assert reinforce.__name__ == 'REINFORCE'
 
     def test_run_online_training(self):
-        """Check that no error occurs when calling online training."""
-        dummy_env = E.DummyContinuous()
+        '''
+        Check that no error occurs when calling online training
+        '''
+
+        dummy_env = E.DummyDiscrete()
         dummy_env = EpisodicEnv(dummy_env)
         reinforce = A.REINFORCE(dummy_env)
         reinforce.train_online(dummy_env, total_iterations=1)
 
     def test_run_offline_training(self):
-        """Check that no error occurs when calling offline training."""
+        '''
+        Check that no error occurs when calling offline training
+        '''
+
         dummy_env = E.DummyDiscrete()
         reinforce = A.REINFORCE(dummy_env)
 
         with pytest.raises(NotImplementedError):
             reinforce.train_offline([], total_iterations=2)
 
     def test_parameter_range(self):
@@ -52,25 +58,28 @@
             A.REINFORCEConfig(num_rollouts_per_train_iteration=-1)
         with pytest.raises(ValueError):
             A.REINFORCEConfig(learning_rate=-0.1)
         with pytest.raises(ValueError):
             A.REINFORCEConfig(clip_grad_norm=-0.1)
 
     def test_latest_iteration_state(self):
-        """Check that latest iteration state has the keys and values we
-        expected."""
+        '''
+        Check that latest iteration state has the keys and values we expected
+        '''
 
-        dummy_env = E.DummyContinuous()
+        dummy_env = E.DummyDiscrete()
         reinforce = A.REINFORCE(dummy_env)
 
         reinforce._policy_trainer_state = {'pi_loss': 0.}
 
         latest_iteration_state = reinforce.latest_iteration_state
         assert 'pi_loss' in latest_iteration_state['scalar']
         assert latest_iteration_state['scalar']['pi_loss'] == 0.
 
 
 if __name__ == "__main__":
+    import sys
+    sys.path.insert(0, "./")
     from testing_utils import EpisodicEnv
     pytest.main()
 else:
-    from ..testing_utils import EpisodicEnv
+    from .testing_utils import EpisodicEnv
```

## tests/algorithms/test_sac.py

```diff
@@ -1,234 +1,75 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from typing import Dict, Optional, Tuple
-
 import numpy as np
 import pytest
 
 import nnabla as nn
-import nnabla.functions as NF
-import nnabla.parametric_functions as NPF
 import nnabla_rl.algorithms as A
-import nnabla_rl.distributions as D
 import nnabla_rl.environments as E
-from nnabla_rl.builders import ModelBuilder
-from nnabla_rl.models import QFunction, StochasticPolicy
 from nnabla_rl.replay_buffer import ReplayBuffer
 
 
-class RNNActorFunction(StochasticPolicy):
-    # type declarations to type check with mypy
-    # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
-    # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
-    _action_dim: int
-    _max_action_value: float
-
-    def __init__(self, scope_name: str, action_dim: int):
-        super(RNNActorFunction, self).__init__(scope_name)
-        self._action_dim = action_dim
-        self._lstm_state_size = action_dim*2
-        self._h = None
-        self._c = None
-
-    def pi(self, s: nn.Variable) -> nn.Variable:
-        with nn.parameter_scope(self.scope_name):
-            with nn.parameter_scope("linear1"):
-                h = NPF.affine(s, n_outmaps=400)
-            h = NF.relu(x=h)
-            with nn.parameter_scope("linear2"):
-                h = NPF.affine(h, n_outmaps=300)
-            h = NF.relu(x=h)
-            if not self._is_internal_state_created():
-                # automatically create internal states if not provided
-                batch_size = h.shape[0]
-                self._create_internal_states(batch_size)
-            with nn.parameter_scope("linear3"):
-                self._h, self._c = NPF.lstm_cell(h, self._h, self._c, self._lstm_state_size)
-            h = self._h
-
-            reshaped = NF.reshape(h, shape=(-1, 2, self._action_dim))
-            mean, ln_sigma = NF.split(reshaped, axis=1)
-            assert mean.shape == ln_sigma.shape
-            assert mean.shape == (s.shape[0], self._action_dim)
-            ln_var = ln_sigma * 2.0
-        return D.SquashedGaussian(mean=mean, ln_var=ln_var)
-
-    def is_recurrent(self) -> bool:
-        return True
-
-    def internal_state_shapes(self) -> Dict[str, Tuple[int, ...]]:
-        shapes: Dict[str, nn.Variable] = {}
-        shapes['lstm_hidden'] = (self._lstm_state_size, )
-        shapes['lstm_cell'] = (self._lstm_state_size, )
-        return shapes
-
-    def get_internal_states(self) -> Dict[str, nn.Variable]:
-        states: Dict[str, nn.Variable] = {}
-        states['lstm_hidden'] = self._h
-        states['lstm_cell'] = self._c
-        return states
-
-    def set_internal_states(self, states: Optional[Dict[str, nn.Variable]] = None):
-        if states is None:
-            if self._h is not None:
-                self._h.data.zero()
-            if self._c is not None:
-                self._c.data.zero()
-        else:
-            self._h = states['lstm_hidden']
-            self._c = states['lstm_cell']
-
-    def _create_internal_states(self, batch_size):
-        self._h = nn.Variable((batch_size, self._lstm_state_size))
-        self._c = nn.Variable((batch_size, self._lstm_state_size))
-
-        self._h.data.zero()
-        self._c.data.zero()
-
-    def _is_internal_state_created(self) -> bool:
-        return self._h is not None and self._c is not None
-
-
-class RNNCriticFunction(QFunction):
-    # type declarations to type check with mypy
-    # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
-    # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
-
-    def __init__(self, scope_name: str):
-        super(RNNCriticFunction, self).__init__(scope_name)
-        self._lstm_state_size = 1
-        self._h = None
-        self._c = None
-
-    def q(self, s: nn.Variable, a: nn.Variable) -> nn.Variable:
-        with nn.parameter_scope(self.scope_name):
-            h = NF.concatenate(s, a)
-            with nn.parameter_scope("linear1"):
-                h = NPF.affine(h, n_outmaps=400)
-            h = NF.relu(x=h)
-            with nn.parameter_scope("linear2"):
-                h = NPF.affine(h, n_outmaps=300)
-            h = NF.relu(x=h)
-            if not self._is_internal_state_created():
-                # automatically create internal states if not provided
-                batch_size = h.shape[0]
-                self._create_internal_states(batch_size)
-            with nn.parameter_scope("linear3"):
-                self._h, self._c = NPF.lstm_cell(h, self._h, self._c, self._lstm_state_size)
-                h = self._h
-        return h
-
-    def is_recurrent(self) -> bool:
-        return True
-
-    def internal_state_shapes(self) -> Dict[str, Tuple[int, ...]]:
-        shapes: Dict[str, nn.Variable] = {}
-        shapes['lstm_hidden'] = (self._lstm_state_size, )
-        shapes['lstm_cell'] = (self._lstm_state_size, )
-        return shapes
-
-    def get_internal_states(self) -> Dict[str, nn.Variable]:
-        states: Dict[str, nn.Variable] = {}
-        states['lstm_hidden'] = self._h
-        states['lstm_cell'] = self._c
-        return states
-
-    def set_internal_states(self, states: Optional[Dict[str, nn.Variable]] = None):
-        if states is None:
-            if self._h is not None:
-                self._h.data.zero()
-            if self._c is not None:
-                self._c.data.zero()
-        else:
-            self._h = states['lstm_hidden']
-            self._c = states['lstm_cell']
-
-    def _create_internal_states(self, batch_size):
-        self._h = nn.Variable((batch_size, self._lstm_state_size))
-        self._c = nn.Variable((batch_size, self._lstm_state_size))
-
-        self._h.data.zero()
-        self._c.data.zero()
-
-    def _is_internal_state_created(self) -> bool:
-        return self._h is not None and self._c is not None
-
-
 class TestSAC(object):
     def setup_method(self, method):
         nn.clear_parameters()
 
     def test_algorithm_name(self):
         dummy_env = E.DummyContinuous()
         sac = A.SAC(dummy_env)
 
         assert sac.__name__ == 'SAC'
 
-    def test_discrete_action_env_unsupported(self):
-        """Check that error occurs when training on discrete action env."""
+    def test_discrete_env_unsupported(self):
+        '''
+        Check that error occurs when training on discrete env
+        '''
+
         dummy_env = E.DummyDiscrete()
         with pytest.raises(Exception):
             A.SAC(dummy_env)
 
     def test_run_online_training(self):
-        """Check that no error occurs when calling online training."""
+        '''
+        Check that no error occurs when calling online training
+        '''
+
         dummy_env = E.DummyContinuous()
         sac = A.SAC(dummy_env)
 
         sac.train_online(dummy_env, total_iterations=10)
 
     def test_run_offline_training(self):
-        """Check that no error occurs when calling offline training."""
+        '''
+        Check that no error occurs when calling offline training
+        '''
+
         batch_size = 5
         dummy_env = E.DummyContinuous()
         config = A.SACConfig(batch_size=batch_size)
         sac = A.SAC(dummy_env, config=config)
 
         experiences = generate_dummy_experiences(dummy_env, batch_size)
         buffer = ReplayBuffer()
         buffer.append_all(experiences)
         sac.train_offline(buffer, total_iterations=10)
 
-    def test_run_online_rnn_training(self):
-        """Check that no error occurs when calling online training with RNN
-        model."""
-        class RNNActorBuilder(ModelBuilder[StochasticPolicy]):
-            def build_model(self, scope_name: str, env_info, algorithm_config, **kwargs):
-                return RNNActorFunction(scope_name, action_dim=env_info.action_dim)
-
-        class RNNCriticBuilder(ModelBuilder[QFunction]):
-            def build_model(self, scope_name: str, env_info, algorithm_config, **kwargs):
-                return RNNCriticFunction(scope_name)
-
-        dummy_env = E.DummyContinuous()
-        config = A.SACConfig()
-        config.actor_unroll_steps = 2
-        config.actor_burn_in_steps = 2
-        config.critic_unroll_steps = 2
-        config.critic_burn_in_steps = 2
-        config.start_timesteps = 7
-        config.batch_size = 2
-        sac = A.SAC(dummy_env, config=config, q_function_builder=RNNCriticBuilder(), policy_builder=RNNActorBuilder())
-
-        sac.train_online(dummy_env, total_iterations=10)
-
     def test_compute_eval_action(self):
         dummy_env = E.DummyContinuous()
         sac = A.SAC(dummy_env)
 
         state = dummy_env.reset()
         state = np.float32(state)
         action = sac.compute_eval_action(state)
@@ -250,16 +91,17 @@
             A.SACConfig(environment_steps=-100)
         with pytest.raises(ValueError):
             A.SACConfig(gradient_steps=-100)
         with pytest.raises(ValueError):
             A.SACConfig(initial_temperature=-100)
 
     def test_latest_iteration_state(self):
-        """Check that latest iteration state has the keys and values we
-        expected."""
+        '''
+        Check that latest iteration state has the keys and values we expected
+        '''
 
         dummy_env = E.DummyContinuous()
         sac = A.SAC(dummy_env)
 
         sac._q_function_trainer_state = {'q_loss': 0., 'td_errors': np.array([0., 1.])}
         sac._policy_trainer_state = {'pi_loss': 1.}
 
@@ -269,11 +111,13 @@
         assert 'td_errors' in latest_iteration_state['histogram']
         assert latest_iteration_state['scalar']['q_loss'] == 0.
         assert latest_iteration_state['scalar']['pi_loss'] == 1.
         assert np.allclose(latest_iteration_state['histogram']['td_errors'], np.array([0., 1.]))
 
 
 if __name__ == "__main__":
+    import sys
+    sys.path.insert(0, "./")
     from testing_utils import generate_dummy_experiences
     pytest.main()
 else:
-    from ..testing_utils import generate_dummy_experiences
+    from .testing_utils import generate_dummy_experiences
```

## tests/algorithms/test_td3.py

```diff
@@ -1,219 +1,67 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from typing import Dict, Optional, Tuple
-
 import numpy as np
 import pytest
 
 import nnabla as nn
-import nnabla.functions as NF
-import nnabla.parametric_functions as NPF
 import nnabla_rl.algorithms as A
 import nnabla_rl.environments as E
-from nnabla_rl.builders import ModelBuilder
-from nnabla_rl.models import DeterministicPolicy, QFunction
 from nnabla_rl.replay_buffer import ReplayBuffer
 
 
-class RNNActorFunction(DeterministicPolicy):
-    # type declarations to type check with mypy
-    # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
-    # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
-    _action_dim: int
-    _max_action_value: float
-
-    def __init__(self, scope_name: str, action_dim: int):
-        super(RNNActorFunction, self).__init__(scope_name)
-        self._action_dim = action_dim
-        self._lstm_state_size = action_dim
-        self._h = None
-        self._c = None
-
-    def pi(self, s: nn.Variable) -> nn.Variable:
-        with nn.parameter_scope(self.scope_name):
-            with nn.parameter_scope("linear1"):
-                h = NPF.affine(s, n_outmaps=400)
-            h = NF.relu(x=h)
-            with nn.parameter_scope("linear2"):
-                h = NPF.affine(h, n_outmaps=300)
-            h = NF.relu(x=h)
-            if not self._is_internal_state_created():
-                # automatically create internal states if not provided
-                batch_size = h.shape[0]
-                self._create_internal_states(batch_size)
-            with nn.parameter_scope("linear3"):
-                self._h, self._c = NPF.lstm_cell(h, self._h, self._c, self._lstm_state_size)
-            h = self._h
-        return NF.tanh(h)
-
-    def is_recurrent(self) -> bool:
-        return True
-
-    def internal_state_shapes(self) -> Dict[str, Tuple[int, ...]]:
-        shapes: Dict[str, nn.Variable] = {}
-        shapes['lstm_hidden'] = (self._lstm_state_size, )
-        shapes['lstm_cell'] = (self._lstm_state_size, )
-        return shapes
-
-    def get_internal_states(self) -> Dict[str, nn.Variable]:
-        states: Dict[str, nn.Variable] = {}
-        states['lstm_hidden'] = self._h
-        states['lstm_cell'] = self._c
-        return states
-
-    def set_internal_states(self, states: Optional[Dict[str, nn.Variable]] = None):
-        if states is None:
-            if self._h is not None:
-                self._h.data.zero()
-            if self._c is not None:
-                self._c.data.zero()
-        else:
-            self._h = states['lstm_hidden']
-            self._c = states['lstm_cell']
-
-    def _create_internal_states(self, batch_size):
-        self._h = nn.Variable((batch_size, self._lstm_state_size))
-        self._c = nn.Variable((batch_size, self._lstm_state_size))
-
-        self._h.data.zero()
-        self._c.data.zero()
-
-    def _is_internal_state_created(self) -> bool:
-        return self._h is not None and self._c is not None
-
-
-class RNNCriticFunction(QFunction):
-    # type declarations to type check with mypy
-    # NOTE: declared variables are instance variable and NOT class variable, unless it is marked with ClassVar
-    # See https://mypy.readthedocs.io/en/stable/class_basics.html for details
-
-    def __init__(self, scope_name: str):
-        super(RNNCriticFunction, self).__init__(scope_name)
-        self._lstm_state_size = 1
-        self._h = None
-        self._c = None
-
-    def q(self, s: nn.Variable, a: nn.Variable) -> nn.Variable:
-        with nn.parameter_scope(self.scope_name):
-            h = NF.concatenate(s, a)
-            with nn.parameter_scope("linear1"):
-                h = NPF.affine(h, n_outmaps=400)
-            h = NF.relu(x=h)
-            with nn.parameter_scope("linear2"):
-                h = NPF.affine(h, n_outmaps=300)
-            h = NF.relu(x=h)
-            if not self._is_internal_state_created():
-                # automatically create internal states if not provided
-                batch_size = h.shape[0]
-                self._create_internal_states(batch_size)
-            with nn.parameter_scope("linear3"):
-                self._h, self._c = NPF.lstm_cell(h, self._h, self._c, self._lstm_state_size)
-                h = self._h
-        return h
-
-    def is_recurrent(self) -> bool:
-        return True
-
-    def internal_state_shapes(self) -> Dict[str, Tuple[int, ...]]:
-        shapes: Dict[str, nn.Variable] = {}
-        shapes['lstm_hidden'] = (self._lstm_state_size, )
-        shapes['lstm_cell'] = (self._lstm_state_size, )
-        return shapes
-
-    def get_internal_states(self) -> Dict[str, nn.Variable]:
-        states: Dict[str, nn.Variable] = {}
-        states['lstm_hidden'] = self._h
-        states['lstm_cell'] = self._c
-        return states
-
-    def set_internal_states(self, states: Optional[Dict[str, nn.Variable]] = None):
-        if states is None:
-            if self._h is not None:
-                self._h.data.zero()
-            if self._c is not None:
-                self._c.data.zero()
-        else:
-            self._h = states['lstm_hidden']
-            self._c = states['lstm_cell']
-
-    def _create_internal_states(self, batch_size):
-        self._h = nn.Variable((batch_size, self._lstm_state_size))
-        self._c = nn.Variable((batch_size, self._lstm_state_size))
-
-        self._h.data.zero()
-        self._c.data.zero()
-
-    def _is_internal_state_created(self) -> bool:
-        return self._h is not None and self._c is not None
-
-
 class TestTD3(object):
     def setup_method(self, method):
         nn.clear_parameters()
 
     def test_algorithm_name(self):
         dummy_env = E.DummyContinuous()
         td3 = A.TD3(dummy_env)
 
         assert td3.__name__ == 'TD3'
 
-    def test_discrete_action_env_unsupported(self):
-        """Check that error occurs when training on discrete action env."""
+    def test_discrete_env_unsupported(self):
+        '''
+        Check that error occurs when training on discrete env
+        '''
+
         dummy_env = E.DummyDiscrete()
         with pytest.raises(Exception):
             A.TD3(dummy_env)
 
     def test_run_online_training(self):
-        """Check that no error occurs when calling online training."""
+        '''
+        Check that no error occurs when calling online training
+        '''
+
         dummy_env = E.DummyContinuous()
         batch_size = 5
         config = A.TD3Config(batch_size=batch_size, start_timesteps=5)
         td3 = A.TD3(dummy_env, config=config)
 
         td3.train_online(dummy_env, total_iterations=10)
 
-    def test_run_online_rnn_training(self):
-        """Check that no error occurs when calling online training with RNN
-        model."""
-        class RNNActorBuilder(ModelBuilder[DeterministicPolicy]):
-            def build_model(self, scope_name: str, env_info, algorithm_config, **kwargs):
-                return RNNActorFunction(scope_name, action_dim=env_info.action_dim)
-
-        class RNNCriticBuilder(ModelBuilder[QFunction]):
-            def build_model(self, scope_name: str, env_info, algorithm_config, **kwargs):
-                return RNNCriticFunction(scope_name)
-
-        dummy_env = E.DummyContinuous()
-        config = A.TD3Config()
-        config.actor_unroll_steps = 2
-        config.actor_burn_in_steps = 2
-        config.critic_unroll_steps = 2
-        config.critic_burn_in_steps = 2
-        config.start_timesteps = 7
-        config.batch_size = 2
-        td3 = A.TD3(dummy_env, config=config, critic_builder=RNNCriticBuilder(), actor_builder=RNNActorBuilder())
-
-        td3.train_online(dummy_env, total_iterations=10)
-
     def test_run_offline_training(self):
-        """Check that no error occurs when calling offline training."""
+        '''
+        Check that no error occurs when calling offline training
+        '''
+
         dummy_env = E.DummyContinuous()
         batch_size = 5
         config = A.TD3Config(batch_size=batch_size)
         td3 = A.TD3(dummy_env, config=config)
 
         experiences = generate_dummy_experiences(dummy_env, batch_size)
         buffer = ReplayBuffer()
@@ -253,16 +101,17 @@
             A.TD3Config(batch_size=-1)
         with pytest.raises(ValueError):
             A.TD3Config(start_timesteps=-1)
         with pytest.raises(ValueError):
             A.TD3Config(replay_buffer_size=-1)
 
     def test_latest_iteration_state(self):
-        """Check that latest iteration state has the keys and values we
-        expected."""
+        '''
+        Check that latest iteration state has the keys and values we expected
+        '''
 
         dummy_env = E.DummyContinuous()
         td3 = A.TD3(dummy_env)
 
         td3._q_function_trainer_state = {'q_loss': 0., 'td_errors': np.array([0., 1.])}
         td3._policy_trainer_state = {'pi_loss': 1.}
 
@@ -272,11 +121,13 @@
         assert 'td_errors' in latest_iteration_state['histogram']
         assert latest_iteration_state['scalar']['q_loss'] == 0.
         assert latest_iteration_state['scalar']['pi_loss'] == 1.
         assert np.allclose(latest_iteration_state['histogram']['td_errors'], np.array([0., 1.]))
 
 
 if __name__ == "__main__":
+    import sys
+    sys.path.insert(0, "./")
     from testing_utils import generate_dummy_experiences
     pytest.main()
 else:
-    from ..testing_utils import generate_dummy_experiences
+    from .testing_utils import generate_dummy_experiences
```

## tests/algorithms/test_trpo.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -27,37 +27,44 @@
 
     def test_algorithm_name(self):
         dummy_env = E.DummyContinuous()
         trpo = A.TRPO(dummy_env)
 
         assert trpo.__name__ == 'TRPO'
 
-    def test_discrete_action_env_unsupported(self):
-        """Check that error occurs when training on discrete action env."""
+    def test_discrete_env_unsupported(self):
+        '''
+        Check that error occurs when training on discrete env
+        '''
+
         dummy_env = E.DummyDiscrete()
-        with pytest.raises(Exception):
+        with pytest.raises(NotImplementedError):
             A.TRPO(dummy_env)
 
     def test_run_online_training(self):
-        """Check that no error occurs when calling online training."""
+        '''
+        Check that no error occurs when calling online training
+        '''
         dummy_env = E.DummyContinuous()
         dummy_env = EpisodicEnv(dummy_env, min_episode_length=3)
 
         config = A.TRPOConfig(num_steps_per_iteration=5,
                               gpu_batch_size=5,
                               pi_batch_size=5,
                               vf_batch_size=2,
                               sigma_kl_divergence_constraint=10.0,
                               maximum_backtrack_numbers=50)
         trpo = A.TRPO(dummy_env, config=config)
 
         trpo.train_online(dummy_env, total_iterations=5)
 
     def test_run_offline_training(self):
-        """Check that raising error when calling offline training."""
+        '''
+        Check that raising error when calling offline training
+        '''
         dummy_env = E.DummyContinuous()
         trpo = A.TRPO(dummy_env)
 
         with pytest.raises(NotImplementedError):
             trpo.train_offline([], total_iterations=10)
 
     def test_compute_eval_action(self):
@@ -87,25 +94,28 @@
             A.TRPOConfig(vf_epochs=-5)
         with pytest.raises(ValueError):
             A.TRPOConfig(vf_batch_size=-5)
         with pytest.raises(ValueError):
             A.TRPOConfig(vf_learning_rate=-0.5)
 
     def test_latest_iteration_state(self):
-        """Check that latest iteration state has the keys and values we
-        expected."""
+        '''
+        Check that latest iteration state has the keys and values we expected
+        '''
 
         dummy_env = E.DummyContinuous()
         trpo = A.TRPO(dummy_env)
 
         trpo._v_function_trainer_state = {'v_loss': 0.}
 
         latest_iteration_state = trpo.latest_iteration_state
         assert 'v_loss' in latest_iteration_state['scalar']
         assert latest_iteration_state['scalar']['v_loss'] == 0.
 
 
 if __name__ == "__main__":
+    import sys
+    sys.path.insert(0, "./")
     from testing_utils import EpisodicEnv
     pytest.main()
 else:
-    from ..testing_utils import EpisodicEnv
+    from .testing_utils import EpisodicEnv
```

## tests/distributions/test_gaussian.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -16,73 +16,32 @@
 from unittest import mock
 
 import numpy as np
 import pytest
 
 import nnabla as nn
 import nnabla_rl.distributions as D
-from nnabla_rl.distributions.gaussian import NnablaGaussian, NumpyGaussian
 
 
-class TestGaussian():
-    def _generate_dummy_mean_var(self):
-        batch_size = 10
-        output_dim = 10
-        input_shape = (batch_size, output_dim)
-        mean = np.zeros(shape=input_shape)
-        sigma = np.ones(shape=input_shape) * 5.
-        ln_var = np.log(sigma) * 2.
-        return mean, ln_var
-
-    def test_nnabla_constructor(self):
-        mean, ln_var = self._generate_dummy_mean_var()
-        distribution = D.Gaussian(nn.Variable.from_numpy_array(mean), nn.Variable.from_numpy_array(ln_var))
-        assert isinstance(distribution._delegate, NnablaGaussian)
-
-    def test_nnabla_constructor_with_wrong_shape(self):
-        mean, ln_var = self._generate_dummy_mean_var()
-        with pytest.raises(AssertionError):
-            D.Gaussian(nn.Variable.from_numpy_array(mean[0]), nn.Variable.from_numpy_array(np.diag(ln_var[0])))
-
-    def test_numpy_constructor(self):
-        mean, ln_var = self._generate_dummy_mean_var()
-        distribution = D.Gaussian(mean[0], np.diag(ln_var[0]))  # without batch
-        assert isinstance(distribution._delegate, NumpyGaussian)
-
-    def test_numpy_constructor_with_wrong_shape(self):
-        mean, ln_var = self._generate_dummy_mean_var()
-        with pytest.raises(AssertionError):
-            D.Gaussian(mean, ln_var)
-
-    def test_mix_constructor(self):
-        mean, ln_var = self._generate_dummy_mean_var()
-        with pytest.raises(ValueError):
-            D.Gaussian(nn.Variable.from_numpy_array(mean), ln_var)
-
-        with pytest.raises(ValueError):
-            D.Gaussian(mean, nn.Variable.from_numpy_array(ln_var))
-
-
-class TestNnablaGaussian(object):
+class TestGaussian(object):
     def setup_method(self, method):
         nn.clear_parameters()
         np.random.seed(0)
 
     def test_sample(self):
         batch_size = 10
         output_dim = 10
 
         input_shape = (batch_size, output_dim)
         mean = np.zeros(shape=input_shape)
         sigma = np.ones(shape=input_shape) * 5.
         ln_var = np.log(sigma) * 2.
 
         with mock.patch('nnabla_rl.functions.sample_gaussian') as mock_sample_gaussian:
-            distribution = NnablaGaussian(mean=nn.Variable.from_numpy_array(mean),
-                                          ln_var=nn.Variable.from_numpy_array(ln_var))
+            distribution = D.Gaussian(mean=mean, ln_var=ln_var)
             noise_clip = None
             sampled = distribution.sample(noise_clip=noise_clip)
             sampled.forward()
 
             assert mock_sample_gaussian.call_count == 1
             args, kwargs = mock_sample_gaussian.call_args
 
@@ -97,22 +56,21 @@
 
         input_shape = (batch_size, output_dim)
 
         mu = np.ones(shape=input_shape) * mean
         ln_var = np.ones(shape=input_shape) * np.log(var)
         var = np.exp(ln_var)
 
-        distribution = NnablaGaussian(mean=nn.Variable.from_numpy_array(mu),
-                                      ln_var=nn.Variable.from_numpy_array(ln_var))
+        distribution = D.Gaussian(mean=mu, ln_var=ln_var)
 
         sample, log_prob = distribution.sample_and_compute_log_prob()
 
         # FIXME: if you enable clear_no_need_grad seems to compute something different
         # Do NOT use forward_all and no_need_grad flag at same time
-        # nnabla's bug?
+        # NNabla's bug?
         nn.forward_all([sample, log_prob])
 
         x = sample.d
         gaussian_log_prob = -0.5 * np.log(2.0 * np.pi) \
             - 0.5 * ln_var \
             - (x - mu) ** 2 / (2.0 * var)
         expected = np.sum(gaussian_log_prob, axis=-1, keepdims=True)
@@ -127,16 +85,15 @@
 
         input_shape = (batch_size, output_dim)
         mean = np.zeros(shape=input_shape)
         sigma = np.ones(shape=input_shape) * 5.
         ln_var = np.log(sigma) * 2.
 
         with mock.patch('nnabla_rl.functions.sample_gaussian_multiple') as mock_sample_multiple_gaussian:
-            distribution = NnablaGaussian(mean=nn.Variable.from_numpy_array(mean),
-                                          ln_var=nn.Variable.from_numpy_array(ln_var))
+            distribution = D.Gaussian(mean=mean, ln_var=ln_var)
             noise_clip = None
             num_samples = 10
             sampled = distribution.sample_multiple(
                 num_samples, noise_clip=noise_clip)
             sampled.forward()
 
             assert mock_sample_multiple_gaussian.call_count == 1
@@ -153,22 +110,21 @@
         output_dim = 10
 
         input_shape = (batch_size, output_dim)
 
         mu = np.ones(shape=input_shape) * mean
         ln_var = np.ones(shape=input_shape) * np.log(var)
 
-        distribution = NnablaGaussian(mean=nn.Variable.from_numpy_array(mu),
-                                      ln_var=nn.Variable.from_numpy_array(ln_var))
+        distribution = D.Gaussian(mean=mu, ln_var=ln_var)
         num_samples = 10
         samples, log_probs = distribution.sample_multiple_and_compute_log_prob(
             num_samples=num_samples)
         # FIXME: if you enable clear_no_need_grad seems to compute something different
         # Do NOT use forward_all and no_need_grad flag at same time
-        # nnabla's bug?
+        # NNabla's bug?
         nn.forward_all([samples, log_probs])
 
         x = samples.d[:, 0, :]
         assert x.shape == (batch_size, output_dim)
         gaussian_log_prob = -0.5 * np.log(2.0 * np.pi) \
             - 0.5 * ln_var \
             - (x - mu) ** 2 / (2.0 * var)
@@ -195,16 +151,15 @@
         output_dim = 10
 
         input_shape = (batch_size, output_dim)
         mean = np.zeros(shape=input_shape)
         sigma = np.ones(shape=input_shape) * 5.
         ln_var = np.log(sigma) * 2.
 
-        distribution = NnablaGaussian(mean=nn.Variable.from_numpy_array(mean),
-                                      ln_var=nn.Variable.from_numpy_array(ln_var))
+        distribution = D.Gaussian(mean=mean, ln_var=ln_var)
         num_samples = 10
         samples, log_probs = distribution.sample_multiple_and_compute_log_prob(
             num_samples=num_samples)
         nn.forward_all([samples, log_probs])
 
         assert samples.shape == (batch_size, num_samples, output_dim)
         assert log_probs.shape == (batch_size, num_samples, 1)
@@ -219,16 +174,15 @@
         ln_var = np.log(sigma) * 2.
         dummy_input = nn.Variable.from_numpy_array(
             np.random.randn(batch_size, output_dim))
 
         with mock.patch('nnabla_rl.distributions.common_utils.gaussian_log_prob',
                         return_value=nn.Variable.from_numpy_array(np.empty(shape=input_shape))) \
                 as mock_gaussian_log_prob:
-            distribution = NnablaGaussian(mean=nn.Variable.from_numpy_array(mean),
-                                          ln_var=nn.Variable.from_numpy_array(ln_var))
+            distribution = D.Gaussian(mean=mean, ln_var=ln_var)
             distribution.log_prob(dummy_input)
 
             assert mock_gaussian_log_prob.call_count == 1
 
             args, _ = mock_gaussian_log_prob.call_args
             assert args == (dummy_input,
                             distribution._mean,
@@ -239,15 +193,15 @@
     @pytest.mark.parametrize("output_dim", range(1, 10))
     def test_entropy(self, batch_size, output_dim):
         input_shape = (batch_size, output_dim)
 
         mean = np.zeros(shape=input_shape)
         sigma = np.ones(shape=input_shape)
         ln_var = np.log(sigma) * 2.
-        distribution = NnablaGaussian(nn.Variable.from_numpy_array(mean), nn.Variable.from_numpy_array(ln_var))
+        distribution = D.Gaussian(mean, ln_var)
 
         actual = distribution.entropy()
         actual.forward()
 
         assert actual.shape == (batch_size, 1)
 
         cov = np.diag(sigma[0] ** 2)
@@ -259,16 +213,16 @@
         batch_size = 10
         output_dim = 10
         input_shape = (batch_size, output_dim)
 
         mean = np.zeros(shape=input_shape)
         sigma = np.ones(shape=input_shape) * 5.
         ln_var = np.log(sigma) * 2.
-        distribution_p = NnablaGaussian(nn.Variable.from_numpy_array(mean), nn.Variable.from_numpy_array(ln_var))
-        distribution_q = NnablaGaussian(nn.Variable.from_numpy_array(mean), nn.Variable.from_numpy_array(ln_var))
+        distribution_p = D.Gaussian(mean, ln_var)
+        distribution_q = D.Gaussian(mean, ln_var)
 
         actual = distribution_p.kl_divergence(distribution_q)
         actual.forward()
 
         assert actual.shape == (batch_size, 1)
 
         expected = np.zeros((batch_size, 1))
@@ -279,68 +233,9 @@
         # Assuming that covariance_matrix is diagonal
         diagonal = covariance_matrix.diagonal()
         determinant = np.prod(diagonal)
 
         return 0.5 * np.log(np.power(2.0 * np.pi * np.e, covariance_matrix.shape[0]) * determinant)
 
 
-class TestNumpyGaussian():
-    def _generate_dummy_mean_var(self, scale=5.):
-        gaussian_dim = 10
-        mean_shape = (gaussian_dim, )
-        mean = np.random.normal(size=mean_shape)
-        sigma = np.diag(np.ones(shape=mean_shape) * scale)
-        sigma_inv = np.diag(1.0 / np.diag(sigma))
-        return mean, sigma, sigma_inv
-
-    def test_sample(self):
-        mean, sigma, _ = self._generate_dummy_mean_var()
-        distribution = NumpyGaussian(mean, np.log(sigma))
-        sample = distribution.sample()
-        assert sample.shape == mean.shape
-
-        with pytest.raises(NotImplementedError):
-            distribution.sample(noise_clip=np.ones(mean.shape))
-
-    def test_numpy_log_prob(self):
-        mean, sigma, sigma_inv = self._generate_dummy_mean_var()
-        distribution = NumpyGaussian(mean, np.log(sigma))
-
-        query = np.random.normal(size=mean.shape)
-        actual = distribution.log_prob(query)
-
-        log_det_term = np.log(np.prod(2.0 * np.pi * np.diag(sigma)))
-        quadratic_term = (mean - query).T.dot(sigma_inv).dot(mean - query)
-        expected = -0.5 * (log_det_term + quadratic_term)
-
-        assert expected == pytest.approx(actual, abs=1e-5)
-
-    def test_numpy_kl_divergence_different_distribution(self):
-        p_mean, p_sigma, _ = self._generate_dummy_mean_var()
-        distribution_p = NumpyGaussian(p_mean, np.log(p_sigma))
-
-        q_mean, q_sigma, _ = self._generate_dummy_mean_var(scale=3)
-        distribution_q = NumpyGaussian(q_mean, np.log(q_sigma))
-
-        actual = distribution_p.kl_divergence(distribution_q)
-
-        q_sigma_inv = np.diag(1.0 / np.diag(q_sigma))
-        trace_term = np.sum(np.diag(q_sigma_inv.dot(p_sigma)))
-        quadratic_term = (q_mean - p_mean).T.dot(q_sigma_inv).dot(q_mean - p_mean)
-        log_det_term = np.log(np.prod(np.diag(q_sigma)) / np.prod(np.diag(p_sigma)))
-        expected = 0.5 * (trace_term + quadratic_term - p_mean.shape[0] + log_det_term)
-
-        assert expected == pytest.approx(actual, abs=1e-5)
-
-    def test_numpy_kl_divergence_identical_distribution(self):
-        mean, sigma, _ = self._generate_dummy_mean_var()
-        distribution_p = NumpyGaussian(mean, np.log(sigma))
-        distribution_q = NumpyGaussian(mean, np.log(sigma))
-
-        actual = distribution_p.kl_divergence(distribution_q)
-        expected = np.zeros((1, ))
-
-        assert expected == pytest.approx(actual, abs=1e-5)
-
-
-if __name__ == '__main__':
+if __name__ == "__main__":
     pytest.main()
```

## tests/distributions/test_softmax.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -17,157 +17,70 @@
 import pytest
 
 import nnabla as nn
 import nnabla_rl.distributions as D
 
 
 class TestSoftmax(object):
-    def setup_method(self, method):
-        nn.clear_parameters()
-
     def test_sample(self):
         z = np.array([[0, 0, 1000, 0],
                       [0, 1000, 0, 0],
                       [1000, 0, 0, 0],
                       [0, 0, 0, 1000]])
 
         batch_size = z.shape[0]
         distribution = D.Softmax(z=z)
         sampled = distribution.sample()
 
         sampled.forward()
         assert sampled.shape == (batch_size, 1)
         assert np.all(sampled.d == np.array([[2], [1], [0], [3]]))
 
-    def test_sample_multi_dimensional(self):
-        z = np.array([[[1000, 0, 0, 0],
-                       [0, 0, 0, 1000],
-                       [0, 1000, 0, 0],
-                       [0, 0, 1000, 0]],
-                      [[0, 0, 1000, 0],
-                       [0, 1000, 0, 0],
-                       [1000, 0, 0, 0],
-                       [0, 0, 0, 1000]]])
-        assert z.shape == (2, 4, 4)
-        batch_size = z.shape[0]
-        category_size = z.shape[1]
-        distribution = D.Softmax(z=z)
-        sampled = distribution.sample()
-
-        sampled.forward()
-        assert sampled.shape == (batch_size, category_size, 1)
-        assert np.all(sampled.d == np.array([[[0], [3], [1], [2]], [[2], [1], [0], [3]]]))
-
-    def test_choose_probable(self):
-        z = np.array([[1.0, 2.0, 3.0, 4.0],
-                      [2.0, 3.0, 1.0, -1.0],
-                      [-5.1, 11.2, 0.8, 0.7],
-                      [-3.0, -2.1, -7.6, -5.4]])
-        assert z.shape == (4, 4)
-        batch_size = z.shape[0]
-        distribution = D.Softmax(z=z)
-        probable = distribution.choose_probable()
-
-        probable.forward()
-        assert probable.shape == (batch_size, )
-        assert np.all(probable.d == np.array([3, 1, 1, 1]))
-
-    def test_choose_probable_multi_dimensional(self):
-        z = np.array([[[1.0, 2.0, 3.0, 4.0],
-                       [2.0, 3.0, 1.0, -1.0],
-                       [-5.1, 11.2, 0.8, 0.7],
-                       [-3.0, -2.1, -7.6, -5.4]],
-                      [[0.9, 0.8, -7.1, 3.2],
-                       [0.1, 0.2, 0.3, 0.4],
-                       [-1.2, -2.7, -3.1, -4.3],
-                       [1.0, 1.2, 1.3, -4.3]]])
-        assert z.shape == (2, 4, 4)
-        batch_size = z.shape[0]
-        category_size = z.shape[1]
-        distribution = D.Softmax(z=z)
-        probable = distribution.choose_probable()
-
-        probable.forward()
-        assert probable.shape == (batch_size, category_size)
-        assert np.all(probable.d == np.array([[3, 1, 1, 1], [3, 3, 0, 2]]))
-
     def test_log_prob(self):
         batch_size = 10
         action_num = 4
         z = np.random.normal(size=(batch_size, action_num))
         actions = np.array([[i % action_num] for i in range(batch_size)])
 
         distribution = D.Softmax(z=z)
         actual = distribution.log_prob(nn.Variable.from_numpy_array(actions))
         actual.forward()
         assert actual.shape == (batch_size, 1)
 
         probabilities = np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)
         log_probabilities = np.log(probabilities)
         indices = np.reshape(actions, newshape=(batch_size, ))
-        one_hot_action = self._to_one_hot_action(indices, action_num=action_num)
-        expected = np.sum(log_probabilities * one_hot_action, axis=1, keepdims=True)
-
-        assert actual.shape == expected.shape
-        assert np.allclose(actual.d, expected)
-
-    def test_log_prob_multi_dimensional(self):
-        batch_size = 10
-        category_num = 3
-        action_num = 4
-        z = np.random.normal(size=(batch_size, category_num, action_num))
-        actions = np.array([[[i % action_num] for i in range(category_num)] for _ in range(batch_size)])
-        assert actions.shape == (batch_size, category_num, 1)
-
-        distribution = D.Softmax(z=z)
-        actual = distribution.log_prob(nn.Variable.from_numpy_array(actions))
-        actual.forward()
-        assert actual.shape == (batch_size, category_num, 1)
-
-        probabilities = np.exp(z) / np.sum(np.exp(z), axis=len(z.shape) - 1, keepdims=True)
-        log_probabilities = np.log(probabilities)
-        indices = np.reshape(actions, newshape=(batch_size, category_num, ))
-        one_hot_action = self._to_one_hot_action(indices, action_num=action_num)
-        expected = np.sum(log_probabilities * one_hot_action, axis=len(z.shape) - 1, keepdims=True)
+        one_hot_action = self._to_one_hot_action(
+            indices, action_num=action_num)
+        expected = np.sum(log_probabilities * one_hot_action,
+                          axis=1,
+                          keepdims=True)
 
         assert actual.shape == expected.shape
         assert np.allclose(actual.d, expected)
 
     def test_entropy(self):
         batch_size = 10
         action_num = 4
         z = np.random.normal(size=(batch_size, action_num))
         distribution = D.Softmax(z=z)
         actual = distribution.entropy()
         actual.forward()
         assert actual.shape == (batch_size, 1)
 
         probabilities = np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)
-        expected = -np.sum(np.log(probabilities) * probabilities, axis=1, keepdims=True)
-
-        assert actual.shape == expected.shape
-        assert np.allclose(actual.d, expected)
-
-    def test_entropy_multi_dimensional(self):
-        batch_size = 10
-        category_num = 3
-        action_num = 4
-        z = np.random.normal(size=(batch_size, category_num, action_num))
-        distribution = D.Softmax(z=z)
-        actual = distribution.entropy()
-        actual.forward()
-        assert actual.shape == (batch_size, category_num, 1)
-
-        probabilities = np.exp(z) / np.sum(np.exp(z), axis=len(z.shape) - 1, keepdims=True)
-        expected = -np.sum(np.log(probabilities) * probabilities, axis=len(z.shape) - 1, keepdims=True)
+        expected = -np.sum(np.log(probabilities) *
+                           probabilities, axis=1, keepdims=True)
 
         assert actual.shape == expected.shape
         assert np.allclose(actual.d, expected)
 
     def test_kl_divergence(self):
+        nn.clear_parameters()
+
         batch_size = 1
         z_p = np.array([[0.25, 0.95]])
         z_p_dist = self._softmax(z_p)
         distribution_p = D.Softmax(z=z_p)
 
         z_q = np.array([[0.5, 1.5]])
         z_q_dist = self._softmax(z_q)
@@ -179,39 +92,18 @@
         assert actual.shape == (batch_size, 1)
 
         expected = z_p_dist[0, 0] * np.log(z_p_dist[0, 0] / z_q_dist[0, 0]) + \
             z_p_dist[0, 1] * np.log(z_p_dist[0, 1] / z_q_dist[0, 1])
 
         assert expected == pytest.approx(actual.d.flatten()[0], abs=1e-5)
 
-    def test_kl_divergence_multi_dimensional(self):
-        batch_size = 1
-        category_num = 2
-        z_p = np.array([[[0.25, 0.95], [0.3, 0.1]]])
-        z_p_dist = self._softmax(z_p)
-        distribution_p = D.Softmax(z=z_p)
-
-        z_q = np.array([[[0.5, 1.5], [5.0, -3.0]]])
-        z_q_dist = self._softmax(z_q)
-        distribution_q = D.Softmax(z=z_q)
-
-        actual = distribution_p.kl_divergence(distribution_q)
-        actual.forward()
-
-        assert actual.shape == (batch_size, category_num, 1)
-
-        for category in range(category_num):
-            expected = z_p_dist[0, category, 0] * np.log(z_p_dist[0, category, 0] / z_q_dist[0, category, 0]) + \
-                z_p_dist[0, category, 1] * np.log(z_p_dist[0, category, 1] / z_q_dist[0, category, 1])
-            assert expected == pytest.approx(actual.d.flatten()[category], abs=1e-5)
-
     def _to_one_hot_action(self, a, action_num):
         action = a
         return np.eye(action_num, dtype=np.float32)[action]
 
     def _softmax(self, z):
-        e_z = np.exp(z - np.max(z, axis=len(z.shape) - 1, keepdims=True))
-        return e_z / np.sum(e_z, axis=len(z.shape) - 1, keepdims=True)
+        e_z = np.exp(z - np.max(z, axis=1, keepdims=True))
+        return e_z / np.sum(e_z, axis=1, keepdims=True)
 
 
 if __name__ == "__main__":
     pytest.main()
```

## tests/distributions/test_squashed_gaussian.py

```diff
@@ -102,15 +102,15 @@
         ln_var = np.array(np.log(var)).reshape((1, 1))
         distribution = D.SquashedGaussian(mean=mean, ln_var=ln_var)
         ln_var = np.log(var)
 
         sample, actual = distribution.sample_and_compute_log_prob()
         # FIXME: if you enable clear_no_need_grad seems to compute something different
         # Do NOT use forward_all and no_need_grad flag at same time
-        # nnabla's bug?
+        # NNabla's bug?
         nn.forward_all([sample, actual])
 
         x = np.arctanh(sample.data.data, dtype=np.float64)
         gaussian_log_prob = -0.5 * \
             np.log(2.0 * np.pi) - 0.5 * ln_var - \
             (x - mean) ** 2 / (2.0 * var)
         log_det_jacobian = np.log(1 - np.tanh(x) ** 2)
@@ -147,15 +147,15 @@
 
         distribution = D.SquashedGaussian(mean=mu, ln_var=ln_var)
         num_samples = 10
         samples, log_probs = distribution.sample_multiple_and_compute_log_prob(
             num_samples=num_samples)
         # FIXME: if you enable clear_no_need_grad seems to compute something different
         # Do NOT use forward_all and no_need_grad flag at same time
-        # nnabla's bug?
+        # NNabla's bug?
         nn.forward_all([samples, log_probs])
 
         assert np.alltrue(-1.0 <= samples.d)
         assert np.alltrue(samples.d <= 1.0)
 
         # Check the first sample independently
         x = np.arctanh(samples.d[:, 0, :], dtype=np.float64)
```

## tests/environment_explorers/test_epsilon_greedy.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023,2024 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -90,39 +90,10 @@
 
         assert np.isclose(explorer._compute_epsilon(1), expected_epsilon(1))
         assert np.isclose(explorer._compute_epsilon(50), expected_epsilon(50))
         assert np.isclose(explorer._compute_epsilon(99), expected_epsilon(99))
         assert np.isclose(explorer._compute_epsilon(100), expected_epsilon(100))
         assert explorer._compute_epsilon(200) == final_epsilon
 
-    def test_return_explorer_info(self, ):
-        initial_epsilon = 1.0
-        final_epsilon = 0.0
-        max_explore_steps = 100
-        greedy_selector_mock = mock.MagicMock(return_value=(1, {}))
-        random_selector_mock = mock.MagicMock(return_value=(2, {}))
-        config = LinearDecayEpsilonGreedyExplorerConfig(initial_epsilon=initial_epsilon,
-                                                        final_epsilon=final_epsilon,
-                                                        max_explore_steps=max_explore_steps,
-                                                        append_explorer_info=True)
-        explorer = LinearDecayEpsilonGreedyExplorer(greedy_selector_mock,
-                                                    random_selector_mock,
-                                                    env_info=None,
-                                                    config=config)
-
-        action, action_info = explorer.action(50, np.random.rand(5))
-
-        assert "greedy_action" in action_info
-        assert "explore_rate" in action_info
-
-        if action == 1:
-            assert action_info["greedy_action"]
-        elif action == 2:
-            assert not action_info["greedy_action"]
-        else:
-            raise RuntimeError("Should not reach here")
-
-        assert action_info["explore_rate"] == 0.5
-
 
 if __name__ == '__main__':
     pytest.main()
```

## tests/environment_explorers/test_gaussian.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -20,15 +20,15 @@
 
 
 class TestRandomGaussianActionStrategy(object):
     @pytest.mark.parametrize('clip_low', np.arange(start=-1.0, stop=0.0, step=0.25))
     @pytest.mark.parametrize("clip_high", np.arange(start=0.0, stop=1.0, step=0.25))
     @pytest.mark.parametrize("sigma", np.arange(start=0.01, stop=5.0, step=1.0))
     def test_random_gaussian_action_selection(self, clip_low, clip_high, sigma):
-        def policy_action_selector(state, *,  begin_of_episode=False):
+        def policy_action_selector(state):
             return np.zeros(shape=state.shape), {'test': 'success'}
         config = GaussianExplorerConfig(
             action_clip_low=clip_low,
             action_clip_high=clip_high,
             sigma=sigma
         )
         explorer = GaussianExplorer(
@@ -40,31 +40,10 @@
         steps = 1
         state = np.empty(shape=(1, 4))
         action, info = explorer.action(steps, state)
 
         assert np.all(clip_low <= action) and np.all(action <= clip_high)
         assert info['test'] == 'success'
 
-    @pytest.mark.parametrize("sigma", np.arange(start=0.01, stop=5.0, step=1.0))
-    def test_random_gaussian_without_clipping(self, sigma):
-        def policy_action_selector(state, *,  begin_of_episode=False):
-            return np.zeros(shape=state.shape), {'test': 'success'}
-
-        config = GaussianExplorerConfig(
-            sigma=sigma
-        )
-        explorer = GaussianExplorer(
-            env_info=None,
-            policy_action_selector=policy_action_selector,
-            config=config
-        )
-
-        steps = 1
-        state = np.empty(shape=(1, 4))
-        action, info = explorer.action(steps, state)
-
-        assert not np.allclose(action, np.zeros(action.shape))
-        assert info['test'] == 'success'
-
 
 if __name__ == '__main__':
     pytest.main()
```

## tests/environments/test_env_info.py

```diff
@@ -1,23 +1,22 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import gym
 import numpy as np
 import pytest
 
 import nnabla_rl.environments as E
 from nnabla_rl.environments.environment_info import EnvironmentInfo
 
 
@@ -42,192 +41,58 @@
     def test_is_continuous_action_env(self):
         dummy_env = E.DummyContinuous()
         env_info = EnvironmentInfo.from_env(dummy_env)
 
         assert not env_info.is_discrete_action_env()
         assert env_info.is_continuous_action_env()
 
-    def test_is_discrete_state_env(self):
-        dummy_env = E.DummyDiscrete()
-        env_info = EnvironmentInfo.from_env(dummy_env)
-
-        assert env_info.is_discrete_state_env()
-        assert not env_info.is_continuous_state_env()
-        assert not env_info.is_tuple_state_env()
-
-    def test_is_continuous_state_env(self):
+    def test_state_shape_continuous(self):
         dummy_env = E.DummyContinuous()
         env_info = EnvironmentInfo.from_env(dummy_env)
 
-        assert not env_info.is_discrete_state_env()
-        assert env_info.is_continuous_state_env()
-        assert not env_info.is_tuple_state_env()
+        assert env_info.state_shape == dummy_env.observation_space.shape
 
-    def test_is_tuple_and_discrete_env(self):
-        dummy_env = E.DummyTupleDiscrete()
+    def test_state_shape_image(self):
+        dummy_env = E.DummyDiscreteImg()
         env_info = EnvironmentInfo.from_env(dummy_env)
 
-        assert env_info.is_discrete_state_env()
-        assert not env_info.is_continuous_state_env()
-        assert env_info.is_tuple_state_env()
-        assert env_info.is_discrete_action_env()
-        assert not env_info.is_continuous_action_env()
-        assert env_info.is_tuple_action_env()
+        assert env_info.state_shape == dummy_env.observation_space.shape
 
-    def test_is_tuple_and_continuous_env(self):
-        dummy_env = E.DummyTupleContinuous()
+    def test_state_dim_continuous(self):
+        dummy_env = E.DummyContinuous()
         env_info = EnvironmentInfo.from_env(dummy_env)
 
-        assert not env_info.is_discrete_state_env()
-        assert env_info.is_continuous_state_env()
-        assert env_info.is_tuple_state_env()
-        assert not env_info.is_discrete_action_env()
-        assert env_info.is_continuous_action_env()
-        assert env_info.is_tuple_action_env()
+        assert env_info.state_dim == dummy_env.observation_space.shape[0]
 
-    def test_is_tuple_and_mixed_env(self):
-        dummy_env = E.DummyTupleMixed()
+    def test_state_dim_image(self):
+        dummy_env = E.DummyDiscreteImg()
         env_info = EnvironmentInfo.from_env(dummy_env)
-        assert env_info.is_tuple_state_env()
-        assert env_info.is_mixed_state_env()
-        assert env_info.is_tuple_action_env()
-        assert env_info.is_mixed_action_env()
+
+        assert env_info.state_dim == np.prod(dummy_env.observation_space.shape)
 
     def test_action_shape_continuous(self):
         dummy_env = E.DummyContinuous()
         env_info = EnvironmentInfo.from_env(dummy_env)
 
         assert env_info.action_shape == dummy_env.action_space.shape
 
     def test_action_shape_discrete(self):
         dummy_env = E.DummyDiscreteImg()
         env_info = EnvironmentInfo.from_env(dummy_env)
 
-        assert env_info.action_shape == (1, )
-
-    def test_action_shape_tuple_continuous(self):
-        dummy_env = E.DummyTupleContinuous()
-        env_info = EnvironmentInfo.from_env(dummy_env)
-
-        assert env_info.action_shape == tuple(space.shape for space in dummy_env.action_space)
-
-    def test_action_shape_tuple_discrete(self):
-        dummy_env = E.DummyTupleDiscrete()
-        env_info = EnvironmentInfo.from_env(dummy_env)
-
-        assert env_info.action_shape == ((1, ), (1, ))
-
-    def test_action_shape_tuple_mixed(self):
-        dummy_env = E.DummyTupleMixed()
-        env_info = EnvironmentInfo.from_env(dummy_env)
-
-        assert env_info.action_shape == tuple(space.shape if isinstance(
-            space, gym.spaces.Box) else (1, ) for space in dummy_env.action_space)
+        assert env_info.action_shape == dummy_env.action_space.shape
 
     def test_action_dim_continuous(self):
         dummy_env = E.DummyContinuous()
         env_info = EnvironmentInfo.from_env(dummy_env)
 
         assert env_info.action_dim == dummy_env.action_space.shape[0]
 
     def test_action_dim_discrete(self):
         dummy_env = E.DummyDiscrete()
         env_info = EnvironmentInfo.from_env(dummy_env)
 
         assert env_info.action_dim == dummy_env.action_space.n
 
-    def test_action_dim_tuple_discrete(self):
-        dummy_env = E.DummyTupleDiscrete()
-        env_info = EnvironmentInfo.from_env(dummy_env)
-
-        assert env_info.action_dim == tuple(space.n for space in env_info.action_space)
-
-    def test_action_dim_tuple_continuous(self):
-        dummy_env = E.DummyTupleContinuous()
-        env_info = EnvironmentInfo.from_env(dummy_env)
-
-        assert env_info.action_dim == tuple(np.prod(space.shape) for space in env_info.action_space)
-
-    def test_action_dim_tuple_mixed(self):
-        dummy_env = E.DummyTupleMixed()
-        env_info = EnvironmentInfo.from_env(dummy_env)
-
-        assert env_info.action_dim == tuple(np.prod(space.shape) if isinstance(
-            space, gym.spaces.Box) else space.n for space in env_info.action_space)
-
-    def test_state_shape_discrete(self):
-        dummy_env = E.DummyDiscrete()
-        env_info = EnvironmentInfo.from_env(dummy_env)
-
-        assert env_info.state_shape == (1, )
-
-    def test_state_shape_continuous(self):
-        dummy_env = E.DummyContinuous()
-        env_info = EnvironmentInfo.from_env(dummy_env)
-
-        assert env_info.state_shape == dummy_env.observation_space.shape
-
-    def test_state_shape_image(self):
-        dummy_env = E.DummyDiscreteImg()
-        env_info = EnvironmentInfo.from_env(dummy_env)
-
-        assert env_info.state_shape == dummy_env.observation_space.shape
-
-    def test_state_shape_tuple_discrete(self):
-        dummy_env = E.DummyTupleDiscrete()
-        env_info = EnvironmentInfo.from_env(dummy_env)
-
-        assert env_info.state_shape == ((1, ), (1, ))
-
-    def test_state_shape_tuple_continuous(self):
-        dummy_env = E.DummyTupleContinuous()
-        env_info = EnvironmentInfo.from_env(dummy_env)
-
-        assert env_info.state_shape == tuple(space.shape for space in dummy_env.observation_space)
-
-    def test_state_shape_tuple_mixed(self):
-        dummy_env = E.DummyTupleMixed()
-        env_info = EnvironmentInfo.from_env(dummy_env)
-
-        assert env_info.state_shape == tuple(space.shape if isinstance(
-            space, gym.spaces.Box) else (1, ) for space in dummy_env.observation_space)
-
-    def test_state_dim_discrete(self):
-        dummy_env = E.DummyDiscrete()
-        env_info = EnvironmentInfo.from_env(dummy_env)
-
-        assert env_info.state_dim == dummy_env.observation_space.n
-
-    def test_state_dim_continuous(self):
-        dummy_env = E.DummyContinuous()
-        env_info = EnvironmentInfo.from_env(dummy_env)
-
-        assert env_info.state_dim == dummy_env.observation_space.shape[0]
-
-    def test_state_dim_image(self):
-        dummy_env = E.DummyDiscreteImg()
-        env_info = EnvironmentInfo.from_env(dummy_env)
-
-        assert env_info.state_dim == np.prod(dummy_env.observation_space.shape)
-
-    def test_state_dim_tuple_discrete(self):
-        dummy_env = E.DummyTupleDiscrete()
-        env_info = EnvironmentInfo.from_env(dummy_env)
-
-        assert env_info.state_dim == tuple(space.n for space in env_info.observation_space)
-
-    def test_state_dim_tuple_continuous(self):
-        dummy_env = E.DummyTupleContinuous()
-        env_info = EnvironmentInfo.from_env(dummy_env)
-
-        assert env_info.state_dim == tuple(np.prod(space.shape) for space in env_info.observation_space)
-
-    def test_state_dim_tuple_mixed(self):
-        dummy_env = E.DummyTupleMixed()
-        env_info = EnvironmentInfo.from_env(dummy_env)
-
-        assert env_info.state_dim == tuple(np.prod(space.shape) if isinstance(
-            space, gym.spaces.Box) else space.n for space in env_info.observation_space)
-
 
 if __name__ == "__main__":
     pytest.main()
```

## tests/environments/wrappers/test_common.py

```diff
@@ -1,45 +1,27 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023,2024 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import gym
 import numpy as np
 import pytest
 
 import nnabla_rl.environments as E
-from nnabla_rl.environments.wrappers.common import FlattenNestedTupleStateWrapper, NumpyFloat32Env, TimestepAsStateEnv
-
-
-class DummyNestedTupleStateEnv(gym.Env):
-    def __init__(self, observation_space) -> None:
-        super().__init__()
-        self.action_space = gym.spaces.Box(low=0.0, high=1.0, shape=(4, ))
-        self.observation_space = observation_space
-
-    def reset(self):
-        return self.observation_space.sample()
-
-    def step(self, a):
-        next_state = self.observation_space.sample()
-        reward = np.random.randn()
-        done = False
-        info = {}
-        return next_state, reward, done, info
+from nnabla_rl.environments.wrappers.common import NumpyFloat32Env
 
 
 class TestCommon(object):
     def test_numpy_float32_env_continuous(self):
         env = E.DummyContinuous()
         env = NumpyFloat32Env(env)
         assert env.observation_space.dtype == np.float32
@@ -50,115 +32,19 @@
 
         assert next_state.dtype == np.float32
         assert reward.dtype == np.float32
 
     def test_numpy_float32_env_discrete(self):
         env = E.DummyDiscrete()
         env = NumpyFloat32Env(env)
+        assert env.observation_space.dtype == np.float32
         assert not env.action_space.dtype == np.float32
 
         action = env.action_space.sample()
         next_state, reward, _, _ = env.step(action)
 
         assert next_state.dtype == np.float32
         assert reward.dtype == np.float32
 
-    def test_numpy_float32_env_tuple_continuous(self):
-        env = E.DummyTupleContinuous()
-        env = NumpyFloat32Env(env)
-
-        action = env.action_space.sample()
-        next_state, reward, _, _ = env.step(action)
-
-        assert next_state[0].dtype == np.float32
-        assert next_state[1].dtype == np.float32
-        assert action[0].dtype == np.float32
-        assert action[1].dtype == np.float32
-        assert reward.dtype == np.float32
-
-    def test_numpy_float32_env_tuple_discrete(self):
-        env = E.DummyTupleDiscrete()
-        env = NumpyFloat32Env(env)
-
-        action = env.action_space.sample()
-        next_state, reward, _, _ = env.step(action)
-
-        assert next_state[0].dtype == np.float32
-        assert next_state[1].dtype == np.float32
-        assert isinstance(action[0], int)
-        assert isinstance(action[0], int)
-        assert reward.dtype == np.float32
-
-    def test_numpy_float32_env_tuple_mixed(self):
-        env = E.DummyTupleMixed()
-        env = NumpyFloat32Env(env)
-
-        action = env.action_space.sample()
-        next_state, reward, _, _ = env.step(action)
-
-        assert next_state[0].dtype == np.float32
-        assert next_state[1].dtype == np.float32
-        assert isinstance(action[0], int)
-        assert action[1].dtype == np.float32
-        assert reward.dtype == np.float32
-
-    def test_timestep_as_state_env_continuous(self):
-        env = E.DummyContinuous()
-        env = TimestepAsStateEnv(env)
-
-        action = env.action_space.sample()
-        next_state, reward, _, _ = env.step(action)
-
-        assert len(next_state) == 2
-        assert next_state[1] == 1
-
-        next_state, reward, _, _ = env.step(action)
-
-        assert len(next_state) == 2
-        assert next_state[1] == 2
-
-    def test_timestep_as_state_env_discrete(self):
-        env = E.DummyDiscrete()
-        env = TimestepAsStateEnv(env)
-
-        action = env.action_space.sample()
-        next_state, reward, _, _ = env.step(action)
-
-        assert len(next_state) == 2
-        assert next_state[1] == 1
-
-        next_state, reward, _, _ = env.step(action)
-
-        assert len(next_state) == 2
-        assert next_state[1] == 2
-
-    def test_flatten_nested_tuple_state(self):
-        box_space_list = [gym.spaces.Box(low=0.0, high=1.0, shape=(i, )) for i in range(5)]
-
-        observation_space = gym.spaces.Tuple(
-            [gym.spaces.Tuple(box_space_list[0:3]),
-             gym.spaces.Tuple(box_space_list[3:])])
-
-        env = DummyNestedTupleStateEnv(observation_space)
-        env = FlattenNestedTupleStateWrapper(env)
-
-        # Check observation space is flattened
-        assert isinstance(env.observation_space, gym.spaces.Tuple)
-        for i, actual_space in enumerate(env.observation_space):
-            assert isinstance(actual_space, gym.spaces.Box)
-            assert actual_space.shape == box_space_list[i].shape
-
-        # Check state shape
-        state = env.reset()
-        self._nested_shape_check(state, box_space_list)
-
-        next_state, _, _, _ = env.step(np.array([1.0]))
-        self._nested_shape_check(state, box_space_list)
-
-    def _nested_shape_check(self, state, box_space_list):
-        assert isinstance(state, tuple)
-        for s, space in zip(state, box_space_list):
-            assert s.shape == space.shape
-
 
 if __name__ == "__main__":
     pytest.main()
```

## tests/model_trainers/test_policy_trainers.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -19,78 +19,22 @@
 import pytest
 
 import nnabla as nn
 import nnabla.functions as NF
 import nnabla.initializer as NI
 import nnabla.parametric_functions as NPF
 import nnabla_rl.model_trainers as MT
-from nnabla_rl.distributions.gaussian import Gaussian
-from nnabla_rl.environments.dummy import DummyContinuous
-from nnabla_rl.environments.environment_info import EnvironmentInfo
-from nnabla_rl.model_trainers.model_trainer import LossIntegration
-from nnabla_rl.model_trainers.policy.dpg_policy_trainer import DPGPolicyTrainer
-from nnabla_rl.model_trainers.policy.soft_policy_trainer import AdjustableTemperature, SoftPolicyTrainer
+from nnabla_rl.model_trainers.policy.soft_policy_trainer import AdjustableTemperature
 from nnabla_rl.model_trainers.policy.trpo_policy_trainer import (_concat_network_params_in_ndarray,
                                                                  _hessian_vector_product,
                                                                  _update_network_params_by_flat_params)
-from nnabla_rl.models import TD3Policy, TD3QFunction
-from nnabla_rl.models.mujoco.policies import SACPolicy
-from nnabla_rl.models.mujoco.q_functions import SACQFunction
-from nnabla_rl.models.policy import DeterministicPolicy, StochasticPolicy
 from nnabla_rl.utils.matrices import compute_hessian
 from nnabla_rl.utils.optimization import conjugate_gradient
 
 
-class DeterministicRnnPolicy(DeterministicPolicy):
-    def __init__(self, scope_name: str):
-        super().__init__(scope_name)
-        self._internal_state_shape = (10, )
-        self._fake_internal_state = None
-
-    def is_recurrent(self) -> bool:
-        return True
-
-    def internal_state_shapes(self):
-        return {'fake': self._internal_state_shape}
-
-    def set_internal_states(self, states):
-        self._fake_internal_state = states['fake']
-
-    def get_internal_states(self):
-        return {'fake': self._fake_internal_state}
-
-    def pi(self, s):
-        self._fake_internal_state = self._fake_internal_state * 2
-        return s
-
-
-class StochasticRnnPolicy(StochasticPolicy):
-    def __init__(self, scope_name: str):
-        super().__init__(scope_name)
-        self._internal_state_shape = (10, )
-        self._fake_internal_state = None
-
-    def is_recurrent(self) -> bool:
-        return True
-
-    def internal_state_shapes(self):
-        return {'fake': self._internal_state_shape}
-
-    def set_internal_states(self, states):
-        self._fake_internal_state = states['fake']
-
-    def get_internal_states(self):
-        return {'fake': self._fake_internal_state}
-
-    def pi(self, s):
-        self._fake_internal_state = self._fake_internal_state * 2
-        return Gaussian(mean=nn.Variable.from_numpy_array(np.zeros(s.shape)),
-                        ln_var=nn.Variable.from_numpy_array(np.ones(s.shape)))
-
-
 class TrainerTest(metaclass=ABCMeta):
     def setup_method(self, method):
         nn.clear_parameters()
         np.random.seed(0)
 
 
 class TestBEARPolicyTrainer(TrainerTest):
@@ -252,98 +196,19 @@
 
 
 class TestTRPOPolicyTrainer(TrainerTest):
     pass
 
 
 class TestDPGPolicyTrainer(TrainerTest):
-    def setup_method(self, method):
-        nn.clear_parameters()
-
-    @pytest.mark.parametrize('unroll_steps', [1, 2])
-    @pytest.mark.parametrize('burn_in_steps', [0, 1, 2])
-    @pytest.mark.parametrize('loss_integration', [LossIntegration.LAST_TIMESTEP_ONLY, LossIntegration.ALL_TIMESTEPS])
-    def test_with_non_rnn_model(self, unroll_steps, burn_in_steps, loss_integration):
-        env_info = EnvironmentInfo.from_env(DummyContinuous())
-
-        policy = TD3Policy('stub_pi', action_dim=env_info.action_dim, max_action_value=1)
-        train_q = TD3QFunction('stub_q', None)
-        # Using DQN Q trainer as representative trainer
-        config = MT.policy_trainers.DPGPolicyTrainerConfig(unroll_steps=unroll_steps,
-                                                           burn_in_steps=burn_in_steps,
-                                                           loss_integration=loss_integration)
-        DPGPolicyTrainer(models=policy, q_function=train_q, solvers={}, env_info=env_info, config=config)
-
-        # pass: If no ecror occurs
-
-    @pytest.mark.parametrize('unroll_steps', [1, 2])
-    @pytest.mark.parametrize('burn_in_steps', [0, 1, 2])
-    @pytest.mark.parametrize('loss_integration', [LossIntegration.LAST_TIMESTEP_ONLY, LossIntegration.ALL_TIMESTEPS])
-    def test_with_rnn_model(self, unroll_steps, burn_in_steps, loss_integration):
-        env_info = EnvironmentInfo.from_env(DummyContinuous())
-
-        policy = DeterministicRnnPolicy('stub_pi')
-        train_q = TD3QFunction('stub_q', None)
-        # Using DQN Q trainer as representative trainer
-        config = MT.policy_trainers.DPGPolicyTrainerConfig(unroll_steps=unroll_steps,
-                                                           burn_in_steps=burn_in_steps,
-                                                           loss_integration=loss_integration)
-        DPGPolicyTrainer(models=policy, q_function=train_q, solvers={}, env_info=env_info, config=config)
-
-        # pass: If no ecror occurs
+    pass
 
 
 class TestSoftPolicyTrainer(TrainerTest):
-    def setup_method(self, method):
-        nn.clear_parameters()
-
-    @pytest.mark.parametrize('unroll_steps', [1, 2])
-    @pytest.mark.parametrize('burn_in_steps', [0, 1, 2])
-    @pytest.mark.parametrize('loss_integration', [LossIntegration.LAST_TIMESTEP_ONLY, LossIntegration.ALL_TIMESTEPS])
-    def test_with_non_rnn_model(self, unroll_steps, burn_in_steps, loss_integration):
-        env_info = EnvironmentInfo.from_env(DummyContinuous())
-
-        policy = SACPolicy('stub_pi', action_dim=env_info.action_dim)
-        train_q1 = SACQFunction('stub_q1', None)
-        train_q2 = SACQFunction('stub_q2', None)
-        # Using DQN Q trainer as representative trainer
-        config = MT.policy_trainers.SoftPolicyTrainerConfig(unroll_steps=unroll_steps,
-                                                            burn_in_steps=burn_in_steps,
-                                                            loss_integration=loss_integration,
-                                                            fixed_temperature=True)
-        SoftPolicyTrainer(policy,
-                          solvers={},
-                          q_functions=[train_q1, train_q2],
-                          temperature=AdjustableTemperature('stub_t'),
-                          temperature_solver=None,
-                          env_info=env_info, config=config)
-
-        # pass: If no ecror occurs
-
-    @pytest.mark.parametrize('unroll_steps', [1, 2])
-    @pytest.mark.parametrize('burn_in_steps', [0, 1, 2])
-    @pytest.mark.parametrize('loss_integration', [LossIntegration.LAST_TIMESTEP_ONLY, LossIntegration.ALL_TIMESTEPS])
-    def test_with_rnn_model(self, unroll_steps, burn_in_steps, loss_integration):
-        env_info = EnvironmentInfo.from_env(DummyContinuous())
-
-        policy = StochasticRnnPolicy('stub_pi')
-        train_q1 = SACQFunction('stub_q1', None)
-        train_q2 = SACQFunction('stub_q2', None)
-        config = MT.policy_trainers.SoftPolicyTrainerConfig(unroll_steps=unroll_steps,
-                                                            burn_in_steps=burn_in_steps,
-                                                            loss_integration=loss_integration,
-                                                            fixed_temperature=True)
-        SoftPolicyTrainer(policy,
-                          solvers={},
-                          q_functions=[train_q1, train_q2],
-                          temperature=AdjustableTemperature('stub_t'),
-                          temperature_solver=None,
-                          env_info=env_info, config=config)
-
-        # pass: If no ecror occurs
+    pass
 
 
 class TestAdjustableTemperature(TrainerTest):
     def test_initial_temperature(self):
         initial_value = 5.0
         temperature = AdjustableTemperature(
             scope_name='test', initial_value=initial_value)
```

## tests/model_trainers/test_q_value_trainers.py

```diff
@@ -1,35 +1,27 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from typing import Optional
-from unittest.mock import patch
-
 import numpy as np
 import pytest
 
 import nnabla as nn
 import nnabla_rl.model_trainers as MT
-from nnabla_rl.environments.dummy import DummyDiscreteImg
-from nnabla_rl.environments.environment_info import EnvironmentInfo
-from nnabla_rl.model_trainers.model_trainer import LossIntegration, TrainingBatch, TrainingVariables
-from nnabla_rl.model_trainers.q_value.dqn_q_trainer import DQNQTrainer
-from nnabla_rl.models import DQNQFunction, DRQNQFunction
 
 
 class TestC51ValueDistributionFunctionTrainer(object):
     def setup_method(self, method):
         nn.clear_parameters()
 
 
@@ -53,226 +45,10 @@
         assert np.allclose(expected, actual)
 
 
 class TestSquaredTDQFunctionTrainer(object):
     def setup_method(self, method):
         nn.clear_parameters()
 
-    @pytest.mark.parametrize('num_steps', [1, 2])
-    @pytest.mark.parametrize('unroll_steps', [1, 2, 3])
-    @pytest.mark.parametrize('burn_in_steps', [0, 1, 2])
-    @pytest.mark.parametrize('loss_integration', [LossIntegration.LAST_TIMESTEP_ONLY, LossIntegration.ALL_TIMESTEPS])
-    def test_with_non_rnn_model(self, num_steps, unroll_steps, burn_in_steps, loss_integration):
-        env_info = EnvironmentInfo.from_env(DummyDiscreteImg())
-
-        env_info = EnvironmentInfo.from_env(DummyDiscreteImg())
-
-        train_q = DQNQFunction('stub', n_action=env_info.action_dim)
-        target_q = train_q.deepcopy('stub2')
-        # Using DQN Q trainer as representative trainer
-        config = MT.q_value_trainers.DQNQTrainerConfig(unroll_steps=unroll_steps,
-                                                       burn_in_steps=burn_in_steps,
-                                                       num_steps=num_steps,
-                                                       loss_integration=loss_integration)
-        DQNQTrainer(train_functions=train_q, solvers={}, target_function=target_q, env_info=env_info, config=config)
-
-        # pass: If no ecror occurs
-
-    @pytest.mark.parametrize('num_steps', [1, 2])
-    @pytest.mark.parametrize('unroll_steps', [1, 2, 3])
-    @pytest.mark.parametrize('burn_in_steps', [0, 1, 2])
-    @pytest.mark.parametrize('loss_integration', [LossIntegration.LAST_TIMESTEP_ONLY, LossIntegration.ALL_TIMESTEPS])
-    def test_with_rnn_model(self, num_steps, unroll_steps, burn_in_steps, loss_integration):
-        env_info = EnvironmentInfo.from_env(DummyDiscreteImg())
-
-        env_info = EnvironmentInfo.from_env(DummyDiscreteImg())
-
-        train_q = DRQNQFunction('stub',  n_action=env_info.action_dim)
-        target_q = train_q.deepcopy('stub2')
-        # Using DQN Q trainer as representative trainer
-        config = MT.q_value_trainers.DQNQTrainerConfig(unroll_steps=unroll_steps,
-                                                       burn_in_steps=burn_in_steps,
-                                                       num_steps=num_steps,
-                                                       loss_integration=loss_integration)
-        DQNQTrainer(train_functions=train_q, solvers={}, target_function=target_q, env_info=env_info, config=config)
-
-        # pass: If no ecror occurs
-
-
-class MultiStepTrainerForTest(MT.q_value_trainers.multi_step_trainer.MultiStepTrainer):
-    def _update_model(self, models, solvers, batch, training_variables, **kwargs):
-        pass
-
-    def support_rnn(self):
-        return True
-
-    def _build_training_graph(self, models, training_variables):
-        pass
-
-    def _setup_training_variables(self, batch_size):
-        return TrainingVariables(batch_size)
-
-    @property
-    def loss_variables(self):
-        return {}
-
-
-class TestMultiStepTrainer(object):
-    def setup_method(self, method):
-        nn.clear_parameters()
-
-    @patch("nnabla_rl.model_trainers.q_value_trainers.multi_step_trainer.MultiStepTrainer.__abstractmethods__", set())
-    def test_n_step_setup_batch(self):
-        batch_size = 5
-        num_steps = 5
-        env_info = EnvironmentInfo.from_env(DummyDiscreteImg())
-
-        train_q = DQNQFunction('stub',  n_action=env_info.action_dim)
-        config = MT.q_value_trainers.multi_step_trainer.MultiStepTrainerConfig(num_steps=num_steps)
-        trainer = MultiStepTrainerForTest(models=train_q, solvers={}, env_info=env_info, config=config)
-
-        batch = _generate_batch(batch_size, num_steps, env_info)
-        assert len(batch) == num_steps
-
-        actual_n_step_batch = trainer._setup_batch(batch)
-
-        assert np.allclose(batch.s_current, actual_n_step_batch.s_current)
-        assert np.allclose(batch.a_current, actual_n_step_batch.a_current)
-        assert not np.allclose(batch.reward, actual_n_step_batch.reward)
-        assert not np.allclose(batch.gamma, actual_n_step_batch.gamma)
-        assert not np.allclose(batch.non_terminal, actual_n_step_batch.non_terminal)
-
-        last_batch = batch[len(batch) - 1]
-        assert np.allclose(last_batch.s_next, actual_n_step_batch.s_next)
-        assert np.allclose(batch.weight, actual_n_step_batch.weight)
-
-        expected_n_step_batch = self._expected_batch(batch, num_steps)
-        assert np.allclose(expected_n_step_batch.reward, actual_n_step_batch.reward)
-        assert np.allclose(expected_n_step_batch.gamma, actual_n_step_batch.gamma)
-        assert np.allclose(expected_n_step_batch.non_terminal, actual_n_step_batch.non_terminal)
-
-    @patch("nnabla_rl.model_trainers.q_value_trainers.multi_step_trainer.MultiStepTrainer.__abstractmethods__", set())
-    def test_rnn_n_step_setup_batch(self):
-        batch_size = 5
-        num_steps = 5
-        unroll_steps = 3
-        env_info = EnvironmentInfo.from_env(DummyDiscreteImg())
-
-        train_q = DRQNQFunction('stub',  n_action=env_info.action_dim)
-        config = MT.q_value_trainers.multi_step_trainer.MultiStepTrainerConfig(num_steps=num_steps,
-                                                                               unroll_steps=unroll_steps)
-        trainer = MultiStepTrainerForTest(models=train_q, solvers={}, env_info=env_info, config=config)
-
-        batch = _generate_batch(batch_size, num_steps + unroll_steps - 1, env_info)
-        assert len(batch) == num_steps + unroll_steps - 1
-
-        actual_n_step_batch = trainer._setup_batch(batch)
-        assert len(actual_n_step_batch) == unroll_steps
-
-        for i, actual_n_step_batch in enumerate(actual_n_step_batch):
-            expected_n_step_batch = self._expected_batch(batch[i], num_steps)
-            assert np.allclose(expected_n_step_batch.s_current, actual_n_step_batch.s_current)
-            assert np.allclose(expected_n_step_batch.a_current, actual_n_step_batch.a_current)
-            assert np.allclose(expected_n_step_batch.reward, actual_n_step_batch.reward)
-            assert np.allclose(expected_n_step_batch.gamma, actual_n_step_batch.gamma)
-            assert np.allclose(expected_n_step_batch.non_terminal, actual_n_step_batch.non_terminal)
-            assert np.allclose(expected_n_step_batch.s_next, actual_n_step_batch.s_next)
-            assert np.allclose(expected_n_step_batch.weight, actual_n_step_batch.weight)
-
-    @patch("nnabla_rl.model_trainers.q_value_trainers.multi_step_trainer.MultiStepTrainer.__abstractmethods__", set())
-    def test_rnn_with_burnin_n_step_setup_batch(self):
-        batch_size = 5
-        num_steps = 5
-        unroll_steps = 3
-        burn_in_steps = 2
-        env_info = EnvironmentInfo.from_env(DummyDiscreteImg())
-
-        train_q = DRQNQFunction('stub',  n_action=env_info.action_dim)
-        config = MT.q_value_trainers.multi_step_trainer.MultiStepTrainerConfig(num_steps=num_steps,
-                                                                               unroll_steps=unroll_steps,
-                                                                               burn_in_steps=burn_in_steps)
-        trainer = MultiStepTrainerForTest(models=train_q, solvers={}, env_info=env_info, config=config)
-
-        batch = _generate_batch(batch_size, num_steps + unroll_steps + burn_in_steps - 1, env_info)
-        assert len(batch) == num_steps + unroll_steps + burn_in_steps - 1
-
-        actual_n_step_batch = trainer._setup_batch(batch)
-        assert len(actual_n_step_batch) == unroll_steps + burn_in_steps
-
-        for i, actual_n_step_batch in enumerate(actual_n_step_batch):
-            expected_n_step_batch = self._expected_batch(batch[i], num_steps)
-            assert np.allclose(expected_n_step_batch.s_current, actual_n_step_batch.s_current)
-            assert np.allclose(expected_n_step_batch.a_current, actual_n_step_batch.a_current)
-            assert np.allclose(expected_n_step_batch.reward, actual_n_step_batch.reward)
-            assert np.allclose(expected_n_step_batch.gamma, actual_n_step_batch.gamma)
-            assert np.allclose(expected_n_step_batch.non_terminal, actual_n_step_batch.non_terminal)
-            assert np.allclose(expected_n_step_batch.s_next, actual_n_step_batch.s_next)
-            assert np.allclose(expected_n_step_batch.weight, actual_n_step_batch.weight)
-
-    def _expected_batch(self, training_batch: TrainingBatch, num_steps) -> TrainingBatch:
-        if num_steps == 1:
-            return training_batch
-        else:
-            n_step_non_terminal = np.copy(training_batch.non_terminal)
-            n_step_reward = np.copy(training_batch.reward)
-            n_step_gamma = np.copy(training_batch.gamma)
-            n_step_state = np.copy(training_batch.s_next)
-            next_batch = training_batch.next_step_batch
-
-            for _ in range(num_steps - 1):
-                # Do not add reward if previous state is terminal state
-                n_step_reward += next_batch.reward * n_step_non_terminal * n_step_gamma
-                n_step_non_terminal *= next_batch.non_terminal
-                n_step_gamma *= next_batch.gamma
-                n_step_state = next_batch.s_next
-
-                next_batch = next_batch.next_step_batch
-
-            return TrainingBatch(batch_size=training_batch.batch_size,
-                                 s_current=training_batch.s_current,
-                                 a_current=training_batch.a_current,
-                                 reward=n_step_reward,
-                                 gamma=n_step_gamma,
-                                 non_terminal=n_step_non_terminal,
-                                 s_next=n_step_state,
-                                 weight=training_batch.weight,
-                                 extra=training_batch.extra,
-                                 next_step_batch=None)
-
-
-def _generate_batch(batch_size, num_steps, env_info) -> TrainingBatch:
-    state_dim = env_info.state_dim
-    action_num = env_info.action_dim
-
-    head_batch: Optional[TrainingBatch] = None
-    tail_batch: Optional[TrainingBatch] = None
-    s_current = np.random.normal(size=(batch_size, state_dim))
-    for _ in range(num_steps):
-        a_current = np.random.randint(action_num, size=(batch_size, 1)).astype('float32')
-        reward = np.random.normal(size=(batch_size, 1))
-        gamma = 0.99
-        non_terminal = np.random.randint(2, size=(batch_size, 1)).astype('float32')
-        s_next = np.random.normal(size=(batch_size, state_dim))
-        weight = np.random.normal(size=(batch_size, 1))
-
-        batch = TrainingBatch(batch_size=batch_size,
-                              s_current=s_current,
-                              a_current=a_current,
-                              reward=reward,
-                              gamma=gamma,
-                              non_terminal=non_terminal,
-                              s_next=s_next,
-                              weight=weight)
-        if head_batch is None:
-            head_batch = batch
-        if tail_batch is None:
-            tail_batch = head_batch
-        else:
-            tail_batch.next_step_batch = batch
-            tail_batch = batch
-        s_current = s_next
-    assert head_batch is not None
-    return head_batch
-
 
 if __name__ == "__main__":
     pytest.main()
```

## tests/models/test_model.py

```diff
@@ -24,22 +24,21 @@
 
 class ModelMock(Model):
     def __init__(self, scope_name, input_dim, output_dim):
         super(ModelMock, self).__init__(scope_name)
         self._input_dim = input_dim
         self._output_dim = output_dim
 
-        self._hidden_sizes = [10, 20]
-
     def __call__(self, s):
         assert s.shape[-1] == self._input_dim
         with nn.parameter_scope(self.scope_name):
-            for i, hidden_size in enumerate(self._hidden_sizes):
-                h = NPF.affine(s, n_outmaps=hidden_size, name=f"linear{i+1}")
-                h = NF.relu(x=h)
+            h = NPF.affine(s, n_outmaps=10, name="linear1")
+            h = NF.relu(x=h)
+            h = NPF.affine(h, n_outmaps=20, name="linear2")
+            h = NF.relu(x=h)
             h = NPF.affine(h, n_outmaps=self._output_dim, name="linear3")
         return NF.tanh(h)
 
 
 class TestModel(object):
     def setup_method(self, method):
         nn.clear_parameters()
@@ -140,40 +139,14 @@
         new_scope_name = 'new'
         model.deepcopy(new_scope_name)
 
         # Can not create with same scope twice
         with pytest.raises(RuntimeError):
             model.deepcopy(new_scope_name)
 
-    def test_shallowcopy(self):
-        scope_name = 'src'
-        input_dim = 5
-        x = nn.Variable.from_numpy_array(np.empty(shape=(1, input_dim)))
-        model = self._create_model_from_input(scope_name=scope_name, x=x)
-
-        # Call once to create params
-        model(x)
-
-        copied = model.shallowcopy()
-        assert copied.scope_name == model.scope_name
-        assert len(copied.get_parameters()) == len(model.get_parameters())
-
-        # Check all attributies are deep copied
-        copied_values = copied.__dict__.values()
-        original_values = model.__dict__.values()
-        for copied_value, original_value in zip(copied_values, original_values):
-            assert isinstance(copied_value, original_value.__class__)
-            if type(copied_value) in (int, float, str, bool, tuple):
-                # immutable objects has same value
-                assert copied_value == original_value
-            else:
-                # mutable objects points different place but has same value
-                assert copied_value is not original_value
-                assert copied_value == original_value
-
     def _create_model_from_input(self, scope_name, x, output_dim=5):
         input_dim = x.shape[-1]
         return ModelMock(scope_name=scope_name, input_dim=input_dim, output_dim=output_dim)
 
 
 if __name__ == "__main__":
     pytest.main()
```

## tests/preprocessors/test_running_mean_normalizer.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023,2024 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -13,15 +13,14 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import numpy as np
 import pytest
 
 import nnabla as nn
-import nnabla.initializer as NI
 from nnabla_rl.preprocessors.running_mean_normalizer import RunningMeanNormalizer
 
 
 class TestRunningMeanNormalizer():
     def setup_method(self, method):
         nn.clear_parameters()
         np.random.seed(0)
@@ -66,57 +65,7 @@
         actual = (s_batch - mean) / np.sqrt(var + 1e-8)
 
         assert np.allclose(filtered_s_batch.d, actual, atol=1e-4)
 
     def test_invalid_value_clip(self):
         with pytest.raises(ValueError):
             RunningMeanNormalizer("test", (1, 1), value_clip=[5., -5.])
-
-    def test_numpy_initializer(self):
-        shape = (6, )
-        mean_initializer = np.random.rand(6)
-        var_initializer = np.random.rand(6)
-        normalizer = RunningMeanNormalizer(scope_name="test", shape=shape, epsilon=0.0,
-                                           mean_initializer=mean_initializer, var_initializer=var_initializer)
-
-        # dummy process
-        output = normalizer.process(nn.Variable.from_numpy_array(np.random.rand(1, 6)))
-        output.forward()
-
-        actual_params = normalizer.get_parameters()
-        assert np.allclose(actual_params["mean"].d, mean_initializer[np.newaxis, :])
-        assert np.allclose(actual_params["var"].d, var_initializer[np.newaxis, :])
-        # count should be default initial value
-        assert np.allclose(actual_params["count"].d, np.ones((1, 1)) * 1e-4)
-
-    def test_nnabla_initializer(self):
-        shape = (6, )
-        mean_initializer = NI.ConstantInitializer(5.0)
-        var_initializer = NI.ConstantInitializer(6.0)
-        normalizer = RunningMeanNormalizer(scope_name="test", shape=shape, epsilon=0.0,
-                                           mean_initializer=mean_initializer, var_initializer=var_initializer)
-
-        # dummy process
-        output = normalizer.process(nn.Variable.from_numpy_array(np.random.rand(1, 6)))
-        output.forward()
-
-        actual_params = normalizer.get_parameters()
-        assert np.allclose(actual_params["mean"].d, np.ones((1, 6)) * 5.0)
-        assert np.allclose(actual_params["var"].d, np.ones((1, 6)) * 6.0)
-        # count should be default initial value
-        assert np.allclose(actual_params["count"].d, np.ones((1, 1)) * 1e-4)
-
-    def test_numpy_initializer_with_invalid_mean_initializer_shape(self):
-        shape = (6, )
-        mean_initializer = np.random.rand(4)
-        var_initializer = np.random.rand(6)
-        with pytest.raises(AssertionError):
-            RunningMeanNormalizer(scope_name="test", shape=shape, epsilon=0.0,
-                                  mean_initializer=mean_initializer, var_initializer=var_initializer)
-
-    def test_numpy_initializer_with_invalid_var_initializer_shape(self):
-        shape = (6, )
-        mean_initializer = np.random.rand(6)
-        var_initializer = np.random.rand(4)
-        with pytest.raises(AssertionError):
-            RunningMeanNormalizer(scope_name="test", shape=shape, epsilon=0.0,
-                                  mean_initializer=mean_initializer, var_initializer=var_initializer)
```

## tests/replay_buffers/test_memory_efficient_atari_buffer.py

```diff
@@ -1,71 +1,49 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-
 import numpy as np
 import pytest
 
 from nnabla_rl.environments.dummy import DummyAtariEnv
 from nnabla_rl.environments.wrappers.atari import MaxAndSkipEnv, NoopResetEnv
-from nnabla_rl.replay_buffers.memory_efficient_atari_buffer import (MemoryEfficientAtariBuffer,
-                                                                    MemoryEfficientAtariTrajectoryBuffer,
-                                                                    ProportionalPrioritizedAtariBuffer,
-                                                                    RankBasedPrioritizedAtariBuffer)
+from nnabla_rl.replay_buffers.memory_efficient_atari_buffer import MemoryEfficientAtariBuffer
 from nnabla_rl.utils.reproductions import build_atari_env
 
 
 class TestMemoryEfficientAtariBuffer(object):
     def test_append_float(self):
-        experience = _generate_atari_experience_mock()[0]
+        experience = self._generate_atari_experience_mock()[0]
 
         capacity = 10
         buffer = MemoryEfficientAtariBuffer(capacity=capacity)
         buffer.append(experience)
 
         s, _, _, _, s_next, *_ = buffer._buffer[len(buffer._buffer)-1]
         assert s.dtype == np.uint8
         assert s_next.dtype == np.uint8
         assert np.alltrue(
             (experience[0][-1] * 255.0).astype(np.uint8) == s)
         assert np.alltrue(
             (experience[4][-1] * 255.0).astype(np.uint8) == s_next)
 
-    def test_unstacked_frame(self):
-        experiences = _generate_atari_experience_mock(num_mocks=10, frame_stack=False)
-
-        capacity = 10
-        buffer = MemoryEfficientAtariBuffer(capacity=capacity, stacked_frames=1)
-
-        for experience in experiences:
-            buffer.append(experience)
-
-        for i, experience in enumerate(experiences):
-            s, _, _, _, s_next, *_ = buffer.__getitem__(i)
-            assert s.shape[0] == 1
-            assert s_next.shape[0] == 1
-            assert s.dtype == np.float32
-            assert s_next.dtype == np.float32
-            assert np.allclose(experience[0], s, atol=1e-2)
-            assert np.allclose(experience[4], s_next, atol=1e-2)
-
     def test_getitem(self):
-        experiences = _generate_atari_experience_mock(num_mocks=10)
+        experiences = self._generate_atari_experience_mock(num_mocks=10)
 
         capacity = 10
         buffer = MemoryEfficientAtariBuffer(capacity=capacity)
 
         for experience in experiences:
             buffer.append(experience)
 
@@ -74,15 +52,15 @@
             assert s.dtype == np.float32
             assert s_next.dtype == np.float32
             assert np.allclose(experience[0], s, atol=1e-2)
             assert np.allclose(experience[4], s_next, atol=1e-2)
 
     def test_full_buffer_getitem(self):
         capacity = 10
-        experiences = _generate_atari_experience_mock(
+        experiences = self._generate_atari_experience_mock(
             num_mocks=(capacity + 5))
         buffer = MemoryEfficientAtariBuffer(capacity=capacity)
 
         for i in range(capacity):
             buffer.append(experiences[i])
         assert len(buffer) == capacity
 
@@ -106,354 +84,33 @@
             assert np.allclose(experience[0], s, atol=1e-2)
             assert np.allclose(experience[4], s_next, atol=1e-2)
 
     def test_buffer_len(self):
         capacity = 10
         buffer = MemoryEfficientAtariBuffer(capacity=capacity)
         for _ in range(10):
-            experience = _generate_atari_experience_mock()[0]
+            experience = self._generate_atari_experience_mock()[0]
             buffer.append(experience)
 
         assert len(buffer) == 10
 
-
-class TestMemoryEfficientAtariTrajectoryBuffer(object):
-    def test_sample_trajectory(self):
-        trajectory1 = _generate_atari_experience_mock(num_mocks=5)
-        trajectory2 = _generate_atari_experience_mock(num_mocks=5)
-        trajectory3 = _generate_atari_experience_mock(num_mocks=10)
-
-        capacity = 10
-        buffer = MemoryEfficientAtariTrajectoryBuffer(num_trajectories=capacity)
-        buffer.append_trajectory(trajectory1)
-        buffer.append_trajectory(trajectory2)
-        buffer.append_trajectory(trajectory3)
-
-        trajectories, *_ = buffer.sample_trajectories(num_samples=2)
-        assert len(trajectories) == 2
-
-    def test_sample_trajectories_portion(self):
-        trajectory1 = _generate_atari_experience_mock(num_mocks=5)
-        trajectory2 = _generate_atari_experience_mock(num_mocks=5)
-        trajectory3 = _generate_atari_experience_mock(num_mocks=10)
-
-        capacity = 10
-        buffer = MemoryEfficientAtariTrajectoryBuffer(num_trajectories=capacity)
-        buffer.append_trajectory(trajectory1)
-        buffer.append_trajectory(trajectory2)
-        buffer.append_trajectory(trajectory3)
-
-        num_samples = 2
-        portion_length = 5
-        trajectories, *_ = buffer.sample_trajectories_portion(num_samples=num_samples, portion_length=portion_length)
-        assert len(trajectories) == num_samples
-        assert all([len(trajectory) == portion_length for trajectory in trajectories])
-
-    def test_append_trajectory(self):
-        trajectory1 = _generate_atari_experience_mock(num_mocks=5)
-        trajectory2 = _generate_atari_experience_mock(num_mocks=5)
-
-        capacity = 10
-        buffer = MemoryEfficientAtariTrajectoryBuffer(num_trajectories=capacity)
-        buffer.append_trajectory(trajectory1)
-        buffer.append_trajectory(trajectory2)
-
-        saved_trajectory1 = buffer.get_trajectory(0)
-        saved_trajectory2 = buffer.get_trajectory(1)
-
-        self._assert_same_trajectory(trajectory1, saved_trajectory1)
-        self._assert_same_trajectory(trajectory2, saved_trajectory2)
-
-    def test_append_compressed_trajectory(self):
-        trajectory1 = _generate_atari_experience_mock(num_mocks=5)
-        trajectory2 = _generate_atari_experience_mock(num_mocks=5)
-
-        capacity = 10
-        buffer = MemoryEfficientAtariTrajectoryBuffer(num_trajectories=capacity)
-        buffer.append_trajectory(self._compress_trajectory(trajectory1))
-        buffer.append_trajectory(self._compress_trajectory(trajectory2))
-
-        saved_trajectory1 = buffer.get_trajectory(0)
-        saved_trajectory2 = buffer.get_trajectory(1)
-
-        self._assert_same_trajectory(trajectory1, saved_trajectory1)
-        self._assert_same_trajectory(trajectory2, saved_trajectory2)
-
-    def test_append_compressed_uint8_trajectory(self):
-        trajectory1 = _generate_atari_experience_mock(num_mocks=5)
-        trajectory2 = _generate_atari_experience_mock(num_mocks=5)
-
-        capacity = 10
-        buffer = MemoryEfficientAtariTrajectoryBuffer(num_trajectories=capacity)
-        buffer.append_trajectory(self._compress_trajectory(trajectory1, to_uint8=True))
-        buffer.append_trajectory(self._compress_trajectory(trajectory2, to_uint8=True))
-
-        saved_trajectory1 = buffer.get_trajectory(0)
-        saved_trajectory2 = buffer.get_trajectory(1)
-
-        self._assert_same_trajectory(trajectory1, saved_trajectory1)
-        self._assert_same_trajectory(trajectory2, saved_trajectory2)
-
-    def _compress_trajectory(self, trajectory, to_uint8=False):
-        def uint8fy(state):
-            return np.asarray(state * 255.0, dtype=np.uint8)
-
-        if to_uint8:
-            return [(uint8fy(s[-1]), a, r, t, uint8fy(s_next[-1]), *_) for (s, a, r, t, s_next, *_) in trajectory]
-        else:
-            return [(s[-1], a, r, t, s_next[-1], *_) for (s, a, r, t, s_next, *_) in trajectory]
-
-    def _assert_same_trajectory(self, expected, actual):
-        for expected_experience, actual_experience in zip(expected, actual):
-            (e_s, e_a, e_r, e_t, e_s_next, *_) = expected_experience
-            (a_s, a_a, a_r, a_t, a_s_next, *_) = actual_experience
-
-            assert np.allclose(e_s, a_s)
-            assert np.allclose(e_a, a_a)
-            assert np.allclose(e_r, a_r)
-            assert np.allclose(e_t, a_t)
-            assert np.allclose(e_s_next, a_s_next)
-
-
-class TestProportionalPrioritizedAtariBuffer(object):
-    def test_append_float(self):
-        experience = _generate_atari_experience_mock()[0]
-
-        capacity = 10
-        buffer = ProportionalPrioritizedAtariBuffer(capacity=capacity)
-        buffer.append(experience)
-
-        s, _, _, _, s_next, *_ = buffer._buffer[len(buffer._buffer)-1]
-        assert s.dtype == np.uint8
-        assert s_next.dtype == np.uint8
-        assert np.alltrue(
-            (experience[0][-1] * 255.0).astype(np.uint8) == s)
-        assert np.alltrue(
-            (experience[4][-1] * 255.0).astype(np.uint8) == s_next)
-
-    def test_unstacked_frame(self):
-        experiences = _generate_atari_experience_mock(num_mocks=10, frame_stack=False)
-
-        capacity = 10
-        buffer = ProportionalPrioritizedAtariBuffer(capacity=capacity, stacked_frames=1)
-
-        for experience in experiences:
-            buffer.append(experience)
-
-        for i, experience in enumerate(experiences):
-            s, _, _, _, s_next, *_ = buffer.__getitem__(i)
-            assert s.shape[0] == 1
-            assert s_next.shape[0] == 1
-            assert s.dtype == np.float32
-            assert s_next.dtype == np.float32
-            assert np.allclose(experience[0], s, atol=1e-2)
-            assert np.allclose(experience[4], s_next, atol=1e-2)
-
-    def test_getitem(self):
-        experiences = _generate_atari_experience_mock(num_mocks=10)
-
-        capacity = 10
-        buffer = ProportionalPrioritizedAtariBuffer(capacity=capacity)
-
-        for experience in experiences:
-            buffer.append(experience)
-
-        for i, experience in enumerate(experiences):
-            s, _, _, _, s_next, *_ = buffer.__getitem__(i)
-            assert s.dtype == np.float32
-            assert s_next.dtype == np.float32
-            assert np.allclose(experience[0], s, atol=1e-2)
-            assert np.allclose(experience[4], s_next, atol=1e-2)
-
-    def test_full_buffer_getitem(self):
-        capacity = 10
-        experiences = _generate_atari_experience_mock(
-            num_mocks=(capacity + 5))
-        buffer = ProportionalPrioritizedAtariBuffer(capacity=capacity)
-
-        for i in range(capacity):
-            buffer.append(experiences[i])
-        assert len(buffer) == capacity
-
-        for i in range(capacity):
-            s, _, _, _, s_next, *_ = buffer[i]
-            experience = experiences[i]
-            assert s.dtype == np.float32
-            assert s_next.dtype == np.float32
-            assert np.allclose(experience[0], s, atol=1e-2)
-            assert np.allclose(experience[4], s_next, atol=1e-2)
-
-        for i in range(5):
-            buffer.append(experiences[i + capacity])
-        assert len(buffer) == capacity
-
-        for i in range(capacity):
-            s, _, _, _, s_next, *_ = buffer[i]
-            experience = experiences[i + 5]
-            assert s.dtype == np.float32
-            assert s_next.dtype == np.float32
-            assert np.allclose(experience[0], s, atol=1e-2)
-            assert np.allclose(experience[4], s_next, atol=1e-2)
-
-    def test_buffer_len(self):
-        capacity = 10
-        buffer = ProportionalPrioritizedAtariBuffer(capacity=capacity)
-        for _ in range(10):
-            experience = _generate_atari_experience_mock()[0]
-            buffer.append(experience)
-
-        assert len(buffer) == 10
-
-    def test_sample_without_update(self):
-        beta = 0.5
-        capacity = 10
-        buffer = ProportionalPrioritizedAtariBuffer(capacity=capacity, beta=beta)
-        for _ in range(5):
-            experience = _generate_atari_experience_mock()[0]
-            buffer.append(experience)
-
-        indices = [1, 3, 2]
-        _, weights = buffer.sample_indices(indices)
-
-        # update the priority and check that following sampling succeeds
-        errors = np.random.sample([len(weights), 1])
-        buffer.update_priorities(errors)
-
-        _, _ = buffer.sample_indices(indices)
-
-        # sample without priority update
-        with pytest.raises(RuntimeError):
-            buffer.sample_indices(indices)
-
-
-class TestRankBasedPrioritizedAtariBuffer(object):
-    def test_append_float(self):
-        experience = _generate_atari_experience_mock()[0]
-
-        capacity = 10
-        buffer = RankBasedPrioritizedAtariBuffer(capacity=capacity)
-        buffer.append(experience)
-
-        s, _, _, _, s_next, *_ = buffer._buffer[len(buffer._buffer)-1]
-        assert s.dtype == np.uint8
-        assert s_next.dtype == np.uint8
-        assert np.alltrue(
-            (experience[0][-1] * 255.0).astype(np.uint8) == s)
-        assert np.alltrue(
-            (experience[4][-1] * 255.0).astype(np.uint8) == s_next)
-
-    def test_unstacked_frame(self):
-        experiences = _generate_atari_experience_mock(num_mocks=10, frame_stack=False)
-
-        capacity = 10
-        buffer = RankBasedPrioritizedAtariBuffer(capacity=capacity, stacked_frames=1)
-
-        for experience in experiences:
-            buffer.append(experience)
-
-        for i, experience in enumerate(experiences):
-            s, _, _, _, s_next, *_ = buffer.__getitem__(i)
-            assert s.shape[0] == 1
-            assert s_next.shape[0] == 1
-            assert s.dtype == np.float32
-            assert s_next.dtype == np.float32
-            assert np.allclose(experience[0], s, atol=1e-2)
-            assert np.allclose(experience[4], s_next, atol=1e-2)
-
-    def test_getitem(self):
-        experiences = _generate_atari_experience_mock(num_mocks=10)
-
-        capacity = 10
-        buffer = RankBasedPrioritizedAtariBuffer(capacity=capacity)
-
-        for experience in experiences:
-            buffer.append(experience)
-
-        for i, experience in enumerate(experiences):
-            s, _, _, _, s_next, *_ = buffer.__getitem__(i)
-            assert s.dtype == np.float32
-            assert s_next.dtype == np.float32
-            assert np.allclose(experience[0], s, atol=1e-2)
-            assert np.allclose(experience[4], s_next, atol=1e-2)
-
-    def test_full_buffer_getitem(self):
-        capacity = 10
-        experiences = _generate_atari_experience_mock(
-            num_mocks=(capacity + 5))
-        buffer = RankBasedPrioritizedAtariBuffer(capacity=capacity)
-
-        for i in range(capacity):
-            buffer.append(experiences[i])
-        assert len(buffer) == capacity
-
-        for i in range(capacity):
-            s, _, _, _, s_next, *_ = buffer[i]
-            experience = experiences[i]
-            assert s.dtype == np.float32
-            assert s_next.dtype == np.float32
-            assert np.allclose(experience[0], s, atol=1e-2)
-            assert np.allclose(experience[4], s_next, atol=1e-2)
-
-        for i in range(5):
-            buffer.append(experiences[i + capacity])
-        assert len(buffer) == capacity
-
-        for i in range(capacity):
-            s, _, _, _, s_next, *_ = buffer[i]
-            experience = experiences[i + 5]
-            assert s.dtype == np.float32
-            assert s_next.dtype == np.float32
-            assert np.allclose(experience[0], s, atol=1e-2)
-            assert np.allclose(experience[4], s_next, atol=1e-2)
-
-    def test_buffer_len(self):
-        capacity = 10
-        buffer = RankBasedPrioritizedAtariBuffer(capacity=capacity)
-        for _ in range(10):
-            experience = _generate_atari_experience_mock()[0]
-            buffer.append(experience)
-
-        assert len(buffer) == 10
-
-    def test_sample_without_update(self):
-        beta = 0.5
-        capacity = 10
-        buffer = RankBasedPrioritizedAtariBuffer(capacity=capacity, beta=beta)
-        for _ in range(5):
-            experience = _generate_atari_experience_mock()[0]
-            buffer.append(experience)
-
-        indices = [1, 3, 2]
-        _, weights = buffer.sample_indices(indices)
-
-        # update the priority and check that following sampling succeeds
-        errors = np.random.sample([len(weights), 1])
-        buffer.update_priorities(errors)
-
-        _, _ = buffer.sample_indices(indices)
-
-        # sample without priority update
-        with pytest.raises(RuntimeError):
-            buffer.sample_indices(indices)
-
-
-def _generate_atari_experience_mock(low=0.0, high=1.0, num_mocks=1, frame_stack=True):
-    env = DummyAtariEnv()
-    env = NoopResetEnv(env)
-    env = MaxAndSkipEnv(env)
-    env = build_atari_env(env, test=True, print_info=False, frame_stack=frame_stack)
-    experiences = []
-    state = env.reset()
-    for _ in range(num_mocks):
-        action = env.action_space.sample()
-        s_next, reward, done, info = env.step(action)
-        experience = (state, action, reward, 1.0 - done, s_next, info)
-        experiences.append(experience)
-        if done:
-            state = env.reset()
-        else:
-            state = s_next
-    return experiences
+    def _generate_atari_experience_mock(self, low=0.0, high=1.0, num_mocks=1):
+        env = DummyAtariEnv()
+        env = NoopResetEnv(env)
+        env = MaxAndSkipEnv(env)
+        env = build_atari_env(env, test=True)
+        experiences = []
+        state = env.reset()
+        for _ in range(num_mocks):
+            action = env.action_space.sample()
+            s_next, reward, done, _ = env.step(action)
+            experience = (state, action, reward, 1.0 - done, s_next)
+            experiences.append(experience)
+            if done:
+                state = env.reset()
+            else:
+                state = s_next
+        return experiences
 
 
 if __name__ == "__main__":
     pytest.main()
```

## tests/replay_buffers/test_prioritized_replay_buffer.py

```diff
@@ -9,750 +9,297 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-
-import math
-
 import numpy as np
 import pytest
 
-from nnabla_rl.replay_buffers.prioritized_replay_buffer import (MaxHeap, MaxHeapDataHolder, MinTree,
-                                                                ProportionalPrioritizedReplayBuffer,
-                                                                RankBasedPrioritizedReplayBuffer, SumTree,
-                                                                SumTreeDataHolder)
-
-
-class TestMinTree(object):
-    def setup_method(self, method):
-        np.random.seed(0)
-
-    def test_finite_capacity(self):
-        requested_capacity = 100
-        buffer = MinTree(capacity=requested_capacity)
-        assert buffer._capacity == requested_capacity
-
-    def test_append_with_capacity(self):
-        capacity = 50
-        append_num = 100
-        min_tree = MinTree(capacity=capacity)
-        for i in range(append_num):
-            min_tree.append(i)
-            if i < capacity:
-                assert len(min_tree) == i + 1
-                assert min_tree.min() == 0
-            else:
-                assert len(min_tree) == capacity
-                assert min_tree.min() == i - capacity + 1
-        assert len(min_tree) == capacity
-
-    def test_update(self):
-        capacity = 50
-        min_tree = MinTree(capacity=capacity)
-        priorities = np.random.randn(capacity)
-        priorities = list(sorted(priorities))
-        min_priority = math.inf
-        for priority in priorities:
-            min_priority = min(min_priority, priority)
-            min_tree.append(priority)
-            assert min_tree.min() == min_priority
-
-        for i in range(capacity - 1):
-            min_tree.update(i, math.inf)
-            assert min_tree.min() == priorities[i + 1]
+from nnabla_rl.replay_buffers.prioritized_replay_buffer import PrioritizedReplayBuffer, SumTree
 
 
 class TestSumTree(object):
     def setup_method(self, method):
         np.random.seed(0)
 
     def test_finite_capacity(self):
         requested_capacity = 100
         buffer = SumTree(capacity=requested_capacity)
         assert buffer._capacity == requested_capacity
 
     def test_append_with_capacity(self):
         capacity = 50
         append_num = 100
-        sum_tree = SumTree(capacity=capacity)
-        expected_sum = 0
-        for i in range(append_num):
-            sum_tree.append(i)
-            if i < capacity:
-                assert len(sum_tree) == i + 1
-            else:
-                expected_sum += i
-                assert len(sum_tree) == capacity
-        assert len(sum_tree) == capacity
-        assert sum_tree.sum() == expected_sum
-
-    def test_update(self):
-        capacity = 50
-        sum_tree = SumTree(capacity=capacity)
-
-        priorities = np.random.randn(capacity)
-        priorities = list(sorted(priorities))
-        sum = 0.0
-        for priority in priorities:
-            sum += priority
-            sum_tree.append(priority)
-            np.testing.assert_almost_equal(sum_tree.sum(), sum)
-
-        for i in range(capacity - 1):
-            tree_index = sum_tree.absolute_to_tree_index(i)
-            current_priority = sum_tree[tree_index]
-            new_priority = np.random.randn()
-            sum_tree.update(i, new_priority)
-            sum += new_priority - current_priority
-            np.testing.assert_almost_equal(sum_tree.sum(), sum)
-
-
-class TestMaxHeap(object):
-    def setup_method(self, method):
-        np.random.seed(0)
-
-    def test_finite_capacity(self):
-        requested_capacity = 100
-        buffer = MaxHeap(capacity=requested_capacity)
-        assert buffer._capacity == requested_capacity
-
-    def test_append_more_than_capacity(self):
-        capacity = 10
-        max_heap = MaxHeap(capacity=capacity)
-        # Fill heap
-        for i in range(capacity):
-            max_heap.append(i)
-            assert max_heap[0][1] == i
-
-        # Fill heap
-        for i in range(5):
-            max_heap.append(i + len(max_heap))
-            # Check appended data is on the top
-            assert max_heap[0][1] == i + len(max_heap)
-
-    def test_update_priorities(self):
-        capacity = 10
-        max_heap = MaxHeap(capacity=capacity)
-        # create positive random priorities to easily manipulate the top priority of heap by inverting the sign
-        priorities = np.abs(np.random.randn(capacity))
-        max_priority = -math.inf
-        for priority in priorities:
-            max_priority = max(max_priority, priority)
-            max_heap.append(priority)
-            assert max_heap[0][1] == max_priority
-        sorted_priority = list(reversed(sorted(priorities)))
-
-        # update data priorities
-        for i in range(capacity - 1):
-            current_priority = max_heap[0][1]
-            # All priorities are positive -> this will be removed from the top
-            new_priority = -current_priority
-            absolute_index = max_heap.heap_to_absolute_index(0)
-            max_heap.update(absolute_index, new_priority)
-            assert max_heap[0][1] == sorted_priority[i + 1]
-
-    def test_sort_data(self):
-        capacity = 10
-        max_heap = MaxHeap(capacity=capacity)
-        priorities = np.random.randn(capacity)
-        max_priority = -math.inf
-        for priority in priorities:
-            max_priority = max(max_priority, priority)
-            max_heap.append(priority)
-            assert max_heap[0][1] == max_priority
-
-        # should not be ideally sorted
-        sorted_order = list(reversed(sorted(priorities)))
-        actual_order = [max_heap._heap[i][1] for i in range(capacity)]
-        assert not np.allclose(actual_order, sorted_order)
-
-        max_heap.sort_data()
-        actual_order = [max_heap._heap[i][1] for i in range(capacity)]
-        assert np.allclose(actual_order, sorted_order)
-
-
-class TestSumTreeDataHolder(object):
-    def setup_method(self, method):
-        np.random.seed(0)
-
-    def test_get_priority(self):
-        capacity = 10
-        initial_max_priority = 1.0
-        holder = SumTreeDataHolder(capacity=capacity, initial_max_priority=initial_max_priority)
-
-        indices = range(10)
-        for i in indices:
-            # save index as data
-            holder.append(i)
-
-        for i in indices:
-            actual_priority = holder.get_priority(i)
-            # Initial priority is initial_max_priority
-            assert initial_max_priority == actual_priority
-
-        priorities = [np.random.uniform() for i in range(10)]
-        for index, priority in zip(indices, priorities):
-            holder.update_priority(index, priority)
-
-        for index, expected_priority in zip(indices, priorities):
-            actual_priority = holder.get_priority(index)
-            # Initial priority is initial_max_priority
-            assert expected_priority == actual_priority
-
-    def test_sum_priority(self):
-        capacity = 10
-        initial_max_priority = 1.0
-        holder = SumTreeDataHolder(capacity=capacity, initial_max_priority=initial_max_priority)
-
-        indices = range(10)
-        for i in indices:
-            # save index as data
-            holder.append(i)
-
-        np.testing.assert_almost_equal(holder.sum_priority(), initial_max_priority * len(indices))
-
-        priorities = [np.random.uniform() for i in range(10)]
-        for index, priority in zip(indices, priorities):
-            holder.update_priority(index, priority)
-
-        np.testing.assert_almost_equal(holder.sum_priority(), np.sum(priorities))
-
-    def test_min_priority(self):
-        capacity = 10
-        initial_max_priority = 1.0
-        holder = SumTreeDataHolder(capacity=capacity, initial_max_priority=initial_max_priority)
-
-        indices = range(10)
-        for i in indices:
-            # save index as data
-            holder.append(i)
-
-        np.testing.assert_almost_equal(holder.min_priority(), initial_max_priority)
-
-        priorities = [np.random.uniform() for i in range(10)]
-        for index, priority in zip(indices, priorities):
-            holder.update_priority(index, priority)
-
-        np.testing.assert_almost_equal(holder.min_priority(), np.min(priorities))
-
-
-class TestMaxHeapDataHolder(object):
-    def setup_method(self, method):
-        np.random.seed(0)
-
-    @pytest.mark.parametrize("alpha", [np.random.random() for _ in range(1, 5)])
-    def test_compute_priority(self, alpha):
-        capacity = 10
-        holder = MaxHeapDataHolder(capacity=capacity, alpha=alpha)
-
-        for rank in range(1, 10):
-            expected = (1 / rank) ** alpha
-            actual = holder._compute_priority(rank)
-            assert np.allclose(expected, actual)
-
-
-class TestProportionalPrioritizedReplayBuffer(object):
-    def setup_method(self, method):
-        np.random.seed(0)
-
-    def test_finite_capacity(self):
-        requested_capacity = 100
-        buffer = ProportionalPrioritizedReplayBuffer(capacity=requested_capacity)
-        assert buffer.capacity == requested_capacity
-
-    def test_infine_capacity(self):
-        with pytest.raises(ValueError):
-            _ = ProportionalPrioritizedReplayBuffer(capacity=None)
-
-    def test_non_positive_capacity(self):
-        with pytest.raises(ValueError):
-            _ = ProportionalPrioritizedReplayBuffer(capacity=-1)
-        with pytest.raises(ValueError):
-            _ = ProportionalPrioritizedReplayBuffer(capacity=0)
-
-    def test_append_with_capacity(self):
-        capacity = 5
-        append_num = 10
-        buffer = ProportionalPrioritizedReplayBuffer(capacity=capacity)
+        init_max_p = 1.0
+        buffer = SumTree(capacity=capacity, init_max_p=init_max_p)
         for i in range(append_num):
-            experience = _generate_experience_mock()
+            experience = self._generate_experience_mock()
             buffer.append(experience)
             if i < capacity:
                 assert len(buffer) == i + 1
             else:
                 assert len(buffer) == capacity
         assert len(buffer) == capacity
+        assert buffer._min_p == init_max_p
+        assert buffer.total == init_max_p * capacity
 
-    @pytest.mark.parametrize("beta", [np.random.uniform(low=0.0, high=1.0) for _ in range(1, 5)])
-    @pytest.mark.parametrize("betasteps", [i for i in range(1, 5)])
-    def test_betasteps(self, beta, betasteps):
-        buffer = self._generate_buffer_with_experiences(experience_num=100, beta=beta, betasteps=betasteps)
-        _, _ = buffer.sample()
-        new_beta = beta + (1.0 - beta) / betasteps
-        np.testing.assert_almost_equal(buffer._beta, new_beta)
-
-    @pytest.mark.parametrize("beta", [np.random.uniform(low=0.0, high=1.0) for _ in range(1, 10)])
-    @pytest.mark.parametrize("normalization_method", ["batch_max", "buffer_max"])
-    def test_sample_one_experience(self, beta, normalization_method):
-        buffer = self._generate_buffer_with_experiences(
-            experience_num=100, beta=beta, normalization_method=normalization_method)
-        experiences, info = buffer.sample()
-        indices = buffer._last_sampled_indices
-        assert len(experiences) == 1
-        assert "weights" in info
-        assert len(info["weights"]) == 1
-        assert len(indices) == 1
-        weights = info["weights"]
-        for index, experience, actual_weight in zip(indices, experiences, weights):
-            expected_weight = self._compute_weight(buffer, index, alpha=buffer._alpha, beta=beta)
-            actual_weight = actual_weight[0]
-            assert experience == buffer[index]
-            np.testing.assert_almost_equal(expected_weight, actual_weight)
-
-    @pytest.mark.parametrize("beta", [np.random.uniform(low=0.0, high=1.0) for _ in range(1, 10)])
-    @pytest.mark.parametrize("normalization_method", ["batch_max", "buffer_max"])
-    def test_sample_multiple_experiences(self, beta, normalization_method):
-        buffer = self._generate_buffer_with_experiences(
-            experience_num=100, beta=beta, normalization_method=normalization_method)
-        num_samples = 10
-        experiences, info = buffer.sample(num_samples=num_samples)
-        indices = buffer._last_sampled_indices
-        assert len(experiences) == num_samples
-        assert "weights" in info
-        assert len(info["weights"]) == num_samples
-        assert len(indices) == num_samples
-        weights = info["weights"]
-        for index, experience, actual_weight in zip(indices, experiences, weights):
-            expected_weight = self._compute_weight(buffer, index, alpha=buffer._alpha, beta=beta)
-            actual_weight = actual_weight[0]
-            assert experience == buffer[index]
-            np.testing.assert_almost_equal(expected_weight, actual_weight)
-
-    @pytest.mark.parametrize("beta", [np.random.uniform(low=0.0, high=1.0) for _ in range(1, 10)])
-    @pytest.mark.parametrize("num_steps", range(1, 5))
-    @pytest.mark.parametrize("normalization_method", ["batch_max", "buffer_max"])
-    def test_sample_multiple_step_experience(self, beta, num_steps, normalization_method):
-        buffer = self._generate_buffer_with_experiences(
-            experience_num=100, beta=beta, normalization_method=normalization_method)
-        experiences_tuple, info = buffer.sample(num_steps=num_steps)
-        if num_steps == 1:
-            experiences_tuple = tuple([experiences_tuple, ])
-        indices = buffer._last_sampled_indices
-        assert len(experiences_tuple) == num_steps
-        assert "weights" in info
-        assert len(info["weights"]) == 1
-        assert len(indices) == 1
-        weights = info["weights"]
-        for i, experiences in enumerate(experiences_tuple):
-            for index, experience, actual_weight in zip(indices, experiences, weights):
-                expected_weight = self._compute_weight(buffer, index, alpha=buffer._alpha, beta=beta)
-                actual_weight = actual_weight[0]
-                assert experience == buffer[index + i]
-                np.testing.assert_almost_equal(expected_weight, actual_weight)
-
-    def test_sample_from_insufficient_size_buffer(self):
-        buffer = self._generate_buffer_with_experiences(experience_num=10)
-        with pytest.raises(ValueError):
-            buffer.sample(num_samples=100)
-
-    @pytest.mark.parametrize("beta", [np.random.uniform(low=0.0, high=1.0) for _ in range(1, 10)])
-    @pytest.mark.parametrize("normalization_method", ["batch_max", "buffer_max"])
-    def test_sample_indices(self, beta, normalization_method):
-        buffer = self._generate_buffer_with_experiences(
-            experience_num=100, beta=beta, normalization_method=normalization_method)
+    @pytest.mark.parametrize(
+        "index, priority",
+        [(np.random.randint(100), np.random.uniform(low=0.1, high=5.0))
+         for _ in range(1, 10)])
+    def test_update(self, index, priority):
+        buffer = self._generate_buffer_with_experiences(experience_num=100)
+        assert buffer.total == 100.0
+        tree_index = index + buffer._capacity - 1
+        prev_priority = buffer._tree[tree_index].value
+        buffer.update(index=index, p=priority)
+        assert buffer._tree[tree_index].value == priority
+        assert buffer.total == 100.0 + (priority - prev_priority)
+
+    @pytest.mark.parametrize(
+        "beta, priority",
+        [(np.random.randint(low=0.0, high=1.0),
+          np.random.uniform(low=0.1, high=5.0))
+         for _ in range(1, 10)])
+    def test_weights_from_priorities(self, beta, priority):
+        buffer = self._generate_buffer_with_experiences(experience_num=100)
+        min_p = buffer._min_p
+        weights = buffer._weights_from_priorities(
+            priorities=np.array([priority]),
+            beta=beta)
+        assert weights == ((priority / min_p) ** (-beta))
+
+    @pytest.mark.parametrize(
+        "beta",
+        [np.random.randint(low=0.0, high=1.0) for _ in range(1, 10)])
+    def test_sample_one_experience(self, beta):
+        buffer = self._generate_buffer_with_experiences(experience_num=100)
+        experience, weights = buffer.sample(beta=beta)
+        lastest_index = buffer._latest_indices
+        assert len(experience) == 1
+        assert len(weights) == 1
+        assert len(lastest_index) == 1
+
+        for weight, index in zip(weights, lastest_index):
+            priority = buffer._get_priority(index)
+            assert weight == buffer._weights_from_priorities(priority, beta)
+
+    @pytest.mark.parametrize(
+        "beta",
+        [np.random.randint(low=0.0, high=1.0) for _ in range(1, 10)])
+    def test_sample_multiple_experiences(self, beta):
+        buffer = self._generate_buffer_with_experiences(experience_num=100)
+        experience, weights = buffer.sample(num_samples=10, beta=beta)
+        lastest_index = buffer._latest_indices
+        assert len(experience) == 10
+        assert len(weights) == 10
+        assert len(lastest_index) == 10
+        for weight, index in zip(weights, lastest_index):
+            priority = buffer._get_priority(index)
+            assert weight == buffer._weights_from_priorities(priority, beta)
+
+    @pytest.mark.parametrize(
+        "beta",
+        [np.random.randint(low=0.0, high=1.0) for _ in range(1, 10)])
+    def test_sample_indices(self, beta):
+        buffer = self._generate_buffer_with_experiences(experience_num=100)
         indices = [1, 67, 50, 4, 99]
-
-        experiences, info = buffer.sample_indices(indices)
+        experiences, weights = buffer.sample_indices(indices, beta=beta)
 
         assert len(experiences) == len(indices)
-        assert "weights" in info
-        assert len(info["weights"]) == len(indices)
-        weights = info["weights"]
-        for index, experience, actual_weight in zip(indices, experiences, weights):
-            expected_weight = self._compute_weight(buffer, index, alpha=buffer._alpha, beta=beta)
-            actual_weight = actual_weight[0]
+        assert len(weights) == len(indices)
+        for experience, weight, index in zip(experiences, weights, indices):
+            priority = buffer._get_priority(index)
             assert experience == buffer[index]
-            np.testing.assert_almost_equal(expected_weight, actual_weight)
-
-    def test_sample_from_empty_indices(self):
-        buffer = self._generate_buffer_with_experiences(experience_num=100)
-        with pytest.raises(ValueError):
-            buffer.sample_indices([])
-
-    def test_sample_from_wrong_indices(self):
-        buffer = self._generate_buffer_with_experiences(experience_num=100)
-        indices = [-99, 100, 101]
-        with pytest.raises(KeyError):
-            buffer.sample_indices(indices)
-
-    def test_random_indices(self):
-        buffer = ProportionalPrioritizedReplayBuffer(capacity=100)
-        for _ in range(100):
-            experience = _generate_experience_mock()
-            buffer.append(experience)
-
-        indices = buffer._random_indices(num_samples=10)
-        indices = np.array(indices, dtype=np.int32)
-        assert len(indices) == 10
-        assert np.alltrue(0 <= indices) and np.alltrue(indices <= 100)
-
-        # check no duplicates
-        assert len(np.unique(indices)) == len(indices)
-
-    def test_buffer_len(self):
-        buffer = ProportionalPrioritizedReplayBuffer(capacity=100)
-        for _ in range(10):
-            experience = _generate_experience_mock()
-            buffer.append(experience)
-
-        assert len(buffer) == 10
+            assert weight == buffer._weights_from_priorities(priority, beta)
 
     def test_sample_without_update(self):
         beta = 0.5
-        buffer = self._generate_buffer_with_experiences(experience_num=100, beta=beta)
+        buffer = self._generate_buffer_with_experiences(experience_num=100)
         indices = [1, 67, 50, 4, 99]
-        _, weights = buffer.sample_indices(indices)
+        _, weights = buffer.sample_indices(indices, beta=beta)
 
         # update the priority and check that following sampling succeeds
         errors = np.random.sample([len(weights), 1])
-        buffer.update_priorities(errors)
+        buffer.update_latest_priorities(errors)
 
-        _, _ = buffer.sample_indices(indices)
+        _, _ = buffer.sample_indices(indices, beta=beta)
 
         # sample without priority update
         with pytest.raises(RuntimeError):
-            buffer.sample_indices(indices)
-
-    def test_update_priorities(self):
-        beta = 0.5
-        buffer = self._generate_buffer_with_experiences(experience_num=100, beta=beta)
-        indices = [1, 67, 50, 4, 99]
-        _, weights = buffer.sample_indices(indices)
-
-        # update the priority and check that following sampling succeeds
-        errors = np.random.uniform(size=(len(indices), 1))
-        buffer.update_priorities(errors)
-
-        expected_priorities = (np.abs(errors) + 1e-8) ** buffer._alpha
-        for index, expected_priority in zip(indices, expected_priorities):
-            actual_priority = buffer._buffer.get_priority(index)
-            np.testing.assert_almost_equal(expected_priority, actual_priority)
-
-        new_indices = [0, 2, 49, 51, 3, 5, 98]
-        _, weights = buffer.sample_indices(new_indices)
-        errors = np.random.uniform(size=(len(new_indices), 1))
-        buffer.update_priorities(errors)
-
-        expected_new_priorities = (np.abs(errors) + 1e-8) ** buffer._alpha
-        for index, expected_priority in zip(new_indices, expected_new_priorities):
-            actual_priority = buffer._buffer.get_priority(index)
-            np.testing.assert_almost_equal(expected_priority, actual_priority)
-
-        # Check old priorities still not changed
-        for index, expected_priority in zip(indices, expected_priorities):
-            # New data is appended -> previous indices - 1 is the correct index
-            actual_priority = buffer._buffer.get_priority(index)
-            np.testing.assert_almost_equal(expected_priority, actual_priority)
-
-        # Append new data and check index 0's priority has changed
-        experience = self._generate_experience_mock()
-        buffer.append(experience)
-
-        old_index0_priority = expected_new_priorities[0]
-        new_index0_priority = buffer._buffer.get_priority(0)
-        with pytest.raises(AssertionError):
-            np.testing.assert_almost_equal(old_index0_priority, new_index0_priority)
-
-        # Check again old priorities still not changed
-        for index, expected_priority in zip(indices, expected_priorities):
-            index = index - 1
-            actual_priority = buffer._buffer.get_priority(index)
-            np.testing.assert_almost_equal(expected_priority, actual_priority)
-
-    @pytest.mark.parametrize("error_clip", [(-np.random.uniform(), np.random.uniform()) for _ in range(1, 10)])
-    def test_error_preprocessing(self, error_clip):
-        buffer = ProportionalPrioritizedReplayBuffer(capacity=100, error_clip=error_clip)
-
-        batch_size = 10
-        errors = np.random.randn(batch_size, 1)
-        processed = buffer._preprocess_errors(errors)
-
-        max_error = np.float32(max(np.abs(error_clip[0]), error_clip[1])) + 1e-5
-        assert all(0 <= processed)
-        assert all(max_error >= processed)
+            buffer.sample_indices(indices, beta=beta)
 
     def _generate_experience_mock(self):
         state_shape = (5, )
         action_shape = (10, )
 
         state = np.empty(shape=state_shape)
         action = np.empty(shape=action_shape)
         reward = np.random.normal()
         non_terminal = 0.0 if np.random.choice([True, False], 1) else 1.0
         next_state = np.empty(shape=state_shape)
         next_action = np.empty(shape=action_shape)
 
         return (state, action, reward, non_terminal, next_state, next_action)
 
-    def _generate_buffer_with_experiences(self,
-                                          experience_num, beta=1.0,
-                                          betasteps=1,
-                                          normalization_method="batch_max"):
-        buffer = ProportionalPrioritizedReplayBuffer(
-            capacity=experience_num, beta=beta, betasteps=betasteps, normalization_method=normalization_method)
+    def _generate_buffer_with_experiences(self, experience_num):
+        buffer = SumTree(capacity=experience_num, init_max_p=1.0)
         for _ in range(experience_num):
-            experience = _generate_experience_mock()
+            experience = self._generate_experience_mock()
             buffer.append(experience)
         return buffer
 
-    def _compute_weight(self, buffer, index, alpha, beta):
-        priority = buffer._buffer.get_priority(index)
-        if buffer._normalization_method == "batch_max":
-            min_priority = np.min(np.array([buffer._buffer.get_priority(index)
-                                            for index in buffer._last_sampled_indices]))
-        elif buffer._normalization_method == "buffer_max":
-            min_priority = buffer._buffer.min_priority()
-        else:
-            raise RuntimeError
-        weights = (priority / min_priority) ** (-beta)
-        return weights
-
 
-class TestRankBasedPrioritizedReplayBuffer(object):
+class TestPrioritizedReplayBuffer(object):
     def setup_method(self, method):
         np.random.seed(0)
 
     def test_finite_capacity(self):
         requested_capacity = 100
-        buffer = RankBasedPrioritizedReplayBuffer(capacity=requested_capacity)
+        buffer = PrioritizedReplayBuffer(capacity=requested_capacity)
         assert buffer.capacity == requested_capacity
 
     def test_infine_capacity(self):
         with pytest.raises(ValueError):
-            _ = RankBasedPrioritizedReplayBuffer(capacity=None)
+            _ = PrioritizedReplayBuffer(capacity=None)
 
-    def test_non_positive_capacity(self):
+    def test_not_positive_capacity(self):
         with pytest.raises(ValueError):
-            _ = RankBasedPrioritizedReplayBuffer(capacity=-1)
+            _ = PrioritizedReplayBuffer(capacity=-1)
         with pytest.raises(ValueError):
-            _ = RankBasedPrioritizedReplayBuffer(capacity=0)
+            _ = PrioritizedReplayBuffer(capacity=0)
 
     def test_append_with_capacity(self):
         capacity = 5
         append_num = 10
-        buffer = RankBasedPrioritizedReplayBuffer(capacity=capacity)
+        buffer = PrioritizedReplayBuffer(capacity=capacity)
         for i in range(append_num):
-            experience = _generate_experience_mock()
+            experience = self._generate_experience_mock()
             buffer.append(experience)
             if i < capacity:
                 assert len(buffer) == i + 1
             else:
                 assert len(buffer) == capacity
         assert len(buffer) == capacity
 
-    @pytest.mark.parametrize("beta", [np.random.uniform(low=0.0, high=1.0) for _ in range(1, 5)])
-    @pytest.mark.parametrize("betasteps", [i for i in range(1, 5)])
-    def test_betasteps(self, beta, betasteps):
-        buffer = self._generate_buffer_with_experiences(experience_num=100, beta=beta, betasteps=betasteps)
-        _, _ = buffer.sample()
-        new_beta = beta + (1.0 - beta) / betasteps
-        np.testing.assert_almost_equal(buffer._beta, new_beta)
-
-    @pytest.mark.parametrize("beta", [np.random.uniform(low=0.0, high=1.0) for _ in range(1, 10)])
+    @pytest.mark.parametrize(
+        "beta",
+        [np.random.randint(low=0.0, high=1.0) for _ in range(1, 10)])
     def test_sample_one_experience(self, beta):
-        buffer = self._generate_buffer_with_experiences(experience_num=100, beta=beta)
+        buffer = self._generate_buffer_with_experiences(experience_num=100,
+                                                        beta=beta)
         experiences, info = buffer.sample()
-        indices = buffer._last_sampled_indices
+        indices = buffer._buffer._latest_indices
         assert len(experiences) == 1
         assert "weights" in info
         assert len(info["weights"]) == 1
         assert len(indices) == 1
         weights = info["weights"]
-        for index, experience, actual_weight in zip(indices, experiences, weights):
-            expected_weight = self._compute_weight(buffer, index, alpha=buffer._alpha, beta=beta)
-            actual_weight = actual_weight[0]
+        for index, experience, weight in zip(indices, experiences, weights):
+            priority = buffer._buffer._get_priority(index)
             assert experience == buffer[index]
-            np.testing.assert_almost_equal(expected_weight, actual_weight)
+            assert weight == buffer._buffer._weights_from_priorities(priority,
+                                                                     beta)
 
-    @pytest.mark.parametrize("beta", [np.random.uniform(low=0.0, high=1.0) for _ in range(1, 10)])
+    @pytest.mark.parametrize(
+        "beta",
+        [np.random.randint(low=0.0, high=1.0) for _ in range(1, 10)])
     def test_sample_multiple_experiences(self, beta):
-        buffer = self._generate_buffer_with_experiences(experience_num=100, beta=beta)
+        buffer = self._generate_buffer_with_experiences(experience_num=100,
+                                                        beta=beta)
         num_samples = 10
         experiences, info = buffer.sample(num_samples=num_samples)
-        indices = buffer._last_sampled_indices
+        indices = buffer._buffer._latest_indices
         assert len(experiences) == num_samples
         assert "weights" in info
         assert len(info["weights"]) == num_samples
         assert len(indices) == num_samples
         weights = info["weights"]
-        for index, experience, actual_weight in zip(indices, experiences, weights):
-            expected_weight = self._compute_weight(buffer, index, alpha=buffer._alpha, beta=beta)
-            actual_weight = actual_weight[0]
+        for index, experience, weight in zip(indices, experiences, weights):
+            priority = buffer._buffer._get_priority(index)
             assert experience == buffer[index]
-            np.testing.assert_almost_equal(expected_weight, actual_weight)
-
-    @pytest.mark.parametrize("beta", [np.random.uniform(low=0.0, high=1.0) for _ in range(1, 10)])
-    @pytest.mark.parametrize("num_steps", range(1, 5))
-    def test_sample_multiple_step_experience(self, beta, num_steps):
-        buffer = self._generate_buffer_with_experiences(experience_num=100,
-                                                        beta=beta)
-        experiences_tuple, info = buffer.sample(num_steps=num_steps)
-        if num_steps == 1:
-            experiences_tuple = tuple([experiences_tuple, ])
-        indices = buffer._last_sampled_indices
-        assert len(experiences_tuple) == num_steps
-        assert "weights" in info
-        assert len(info["weights"]) == 1
-        assert len(indices) == 1
-        weights = info["weights"]
-        for i, experiences in enumerate(experiences_tuple):
-            for index, experience, actual_weight in zip(indices, experiences, weights):
-                expected_weight = self._compute_weight(buffer, index, alpha=buffer._alpha, beta=beta)
-                actual_weight = actual_weight[0]
-                assert experience == buffer[index + i]
-                np.testing.assert_almost_equal(expected_weight, actual_weight)
+            assert weight == buffer._buffer._weights_from_priorities(priority,
+                                                                     beta)
 
     def test_sample_from_insufficient_size_buffer(self):
         buffer = self._generate_buffer_with_experiences(experience_num=10)
         with pytest.raises(ValueError):
             buffer.sample(num_samples=100)
 
-    @pytest.mark.parametrize("beta", [np.random.uniform(low=0.0, high=1.0) for _ in range(1, 10)])
+    @pytest.mark.parametrize(
+        "beta",
+        [np.random.randint(low=0.0, high=1.0) for _ in range(1, 10)])
     def test_sample_indices(self, beta):
-        buffer = self._generate_buffer_with_experiences(experience_num=100, beta=beta)
+        buffer = self._generate_buffer_with_experiences(experience_num=100,
+                                                        beta=beta)
         indices = [1, 67, 50, 4, 99]
 
         experiences, info = buffer.sample_indices(indices)
 
         assert len(experiences) == len(indices)
         assert "weights" in info
         assert len(info["weights"]) == len(indices)
         weights = info["weights"]
-        for index, experience, actual_weight in zip(indices, experiences, weights):
-            expected_weight = self._compute_weight(buffer, index, alpha=buffer._alpha, beta=beta)
-            actual_weight = actual_weight[0]
+        for index, experience, weight in zip(indices, experiences, weights):
+            priority = buffer._buffer._get_priority(index)
             assert experience == buffer[index]
-            np.testing.assert_almost_equal(expected_weight, actual_weight)
+            assert weight == buffer._buffer._weights_from_priorities(priority,
+                                                                     beta)
 
     def test_sample_from_empty_indices(self):
         buffer = self._generate_buffer_with_experiences(experience_num=100)
         with pytest.raises(ValueError):
             buffer.sample_indices([])
 
     def test_sample_from_wrong_indices(self):
         buffer = self._generate_buffer_with_experiences(experience_num=100)
         indices = [-99, 100, 101]
-        with pytest.raises(KeyError):
+        with pytest.raises(IndexError):
             buffer.sample_indices(indices)
 
     def test_random_indices(self):
-        buffer = RankBasedPrioritizedReplayBuffer(capacity=100)
+        buffer = PrioritizedReplayBuffer(capacity=100)
         for _ in range(100):
-            experience = _generate_experience_mock()
+            experience = self._generate_experience_mock()
             buffer.append(experience)
 
         indices = buffer._random_indices(num_samples=10)
         indices = np.array(indices, dtype=np.int32)
         assert len(indices) == 10
         assert np.alltrue(0 <= indices) and np.alltrue(indices <= 100)
 
         # check no duplicates
         assert len(np.unique(indices)) == len(indices)
 
     def test_buffer_len(self):
-        buffer = RankBasedPrioritizedReplayBuffer(capacity=100)
+        buffer = PrioritizedReplayBuffer(capacity=100)
         for _ in range(10):
-            experience = _generate_experience_mock()
+            experience = self._generate_experience_mock()
             buffer.append(experience)
 
         assert len(buffer) == 10
 
-    def test_sample_without_update(self):
-        beta = 0.5
-        buffer = self._generate_buffer_with_experiences(experience_num=100, beta=beta)
-        indices = [1, 67, 50, 4, 99]
-        _, weights = buffer.sample_indices(indices)
-
-        # update the priority and check that following sampling succeeds
-        errors = np.random.sample([len(weights), 1])
-        buffer.update_priorities(errors)
-
-        _, _ = buffer.sample_indices(indices)
-
-        # sample without priority update
-        with pytest.raises(RuntimeError):
-            buffer.sample_indices(indices)
-
-    @pytest.mark.parametrize("N, k", [(1, 1), (2, 1), (3, 2), (4, 4), (10, 5)])
-    def test_compute_segment_boundaries(self, N, k):
-        buffer = RankBasedPrioritizedReplayBuffer(capacity=10)
-        actual = buffer._compute_segment_boundaries(N, k)
-
-        assert len(actual) == k
-        assert actual[-1] == N
-
-    def test_compute_segment_boundaries_batch_size_greater_than_buffer(self):
-        buffer = RankBasedPrioritizedReplayBuffer(capacity=10)
+    def _generate_experience_mock(self):
+        state_shape = (5, )
+        action_shape = (10, )
 
-        with pytest.raises(ValueError):
-            buffer._compute_segment_boundaries(N=5, k=10)
+        state = np.empty(shape=state_shape)
+        action = np.empty(shape=action_shape)
+        reward = np.random.normal()
+        non_terminal = 0.0 if np.random.choice([True, False], 1) else 1.0
+        next_state = np.empty(shape=state_shape)
+        next_action = np.empty(shape=action_shape)
 
-    def test_sort_interval(self):
-        sort_interval = 5
-        buffer = RankBasedPrioritizedReplayBuffer(capacity=10, sort_interval=sort_interval)
-        for i in range(10):
-            experience = _generate_experience_mock()
-            buffer.append(experience)
-            buffer._last_sampled_indices = [i]
-            buffer.update_priorities(errors=[np.random.randint(100)])
-            if (i + 1) % sort_interval == 0:
-                sorted_heap = sorted(buffer._buffer._max_heap._heap,
-                                     key=lambda item: -math.inf if item is None else item[1],
-                                     reverse=True)
-                assert np.alltrue(buffer._buffer._max_heap._heap == sorted_heap)
-
-    @pytest.mark.parametrize("error_clip", [(-np.random.uniform(), np.random.uniform()) for _ in range(1, 10)])
-    def test_error_preprocessing(self, error_clip):
-        buffer = RankBasedPrioritizedReplayBuffer(capacity=100, error_clip=error_clip)
-
-        batch_size = 10
-        errors = np.random.randn(batch_size, 1)
-        processed = buffer._preprocess_errors(errors)
-
-        max_error = np.float32(max(np.abs(error_clip[0]), error_clip[1])) + 1e-5
-        assert all(0 <= processed)
-        assert all(max_error >= processed)
+        return (state, action, reward, non_terminal, next_state, next_action)
 
-    def _generate_buffer_with_experiences(self, experience_num, beta=1.0, betasteps=1):
-        buffer = RankBasedPrioritizedReplayBuffer(capacity=experience_num, beta=beta, betasteps=betasteps)
+    def _generate_buffer_with_experiences(self, experience_num, beta=1.0):
+        buffer = PrioritizedReplayBuffer(capacity=experience_num, beta=beta)
         for _ in range(experience_num):
-            experience = _generate_experience_mock()
+            experience = self._generate_experience_mock()
             buffer.append(experience)
         return buffer
 
-    def _compute_weight(self, buffer, index, alpha, beta):
-        priority = buffer._buffer.get_priority(index)
-        worst_rank = len(buffer._buffer)
-        min_priority = (1 / worst_rank) ** alpha
-        weights = (priority / min_priority) ** (-beta)
-        return weights
-
-
-def _generate_experience_mock():
-    state_shape = (5, )
-    action_shape = (10, )
-
-    state = np.empty(shape=state_shape)
-    action = np.empty(shape=action_shape)
-    reward = np.random.normal()
-    non_terminal = 0.0 if np.random.choice([True, False], 1) else 1.0
-    next_state = np.empty(shape=state_shape)
-    next_action = np.empty(shape=action_shape)
-
-    return (state, action, reward, non_terminal, next_state, next_action)
-
 
 if __name__ == "__main__":
     pytest.main()
```

## tests/utils/test_copy.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -39,14 +39,15 @@
             h = NPF.affine(dummy_variable, 1,
                            w_init=self._weight_initializer,
                            b_init=self._bias_initialzier)
         return h
 
 
 class TestCopy(object):
+
     def test_copy_network_parameters(self):
         nn.clear_parameters()
 
         base = DummyNetwork('base',
                             NI.ConstantInitializer(1),
                             NI.ConstantInitializer(1))
         target = DummyNetwork('target',
```

## tests/utils/test_data.py

```diff
@@ -1,233 +1,25 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023,2024 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import numpy as np
 import pytest
-from packaging.version import parse
 
-import nnabla as nn
-import nnabla_rl.environments as E
-from nnabla_rl.utils.data import (RingBuffer, add_batch_dimension, compute_std_ndarray, list_of_dict_to_dict_of_list,
-                                  marshal_dict_experiences, marshal_experiences, normalize_ndarray,
-                                  set_data_to_variable, unnormalize_ndarray)
-
-
-class TestData():
-    def test_set_data_to_variable(self):
-        variable = nn.Variable((3,))
-        array = np.random.rand(3)
-
-        set_data_to_variable(variable, array)
-
-        assert np.allclose(variable.d, array)
-
-    def test_set_data_to_variable_tuple(self):
-        variables = (nn.Variable((3,)), nn.Variable((5,)))
-        arrays = (np.random.rand(3), np.random.rand(5))
-
-        set_data_to_variable(variables, arrays)
-
-        assert np.allclose(variables[0].d, arrays[0])
-        assert np.allclose(variables[1].d, arrays[1])
-
-    def test_set_data_to_variable_wrong_length(self):
-        variables = (nn.Variable((3,)), nn.Variable((5,)), nn.Variable((7,)))
-        arrays = (np.random.rand(3), np.random.rand(5))
-
-        with pytest.raises(AssertionError):
-            set_data_to_variable(variables, arrays)
-
-    def test_marshal_experiences(self):
-        batch_size = 3
-        dummy_env = E.DummyContinuous()
-        experiences = generate_dummy_experiences(dummy_env, batch_size)
-        state, action, reward, done, next_state, info = marshal_experiences(experiences)
-        rnn_states = info['rnn_states']
-        rnn_dummy_state1 = rnn_states['dummy_scope']['dummy_state1']
-        rnn_dummy_state2 = rnn_states['dummy_scope']['dummy_state2']
-
-        assert state.shape == (batch_size, dummy_env.observation_space.shape[0])
-        assert action.shape == (batch_size, dummy_env.action_space.shape[0])
-        assert reward.shape == (batch_size, 1)
-        assert done.shape == (batch_size, 1)
-        assert next_state.shape == (batch_size, dummy_env.observation_space.shape[0])
-        assert rnn_dummy_state1.shape == (batch_size, 1)
-        assert rnn_dummy_state2.shape == (batch_size, 1)
-
-    def test_marshal_experiences_tuple_continous(self):
-        batch_size = 2
-        dummy_env = E.DummyTupleContinuous()
-        experiences = generate_dummy_experiences(dummy_env, batch_size)
-        state, action, reward, done, next_state, info = marshal_experiences(experiences)
-        rnn_states = info['rnn_states']
-        rnn_dummy_state1 = rnn_states['dummy_scope']['dummy_state1']
-        rnn_dummy_state2 = rnn_states['dummy_scope']['dummy_state2']
-
-        assert state[0].shape == (batch_size, dummy_env.observation_space[0].shape[0])
-        assert state[1].shape == (batch_size, dummy_env.observation_space[1].shape[0])
-        assert action[0].shape == (batch_size, dummy_env.action_space[0].shape[0])
-        assert action[1].shape == (batch_size, dummy_env.action_space[1].shape[0])
-        assert reward.shape == (batch_size, 1)
-        assert done.shape == (batch_size, 1)
-        assert next_state[0].shape == (batch_size, dummy_env.observation_space[0].shape[0])
-        assert next_state[1].shape == (batch_size, dummy_env.observation_space[1].shape[0])
-        assert rnn_dummy_state1.shape == (batch_size, 1)
-        assert rnn_dummy_state2.shape == (batch_size, 1)
-
-    def test_marshal_experiences_tuple_discrete(self):
-        batch_size = 2
-        dummy_env = E.DummyTupleDiscrete()
-        experiences = generate_dummy_experiences(dummy_env, batch_size)
-        state, action, reward, done, next_state, info = marshal_experiences(experiences)
-        rnn_states = info['rnn_states']
-        rnn_dummy_state1 = rnn_states['dummy_scope']['dummy_state1']
-        rnn_dummy_state2 = rnn_states['dummy_scope']['dummy_state2']
-
-        assert state[0].shape == (batch_size, 1)
-        assert state[1].shape == (batch_size, 1)
-        assert action[0].shape == (batch_size, 1)
-        assert action[1].shape == (batch_size, 1)
-        assert reward.shape == (batch_size, 1)
-        assert done.shape == (batch_size, 1)
-        assert next_state[0].shape == (batch_size, 1)
-        assert next_state[1].shape == (batch_size, 1)
-        assert rnn_dummy_state1.shape == (batch_size, 1)
-        assert rnn_dummy_state2.shape == (batch_size, 1)
-
-    def test_marshal_dict_experiences(self):
-        experiences = {'key1': 1, 'key2': 2}
-        dict_experiences = [{'key_parent': experiences}, {'key_parent': experiences}]
-        marshaled_experience = marshal_dict_experiences(dict_experiences)
-
-        key1_experiences = marshaled_experience['key_parent']['key1']
-        key2_experiences = marshaled_experience['key_parent']['key2']
-
-        assert key1_experiences.shape == (2, 1)
-        assert key2_experiences.shape == (2, 1)
-
-        np.testing.assert_allclose(np.asarray(key1_experiences), 1)
-        np.testing.assert_allclose(np.asarray(key2_experiences), 2)
-
-    def test_marshal_triple_nested_dict_experiences(self):
-        experiences = {'key1': 1, 'key2': 2}
-        nested_experiences = {'nest1': experiences, 'nest2': experiences}
-        dict_experiences = [{'key_parent': nested_experiences}, {'key_parent': nested_experiences}]
-        marshaled_experience = marshal_dict_experiences(dict_experiences)
-
-        key1_experiences = marshaled_experience['key_parent']['nest1']['key1']
-        key2_experiences = marshaled_experience['key_parent']['nest2']['key2']
-
-        assert len(key1_experiences) == 2
-        assert len(key2_experiences) == 2
-
-        np.testing.assert_allclose(np.asarray(key1_experiences), 1)
-        np.testing.assert_allclose(np.asarray(key2_experiences), 2)
-
-    def test_marashal_dict_experiences_with_inhomogeneous_part(self):
-        installed_numpy_version = parse(np.__version__)
-        numpy_version1_24 = parse('1.24.0')
-
-        if installed_numpy_version < numpy_version1_24:
-            # no need to test
-            return
-
-        experiences = {'key1': 1, 'key2': 2}
-        inhomgeneous_experiences = {'key1': np.empty(shape=(6, )), 'key2': 2}
-        dict_experiences = [{'key_parent': experiences}, {'key_parent': inhomgeneous_experiences}]
-
-        marshaled_experience = marshal_dict_experiences(dict_experiences)
-
-        assert 'key1' not in marshaled_experience['key_parent']
-
-        key2_experiences = marshaled_experience['key_parent']['key2']
-        assert key2_experiences.shape == (2, 1)
-
-        np.testing.assert_allclose(np.asarray(key2_experiences), 2)
-
-    def test_list_of_dict_to_dict_of_list(self):
-        list_of_dict = [{'key1': 1, 'key2': 2}, {'key1': 1, 'key2': 2}]
-        dict_of_list = list_of_dict_to_dict_of_list(list_of_dict)
-
-        key1_list = dict_of_list['key1']
-        key2_list = dict_of_list['key2']
-
-        assert len(key1_list) == 2
-        assert len(key2_list) == 2
-
-        np.testing.assert_allclose(np.asarray(key1_list), 1)
-        np.testing.assert_allclose(np.asarray(key2_list), 2)
-
-    def test_add_batch_dimension_array(self):
-        array = np.random.randn(4)
-        actual_array = add_batch_dimension(array)
-
-        assert actual_array.shape == (1, *array.shape)
-
-    def test_add_batch_dimension_tuple(self):
-        array1 = np.random.randn(4)
-        array2 = np.random.randn(3)
-
-        actual_array = add_batch_dimension((array1, array2))
-
-        assert actual_array[0].shape == (1, *array1.shape)
-        assert actual_array[1].shape == (1, *array2.shape)
-
-    @pytest.mark.parametrize("x, mean, std, value_clip, expected",
-                             [
-                                 (np.array([2.0]), np.array([1.0]), np.array([0.2]), None,  np.array([5.0])),
-                                 (np.array([2.0]), np.array([1.0]), np.array([0.2]),  (-1.5, 1.5), np.array([1.5])),
-                                 (np.array([-2.0]), np.array([1.0]), np.array([0.2]),  (-1.5, 1.5), np.array([-1.5])),
-                                 (np.array([[2.0], [1.0]]), np.array([[1.0]]), np.array([[0.2]]), None,
-                                  np.array([[5.0], [0.0]])),
-                             ])
-    def test_normalize_ndarray(self, x, expected, mean, std, value_clip):
-        actual_var = normalize_ndarray(x, mean, std, value_clip=value_clip)
-        assert np.allclose(actual_var, expected)
-
-    @pytest.mark.parametrize("x, mean, std, value_clip, expected",
-                             [
-                                 (np.array([2.0]), np.array([1.0]), np.array([0.2]), None, np.array([1.4])),
-                                 (np.array([2.0]), np.array([1.0]), np.array([0.2]),  (-1.0, 1.0), np.array([1.0])),
-                                 (np.array([-2.0]), np.array([-1.0]), np.array([0.2]),  (-1.0, 1.0),  np.array([-1.0])),
-                                 (np.array([[2.0], [1.0]]), np.array([[1.0]]), np.array([[0.2]]), None,
-                                  np.array([[1.4], [1.2]])),
-                             ])
-    def test_unnormalize_ndarray(self, x, expected, mean, std, value_clip):
-        actual_var = unnormalize_ndarray(x, mean, std, value_clip=value_clip)
-        assert np.allclose(actual_var, expected)
-
-    @pytest.mark.parametrize("var, epsilon, mode_for_floating_point_error, expected",
-                             [
-                                 (np.array([3.0]), 1.0, "add", np.array([2.0])),
-                                 (np.array([4.0]), 0.01, "max", np.array([2.0])),
-                                 (np.array([0.4]), 1.0, "max", np.array([1.0])),
-                                 (np.array([[3.0], [8.0]]), 1.0, "add", np.array([[2.0], [3.0]])),
-                                 (np.array([[4.0], [9.0]]), 0.01, "max", np.array([[2.0], [3.0]])),
-                                 (np.array([[0.4], [0.9]]), 1.0, "max", np.array([[1.0], [1.0]])),
-                             ])
-    def test_compute_std_ndarray(self, var, epsilon, mode_for_floating_point_error, expected):
-        actual_var = compute_std_ndarray(var, epsilon, mode_for_floating_point_error)
-        assert np.allclose(actual_var, expected)
-
-    def test_compute_std_ndarray_with_invalid_args(self):
-        with pytest.raises(ValueError):
-            compute_std_ndarray(np.ones(1), 0.01, "dummy_add")
+from nnabla_rl.utils.data import RingBuffer
 
 
 class TestRingBuffer(object):
     def test_append(self):
         maxlen = 10
         buffer = RingBuffer(maxlen)
         for i in range(maxlen):
@@ -263,11 +55,8 @@
         for i in range(maxlen):
             assert len(buffer) == maxlen
             buffer.append(i)
         assert len(buffer) == maxlen
 
 
 if __name__ == "__main__":
-    from testing_utils import generate_dummy_experiences
     pytest.main()
-else:
-    from ..testing_utils import generate_dummy_experiences
```

## tests/utils/test_serializers.py

```diff
@@ -15,34 +15,27 @@
 
 import pathlib
 
 import numpy as np
 import pytest
 
 import nnabla_rl.algorithms as A
-from nnabla_rl.environments.dummy import DummyContinuous
 from nnabla_rl.utils.serializers import load_snapshot
 
 
 class TestLoadSnapshot(object):
     def test_load_snapshot(self):
         snapshot_path = pathlib.Path('test_resources/utils/ddpg-snapshot')
-        env = DummyContinuous(observation_shape=(3, ), action_shape=(1, ))
-        ddpg = load_snapshot(snapshot_path, env)
+        ddpg = load_snapshot(snapshot_path)
 
         assert isinstance(ddpg, A.DDPG)
         assert ddpg.iteration_num == 10000
         assert np.isclose(ddpg._config.tau, 0.05)
         assert np.isclose(ddpg._config.gamma, 0.99)
         assert np.isclose(ddpg._config.learning_rate, 0.001)
         assert np.isclose(ddpg._config.batch_size, 100)
         assert np.isclose(ddpg._config.start_timesteps, 200)
         assert ddpg._config.replay_buffer_size == 1000000
 
-    def test_load_snapshot_no_env(self):
-        snapshot_path = pathlib.Path('test_resources/utils/ddpg-snapshot')
-        with pytest.raises(RuntimeError):
-            load_snapshot(snapshot_path, {})
-
 
 if __name__ == '__main__':
     pytest.main()
```

## tests/writers/test_file_writer.py

```diff
@@ -1,9 +1,9 @@
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023,2024 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -13,15 +13,14 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import os
 import tempfile
 
 import numpy as np
-import pytest
 
 from nnabla_rl.writers.file_writer import FileWriter
 
 
 class TestFileWriter():
     def test_write_scalar(self):
         with tempfile.TemporaryDirectory() as tmpdir:
@@ -59,38 +58,18 @@
                 os.path.join(tmpdir, 'evaluation_results_histogram.tsv')
             this_file_dir = os.path.dirname(__file__)
             test_file_dir = this_file_dir.replace('tests', 'test_resources')
             test_file_path = \
                 os.path.join(test_file_dir, 'evaluation_results_histogram.tsv')
             self._check_same_tsv_file(file_path, test_file_path)
 
-    @pytest.mark.parametrize("format", ["%f", "%.3f", "%.5f"])
-    def test_data_formatting(self, format):
-        with tempfile.TemporaryDirectory() as tmpdir:
-            test_returns = np.arange(5)
-            test_results = {}
-            test_results['mean'] = np.mean(test_returns)
-            test_results['std_dev'] = np.std(test_returns)
-            test_results['min'] = np.min(test_returns)
-            test_results['max'] = np.max(test_returns)
-            test_results['median'] = np.median(test_returns)
-
-            writer = FileWriter(outdir=tmpdir, file_prefix='actual_results', fmt=format)
-            writer.write_scalar(1, test_results)
-
-            actual_file_path = os.path.join(tmpdir, 'actual_results_scalar.tsv')
-
-            this_file_dir = os.path.dirname(__file__)
-            expected_file_dir = this_file_dir.replace('tests', 'test_resources')
-            expected_file_path = os.path.join(expected_file_dir, f'evaluation_results_scalar{format}.tsv')
-            self._check_same_tsv_file(actual_file_path, expected_file_path)
-
     def _check_same_tsv_file(self, file_path1, file_path2):
         # check each line
         with open(file_path1, mode='rt') as data_1, \
                 open(file_path2, mode='rt') as data_2:
             for d_1, d_2 in zip(data_1, data_2):
                 assert d_1 == d_2
 
 
 if __name__ == "__main__":
+    import pytest
     pytest.main()
```

## Comparing `nnabla_rl/numpy_models/distribution_parameter.py` & `nnabla_rl-0.9.0.data/scripts/train_with_seeds`

 * *Files 26% similar despite different names*

```diff
@@ -1,25 +1,24 @@
-# Copyright 2022 Sony Group Corporation.
+#!/bin/bash
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from abc import ABCMeta
-
-from nnabla_rl.numpy_models.numpy_model import NumpyModel
-
-
-class DistributionParameter(NumpyModel, metaclass=ABCMeta):
-    def __init__(self) -> None:
-        super().__init__()
-
-    def update_parameter(self, *args):
-        raise NotImplementedError
+if [ $# -ne 4 ]; then
+    echo "usage: $0 <script_file_name> <gpu_id> <env> <save_dir>"
+    exit 1
+fi
+RESULTDIR="$4/$3_results"
+for seed in 1 10 100
+do
+    python $1 --gpu $2 --seed $seed --env $3 --save-dir $4
+done
```

## Comparing `nnabla_rl-0.15.0.data/scripts/check_best_iteration` & `nnabla_rl-0.9.0.data/scripts/check_best_iteration`

 * *Files identical despite different names*

## Comparing `nnabla_rl-0.15.0.data/scripts/compile_results` & `nnabla_rl-0.9.0.data/scripts/compile_results`

 * *Files identical despite different names*

## Comparing `nnabla_rl-0.15.0.data/scripts/insert_copyright` & `nnabla_rl-0.9.0.data/scripts/insert_copyright`

 * *Files 26% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 #!python
-# Copyright 2021,2022 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -15,15 +15,15 @@
 
 import argparse
 import os
 import pathlib
 import re
 import subprocess
 
-_exclude_dirs = ['external', '.git']
+_exclude_dirs = ['.git', '.egg', 'cache', '.vscode', 'external']
 _insert_file_regex = re.compile('.*.(py|cfg|ini|sh)')
 _header_extract_regex = re.compile('# Copyright \\d{4}.*? limitations under the License.', re.DOTALL)
 _shebang_extract_regex = re.compile('#!.+')
 _date_extract_regex = re.compile('(\\d{4}-\\d{2}-\\d{2})')
 
 COPYRIGHT_TEMPLATE_SONY_CORP = '# Copyright {} Sony Corporation.'
 
@@ -39,109 +39,25 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.'''
 
 
-def find_dot_git(rootdir):
-    path = pathlib.Path(rootdir)
-    for f in path.iterdir():
-        if f.name == '.git':
-            return f
-    parent = path.parent
-    return None if parent == path else find_dot_git(parent)
-
-
-def find_gitignore(rootdir):
-    if rootdir.name == '.git':
-        return []
-    path = pathlib.Path(rootdir)
-    gitignore_files = []
-    for f in path.iterdir():
-        if f.is_dir():
-            gitignores = find_gitignore(f)
-            gitignore_files.extend(gitignores)
-        if f.name == '.gitignore':
-            gitignore_files.append(f)
-    return gitignore_files
-
-
-def read_gitignore(gitignore_file):
-    exclude_dirs = []
-    with open(gitignore_file) as f:
-        lines = f.readlines()
-    for l in lines:
-        l = l.replace('\n', '')
-        if '#' in l:
-            continue
-        if '*' in l:
-            l = l.replace('*', '')
-        if len(l) == 0:
-            continue
-        f = gitignore_file.parent / l
-        if f.is_dir():
-            exclude_dirs.append(f)
-    return exclude_dirs
-
-
-def read_exclude(exclude_file):
-    exclude_dirs = []
-    with open(exclude_file) as f:
-        lines = f.readlines()
-    for l in lines:
-        l = l.replace('\n', '')
-        if '#' in l:
-            continue
-        if '*' in l:
-            l = l.replace('*', '')
-        if len(l) == 0:
-            continue
-        exclude_dirs.append(l)
-    return exclude_dirs
-
-
-def list_exclude_dirs(rootdir):
-    dot_git_file = find_dot_git(rootdir)
-    if dot_git_file is None:
-        return []
-    exclude_file = dot_git_file / 'info/exclude'
-    git_rootdir = dot_git_file.parent
-
-    excludes = []
-    additional_excludes = read_exclude(exclude_file)
-    for additional in additional_excludes:
-        additional_exclude = git_rootdir / additional
-        if additional_exclude.exists() and additional_exclude.is_dir():
-            excludes.append(additional_exclude)
-
-    gitignore_files = find_gitignore(git_rootdir)
-    for f in gitignore_files:
-        excludes.extend(read_gitignore(f))
-        for additional in additional_excludes:
-            additional_exclude = f.parent / additional
-            if additional_exclude.exists() and additional_exclude.is_dir():
-                excludes.append(additional_exclude)
-    return excludes
-
-
-def listup_files(rootdir, exclude_dirs):
+def listup_files(rootdir):
     files = []
     path = pathlib.Path(rootdir)
     dirname = str(path)
-    for exclude in exclude_dirs:
-        if path == exclude:
-            return files
     for exclude in _exclude_dirs:
         if exclude in dirname:
             return files
 
     for f in path.iterdir():
         if f.is_dir():
-            files.extend(listup_files(f, exclude_dirs))
+            files.extend(listup_files(f))
         else:
             # empty suffix means binary file
             matched = _insert_file_regex.fullmatch(str(f)) is not None or (f.suffix == '' and has_shebang(f))
             if matched:
                 files.append(f)
     return files
 
@@ -274,16 +190,15 @@
     else:
         replaced_text = text.replace(old_header, new_header)
     with open(filepath, 'w') as f:
         f.write(replaced_text)
 
 
 def main(args):
-    exclude_dirs = list_exclude_dirs(args.rootdir)
-    files = listup_files(args.rootdir, exclude_dirs=exclude_dirs)
+    files = listup_files(args.rootdir)
     for f in files:
         if args.diff:
             check_diff(f)
         else:
             insert_copyright_header(f)
```

## Comparing `nnabla_rl-0.15.0.data/scripts/plot_result` & `nnabla_rl-0.9.0.data/scripts/plot_result`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 #!python
 # Copyright 2020,2021 Sony Corporation.
-# Copyright 2021,2022,2023 Sony Group Corporation.
+# Copyright 2021 Sony Group Corporation.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -12,39 +12,28 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import argparse
 import pathlib
+import sys
+from datetime import datetime as dt
 
 import matplotlib.pyplot as plt
 import numpy as np
 from matplotlib.ticker import ScalarFormatter
 
 
-def smooth_with_moving_average(data, k=1):
-    if k == 1:
-        # no smoothing
-        return data
-    # assuming that the data is flat
-    data = np.insert(data, obj=0, values=[data[0]] * (k - 1))
-    cumsum = np.cumsum(data, dtype=float)
-    cumsum[k:] = cumsum[k:] - cumsum[:-k]
-    moving_average = cumsum[k - 1:] / k
-    return moving_average
-
-
 def load_data(path):
     path = pathlib.Path(path)
     return np.loadtxt(str(path), delimiter='\t', skiprows=1)
 
 
 def plot_results(args):
-    plt.style.use('tableau-colorblind10')
     fig = plt.figure(figsize=(5, 4), dpi=80)
     ax = fig.add_subplot(111)
 
     xnlim = args.xnlim
     xplim = args.xplim
     ynlim = args.ynlim
     yplim = args.yplim
@@ -76,24 +65,21 @@
             yplim = np.amax(max_y)
         else:
             yplim = max(np.amax(max_y), yplim)
 
         mean_label = 'mean'
         if i < len(args.tsvlabels):
             mean_label = f'{mean_label}({args.tsvlabels[i]})'
-        avg_y = smooth_with_moving_average(avg_y, k=args.smooth_k)
         ax.plot(x, avg_y, label=mean_label, linewidth=1)
-        if not args.no_stddev:
-            ax.fill_between(x, avg_y + std_y, avg_y - std_y, alpha=0.3)
+        ax.fill_between(x, avg_y + std_y, avg_y - std_y, alpha=0.3)
 
         if args.plot_median:
-            median_label = 'median'
+            median_label = 'mean'
             if i < len(args.tsvlabels):
                 median_label = f'{median_label}({args.tsvlabels[i]})'
-                med_y = smooth_with_moving_average(med_y, k=args.smooth_k)
                 ax.plot(x, med_y, label=median_label, linewidth=1)
     ax.set_xlim(args.xnlim, args.xplim)
     ax.set_ylim(args.ynlim, args.yplim)
     ax.set_xlabel(args.xlabel)
     ax.set_ylabel(args.ylabel)
     ax.xaxis.set_major_formatter(ScalarFormatter(useMathText=True))
     ax.ticklabel_format(style="sci",  axis="x", scilimits=(0, 0))
@@ -136,15 +122,13 @@
     parser.add_argument('--yplim', type=int, default=None)
 
     parser.add_argument('--legend-pos', type=str, default='upper left')
 
     parser.add_argument('--hlines', type=float, nargs='*', default=[])
 
     parser.add_argument('--plot-median', action='store_true')
-    parser.add_argument('--no-stddev', action='store_true')
-    parser.add_argument('--smooth-k', type=int, default=1)
 
     parser.add_argument('--show-fig', action='store_true')
 
     args = parser.parse_args()
 
     plot_results(args)
```

## Comparing `nnabla_rl-0.15.0.dist-info/LICENSE` & `nnabla_rl-0.9.0.dist-info/LICENSE`

 * *Files identical despite different names*

