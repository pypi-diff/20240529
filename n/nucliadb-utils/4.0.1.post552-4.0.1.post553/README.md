# Comparing `tmp/nucliadb_utils-4.0.1.post552-py3-none-any.whl.zip` & `tmp/nucliadb_utils-4.0.1.post553-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,80 +1,80 @@
-Zip file size: 118007 bytes, number of entries: 78
--rw-r--r--  2.0 unx      895 b- defN 24-May-28 14:23 nucliadb_utils/__init__.py
--rw-r--r--  2.0 unx     2894 b- defN 24-May-28 14:23 nucliadb_utils/asyncio_utils.py
--rw-r--r--  2.0 unx     6071 b- defN 24-May-28 14:23 nucliadb_utils/authentication.py
--rw-r--r--  2.0 unx     2391 b- defN 24-May-28 14:23 nucliadb_utils/const.py
--rw-r--r--  2.0 unx     2470 b- defN 24-May-28 14:23 nucliadb_utils/debug.py
--rw-r--r--  2.0 unx     1094 b- defN 24-May-28 14:23 nucliadb_utils/exceptions.py
--rw-r--r--  2.0 unx     2740 b- defN 24-May-28 14:23 nucliadb_utils/featureflagging.py
--rw-r--r--  2.0 unx     3336 b- defN 24-May-28 14:23 nucliadb_utils/grpc.py
--rw-r--r--  2.0 unx     1599 b- defN 24-May-28 14:23 nucliadb_utils/helpers.py
--rw-r--r--  2.0 unx     3462 b- defN 24-May-28 14:23 nucliadb_utils/indexing.py
--rw-r--r--  2.0 unx     8224 b- defN 24-May-28 14:23 nucliadb_utils/nats.py
--rw-r--r--  2.0 unx     1173 b- defN 24-May-28 14:23 nucliadb_utils/partition.py
--rw-r--r--  2.0 unx        0 b- defN 24-May-28 14:23 nucliadb_utils/py.typed
--rw-r--r--  2.0 unx     1713 b- defN 24-May-28 14:23 nucliadb_utils/run.py
--rw-r--r--  2.0 unx     7252 b- defN 24-May-28 14:23 nucliadb_utils/settings.py
--rw-r--r--  2.0 unx     2704 b- defN 24-May-28 14:23 nucliadb_utils/signals.py
--rw-r--r--  2.0 unx      890 b- defN 24-May-28 14:23 nucliadb_utils/store.py
--rw-r--r--  2.0 unx     7212 b- defN 24-May-28 14:23 nucliadb_utils/transaction.py
--rw-r--r--  2.0 unx    14178 b- defN 24-May-28 14:23 nucliadb_utils/utilities.py
--rw-r--r--  2.0 unx      835 b- defN 24-May-28 14:23 nucliadb_utils/audit/__init__.py
--rw-r--r--  2.0 unx     2834 b- defN 24-May-28 14:23 nucliadb_utils/audit/audit.py
--rw-r--r--  2.0 unx     3430 b- defN 24-May-28 14:23 nucliadb_utils/audit/basic.py
--rw-r--r--  2.0 unx    12042 b- defN 24-May-28 14:23 nucliadb_utils/audit/stream.py
--rw-r--r--  2.0 unx      833 b- defN 24-May-28 14:23 nucliadb_utils/cache/__init__.py
--rw-r--r--  2.0 unx      975 b- defN 24-May-28 14:23 nucliadb_utils/cache/exceptions.py
--rw-r--r--  2.0 unx     7089 b- defN 24-May-28 14:23 nucliadb_utils/cache/nats.py
--rw-r--r--  2.0 unx     1654 b- defN 24-May-28 14:23 nucliadb_utils/cache/pubsub.py
--rw-r--r--  2.0 unx     1059 b- defN 24-May-28 14:23 nucliadb_utils/cache/settings.py
--rw-r--r--  2.0 unx      833 b- defN 24-May-28 14:23 nucliadb_utils/fastapi/__init__.py
--rw-r--r--  2.0 unx     1737 b- defN 24-May-28 14:23 nucliadb_utils/fastapi/openapi.py
--rw-r--r--  2.0 unx     3631 b- defN 24-May-28 14:23 nucliadb_utils/fastapi/run.py
--rw-r--r--  2.0 unx     3774 b- defN 24-May-28 14:23 nucliadb_utils/fastapi/versioning.py
--rw-r--r--  2.0 unx      835 b- defN 24-May-28 14:23 nucliadb_utils/nuclia_usage/__init__.py
--rw-r--r--  2.0 unx      835 b- defN 24-May-28 14:23 nucliadb_utils/nuclia_usage/protos/__init__.py
--rw-r--r--  2.0 unx     5934 b- defN 24-May-28 14:23 nucliadb_utils/nuclia_usage/protos/kb_usage_pb2.py
--rw-r--r--  2.0 unx    15983 b- defN 24-May-28 14:23 nucliadb_utils/nuclia_usage/protos/kb_usage_pb2.pyi
--rw-r--r--  2.0 unx     1008 b- defN 24-May-28 14:23 nucliadb_utils/nuclia_usage/protos/kb_usage_pb2_grpc.py
--rw-r--r--  2.0 unx      911 b- defN 24-May-28 14:23 nucliadb_utils/nuclia_usage/protos/kb_usage_pb2_grpc.pyi
--rw-r--r--  2.0 unx      835 b- defN 24-May-28 14:23 nucliadb_utils/nuclia_usage/utils/__init__.py
--rw-r--r--  2.0 unx     3870 b- defN 24-May-28 14:23 nucliadb_utils/nuclia_usage/utils/kb_usage_report.py
--rw-r--r--  2.0 unx      872 b- defN 24-May-28 14:23 nucliadb_utils/storages/__init__.py
--rw-r--r--  2.0 unx     2417 b- defN 24-May-28 14:23 nucliadb_utils/storages/exceptions.py
--rw-r--r--  2.0 unx    27099 b- defN 24-May-28 14:23 nucliadb_utils/storages/gcs.py
--rw-r--r--  2.0 unx    10097 b- defN 24-May-28 14:23 nucliadb_utils/storages/local.py
--rw-r--r--  2.0 unx     2097 b- defN 24-May-28 14:23 nucliadb_utils/storages/nuclia.py
--rw-r--r--  2.0 unx    18633 b- defN 24-May-28 14:23 nucliadb_utils/storages/pg.py
--rw-r--r--  2.0 unx    19269 b- defN 24-May-28 14:23 nucliadb_utils/storages/s3.py
--rw-r--r--  2.0 unx     1285 b- defN 24-May-28 14:23 nucliadb_utils/storages/settings.py
--rw-r--r--  2.0 unx    20322 b- defN 24-May-28 14:23 nucliadb_utils/storages/storage.py
--rw-r--r--  2.0 unx     1456 b- defN 24-May-28 14:23 nucliadb_utils/tests/__init__.py
--rw-r--r--  2.0 unx    10698 b- defN 24-May-28 14:23 nucliadb_utils/tests/asyncbenchmark.py
--rw-r--r--  2.0 unx     1876 b- defN 24-May-28 14:23 nucliadb_utils/tests/conftest.py
--rw-r--r--  2.0 unx     3098 b- defN 24-May-28 14:23 nucliadb_utils/tests/gcs.py
--rw-r--r--  2.0 unx     1513 b- defN 24-May-28 14:23 nucliadb_utils/tests/indexing.py
--rw-r--r--  2.0 unx     1343 b- defN 24-May-28 14:23 nucliadb_utils/tests/local.py
--rw-r--r--  2.0 unx     7701 b- defN 24-May-28 14:23 nucliadb_utils/tests/nats.py
--rw-r--r--  2.0 unx     1819 b- defN 24-May-28 14:23 nucliadb_utils/tests/pg.py
--rw-r--r--  2.0 unx     2330 b- defN 24-May-28 14:23 nucliadb_utils/tests/s3.py
--rw-r--r--  2.0 unx      833 b- defN 24-May-28 14:23 nucliadb_utils/tests/unit/__init__.py
--rw-r--r--  2.0 unx     2359 b- defN 24-May-28 14:23 nucliadb_utils/tests/unit/test_asyncio_utils.py
--rw-r--r--  2.0 unx     5301 b- defN 24-May-28 14:23 nucliadb_utils/tests/unit/test_authentication.py
--rw-r--r--  2.0 unx     1574 b- defN 24-May-28 14:23 nucliadb_utils/tests/unit/test_helpers.py
--rw-r--r--  2.0 unx     4436 b- defN 24-May-28 14:23 nucliadb_utils/tests/unit/test_nats.py
--rw-r--r--  2.0 unx     1910 b- defN 24-May-28 14:23 nucliadb_utils/tests/unit/test_run.py
--rw-r--r--  2.0 unx     2647 b- defN 24-May-28 14:23 nucliadb_utils/tests/unit/test_signals.py
--rw-r--r--  2.0 unx     1189 b- defN 24-May-28 14:23 nucliadb_utils/tests/unit/test_tests.py
--rw-r--r--  2.0 unx     3917 b- defN 24-May-28 14:23 nucliadb_utils/tests/unit/test_transaction.py
--rw-r--r--  2.0 unx     5377 b- defN 24-May-28 14:23 nucliadb_utils/tests/unit/test_utilities.py
--rw-r--r--  2.0 unx      833 b- defN 24-May-28 14:23 nucliadb_utils/tests/unit/storages/__init__.py
--rw-r--r--  2.0 unx     1924 b- defN 24-May-28 14:23 nucliadb_utils/tests/unit/storages/test_aws.py
--rw-r--r--  2.0 unx     3554 b- defN 24-May-28 14:23 nucliadb_utils/tests/unit/storages/test_gcs.py
--rw-r--r--  2.0 unx    17000 b- defN 24-May-28 14:23 nucliadb_utils/tests/unit/storages/test_pg.py
--rw-r--r--  2.0 unx     7107 b- defN 24-May-28 14:23 nucliadb_utils/tests/unit/storages/test_storage.py
--rw-r--r--  2.0 unx     2030 b- defN 24-May-28 14:24 nucliadb_utils-4.0.1.post552.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-May-28 14:24 nucliadb_utils-4.0.1.post552.dist-info/WHEEL
--rw-r--r--  2.0 unx       15 b- defN 24-May-28 14:24 nucliadb_utils-4.0.1.post552.dist-info/top_level.txt
--rw-r--r--  2.0 unx        1 b- defN 24-May-28 14:24 nucliadb_utils-4.0.1.post552.dist-info/zip-safe
--rw-rw-r--  2.0 unx     7121 b- defN 24-May-28 14:24 nucliadb_utils-4.0.1.post552.dist-info/RECORD
-78 files, 344380 bytes uncompressed, 106573 bytes compressed:  69.1%
+Zip file size: 118332 bytes, number of entries: 78
+-rw-r--r--  2.0 unx      895 b- defN 24-May-29 12:14 nucliadb_utils/__init__.py
+-rw-r--r--  2.0 unx     2894 b- defN 24-May-29 12:14 nucliadb_utils/asyncio_utils.py
+-rw-r--r--  2.0 unx     6071 b- defN 24-May-29 12:14 nucliadb_utils/authentication.py
+-rw-r--r--  2.0 unx     2391 b- defN 24-May-29 12:14 nucliadb_utils/const.py
+-rw-r--r--  2.0 unx     2470 b- defN 24-May-29 12:14 nucliadb_utils/debug.py
+-rw-r--r--  2.0 unx     1094 b- defN 24-May-29 12:14 nucliadb_utils/exceptions.py
+-rw-r--r--  2.0 unx     2740 b- defN 24-May-29 12:14 nucliadb_utils/featureflagging.py
+-rw-r--r--  2.0 unx     3336 b- defN 24-May-29 12:14 nucliadb_utils/grpc.py
+-rw-r--r--  2.0 unx     1599 b- defN 24-May-29 12:14 nucliadb_utils/helpers.py
+-rw-r--r--  2.0 unx     3462 b- defN 24-May-29 12:14 nucliadb_utils/indexing.py
+-rw-r--r--  2.0 unx     8224 b- defN 24-May-29 12:14 nucliadb_utils/nats.py
+-rw-r--r--  2.0 unx     1173 b- defN 24-May-29 12:14 nucliadb_utils/partition.py
+-rw-r--r--  2.0 unx        0 b- defN 24-May-29 12:14 nucliadb_utils/py.typed
+-rw-r--r--  2.0 unx     1713 b- defN 24-May-29 12:14 nucliadb_utils/run.py
+-rw-r--r--  2.0 unx     7252 b- defN 24-May-29 12:14 nucliadb_utils/settings.py
+-rw-r--r--  2.0 unx     2704 b- defN 24-May-29 12:14 nucliadb_utils/signals.py
+-rw-r--r--  2.0 unx      890 b- defN 24-May-29 12:14 nucliadb_utils/store.py
+-rw-r--r--  2.0 unx     7212 b- defN 24-May-29 12:14 nucliadb_utils/transaction.py
+-rw-r--r--  2.0 unx    14178 b- defN 24-May-29 12:14 nucliadb_utils/utilities.py
+-rw-r--r--  2.0 unx      835 b- defN 24-May-29 12:14 nucliadb_utils/audit/__init__.py
+-rw-r--r--  2.0 unx     2834 b- defN 24-May-29 12:14 nucliadb_utils/audit/audit.py
+-rw-r--r--  2.0 unx     3430 b- defN 24-May-29 12:14 nucliadb_utils/audit/basic.py
+-rw-r--r--  2.0 unx    12042 b- defN 24-May-29 12:14 nucliadb_utils/audit/stream.py
+-rw-r--r--  2.0 unx      833 b- defN 24-May-29 12:14 nucliadb_utils/cache/__init__.py
+-rw-r--r--  2.0 unx      975 b- defN 24-May-29 12:14 nucliadb_utils/cache/exceptions.py
+-rw-r--r--  2.0 unx     7089 b- defN 24-May-29 12:14 nucliadb_utils/cache/nats.py
+-rw-r--r--  2.0 unx     1654 b- defN 24-May-29 12:14 nucliadb_utils/cache/pubsub.py
+-rw-r--r--  2.0 unx     1059 b- defN 24-May-29 12:14 nucliadb_utils/cache/settings.py
+-rw-r--r--  2.0 unx      833 b- defN 24-May-29 12:14 nucliadb_utils/fastapi/__init__.py
+-rw-r--r--  2.0 unx     1737 b- defN 24-May-29 12:14 nucliadb_utils/fastapi/openapi.py
+-rw-r--r--  2.0 unx     3631 b- defN 24-May-29 12:14 nucliadb_utils/fastapi/run.py
+-rw-r--r--  2.0 unx     3774 b- defN 24-May-29 12:14 nucliadb_utils/fastapi/versioning.py
+-rw-r--r--  2.0 unx      835 b- defN 24-May-29 12:14 nucliadb_utils/nuclia_usage/__init__.py
+-rw-r--r--  2.0 unx      835 b- defN 24-May-29 12:14 nucliadb_utils/nuclia_usage/protos/__init__.py
+-rw-r--r--  2.0 unx     5934 b- defN 24-May-29 12:14 nucliadb_utils/nuclia_usage/protos/kb_usage_pb2.py
+-rw-r--r--  2.0 unx    15983 b- defN 24-May-29 12:14 nucliadb_utils/nuclia_usage/protos/kb_usage_pb2.pyi
+-rw-r--r--  2.0 unx     1008 b- defN 24-May-29 12:14 nucliadb_utils/nuclia_usage/protos/kb_usage_pb2_grpc.py
+-rw-r--r--  2.0 unx      911 b- defN 24-May-29 12:14 nucliadb_utils/nuclia_usage/protos/kb_usage_pb2_grpc.pyi
+-rw-r--r--  2.0 unx      835 b- defN 24-May-29 12:14 nucliadb_utils/nuclia_usage/utils/__init__.py
+-rw-r--r--  2.0 unx     3870 b- defN 24-May-29 12:14 nucliadb_utils/nuclia_usage/utils/kb_usage_report.py
+-rw-r--r--  2.0 unx      872 b- defN 24-May-29 12:14 nucliadb_utils/storages/__init__.py
+-rw-r--r--  2.0 unx     2417 b- defN 24-May-29 12:14 nucliadb_utils/storages/exceptions.py
+-rw-r--r--  2.0 unx    27303 b- defN 24-May-29 12:14 nucliadb_utils/storages/gcs.py
+-rw-r--r--  2.0 unx    10447 b- defN 24-May-29 12:14 nucliadb_utils/storages/local.py
+-rw-r--r--  2.0 unx     2097 b- defN 24-May-29 12:14 nucliadb_utils/storages/nuclia.py
+-rw-r--r--  2.0 unx    18976 b- defN 24-May-29 12:14 nucliadb_utils/storages/pg.py
+-rw-r--r--  2.0 unx    19447 b- defN 24-May-29 12:14 nucliadb_utils/storages/s3.py
+-rw-r--r--  2.0 unx     1285 b- defN 24-May-29 12:14 nucliadb_utils/storages/settings.py
+-rw-r--r--  2.0 unx    20585 b- defN 24-May-29 12:14 nucliadb_utils/storages/storage.py
+-rw-r--r--  2.0 unx     1456 b- defN 24-May-29 12:14 nucliadb_utils/tests/__init__.py
+-rw-r--r--  2.0 unx    10698 b- defN 24-May-29 12:14 nucliadb_utils/tests/asyncbenchmark.py
+-rw-r--r--  2.0 unx     1876 b- defN 24-May-29 12:14 nucliadb_utils/tests/conftest.py
+-rw-r--r--  2.0 unx     3098 b- defN 24-May-29 12:14 nucliadb_utils/tests/gcs.py
+-rw-r--r--  2.0 unx     1513 b- defN 24-May-29 12:14 nucliadb_utils/tests/indexing.py
+-rw-r--r--  2.0 unx     1343 b- defN 24-May-29 12:14 nucliadb_utils/tests/local.py
+-rw-r--r--  2.0 unx     7701 b- defN 24-May-29 12:14 nucliadb_utils/tests/nats.py
+-rw-r--r--  2.0 unx     1819 b- defN 24-May-29 12:14 nucliadb_utils/tests/pg.py
+-rw-r--r--  2.0 unx     2330 b- defN 24-May-29 12:14 nucliadb_utils/tests/s3.py
+-rw-r--r--  2.0 unx      833 b- defN 24-May-29 12:14 nucliadb_utils/tests/unit/__init__.py
+-rw-r--r--  2.0 unx     2359 b- defN 24-May-29 12:14 nucliadb_utils/tests/unit/test_asyncio_utils.py
+-rw-r--r--  2.0 unx     5301 b- defN 24-May-29 12:14 nucliadb_utils/tests/unit/test_authentication.py
+-rw-r--r--  2.0 unx     1574 b- defN 24-May-29 12:14 nucliadb_utils/tests/unit/test_helpers.py
+-rw-r--r--  2.0 unx     4436 b- defN 24-May-29 12:14 nucliadb_utils/tests/unit/test_nats.py
+-rw-r--r--  2.0 unx     1910 b- defN 24-May-29 12:14 nucliadb_utils/tests/unit/test_run.py
+-rw-r--r--  2.0 unx     2647 b- defN 24-May-29 12:14 nucliadb_utils/tests/unit/test_signals.py
+-rw-r--r--  2.0 unx     1189 b- defN 24-May-29 12:14 nucliadb_utils/tests/unit/test_tests.py
+-rw-r--r--  2.0 unx     3917 b- defN 24-May-29 12:14 nucliadb_utils/tests/unit/test_transaction.py
+-rw-r--r--  2.0 unx     5377 b- defN 24-May-29 12:14 nucliadb_utils/tests/unit/test_utilities.py
+-rw-r--r--  2.0 unx      833 b- defN 24-May-29 12:14 nucliadb_utils/tests/unit/storages/__init__.py
+-rw-r--r--  2.0 unx     1924 b- defN 24-May-29 12:14 nucliadb_utils/tests/unit/storages/test_aws.py
+-rw-r--r--  2.0 unx     3554 b- defN 24-May-29 12:14 nucliadb_utils/tests/unit/storages/test_gcs.py
+-rw-r--r--  2.0 unx    17016 b- defN 24-May-29 12:14 nucliadb_utils/tests/unit/storages/test_pg.py
+-rw-r--r--  2.0 unx     7131 b- defN 24-May-29 12:14 nucliadb_utils/tests/unit/storages/test_storage.py
+-rw-r--r--  2.0 unx     2030 b- defN 24-May-29 12:16 nucliadb_utils-4.0.1.post553.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-May-29 12:16 nucliadb_utils-4.0.1.post553.dist-info/WHEEL
+-rw-r--r--  2.0 unx       15 b- defN 24-May-29 12:16 nucliadb_utils-4.0.1.post553.dist-info/top_level.txt
+-rw-r--r--  2.0 unx        1 b- defN 24-May-29 12:15 nucliadb_utils-4.0.1.post553.dist-info/zip-safe
+-rw-rw-r--  2.0 unx     7121 b- defN 24-May-29 12:16 nucliadb_utils-4.0.1.post553.dist-info/RECORD
+78 files, 345758 bytes uncompressed, 106898 bytes compressed:  69.1%
```

## zipnote {}

```diff
@@ -213,23 +213,23 @@
 
 Filename: nucliadb_utils/tests/unit/storages/test_pg.py
 Comment: 
 
 Filename: nucliadb_utils/tests/unit/storages/test_storage.py
 Comment: 
 
-Filename: nucliadb_utils-4.0.1.post552.dist-info/METADATA
+Filename: nucliadb_utils-4.0.1.post553.dist-info/METADATA
 Comment: 
 
-Filename: nucliadb_utils-4.0.1.post552.dist-info/WHEEL
+Filename: nucliadb_utils-4.0.1.post553.dist-info/WHEEL
 Comment: 
 
-Filename: nucliadb_utils-4.0.1.post552.dist-info/top_level.txt
+Filename: nucliadb_utils-4.0.1.post553.dist-info/top_level.txt
 Comment: 
 
-Filename: nucliadb_utils-4.0.1.post552.dist-info/zip-safe
+Filename: nucliadb_utils-4.0.1.post553.dist-info/zip-safe
 Comment: 
 
-Filename: nucliadb_utils-4.0.1.post552.dist-info/RECORD
+Filename: nucliadb_utils-4.0.1.post553.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## nucliadb_utils/storages/gcs.py

```diff
@@ -22,15 +22,15 @@
 import asyncio
 import base64
 import json
 import socket
 from concurrent.futures import ThreadPoolExecutor
 from copy import deepcopy
 from datetime import datetime
-from typing import Any, AsyncGenerator, AsyncIterator, Dict, List, Optional
+from typing import AsyncGenerator, AsyncIterator, Dict, List, Optional
 from urllib.parse import quote_plus
 
 import aiohttp
 import aiohttp.client_exceptions
 import backoff  # type: ignore
 import google.auth.transport.requests  # type: ignore
 import yarl
@@ -43,15 +43,20 @@
 from nucliadb_utils.storages import CHUNK_SIZE
 from nucliadb_utils.storages.exceptions import (
     CouldNotCopyNotFound,
     CouldNotCreateBucket,
     InvalidOffset,
     ResumableUploadGone,
 )
-from nucliadb_utils.storages.storage import Storage, StorageField
+from nucliadb_utils.storages.storage import (
+    ObjectInfo,
+    ObjectMetadata,
+    Storage,
+    StorageField,
+)
 
 storage_ops_observer = metrics.Observer("gcs_ops", labels={"type": ""})
 
 
 def strip_query_params(url: yarl.URL) -> str:
     return str(url.with_query(None))
 
@@ -407,15 +412,15 @@
     @backoff.on_exception(
         backoff.expo,
         RETRIABLE_EXCEPTIONS,
         jitter=backoff.random_jitter,
         max_tries=MAX_TRIES,
     )
     @storage_ops_observer.wrap({"type": "exists"})
-    async def exists(self) -> Optional[Dict[str, str]]:
+    async def exists(self) -> Optional[ObjectMetadata]:
         """
         Existence can be checked either with a CloudFile data in the field attribute
         or own StorageField key and bucket. Field takes precendece
         """
         if self.storage.session is None:
             raise AttributeError()
         key = None
@@ -434,24 +439,26 @@
             bucket,
             quote_plus(key),
         )
         headers = await self.storage.get_access_headers()
         async with self.storage.session.get(url, headers=headers) as api_resp:
             if api_resp.status == 200:
                 data = await api_resp.json()
-                metadata = data.get("metadata")
-                if metadata is None:
-                    metadata = {}
-                if metadata.get("SIZE") is None:
-                    metadata["SIZE"] = data.get("size")
-                if metadata.get("CONTENT_TYPE") is None:
-                    metadata["CONTENT_TYPE"] = data.get("contentType")
-                if metadata.get("FILENAME") is None:
-                    metadata["FILENAME"] = key.split("/")[-1]
-                return metadata
+                metadata = data.get("metadata") or {}
+                metadata = {k.lower(): v for k, v in metadata.items()}
+                size = metadata.get("size") or data.get("size") or 0
+                content_type = (
+                    metadata.get("content_type") or data.get("contentType") or ""
+                )
+                filename = metadata.get("filename") or key.split("/")[-1]
+                return ObjectMetadata(
+                    filename=filename,
+                    size=int(size),
+                    content_type=content_type,
+                )
             else:
                 return None
 
     async def upload(self, iterator: AsyncIterator, origin: CloudFile) -> CloudFile:
         self.field = await self.start(origin)
         if self.field is None:
             raise AttributeError()
@@ -714,38 +721,40 @@
                 logger.error(msg, extra={"kbid": kbid})
                 with errors.push_scope() as scope:
                     scope.set_extra("kbid", kbid)
                     scope.set_extra("status_code", resp.status)
                     errors.capture_message(msg, "error", scope)
         return deleted, conflict
 
-    async def iterate_bucket(self, bucket: str, prefix: str) -> AsyncIterator[Any]:
+    async def iterate_objects(
+        self, bucket: str, prefix: str
+    ) -> AsyncGenerator[ObjectInfo, None]:
         if self.session is None:
             raise AttributeError()
         url = "{}/{}/o".format(self.object_base_url, bucket)
         headers = await self.get_access_headers()
         async with self.session.get(
             url,
             headers=headers,
             params={"prefix": prefix},
         ) as resp:
             assert resp.status == 200
             data = await resp.json()
             if "items" in data:
                 for item in data["items"]:
-                    yield item
+                    yield ObjectInfo(name=item["name"])
 
         page_token = data.get("nextPageToken")
         while page_token is not None:
             headers = await self.get_access_headers()
             async with self.session.get(
                 url,
                 headers=headers,
                 params={"prefix": prefix, "pageToken": page_token},
             ) as resp:
                 data = await resp.json()
                 items = data.get("items", [])
                 if len(items) == 0:
                     break
                 for item in items:
-                    yield item
+                    yield ObjectInfo(name=item["name"])
                 page_token = data.get("nextPageToken")
```

## nucliadb_utils/storages/local.py

```diff
@@ -20,21 +20,26 @@
 from __future__ import annotations
 
 import glob
 import json
 import os
 import shutil
 from datetime import datetime
-from typing import Any, AsyncGenerator, AsyncIterator, Dict, Optional
+from typing import AsyncGenerator, AsyncIterator, Dict, Optional
 
 import aiofiles
 from nucliadb_protos.resources_pb2 import CloudFile
 
 from nucliadb_utils.storages import CHUNK_SIZE
-from nucliadb_utils.storages.storage import Storage, StorageField
+from nucliadb_utils.storages.storage import (
+    ObjectInfo,
+    ObjectMetadata,
+    Storage,
+    StorageField,
+)
 
 
 class LocalStorageField(StorageField):
     storage: LocalStorage
     _handler = None
 
     def metadata_key(self, uri: Optional[str] = None):
@@ -138,22 +143,24 @@
                 bucket_name=self.bucket,
                 source=CloudFile.LOCAL,
             )
             upload_uri = self.key
 
         init_url = self.storage.get_file_path(self.bucket, upload_uri)
         metadata_init_url = self.metadata_key(init_url)
-        metadata = json.dumps(
-            {"FILENAME": cf.filename, "SIZE": cf.size, "CONTENT_TYPE": cf.content_type}
+        object_metadata = ObjectMetadata(
+            filename=cf.filename,
+            content_type=cf.content_type,
+            size=cf.size,
         )
-
+        raw_metadata = json.dumps(object_metadata.model_dump())
         path_to_create = os.path.dirname(metadata_init_url)
         os.makedirs(path_to_create, exist_ok=True)
         async with aiofiles.open(metadata_init_url, "w+") as resp:
-            await resp.write(metadata)
+            await resp.write(raw_metadata)
 
         self._handler = await aiofiles.threadpool.open(init_url, "wb+")
         field.offset = 0
         field.upload_uri = upload_uri
         return field
 
     async def _append(self, cf: CloudFile, data: bytes):
@@ -186,20 +193,23 @@
             )
 
         await self._handler.close()
         self.field.uri = self.key
         self.field.ClearField("offset")
         self.field.ClearField("upload_uri")
 
-    async def exists(self) -> Optional[Dict[str, str]]:
+    async def exists(self) -> Optional[ObjectMetadata]:
         file_path = self.storage.get_file_path(self.bucket, self.key)
         metadata_path = self.metadata_key(file_path)
         if os.path.exists(metadata_path):
             async with aiofiles.open(metadata_path, "r") as metadata:
-                return json.loads(await metadata.read())
+                raw_metadata = await metadata.read()
+                metadata_dict = json.loads(raw_metadata)
+                metadata_dict = {k.lower(): v for k, v in metadata_dict.items()}
+                return ObjectMetadata.model_validate(metadata_dict)
         return None
 
     async def upload(self, iterator: AsyncIterator, origin: CloudFile) -> CloudFile:
         self.field = await self.start(origin)
         if self.field is None:
             raise AttributeError()
         await self.append(origin, iterator)
@@ -265,18 +275,19 @@
         try:
             shutil.rmtree(path)
             deleted = True
         except Exception:
             deleted = False
         return deleted
 
-    async def iterate_bucket(self, bucket: str, prefix: str) -> AsyncIterator[Any]:
+    async def iterate_objects(
+        self, bucket: str, prefix: str
+    ) -> AsyncGenerator[ObjectInfo, None]:
         for key in glob.glob(f"{bucket}/{prefix}*"):
-            item = {"name": key}
-            yield item
+            yield ObjectInfo(name=key)
 
     async def download(
         self, bucket_name: str, key: str, headers: Optional[Dict[str, str]] = None
     ):
         key_path = self.get_file_path(bucket_name, key)
         if not os.path.exists(key_path):
             return
```

## nucliadb_utils/storages/pg.py

```diff
@@ -24,15 +24,20 @@
 import uuid
 from typing import Any, AsyncGenerator, AsyncIterator, Optional, TypedDict
 
 import asyncpg
 from nucliadb_protos.resources_pb2 import CloudFile
 
 from nucliadb_utils.storages import CHUNK_SIZE
-from nucliadb_utils.storages.storage import Storage, StorageField
+from nucliadb_utils.storages.storage import (
+    ObjectInfo,
+    ObjectMetadata,
+    Storage,
+    StorageField,
+)
 
 logger = logging.getLogger(__name__)
 
 # Table design notes
 # - No foreign key constraints ON PURPOSE
 # - No cascade handling ON PURPOSE
 CREATE_TABLE = """
@@ -284,15 +289,15 @@
                 size=chunk["size"],
             )
             for chunk in chunks
         ]
 
     async def iterate_kb(
         self, bucket: str, prefix: Optional[str] = None
-    ) -> AsyncIterator[FileInfo]:
+    ) -> AsyncGenerator[FileInfo, None]:
         query = """
 SELECT filename, size, content_type, file_id
 FROM kb_files
 WHERE kb_id = $1
 """
         args: list[Any] = [bucket]
         if prefix:
@@ -513,18 +518,25 @@
                     )
                     raise
 
         self.field.uri = self.key
         self.field.ClearField("offset")
         self.field.ClearField("upload_uri")
 
-    async def exists(self) -> Optional[FileInfo]:  # type:ignore
+    async def exists(self) -> Optional[ObjectMetadata]:
         async with self.storage.pool.acquire() as conn:
             dl = PostgresFileDataLayer(conn)
-            return await dl.get_file_info(self.bucket, self.key)
+            file_info = await dl.get_file_info(self.bucket, self.key)
+            if file_info is None:
+                return None
+            return ObjectMetadata(
+                filename=file_info["filename"],
+                size=file_info["size"],
+                content_type=file_info["content_type"],
+            )
 
     async def upload(self, iterator: AsyncIterator, origin: CloudFile) -> CloudFile:
         self.field = await self.start(origin)
         await self.append(origin, iterator)
         await self.finish()
         return self.field
 
@@ -584,19 +596,21 @@
             dl = PostgresFileDataLayer(conn)
             await dl.delete_file(bucket_name, uri)
 
     async def schedule_delete_kb(self, kbid: str) -> bool:
         await self.delete_kb(kbid)
         return True
 
-    async def iterate_bucket(self, bucket: str, prefix: str) -> AsyncIterator[Any]:
+    async def iterate_objects(
+        self, bucket: str, prefix: str
+    ) -> AsyncGenerator[ObjectInfo, None]:
         async with self.pool.acquire() as conn:
             dl = PostgresFileDataLayer(conn)
             async for file_data in dl.iterate_kb(bucket, prefix):
-                yield {"name": file_data["key"]}
+                yield ObjectInfo(name=file_data["key"])
 
     async def download(
         self, bucket_name: str, key: str, headers: Optional[dict[str, str]] = None
     ) -> AsyncIterator[bytes]:
         async with self.pool.acquire() as conn:
             dl = PostgresFileDataLayer(conn)
             async for chunk in dl.iterate_chunks(bucket_name, key):
```

## nucliadb_utils/storages/s3.py

```diff
@@ -17,28 +17,33 @@
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 from __future__ import annotations
 
 from contextlib import AsyncExitStack
 from datetime import datetime
-from typing import Any, AsyncGenerator, AsyncIterator, Optional
+from typing import AsyncGenerator, AsyncIterator, Optional
 
 import aiobotocore  # type: ignore
 import aiohttp
 import backoff  # type: ignore
 import botocore  # type: ignore
 from aiobotocore.client import AioBaseClient  # type: ignore
 from aiobotocore.session import AioSession, get_session  # type: ignore
 from nucliadb_protos.resources_pb2 import CloudFile
 
 from nucliadb_telemetry import errors
 from nucliadb_utils import logger
 from nucliadb_utils.storages.exceptions import UnparsableResponse
-from nucliadb_utils.storages.storage import Storage, StorageField
+from nucliadb_utils.storages.storage import (
+    ObjectInfo,
+    ObjectMetadata,
+    Storage,
+    StorageField,
+)
 
 MB = 1024 * 1024
 MIN_UPLOAD_SIZE = 5 * MB
 CHUNK_SIZE = MIN_UPLOAD_SIZE
 MAX_TRIES = 3
 
 RETRIABLE_EXCEPTIONS = (
@@ -268,15 +273,15 @@
         await self.storage._s3aioclient.complete_multipart_upload(
             Bucket=self.field.bucket_name,
             Key=self.field.upload_uri,
             UploadId=self.field.resumable_uri,
             MultipartUpload=part_info,
         )
 
-    async def exists(self):
+    async def exists(self) -> Optional[ObjectMetadata]:
         """
         Existence can be checked either with a CloudFile data in the field attribute
         or own StorageField key and bucket. Field takes precendece
         """
 
         key = None
         bucket = None
@@ -288,21 +293,23 @@
             bucket = self.bucket
         else:
             return None
 
         try:
             obj = await self.storage._s3aioclient.head_object(Bucket=bucket, Key=key)
             if obj is not None:
-                metadata = obj.get("Metadata", {})
-                return {
-                    "SIZE": metadata.get("size") or obj.get("ContentLength"),
-                    "CONTENT_TYPE": metadata.get("content_type")
-                    or obj.get("ContentType"),
-                    "FILENAME": metadata.get("filename") or key.split("/")[-1],
-                }
+                metadata = obj.get("Metadata") or {}
+                size = metadata.get("size") or obj.get("ContentLength") or 0
+                content_type = (
+                    metadata.get("content_type") or obj.get("ContentType") or ""
+                )
+                filename = metadata.get("filename") or key.split("/")[-1]
+                return ObjectMetadata(
+                    size=int(size), content_type=content_type, filename=filename
+                )
             else:
                 return None
         except botocore.exceptions.ClientError as e:
             error_code = parse_status_code(e)
             if error_code == 404:
                 return None
             raise
@@ -420,22 +427,21 @@
             try:
                 await self._s3aioclient.delete_object(Bucket=bucket_name, Key=uri)
             except botocore.exceptions.ClientError:
                 logger.warning("Error deleting object", exc_info=True)
         else:
             raise AttributeError("No valid uri")
 
-    async def iterate_bucket(
+    async def iterate_objects(
         self, bucket: str, prefix: str = "/"
-    ) -> AsyncIterator[Any]:
+    ) -> AsyncGenerator[ObjectInfo, None]:
         paginator = self._s3aioclient.get_paginator("list_objects")
         async for result in paginator.paginate(Bucket=bucket, Prefix=prefix):
             for item in result.get("Contents", []):
-                item["name"] = item["Key"]
-                yield item
+                yield ObjectInfo(name=item["Key"])
 
     async def create_kb(self, kbid: str):
         bucket_name = self.get_bucket_name(kbid)
         return await self._create_bucket_if_not_exists(bucket_name)
 
     async def bucket_exists(self, bucket_name: str) -> bool:
         return await bucket_exists(self._s3aioclient, bucket_name)
```

## nucliadb_utils/storages/storage.py

```diff
@@ -36,14 +36,15 @@
     cast,
 )
 
 from nucliadb_protos.noderesources_pb2 import Resource as BrainResource
 from nucliadb_protos.nodewriter_pb2 import IndexMessage
 from nucliadb_protos.resources_pb2 import CloudFile
 from nucliadb_protos.writer_pb2 import BrokerMessage
+from pydantic import BaseModel
 
 from nucliadb_utils import logger
 from nucliadb_utils.helpers import async_gen_lookahead
 from nucliadb_utils.storages import CHUNK_SIZE
 from nucliadb_utils.storages.exceptions import IndexDataNotFound, InvalidCloudFile
 from nucliadb_utils.utilities import get_local_storage, get_nuclia_storage
 
@@ -56,14 +57,24 @@
 DEADLETTER = "deadletter/{partition}/{seqid}/{seq}"
 OLD_INDEXING_KEY = "index/{node}/{shard}/{txid}"
 INDEXING_KEY = "index/{kb}/{shard}/{resource}/{txid}"
 # temporary storage for large stream data
 MESSAGE_KEY = "message/{kbid}/{rid}/{mid}"
 
 
+class ObjectInfo(BaseModel):
+    name: str
+
+
+class ObjectMetadata(BaseModel):
+    filename: str
+    content_type: str
+    size: int
+
+
 class StorageField(abc.ABC, metaclass=abc.ABCMeta):
     storage: Storage
     bucket: str
     key: str
     field: Optional[CloudFile] = None
 
     def __init__(
@@ -95,15 +106,15 @@
         deleted = False
         if self.field is not None:
             await self.storage.delete_upload(self.field.uri, self.bucket)
             deleted = True
         return deleted
 
     @abc.abstractmethod
-    async def exists(self) -> Optional[Dict[str, str]]: ...
+    async def exists(self) -> Optional[ObjectMetadata]: ...
 
     @abc.abstractmethod
     async def copy(
         self,
         origin_uri: str,
         destination_uri: str,
         origin_bucket_name: str,
@@ -137,18 +148,18 @@
     cached_buckets: List[str] = []
     chunk_size = CHUNK_SIZE
 
     async def delete_resource(self, kbid: str, uuid: str):
         # Delete all keys inside a resource
         bucket = self.get_bucket_name(kbid)
         resource_storage_base_path = STORAGE_RESOURCE.format(kbid=kbid, uuid=uuid)
-        async for bucket_info in self.iterate_bucket(
+        async for object_info in self.iterate_objects(
             bucket, resource_storage_base_path
         ):
-            await self.delete_upload(bucket_info["name"], bucket)
+            await self.delete_upload(object_info.name, bucket)
 
     async def deadletter(
         self, message: BrokerMessage, seq: int, seqid: int, partition: str
     ):
         if self.deadletter_bucket is None:
             logger.error("No Deadletter Bucket defined will not store the error")
             return
@@ -282,15 +293,15 @@
             new_cf = CloudFile()
             new_cf.CopyFrom(file)
             new_cf.bucket_name = destination.bucket
             new_cf.uri = destination.key
         elif file.source == self.source:
             # This is the case for NucliaDB hosted deployment (Nuclia's cloud deployment):
             # The data is already stored in the right place by the processing
-            logger.debug(f"[Nuclia hosted]")
+            logger.debug("[Nuclia hosted]")
             return file
         elif file.source == CloudFile.EXPORT:
             # This is for files coming from an export
             logger.debug(f"[Exported file]: {file.uri}")
             new_cf = CloudFile()
             new_cf.CopyFrom(file)
             new_cf.bucket_name = destination.bucket
@@ -507,15 +518,19 @@
     @abc.abstractmethod
     async def initialize(self) -> None: ...
 
     @abc.abstractmethod
     async def finalize(self) -> None: ...
 
     @abc.abstractmethod
-    def iterate_bucket(self, bucket: str, prefix: str) -> AsyncIterator[Any]: ...
+    async def iterate_objects(
+        self, bucket: str, prefix: str
+    ) -> AsyncGenerator[ObjectInfo, None]:
+        raise NotImplementedError()
+        yield ObjectInfo(name="")
 
     async def copy(self, file: CloudFile, destination: StorageField) -> None:
         await destination.copy(
             file.uri, destination.key, file.bucket_name, destination.bucket
         )
 
     async def move(self, file: CloudFile, destination: StorageField) -> None:
```

## nucliadb_utils/tests/unit/storages/test_pg.py

```diff
@@ -519,15 +519,15 @@
             ]
         )
 
     async def test_delete_upload(self, storage: pg.PostgresStorage, connection):
         await storage.delete_upload("file_id", "kb_id")
         connection.execute.assert_awaited_with(ANY, "kb_id", "file_id")
 
-    async def test_iterate_bucket(self, storage: pg.PostgresStorage, connection):
+    async def test_iterate_objects(self, storage: pg.PostgresStorage, connection):
         connection.cursor = MagicMock(
             return_value=iter_result(
                 [
                     {
                         "file_id": "file_id1",
                         "filename": "filename",
                         "size": 1,
@@ -539,19 +539,18 @@
                         "size": 1,
                         "content_type": "content_type",
                     },
                 ]
             )
         )
 
-        chunks = []
-        async for chunk in storage.iterate_bucket("kb_id", "file_id"):
-            chunks.append(chunk)
-
-        assert chunks == [{"name": "file_id1"}, {"name": "file_id2"}]
+        object_names = []
+        async for object_info in storage.iterate_objects("kb_id", "file_id"):
+            object_names.append(object_info.name)
+        assert object_names == ["file_id1", "file_id2"]
 
     async def test_download(
         self, storage: pg.PostgresStorage, connection, chunk_info, chunk_data
     ):
         connection.fetch.return_value = chunk_info
         connection.fetchrow.side_effect = chunk_data
```

## nucliadb_utils/tests/unit/storages/test_storage.py

```diff
@@ -24,14 +24,15 @@
 from nucliadb_protos.noderesources_pb2 import Resource as BrainResource
 from nucliadb_protos.noderesources_pb2 import ResourceID
 from nucliadb_protos.nodewriter_pb2 import IndexMessage
 from nucliadb_protos.resources_pb2 import CloudFile
 
 from nucliadb_utils.storages.local import LocalStorageField
 from nucliadb_utils.storages.storage import (
+    ObjectInfo,
     Storage,
     StorageField,
     iter_and_add_size,
     iter_in_chunk_size,
 )
 
 
@@ -63,16 +64,16 @@
         self.delete_upload = AsyncMock()
         self.uploadbytes = AsyncMock()
         self.move = AsyncMock()
 
     def get_bucket_name(self, kbid):
         return "bucket"
 
-    async def iterate_bucket(self, bucket_name, prefix):
-        yield {"name": "uri"}
+    async def iterate_objects(self, bucket_name, prefix):
+        yield ObjectInfo(name="uri")
 
     async def download(self, bucket_name, uri):
         br = BrainResource(labels=["label"])
         yield br.SerializeToString()
 
     async def create_kb(self, kbid):
         return True
```

## Comparing `nucliadb_utils-4.0.1.post552.dist-info/METADATA` & `nucliadb_utils-4.0.1.post553.dist-info/METADATA`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: nucliadb_utils
-Version: 4.0.1.post552
+Version: 4.0.1.post553
 Home-page: https://nuclia.com
 License: BSD
 Classifier: Development Status :: 4 - Beta
 Classifier: Programming Language :: Python
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Programming Language :: Python :: 3.10
 Classifier: Programming Language :: Python :: 3.11
@@ -19,16 +19,16 @@
 Requires-Dist: types-requests >=2.27.7
 Requires-Dist: mmh3 >=3.0.0
 Requires-Dist: nats-py[nkeys] >=2.6.0
 Requires-Dist: PyNaCl
 Requires-Dist: pyjwt >=2.4.0
 Requires-Dist: memorylru >=1.1.2
 Requires-Dist: mrflagly
-Requires-Dist: nucliadb-protos >=4.0.1.post552
-Requires-Dist: nucliadb-telemetry >=4.0.1.post552
+Requires-Dist: nucliadb-protos >=4.0.1.post553
+Requires-Dist: nucliadb-telemetry >=4.0.1.post553
 Provides-Extra: cache
 Requires-Dist: redis >=4.3.4 ; extra == 'cache'
 Requires-Dist: orjson >=3.6.7 ; extra == 'cache'
 Requires-Dist: lru-dict >=1.1.7 ; extra == 'cache'
 Provides-Extra: fastapi
 Requires-Dist: fastapi >=0.95.2 ; extra == 'fastapi'
 Requires-Dist: uvicorn <0.19.0,>=0.16.0 ; extra == 'fastapi'
```

## Comparing `nucliadb_utils-4.0.1.post552.dist-info/RECORD` & `nucliadb_utils-4.0.1.post553.dist-info/RECORD`

 * *Files 3% similar despite different names*

```diff
@@ -36,21 +36,21 @@
 nucliadb_utils/nuclia_usage/protos/kb_usage_pb2.pyi,sha256=xhyc3jJBh0KZuWcgmIbwSkWYfoUKHDC8Fqa7GBjwkmE,15983
 nucliadb_utils/nuclia_usage/protos/kb_usage_pb2_grpc.py,sha256=dhop8WwjplPfORYPYb9HtcS9gHMzqXPJQGqXYRjV-6M,1008
 nucliadb_utils/nuclia_usage/protos/kb_usage_pb2_grpc.pyi,sha256=6RIsZ2934iodEckflpBStgLKEkFhKfNmZ72UKg2Bwb4,911
 nucliadb_utils/nuclia_usage/utils/__init__.py,sha256=cp15ZcFnHvpcu_5-aK2A4uUyvuZVV_MJn4bIXMa20ks,835
 nucliadb_utils/nuclia_usage/utils/kb_usage_report.py,sha256=E1eUSFXBVNzQP9Q2rWj9y3koCO5S7iKwckny_AoLKuk,3870
 nucliadb_utils/storages/__init__.py,sha256=5Qc8AUWiJv9_JbGCBpAn88AIJhwDlm0OPQpg2ZdRL4U,872
 nucliadb_utils/storages/exceptions.py,sha256=n6aBOyurWMo8mXd1XY6Psgno4VfXJ9TRbxCy67c08-g,2417
-nucliadb_utils/storages/gcs.py,sha256=3A4eqJe6PoF3oE7c9JSCpRtbcv6dtg6Pd28u4cXKwyE,27099
-nucliadb_utils/storages/local.py,sha256=jgUn3AwaQWn5VBKAvikd97sLAS3jx9PaUE7pwq1iJrk,10097
+nucliadb_utils/storages/gcs.py,sha256=krBkNd7wkHhfIn3T-4QvYu1Rw-envYCa6G4G90oOjvM,27303
+nucliadb_utils/storages/local.py,sha256=JewYQ-fes9iUtUjlbHgWXrG1RsQWh16TJDunJnwfbTg,10447
 nucliadb_utils/storages/nuclia.py,sha256=UfvRu92eqG1v-PE-UWH2x8KEJFqDqATMmUGFmEuqSSs,2097
-nucliadb_utils/storages/pg.py,sha256=yFk6AVgZHPgQq6NwLN_qN7fwD05WgCU5XE7gsFt-B0Q,18633
-nucliadb_utils/storages/s3.py,sha256=8IZoDlTeICZtU1Z0eouaxvafSR6y4GqCtLjFUBxTd1E,19269
+nucliadb_utils/storages/pg.py,sha256=DxXNwcstAFOTC6kaXlWp-b4WrvR8aSSOfgVJNDQ5oDI,18976
+nucliadb_utils/storages/s3.py,sha256=f2bjgmT6JRlUr5DHy3tRUip4kYSA1MzXfYrLNVUp_Cg,19447
 nucliadb_utils/storages/settings.py,sha256=ugCPy1zxBOmA2KosT-4tsjpvP002kg5iQyi42yCGCJA,1285
-nucliadb_utils/storages/storage.py,sha256=lrXa6eWY7HMCUgrEz9-jTk7PBPwP1Nor_zi3oZMiVZ4,20322
+nucliadb_utils/storages/storage.py,sha256=sR2Qvev6eLUvbH1WTXjqXIOnKRy1YMMx6Vsj0wZ2x8A,20585
 nucliadb_utils/tests/__init__.py,sha256=Oo9CAE7B0eW5VHn8sHd6o30SQzOWUhktLPRXdlDOleA,1456
 nucliadb_utils/tests/asyncbenchmark.py,sha256=rN_NNDk4ras0qgFp0QlRyAi9ZU9xITdzxl2s5CigzBo,10698
 nucliadb_utils/tests/conftest.py,sha256=gPYVuVhj_e6Aeanb91wvUerwuxZgaS7d3luIBRQFIU0,1876
 nucliadb_utils/tests/gcs.py,sha256=1dbt_zG3uZPZDF3Nyrgrvi_bsKmafAUOm4Pu4bzt7wI,3098
 nucliadb_utils/tests/indexing.py,sha256=YW2QhkhO9Q_8A4kKWJaWSvXvyQ_AiAwY1VylcfVQFxk,1513
 nucliadb_utils/tests/local.py,sha256=c3gZJJWmvOftruJkIQIwB3q_hh3uxEhqGIAVWim1Bbk,1343
 nucliadb_utils/tests/nats.py,sha256=lgRe6YH9LSoI7XgcyKAC2VTSAtuu8EeMve0jWWC_kOY,7701
@@ -65,14 +65,14 @@
 nucliadb_utils/tests/unit/test_signals.py,sha256=Br3BjBZpAtKPqeSmfHSGf1Seu4mB2OBWFCgLcNEH0PQ,2647
 nucliadb_utils/tests/unit/test_tests.py,sha256=-YHgVMKJr_3WYwrBj9TnwpVLIkP80PLZpWnvAIYvnz8,1189
 nucliadb_utils/tests/unit/test_transaction.py,sha256=ULVALWbMWEkH2Gq5Q8olEL8k8Kqyh5RAe0Lp9qdF1V4,3917
 nucliadb_utils/tests/unit/test_utilities.py,sha256=KcHSPp3RZyKAnscJrIwc2M3PCD3l1DWb6WuqtI6cl5o,5377
 nucliadb_utils/tests/unit/storages/__init__.py,sha256=itSI7dtTwFP55YMX4iK7JzdMHS5CQVUiB1XzQu4UBh8,833
 nucliadb_utils/tests/unit/storages/test_aws.py,sha256=GCsB_jwCUNV3Ogt8TZZEmNKAHvOlR0HGU7blrFbtJqs,1924
 nucliadb_utils/tests/unit/storages/test_gcs.py,sha256=2XzJwgNpfjVGjtE-QdZhu3ayuT1EMEXINdM-_SatPCY,3554
-nucliadb_utils/tests/unit/storages/test_pg.py,sha256=sJfUttMSzq8W1XYolAUcMxl_R5HcEzb5fpCklPeMJiY,17000
-nucliadb_utils/tests/unit/storages/test_storage.py,sha256=54nUtElPwT3GQPTT4638F2awBYRl8nD6me9wWijhtbA,7107
-nucliadb_utils-4.0.1.post552.dist-info/METADATA,sha256=vpP4LjOwypL6nmadjZg8BouwRLZGF3gaR1PavmNTRzs,2030
-nucliadb_utils-4.0.1.post552.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
-nucliadb_utils-4.0.1.post552.dist-info/top_level.txt,sha256=fE3vJtALTfgh7bcAWcNhcfXkNPp_eVVpbKK-2IYua3E,15
-nucliadb_utils-4.0.1.post552.dist-info/zip-safe,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
-nucliadb_utils-4.0.1.post552.dist-info/RECORD,,
+nucliadb_utils/tests/unit/storages/test_pg.py,sha256=hOR8WSvnuRJKH_rD2vcSMDOJqs7EndJqaPNrTENBOy8,17016
+nucliadb_utils/tests/unit/storages/test_storage.py,sha256=OT21FUnQTU2oxxaFfVC1LMkJ34jyW7Leit5ZCQgNyZ0,7131
+nucliadb_utils-4.0.1.post553.dist-info/METADATA,sha256=ZtL1febvSlU2s6HaxeRuUpMHxQ7Sn4g7vQjLuaRhSUk,2030
+nucliadb_utils-4.0.1.post553.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+nucliadb_utils-4.0.1.post553.dist-info/top_level.txt,sha256=fE3vJtALTfgh7bcAWcNhcfXkNPp_eVVpbKK-2IYua3E,15
+nucliadb_utils-4.0.1.post553.dist-info/zip-safe,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
+nucliadb_utils-4.0.1.post553.dist-info/RECORD,,
```

