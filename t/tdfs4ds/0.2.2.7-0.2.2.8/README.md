# Comparing `tmp/tdfs4ds-0.2.2.7-py3-none-any.whl.zip` & `tmp/tdfs4ds-0.2.2.8-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,32 +1,33 @@
-Zip file size: 155542 bytes, number of entries: 30
+Zip file size: 158398 bytes, number of entries: 31
 -rw-rw-rw-  2.0 fat       23 b- defN 23-Jul-05 10:56 tdfs/__init__.py
 -rw-rw-rw-  2.0 fat      985 b- defN 23-Jul-05 13:45 tdfs/datasets.py
 -rw-rw-rw-  2.0 fat    31354 b- defN 23-Jul-05 14:47 tdfs/feature_store.py
 -rw-rw-rw-  2.0 fat   112488 b- defN 23-Jul-05 13:43 tdfs/data/curves.csv
--rw-rw-rw-  2.0 fat    44572 b- defN 24-Mar-26 20:48 tdfs4ds/__init__.py
+-rw-rw-rw-  2.0 fat    46622 b- defN 24-Mar-27 13:50 tdfs4ds/__init__.py
 -rw-rw-rw-  2.0 fat     3524 b- defN 24-Feb-09 07:20 tdfs4ds/datasets.py
 -rw-rw-rw-  2.0 fat     7081 b- defN 23-Nov-08 07:57 tdfs4ds/feature_engineering.py
 -rw-rw-rw-  2.0 fat    68576 b- defN 24-Jan-18 13:59 tdfs4ds/feature_store.py
 -rw-rw-rw-  2.0 fat    16939 b- defN 24-Jan-15 13:37 tdfs4ds/process_store.py
 -rw-rw-rw-  2.0 fat    23312 b- defN 24-Jan-29 09:10 tdfs4ds/utils.py
 -rw-rw-rw-  2.0 fat   112488 b- defN 23-Jul-05 13:43 tdfs4ds/data/curves.csv
 -rw-rw-rw-  2.0 fat      209 b- defN 24-Feb-09 10:12 tdfs4ds/feature_store/__init__.py
 -rw-rw-rw-  2.0 fat     9609 b- defN 24-Mar-26 09:22 tdfs4ds/feature_store/entity_management.py
 -rw-rw-rw-  2.0 fat    26340 b- defN 24-Mar-19 12:09 tdfs4ds/feature_store/feature_data_processing.py
 -rw-rw-rw-  2.0 fat    25246 b- defN 24-Mar-26 09:49 tdfs4ds/feature_store/feature_query_retrieval.py
 -rw-rw-rw-  2.0 fat    38568 b- defN 24-Mar-26 20:53 tdfs4ds/feature_store/feature_store_management.py
 -rw-rw-rw-  2.0 fat      182 b- defN 24-Feb-09 10:13 tdfs4ds/process_store/__init__.py
--rw-rw-rw-  2.0 fat     6217 b- defN 24-Feb-06 14:19 tdfs4ds/process_store/process_query_administration.py
--rw-rw-rw-  2.0 fat    12641 b- defN 24-Mar-25 19:03 tdfs4ds/process_store/process_registration_management.py
--rw-rw-rw-  2.0 fat     7190 b- defN 24-Mar-25 16:59 tdfs4ds/process_store/process_store_catalog_management.py
+-rw-rw-rw-  2.0 fat     6722 b- defN 24-Mar-27 13:37 tdfs4ds/process_store/process_query_administration.py
+-rw-rw-rw-  2.0 fat    14892 b- defN 24-Mar-27 09:41 tdfs4ds/process_store/process_registration_management.py
+-rw-rw-rw-  2.0 fat     9405 b- defN 24-Mar-27 09:26 tdfs4ds/process_store/process_store_catalog_management.py
 -rw-rw-rw-  2.0 fat      168 b- defN 24-Feb-09 10:14 tdfs4ds/utils/__init__.py
+-rw-rw-rw-  2.0 fat     5974 b- defN 24-Mar-27 14:01 tdfs4ds/utils/filter_management.py
 -rw-rw-rw-  2.0 fat     4034 b- defN 24-Mar-08 10:55 tdfs4ds/utils/info.py
 -rw-rw-rw-  2.0 fat    22173 b- defN 24-Feb-15 17:45 tdfs4ds/utils/lineage.py
 -rw-rw-rw-  2.0 fat     6356 b- defN 24-Feb-06 14:39 tdfs4ds/utils/query_management.py
 -rw-rw-rw-  2.0 fat     6459 b- defN 24-Mar-26 16:01 tdfs4ds/utils/time_management.py
 -rw-rw-rw-  2.0 fat    22812 b- defN 24-Feb-09 08:28 tdfs4ds/utils/visualization.py
--rw-rw-rw-  2.0 fat    11708 b- defN 24-Mar-26 21:06 tdfs4ds-0.2.2.7.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 24-Mar-26 21:06 tdfs4ds-0.2.2.7.dist-info/WHEEL
--rw-rw-rw-  2.0 fat        8 b- defN 24-Mar-26 21:06 tdfs4ds-0.2.2.7.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     2609 b- defN 24-Mar-26 21:06 tdfs4ds-0.2.2.7.dist-info/RECORD
-30 files, 623963 bytes uncompressed, 151344 bytes compressed:  75.7%
+-rw-rw-rw-  2.0 fat    11708 b- defN 24-Mar-27 14:18 tdfs4ds-0.2.2.8.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 24-Mar-27 14:18 tdfs4ds-0.2.2.8.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat        8 b- defN 24-Mar-27 14:18 tdfs4ds-0.2.2.8.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat     2700 b- defN 24-Mar-27 14:18 tdfs4ds-0.2.2.8.dist-info/RECORD
+31 files, 637049 bytes uncompressed, 154056 bytes compressed:  75.8%
```

## zipnote {}

```diff
@@ -57,14 +57,17 @@
 
 Filename: tdfs4ds/process_store/process_store_catalog_management.py
 Comment: 
 
 Filename: tdfs4ds/utils/__init__.py
 Comment: 
 
+Filename: tdfs4ds/utils/filter_management.py
+Comment: 
+
 Filename: tdfs4ds/utils/info.py
 Comment: 
 
 Filename: tdfs4ds/utils/lineage.py
 Comment: 
 
 Filename: tdfs4ds/utils/query_management.py
@@ -72,20 +75,20 @@
 
 Filename: tdfs4ds/utils/time_management.py
 Comment: 
 
 Filename: tdfs4ds/utils/visualization.py
 Comment: 
 
-Filename: tdfs4ds-0.2.2.7.dist-info/METADATA
+Filename: tdfs4ds-0.2.2.8.dist-info/METADATA
 Comment: 
 
-Filename: tdfs4ds-0.2.2.7.dist-info/WHEEL
+Filename: tdfs4ds-0.2.2.8.dist-info/WHEEL
 Comment: 
 
-Filename: tdfs4ds-0.2.2.7.dist-info/top_level.txt
+Filename: tdfs4ds-0.2.2.8.dist-info/top_level.txt
 Comment: 
 
-Filename: tdfs4ds-0.2.2.7.dist-info/RECORD
+Filename: tdfs4ds-0.2.2.8.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## tdfs4ds/__init__.py

```diff
@@ -1,28 +1,30 @@
-__version__ = '0.2.2.7'
+__version__ = '0.2.2.8'
 
 DATA_DOMAIN             = None
 SCHEMA                  = None
 FEATURE_CATALOG_NAME    = 'FS_FEATURE_CATALOG'
 PROCESS_CATALOG_NAME    = 'FS_PROCESS_CATALOG'
 DATA_DISTRIBUTION_NAME  = 'FS_DATA_DISTRIBUTION'
+FILTER_MANAGER_NAME     = 'FS_FILTER_MANAGER'
 
 END_PERIOD              = 'UNTIL_CHANGED' #'9999-01-01 00:00:00'
 FEATURE_STORE_TIME      = None
 FEATURE_VERSION_DEFAULT = 'dev.0.0'
 DISPLAY_LOGS            = True
 DEBUG_MODE              = False
 
 USE_VOLATILE_TABLE      = True
 STORE_FEATURE           = 'MERGE' #'UPDATE_INSERT'
 import warnings
 warnings.filterwarnings('ignore')
 
 import teradataml as tdml
 from tdfs4ds.utils.lineage import crystallize_view
+from tdfs4ds.utils.filter_management import FilterManager
 import tdfs4ds.feature_store
 import tdfs4ds.process_store
 import tdfs4ds.datasets
 
 import inspect
 import tqdm
 
@@ -215,29 +217,46 @@
 
     # Construct SQL query to retrieve process details by process ID
     query = f"""
     {validtime_statement}
     SEL 
         A.*,
         B.FOR_PRIMARY_INDEX,
-        CASE WHEN B.FOR_DATA_PARTITIONING IS NULL THEN '' ELSE B.FOR_DATA_PARTITIONING END AS FOR_DATA_PARTITIONING
+        CASE WHEN B.FOR_DATA_PARTITIONING IS NULL THEN '' ELSE B.FOR_DATA_PARTITIONING END AS FOR_DATA_PARTITIONING,
+        D.DATABASE_NAME AS FILTER_DATABASE_NAME,
+        D.VIEW_NAME AS FILTER_VIEW_NAME,
+        D.TABLE_NAME AS FILTER_TABLE_NAME
     FROM {tdfs4ds.SCHEMA}.{tdfs4ds.PROCESS_CATALOG_NAME} A
     LEFT JOIN {tdfs4ds.SCHEMA}.{tdfs4ds.DATA_DISTRIBUTION_NAME} B
     ON A.PROCESS_ID = B.PROCESS_ID
+    LEFT JOIN {tdfs4ds.SCHEMA}.{tdfs4ds.FILTER_MANAGER_NAME} D
+    ON A.PROCESS_ID = D.PROCESS_ID
     WHERE A.PROCESS_ID = '{process_id}' 
     """
 
     # Executing the query and converting the result to Pandas DataFrame
     df = tdml.DataFrame.from_query(query).to_pandas()
 
     # Check if exactly one record is returned, else print an error
     if df.shape[0] != 1:
         print('error - there is ', df.shape[0], f' records. Check table {tdfs4ds.SCHEMA}.{tdfs4ds.PROCESS_CATALOG_NAME}')
+        print('check ou this query:')
+        print(query)
         return
 
+
+    # Fetching the filter manager
+    filter_schema_name = df['FILTER_DATABASE_NAME'].values[0]
+    if filter_schema_name is None:
+        filtermanager = None
+    else:
+        filter_view_name = df['FILTER_VIEW_NAME'].values[0]
+        filter_table_name = df['FILTER_TABLE_NAME'].values[0]
+        filtermanager = FilterManager(table_name=filter_view_name, schema_name=filter_schema_name)
+
     # Fetching the process type from the query result
     process_type = df['PROCESS_TYPE'].values[0]
 
     # Fetching the primary index from the query result
     primary_index = df['FOR_PRIMARY_INDEX'].values[0]
     if primary_index is not None:
         primary_index = primary_index.split(',')
@@ -266,27 +285,28 @@
             print('run','partitioning',partitioning)
         dataset = _upload_features(
             df_data,
             entity_id,
             feature_names,
             feature_versions = process_id,
             primary_index = primary_index,
-            partitioning = partitioning
+            partitioning = partitioning,
+            filtermanager = filtermanager
         )
 
     # Handling 'tdstone2 view' process type
     elif process_type == 'tdstone2 view':
         print('not implemented yet')
 
     if return_dataset:
         return dataset
     else:
         return
 
-def upload_features(df, entity_id, feature_names, metadata={}, primary_index = None, partitioning = ''):
+def upload_features(df, entity_id, feature_names, metadata={}, primary_index = None, partitioning = '', filtermanager = None):
     """
     Uploads feature data from a DataFrame to the feature store for a specified entity. This involves registering the
     process in the feature store, executing the necessary SQL to insert the data, and returning the resulting dataset
     for further use or inspection.
 
     The function supports dynamic entity ID interpretation and flexible feature name handling, ensuring compatibility
     with various data schemas. It automatically registers the data upload process and applies additional metadata,
@@ -362,36 +382,43 @@
             feature_names = feature_names.split(',')
         else:
             feature_names = [feature_names]
         if tdfs4ds.DISPLAY_LOGS:
             print('it has been converted to : ', feature_names)
             print('check it is a expected.')
 
+    if tdfs4ds.DISPLAY_LOGS:
+        print("filtermanager", filtermanager)
+
     # Register the process and retrieve the SQL query to insert the features, and the process ID
-    query_insert, process_id, query_insert_dist = register_process_view.__wrapped__(
+    query_insert, process_id, query_insert_dist, query_insert_filtermanager = register_process_view.__wrapped__(
         view_name       = df,
         entity_id       = entity_id,
         feature_names   = feature_names,
         metadata        = metadata,
         with_process_id = True,
         primary_index   = primary_index,
-        partitioning    = partitioning
+        partitioning    = partitioning,
+        filtermanager   = filtermanager
     )
 
     # Execute the SQL query to insert the features into the database
     execute_query(query_insert)
     execute_query(query_insert_dist)
+    print("query_insert_filtermanager",query_insert_filtermanager)
+    if query_insert_filtermanager is not None:
+        execute_query(query_insert_filtermanager)
 
     # Run the registered process and return the resulting dataset
     dataset = run(process_id=process_id, return_dataset=True)
 
     return dataset
 
 def _upload_features(df, entity_id, feature_names,
-                   feature_versions=FEATURE_VERSION_DEFAULT, primary_index = None, partitioning = ''):
+                   feature_versions=FEATURE_VERSION_DEFAULT, primary_index = None, partitioning = '', filtermanager=None):
     """
     Uploads features from a DataFrame to the feature store, handling entity registration, feature type determination,
     feature registration, preparation for ingestion, and storage in the designated feature tables.
 
     Parameters:
     - df (DataFrame): The input DataFrame containing the feature data.
     - entity_id (str or dict): The identifier for the entity to which these features belong. This can be a single ID
@@ -456,31 +483,52 @@
     # Register the features in the feature catalog.
     register_features(
         entity_id,
         feature_names_types,
         primary_index,
         partitioning
     )
+    if tdfs4ds.DISPLAY_LOGS:
+        print("filtermanager",filtermanager)
+    if filtermanager is None:
+        # Prepare the features for ingestion.
+        prepared_features, volatile_table_name = prepare_feature_ingestion(
+            df,
+            entity_id,
+            feature_names,
+            feature_versions=selected_features
+        )
 
-    # Prepare the features for ingestion.
-    prepared_features, volatile_table_name = prepare_feature_ingestion(
-        df,
-        entity_id,
-        feature_names,
-        feature_versions=selected_features
-    )
-
-    # Store the prepared features in the feature store.
-    store_feature(
-        entity_id,
-        prepared_features
-    )
+        # Store the prepared features in the feature store.
+        store_feature(
+            entity_id,
+            prepared_features
+        )
+    else:
+        nb_filters = filtermanager.nb_filters
+        for i in range(nb_filters):
+            filtermanager.update(i+1)
+            if tdfs4ds.DISPLAY_LOGS:
+                print(filtermanager.display())
+            # Prepare the features for ingestion.
+            prepared_features, volatile_table_name = prepare_feature_ingestion(
+                df,
+                entity_id,
+                feature_names,
+                feature_versions=selected_features
+            )
+
+            # Store the prepared features in the feature store.
+            store_feature(
+                entity_id,
+                prepared_features
+            )
 
-    # Clean up by dropping the temporary volatile table.
-    tdml.execute_sql(f'DROP TABLE {volatile_table_name}')
+            # Clean up by dropping the temporary volatile table.
+            tdml.execute_sql(f'DROP TABLE {volatile_table_name}')
 
     # Build a dataset view in the feature store.
     dataset = build_dataset(
         entity_id,
         selected_features,
         view_name=None
     )
```

## tdfs4ds/process_store/process_query_administration.py

```diff
@@ -28,23 +28,32 @@
     else:
         validtime_statement = f"VALIDTIME AS OF TIMESTAMP '{tdfs4ds.FEATURE_STORE_TIME}'"
 
     # Constructing the SQL query to fetch process details
     query = f"""
     {validtime_statement}
     SELECT 
-        PROCESS_ID ,
-        PROCESS_TYPE ,
-        VIEW_NAME ,
-        ENTITY_ID ,
-        FEATURE_NAMES ,
-        FEATURE_VERSION AS PROCESS_VERSION,
-        DATA_DOMAIN,
-        METADATA
-    FROM {tdfs4ds.SCHEMA}.{tdfs4ds.PROCESS_CATALOG_NAME}
+        A.PROCESS_ID ,
+        A.PROCESS_TYPE ,
+        A.VIEW_NAME ,
+        A.ENTITY_ID ,
+        A.FEATURE_NAMES ,
+        A.FEATURE_VERSION AS PROCESS_VERSION,
+        A.DATA_DOMAIN,
+        A.METADATA,
+        B.FOR_PRIMARY_INDEX,
+        CASE WHEN B.FOR_DATA_PARTITIONING IS NULL THEN '' ELSE B.FOR_DATA_PARTITIONING END AS FOR_DATA_PARTITIONING,
+        D.DATABASE_NAME AS FILTER_DATABASE_NAME,
+        D.VIEW_NAME AS FILTER_VIEW_NAME,
+        D.TABLE_NAME AS FILTER_TABLE_NAME
+    FROM {tdfs4ds.SCHEMA}.{tdfs4ds.PROCESS_CATALOG_NAME} A
+    LEFT JOIN {tdfs4ds.SCHEMA}.{tdfs4ds.DATA_DISTRIBUTION_NAME} B
+    ON A.PROCESS_ID = B.PROCESS_ID
+    LEFT JOIN {tdfs4ds.SCHEMA}.{tdfs4ds.FILTER_MANAGER_NAME} D
+    ON A.PROCESS_ID = D.PROCESS_ID
     """
 
     # Optionally printing the query if configured to do so
     if tdml.display.print_sqlmr_query:
         print(query)
 
     # Executing the query and returning the result as a DataFrame
```

## tdfs4ds/process_store/process_registration_management.py

```diff
@@ -37,14 +37,19 @@
     if type(view_name) == tdml.dataframe.dataframe.DataFrame:
         try:
             view_name = view_name._table_name
         except:
             print('create your teradata dataframe using tdml.DataFrame(<view name>). Crystallize your view if needed')
             return []
 
+    # Get filter manager:
+    filtermanager = kwargs.get('filtermanager',None)
+    if filtermanager is None:
+        query_insert_filtermanager = None
+
     # Get data distribution related inputs:
     primary_index = kwargs.get('primary_index', [e for e in entity_id.keys()])
     partitioning  = kwargs.get('partitioning','').replace("'",'"')
 
     if primary_index is None:
         primary_index = [e for e in entity_id.keys()]
 
@@ -90,14 +95,24 @@
             query_insert_dist = f"""
                 CURRENT VALIDTIME INSERT INTO {tdfs4ds.SCHEMA}.{tdfs4ds.DATA_DISTRIBUTION_NAME} (PROCESS_ID, FOR_PRIMARY_INDEX, FOR_DATA_PARTITIONING)
                     VALUES ('{process_id}',
                     '{primary_index}',
                     '{partitioning}'
                     )
                 """
+
+            if filtermanager is not None:
+                query_insert_filtermanager = f"""
+                CURRENT VALIDTIME INSERT INTO {tdfs4ds.SCHEMA}.{tdfs4ds.FILTER_MANAGER_NAME} (PROCESS_ID, DATABASE_NAME, VIEW_NAME, TABLE_NAME)
+                    VALUES ('{process_id}',
+                    '{filtermanager.schema_name}',
+                    '{filtermanager.view_name}',
+                    '{filtermanager.table_name}'
+                    )
+                """
         # Constructing the query for updating existing views
         else:
             query_insert = f"""
                             CURRENT VALIDTIME UPDATE {tdfs4ds.SCHEMA}.{tdfs4ds.PROCESS_CATALOG_NAME} 
                             SET 
                                 PROCESS_TYPE = 'denormalized view'
                             ,   ENTITY_ID = '{json.dumps(entity_id).replace("'", '"')}'
@@ -112,14 +127,24 @@
             query_insert_dist = f"""
                 CURRENT VALIDTIME UPDATE {tdfs4ds.SCHEMA}.{tdfs4ds.DATA_DISTRIBUTION_NAME}
                 SET 
                     FOR_PRIMARY_INDEX = '{",".join(primary_index)}',
                     FOR_DATA_PARTITIONING = '{partitioning}' 
                 WHERE PROCESS_ID = '{process_id}'
                 """
+
+            if filtermanager is not None:
+                query_insert_filtermanager = f"""
+                CURRENT VALIDTIME UPDATE {tdfs4ds.SCHEMA}.{tdfs4ds.FILTER_MANAGER_NAME}
+                SET 
+                    DATABASE_NAME = '{filtermanager.schema_name}',
+                    VIEW_NAME     = '{filtermanager.view_name}',
+                    TABLE_NAME    = '{filtermanager.table_name}'
+                WHERE PROCESS_ID = '{process_id}'
+                """
     else:
         # Handling the case when the date is in the past
         df = tdml.DataFrame.from_query(f"VALIDTIME AS OF TIMESTAMP '{tdfs4ds.FEATURE_STORE_TIME}' SEL * FROM {tdfs4ds.SCHEMA}.{tdfs4ds.PROCESS_CATALOG_NAME} WHERE view_name = '{view_name}'")
 
 
 
         # Constructing the query for new views with a past date
@@ -143,14 +168,26 @@
                 INSERT INTO {tdfs4ds.SCHEMA}.{tdfs4ds.DATA_DISTRIBUTION_NAME} (PROCESS_ID, FOR_PRIMARY_INDEX, FOR_DATA_PARTITIONING,ValidStart, ValidEnd)
                     VALUES ('{process_id}',
                     '{primary_index}',
                     '{partitioning}',
                     TIMESTAMP '{tdfs4ds.FEATURE_STORE_TIME}',
                     TIMESTAMP '{end_period_}' 
                 """
+
+            if filtermanager is not None:
+                query_insert_filtermanager = f"""
+                INSERT INTO {tdfs4ds.SCHEMA}.{tdfs4ds.FILTER_MANAGER_NAME} (PROCESS_ID, DATABASE_NAME, VIEW_NAME, TABLE_NAME,ValidStart, ValidEnd)
+                    VALUES ('{process_id}',
+                    '{filtermanager.schema_name}',
+                    '{filtermanager.view_name}',
+                    '{filtermanager.table_name}',
+                    TIMESTAMP '{tdfs4ds.FEATURE_STORE_TIME}',
+                    TIMESTAMP '{end_period_}' 
+                    )
+                """
         # Constructing the query for updating existing views with a past date
         else:
             query_insert = f"""{validtime_statement}
                             UPDATE {tdfs4ds.SCHEMA}.{tdfs4ds.PROCESS_CATALOG_NAME} 
                             SET 
                                 PROCESS_TYPE = 'denormalized view'
                             ,   ENTITY_ID = '{json.dumps(entity_id).replace("'", '"')}'
@@ -168,24 +205,33 @@
                 UPDATE {tdfs4ds.SCHEMA}.{tdfs4ds.DATA_DISTRIBUTION_NAME}
                 SET 
                     FOR_PRIMARY_INDEX = '{",".join(primary_index)}',
                     FOR_DATA_PARTITIONING = '{partitioning}' 
                 WHERE PROCESS_ID = '{process_id}'
                 """
 
+            if filtermanager is not None:
+                query_insert_filtermanager = f"""{validtime_statement}
+                UPDATE {tdfs4ds.SCHEMA}.{tdfs4ds.FILTER_MANAGER_NAME}
+                SET 
+                    DATABASE_NAME = '{filtermanager.schema_name}',
+                    VIEW_NAME     = '{filtermanager.view_name}',
+                    TABLE_NAME    = '{filtermanager.table_name}'
+                WHERE PROCESS_ID = '{process_id}'
+                """
 
     # Logging the process registration
     print(f'register process with id : {process_id}')
     print(f'to run the process again just type : run(process_id={process_id})')
     print(f'to update your dataset : dataset = run(process_id={process_id},return_dataset=True)')
 
     if kwargs.get('with_process_id'):
-        return query_insert, process_id, query_insert_dist
+        return query_insert, process_id, query_insert_dist, query_insert_filtermanager
     else:
-        return query_insert, query_insert_dist
+        return query_insert, query_insert_dist, query_insert_filtermanager
 
 @execute_query_wrapper
 def register_process_tdstone(model, metadata={}):
     """
     Registers a 'tdstone2 view' process in the feature store with specified model details and metadata.
     This function is designed for registering hyper-segmented models created using the 'tdstone2' Python package.
     It handles both scenarios where the feature store date is current or in the past.
```

## tdfs4ds/process_store/process_store_catalog_management.py

```diff
@@ -84,14 +84,37 @@
             )
             PRIMARY INDEX (PROCESS_ID);
     """
 
     # SQL query to comment the table
     query5 = f"COMMENT ON TABLE {tdfs4ds.SCHEMA}.{tdfs4ds.DATA_DISTRIBUTION_NAME} IS 'DESCRIBES THE DATA DISTRIBUTION OF THE PROCESSES'"
 
+    query6 = f"""
+    CREATE MULTISET TABLE {tdfs4ds.SCHEMA}.{tdfs4ds.FILTER_MANAGER_NAME},
+            FALLBACK,
+            NO BEFORE JOURNAL,
+            NO AFTER JOURNAL,
+            CHECKSUM = DEFAULT,
+            DEFAULT MERGEBLOCKRATIO,
+            MAP = TD_MAP1
+            (
+
+                PROCESS_ID VARCHAR(36) NOT NULL,
+                DATABASE_NAME VARCHAR(2048),
+                VIEW_NAME VARCHAR(2048),
+                TABLE_NAME VARCHAR(2048),
+                ValidStart TIMESTAMP(0) WITH TIME ZONE NOT NULL,
+                ValidEnd TIMESTAMP(0) WITH TIME ZONE NOT NULL,
+                PERIOD FOR ValidPeriod  (ValidStart, ValidEnd) AS VALIDTIME
+            )
+            PRIMARY INDEX (PROCESS_ID);
+    """
+
+    # SQL query to comment the table
+    query7 = f"COMMENT ON TABLE {tdfs4ds.SCHEMA}.{tdfs4ds.FILTER_MANAGER_NAME} IS 'LIST THE FILTER MANAGER ATTACHED TO THE PROCESSES'"
     try:
         # Attempt to execute the create table query
         execute_query(query)
         if tdml.display.print_sqlmr_query:
             print(query)
         if tdfs4ds.DISPLAY_LOGS: print(f'TABLE {tdfs4ds.SCHEMA}.{tdfs4ds.PROCESS_CATALOG_NAME} has been created')
         execute_query(query3)
@@ -132,16 +155,40 @@
                 if tdml.display.print_sqlmr_query:
                     print(query4)
                 execute_query(query5)
             except Exception as e:
                 print(str(e).split('\n')[0])
 
     try:
+        # Attempt to execute the create table query
+        execute_query(query6)
+        if tdml.display.print_sqlmr_query:
+            print(query6)
+        if tdfs4ds.DISPLAY_LOGS: print(f'TABLE {tdfs4ds.SCHEMA}.{tdfs4ds.FILTER_MANAGER_NAME} has been created')
+        execute_query(query7)
+    except Exception as e:
+        # If the table already exists and if_exists is set to 'replace', drop the table and recreate it
+        if tdfs4ds.DISPLAY_LOGS: print(str(e).split('\n')[0])
+        if str(e).split('\n')[0].endswith('already exists.') and (if_exists == 'replace'):
+            execute_query(f'DROP TABLE  {tdfs4ds.SCHEMA}.{tdfs4ds.FILTER_MANAGER_NAME}')
+            print(f'TABLE {tdfs4ds.SCHEMA}.{tdfs4ds.FILTER_MANAGER_NAME} has been dropped')
+            try:
+                # Attempt to recreate the table after dropping it
+                execute_query(query6)
+                if tdfs4ds.DISPLAY_LOGS: print(
+                    f'TABLE {tdfs4ds.SCHEMA}.{tdfs4ds.FILTER_MANAGER_NAME} has been re-created')
+                if tdml.display.print_sqlmr_query:
+                    print(query6)
+                execute_query(query7)
+            except Exception as e:
+                print(str(e).split('\n')[0])
+
+    try:
         # Attempt to create the secondary index
         execute_query(query2)
         if tdml.display.print_sqlmr_query:
             print(query)
         if tdfs4ds.DISPLAY_LOGS: print(f'SECONDARY INDEX ON TABLE {tdfs4ds.SCHEMA}.{tdfs4ds.PROCESS_CATALOG_NAME} has been created')
     except Exception as e:
         print(str(e).split('\n')[0])
 
-    return tdfs4ds.PROCESS_CATALOG_NAME, tdfs4ds.DATA_DISTRIBUTION_NAME
+    return tdfs4ds.PROCESS_CATALOG_NAME, tdfs4ds.DATA_DISTRIBUTION_NAME, tdfs4ds.FILTER_MANAGER_NAME
```

## Comparing `tdfs4ds-0.2.2.7.dist-info/METADATA` & `tdfs4ds-0.2.2.8.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: tdfs4ds
-Version: 0.2.2.7
+Version: 0.2.2.8
 Summary: A python package to simplify the usage of feature store using Teradata Vantage ...
 Author: Denis Molin
 Requires-Python: >=3.6
 Description-Content-Type: text/markdown
 Requires-Dist: teradataml (>=17.20)
 Requires-Dist: pandas
 Requires-Dist: numpy
```

## Comparing `tdfs4ds-0.2.2.7.dist-info/RECORD` & `tdfs4ds-0.2.2.8.dist-info/RECORD`

 * *Files 9% similar despite different names*

```diff
@@ -1,30 +1,31 @@
 tdfs/__init__.py,sha256=7AcO7uB1opRCt7t2JOHworKimfAaDeO3boRW7u9Geo8,23
 tdfs/datasets.py,sha256=-b2MPEKGki2V1M8iUcoDR9uc2krIK7u1CK-EhChvihs,985
 tdfs/feature_store.py,sha256=Honu7eOAXxP4Ivz0mRlhuNkfTDzgZl5HB1WlQUwzcZ0,31354
 tdfs/data/curves.csv,sha256=q0Tm-0yu7VMK4lHvHpgi1LMeRq0lO5gJy2Q17brKbEM,112488
-tdfs4ds/__init__.py,sha256=xW7yj5FKNRTA3mtWD2Cv36JMXyTKKK5AmlR-uqkfpGQ,44572
+tdfs4ds/__init__.py,sha256=W6L6HXq6odM9gwC99IoVaw25Kvn233fasC4WdAXR8Ks,46622
 tdfs4ds/datasets.py,sha256=LE4Gn0muwdyrIrCrbkE92cnafUML63z1lj5bFIIVzmc,3524
 tdfs4ds/feature_engineering.py,sha256=oVnZ2V_XNGE12LKC_fNfkrWSQZLgtYRmaf8Dispi6S4,7081
 tdfs4ds/feature_store.py,sha256=y-oItPZw6nBkBcGAceaATZbkLPTsvpk0OnpzTxYofDs,68576
 tdfs4ds/process_store.py,sha256=W97pwqOwabo062ow_LfAXZmlSkcq8xTuwhwAX1EStlQ,16939
 tdfs4ds/utils.py,sha256=xF1VP0NCgosXcKymOo_ofMMnvLEF228IxaxIl-f65uA,23312
 tdfs4ds/data/curves.csv,sha256=q0Tm-0yu7VMK4lHvHpgi1LMeRq0lO5gJy2Q17brKbEM,112488
 tdfs4ds/feature_store/__init__.py,sha256=a7NPCkpTx40UR5LRErwnskpABG2Vuib7F5wUjaUGCnI,209
 tdfs4ds/feature_store/entity_management.py,sha256=JqE0qIzu6-YlXlLmiqMKx9SHQYeLhB_sD7no3vYnI-w,9609
 tdfs4ds/feature_store/feature_data_processing.py,sha256=vcZemKqwDDKkvg6nhy1sZV1ZAbbX4Xa-O3XiD4H8nvs,26340
 tdfs4ds/feature_store/feature_query_retrieval.py,sha256=Fj8yMX372hYNm3vkX29thDysfhEAf4bUJw4YjPVGD94,25246
 tdfs4ds/feature_store/feature_store_management.py,sha256=wz_OuTMK-GM-vYNL9XYAPi56h3IR9lo8kNNfFb3LpqA,38568
 tdfs4ds/process_store/__init__.py,sha256=6hiuFYdOYBiFG2J7we-DH3PBnssG3S0HILDj72X4pbw,182
-tdfs4ds/process_store/process_query_administration.py,sha256=j4YMPOm3tqZcgUZvRQs-tzVIyM2LCQbX67G8hW1-qG8,6217
-tdfs4ds/process_store/process_registration_management.py,sha256=zyJPIqLhmZJL3D0S8OuQo_cDpSTpet92eyfQ_5fM1i4,12641
-tdfs4ds/process_store/process_store_catalog_management.py,sha256=oS69aApTVqy-QI0oGfNyags50cL-JqJZN3ZSnj0cEO4,7190
+tdfs4ds/process_store/process_query_administration.py,sha256=Nf4gX2PSXOWzZuxSKz-VVHJJaCkyCF1UKnCLALhxqFg,6722
+tdfs4ds/process_store/process_registration_management.py,sha256=IqL1bjY7uS2BKe5U-FhyA9alfmSSdQIsRiWwWRV1tO4,14892
+tdfs4ds/process_store/process_store_catalog_management.py,sha256=BKmMEjMJ8p1iJeO6jvhXtMGDE_9Muys_e6C0DEa7Q0I,9405
 tdfs4ds/utils/__init__.py,sha256=-yTMfDLZbQnIRQ64s_bczzT21tDW2A8FZeq9PX5SgFU,168
+tdfs4ds/utils/filter_management.py,sha256=B67IfuENEMH6p8qyPXGjaw3s_Cn-CRwmDDI1X0XaL58,5974
 tdfs4ds/utils/info.py,sha256=tfdDt7tlDLVh0VHPgKzRYQ-wra25Sozf3pKrrr0A2xs,4034
 tdfs4ds/utils/lineage.py,sha256=QLDwGEXE2yn1u1fdwostCNqp7ka-tapeFPstuxjjaOE,22173
 tdfs4ds/utils/query_management.py,sha256=nAcE8QY1GWAKgOtb-ubSfDVcnYbU7Ge8CruVRLoPtmY,6356
 tdfs4ds/utils/time_management.py,sha256=_7rq-flQ-Gmhxz9n72L2BDx1WWTvVArx9_uMdLEjtmk,6459
 tdfs4ds/utils/visualization.py,sha256=yd5FvRxWmuhggxDW3euMTKi_ufBOt0kKIB3Zl3XAW0I,22812
-tdfs4ds-0.2.2.7.dist-info/METADATA,sha256=e1Aj9tP-XqW0F8e6VzsMVY5d6syzxwFyPUD8h49w7Eo,11708
-tdfs4ds-0.2.2.7.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-tdfs4ds-0.2.2.7.dist-info/top_level.txt,sha256=wMyVkMvnBn8RRt1xBveGQxOpWFijPMPkMiE7G2mi8zo,8
-tdfs4ds-0.2.2.7.dist-info/RECORD,,
+tdfs4ds-0.2.2.8.dist-info/METADATA,sha256=3ToTlpnv5sVK-9HtZfTtNNo1GVWTg12p3CmFZ1zzSio,11708
+tdfs4ds-0.2.2.8.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
+tdfs4ds-0.2.2.8.dist-info/top_level.txt,sha256=wMyVkMvnBn8RRt1xBveGQxOpWFijPMPkMiE7G2mi8zo,8
+tdfs4ds-0.2.2.8.dist-info/RECORD,,
```

