# Comparing `tmp/pami-2024.5.28.1.tar.gz` & `tmp/pami-2024.5.7.1.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "pami-2024.5.28.1.tar", last modified: Tue May 28 08:48:12 2024, max compression
+gzip compressed data, was "pami-2024.5.7.1.tar", last modified: Tue May  7 07:53:15 2024, max compression
```

## Comparing `pami-2024.5.28.1.tar` & `pami-2024.5.7.1.tar`

### file list

```diff
@@ -1,515 +1,512 @@
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.920555 pami-2024.5.28.1/
--rw-r--r--   0 vanithak   (502) staff       (20)    35149 2024-03-12 04:33:29.000000 pami-2024.5.28.1/LICENSE
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.287507 pami-2024.5.28.1/PAMI/
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.288419 pami-2024.5.28.1/PAMI/AssociationRules/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/AssociationRules/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.299456 pami-2024.5.28.1/PAMI/AssociationRules/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    14350 2024-05-03 04:25:04.000000 pami-2024.5.28.1/PAMI/AssociationRules/basic/_ARWithLeverage.py
--rw-r--r--   0 vanithak   (502) staff       (20)    14263 2024-05-03 04:25:04.000000 pami-2024.5.28.1/PAMI/AssociationRules/basic/_ARWithLift.py
--rw-r--r--   0 vanithak   (502) staff       (20)    19416 2024-05-03 04:25:04.000000 pami-2024.5.28.1/PAMI/AssociationRules/basic/_RuleMiner.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/AssociationRules/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6613 2024-05-03 04:25:04.000000 pami-2024.5.28.1/PAMI/AssociationRules/basic/abstract.py
--rw-r--r--   0 vanithak   (502) staff       (20)    13089 2024-05-27 02:58:37.000000 pami-2024.5.28.1/PAMI/AssociationRules/basic/confidence.py
--rw-r--r--   0 vanithak   (502) staff       (20)    13090 2024-05-27 02:58:37.000000 pami-2024.5.28.1/PAMI/AssociationRules/basic/leverage.py
--rw-r--r--   0 vanithak   (502) staff       (20)    12873 2024-05-27 02:58:37.000000 pami-2024.5.28.1/PAMI/AssociationRules/basic/lift.py
--rw-r--r--   0 vanithak   (502) staff       (20)      139 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.300492 pami-2024.5.28.1/PAMI/correlatedPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/correlatedPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.307312 pami-2024.5.28.1/PAMI/correlatedPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    26430 2024-05-28 08:47:44.000000 pami-2024.5.28.1/PAMI/correlatedPattern/basic/CoMine.py
--rw-r--r--   0 vanithak   (502) staff       (20)    27988 2024-05-03 04:25:04.000000 pami-2024.5.28.1/PAMI/correlatedPattern/basic/CoMinePlus.py
--rw-r--r--   0 vanithak   (502) staff       (20)    26444 2024-05-27 02:58:37.000000 pami-2024.5.28.1/PAMI/correlatedPattern/basic/_CoMine.py
--rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/correlatedPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6208 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/correlatedPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.308498 pami-2024.5.28.1/PAMI/coveragePattern/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/coveragePattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.312487 pami-2024.5.28.1/PAMI/coveragePattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    14643 2024-05-01 06:38:00.000000 pami-2024.5.28.1/PAMI/coveragePattern/basic/CMine.py
--rw-r--r--   0 vanithak   (502) staff       (20)    17200 2024-05-01 06:38:00.000000 pami-2024.5.28.1/PAMI/coveragePattern/basic/CPPG.py
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/coveragePattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     7155 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/coveragePattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.319566 pami-2024.5.28.1/PAMI/extras/
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.336496 pami-2024.5.28.1/PAMI/extras/DF2DB/
--rw-r--r--   0 vanithak   (502) staff       (20)     4360 2024-04-09 02:02:26.000000 pami-2024.5.28.1/PAMI/extras/DF2DB/DF2DB.py
--rw-r--r--   0 vanithak   (502) staff       (20)     4287 2024-04-09 02:02:26.000000 pami-2024.5.28.1/PAMI/extras/DF2DB/DF2DBPlus.py
--rw-r--r--   0 vanithak   (502) staff       (20)    10331 2024-04-09 02:02:26.000000 pami-2024.5.28.1/PAMI/extras/DF2DB/DenseFormatDF.py
--rw-r--r--   0 vanithak   (502) staff       (20)     5413 2024-04-09 02:02:26.000000 pami-2024.5.28.1/PAMI/extras/DF2DB/SparseFormatDF.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/extras/DF2DB/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     3103 2024-04-09 02:02:26.000000 pami-2024.5.28.1/PAMI/extras/DF2DB/createTDB.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6948 2024-04-09 02:02:26.000000 pami-2024.5.28.1/PAMI/extras/DF2DB/denseDF2DBPlus.py
--rw-r--r--   0 vanithak   (502) staff       (20)    11940 2024-04-09 02:02:26.000000 pami-2024.5.28.1/PAMI/extras/DF2DB/denseDF2DB_dump.py
--rw-r--r--   0 vanithak   (502) staff       (20)     5336 2024-04-09 02:02:26.000000 pami-2024.5.28.1/PAMI/extras/DF2DB/sparseDF2DBPlus.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/extras/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.340113 pami-2024.5.28.1/PAMI/extras/calculateMISValues/
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/extras/calculateMISValues/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6468 2024-03-29 21:11:29.000000 pami-2024.5.28.1/PAMI/extras/calculateMISValues/usingBeta.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6499 2024-03-29 21:11:29.000000 pami-2024.5.28.1/PAMI/extras/calculateMISValues/usingSD.py
--rw-r--r--   0 vanithak   (502) staff       (20)     7345 2024-04-17 06:00:20.000000 pami-2024.5.28.1/PAMI/extras/convertMultiTSIntoFuzzy.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.354025 pami-2024.5.28.1/PAMI/extras/dbStats/
--rw-r--r--   0 vanithak   (502) staff       (20)    14951 2024-03-29 21:11:29.000000 pami-2024.5.28.1/PAMI/extras/dbStats/FuzzyDatabase.py
--rw-r--r--   0 vanithak   (502) staff       (20)    13796 2024-03-29 21:11:29.000000 pami-2024.5.28.1/PAMI/extras/dbStats/MultipleTimeSeriesFuzzyDatabaseStats.py
--rw-r--r--   0 vanithak   (502) staff       (20)    16034 2024-04-09 02:02:26.000000 pami-2024.5.28.1/PAMI/extras/dbStats/SequentialDatabase.py
--rw-r--r--   0 vanithak   (502) staff       (20)    16883 2024-03-29 21:11:29.000000 pami-2024.5.28.1/PAMI/extras/dbStats/TemporalDatabase.py
--rw-r--r--   0 vanithak   (502) staff       (20)    12839 2024-03-29 21:11:29.000000 pami-2024.5.28.1/PAMI/extras/dbStats/TransactionalDatabase.py
--rw-r--r--   0 vanithak   (502) staff       (20)    15120 2024-03-29 21:11:29.000000 pami-2024.5.28.1/PAMI/extras/dbStats/UncertainTemporalDatabase.py
--rw-r--r--   0 vanithak   (502) staff       (20)    11953 2024-03-29 21:11:29.000000 pami-2024.5.28.1/PAMI/extras/dbStats/UncertainTransactionalDatabase.py
--rw-r--r--   0 vanithak   (502) staff       (20)    12679 2024-04-09 02:02:26.000000 pami-2024.5.28.1/PAMI/extras/dbStats/UtilityDatabase.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/extras/dbStats/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.358621 pami-2024.5.28.1/PAMI/extras/fuzzyTransformation/
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/extras/fuzzyTransformation/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     5238 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/extras/fuzzyTransformation/abstract.py
--rw-r--r--   0 vanithak   (502) staff       (20)     8594 2024-04-09 02:02:26.000000 pami-2024.5.28.1/PAMI/extras/fuzzyTransformation/temporalToFuzzy.py
--rw-r--r--   0 vanithak   (502) staff       (20)     8792 2024-04-09 02:02:26.000000 pami-2024.5.28.1/PAMI/extras/fuzzyTransformation/transactionalToFuzzy.py
--rw-r--r--   0 vanithak   (502) staff       (20)     8313 2024-04-09 02:02:26.000000 pami-2024.5.28.1/PAMI/extras/fuzzyTransformation/utilityToFuzzy.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.362595 pami-2024.5.28.1/PAMI/extras/generateDatabase/
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/extras/generateDatabase/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     5685 2024-04-09 02:02:26.000000 pami-2024.5.28.1/PAMI/extras/generateDatabase/generateSpatioTemporalDatabase.py
--rw-r--r--   0 vanithak   (502) staff       (20)     9558 2024-04-09 02:02:26.000000 pami-2024.5.28.1/PAMI/extras/generateDatabase/generateTemporalDatabase.py
--rw-r--r--   0 vanithak   (502) staff       (20)     5971 2024-04-09 02:02:26.000000 pami-2024.5.28.1/PAMI/extras/generateDatabase/generateTransactionalDatabase.py
--rw-r--r--   0 vanithak   (502) staff       (20)     5157 2024-04-10 08:52:08.000000 pami-2024.5.28.1/PAMI/extras/generateLatexGraphFile.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.369318 pami-2024.5.28.1/PAMI/extras/graph/
--rw-r--r--   0 vanithak   (502) staff       (20)     3223 2024-04-09 02:02:26.000000 pami-2024.5.28.1/PAMI/extras/graph/DF2Fig.py
--rw-r--r--   0 vanithak   (502) staff       (20)     3577 2024-04-09 02:02:26.000000 pami-2024.5.28.1/PAMI/extras/graph/DF2Tex.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/extras/graph/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     2750 2024-04-09 02:02:26.000000 pami-2024.5.28.1/PAMI/extras/graph/plotLineGraphFromDictionary.py
--rw-r--r--   0 vanithak   (502) staff       (20)     3599 2024-04-09 02:02:26.000000 pami-2024.5.28.1/PAMI/extras/graph/plotLineGraphsFromDataFrame.py
--rw-r--r--   0 vanithak   (502) staff       (20)     4465 2024-04-09 02:02:26.000000 pami-2024.5.28.1/PAMI/extras/graph/visualizeFuzzyPatterns.py
--rw-r--r--   0 vanithak   (502) staff       (20)     4240 2024-04-09 02:02:26.000000 pami-2024.5.28.1/PAMI/extras/graph/visualizePatterns.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.370415 pami-2024.5.28.1/PAMI/extras/image2Database/
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/extras/image2Database/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.372490 pami-2024.5.28.1/PAMI/extras/imageProcessing/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/extras/imageProcessing/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6488 2024-04-09 02:02:26.000000 pami-2024.5.28.1/PAMI/extras/imageProcessing/imagery2Databases.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.375642 pami-2024.5.28.1/PAMI/extras/messaging/
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/extras/messaging/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)      533 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/extras/messaging/discord.py
--rw-r--r--   0 vanithak   (502) staff       (20)     1575 2024-04-09 02:02:26.000000 pami-2024.5.28.1/PAMI/extras/messaging/gmail.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.379480 pami-2024.5.28.1/PAMI/extras/neighbours/
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/extras/neighbours/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     4784 2024-04-17 06:00:20.000000 pami-2024.5.28.1/PAMI/extras/neighbours/findNeighborsUsingEuclideanDistanceforPointInfo.py
--rw-r--r--   0 vanithak   (502) staff       (20)     4410 2024-04-17 06:00:20.000000 pami-2024.5.28.1/PAMI/extras/neighbours/findNeighboursUsingEuclidean.py
--rw-r--r--   0 vanithak   (502) staff       (20)     4305 2024-04-17 06:00:20.000000 pami-2024.5.28.1/PAMI/extras/neighbours/findNeighboursUsingGeodesic.py
--rw-r--r--   0 vanithak   (502) staff       (20)     5013 2024-04-10 08:52:08.000000 pami-2024.5.28.1/PAMI/extras/plotPointOnMap.py
--rw-r--r--   0 vanithak   (502) staff       (20)     5183 2024-04-10 08:52:08.000000 pami-2024.5.28.1/PAMI/extras/plotPointOnMap_dump.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.380535 pami-2024.5.28.1/PAMI/extras/sampleDatasets/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/extras/sampleDatasets/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     4024 2024-04-10 08:52:08.000000 pami-2024.5.28.1/PAMI/extras/scatterPlotSpatialPoints.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.386627 pami-2024.5.28.1/PAMI/extras/stats/
--rw-r--r--   0 vanithak   (502) staff       (20)    12724 2024-04-09 02:02:26.000000 pami-2024.5.28.1/PAMI/extras/stats/TransactionalDatabase.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/extras/stats/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     4144 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/extras/stats/graphDatabase.py
--rw-r--r--   0 vanithak   (502) staff       (20)    15998 2024-04-09 02:02:26.000000 pami-2024.5.28.1/PAMI/extras/stats/sequentialDatabase.py
--rw-r--r--   0 vanithak   (502) staff       (20)    16926 2024-04-09 02:02:26.000000 pami-2024.5.28.1/PAMI/extras/stats/temporalDatabase.py
--rw-r--r--   0 vanithak   (502) staff       (20)    12692 2024-04-09 02:02:26.000000 pami-2024.5.28.1/PAMI/extras/stats/utilityDatabase.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.411961 pami-2024.5.28.1/PAMI/extras/syntheticDataGenerator/
--rw-r--r--   0 vanithak   (502) staff       (20)     7276 2024-05-01 06:38:00.000000 pami-2024.5.28.1/PAMI/extras/syntheticDataGenerator/TemporalDatabase.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6291 2024-05-28 08:47:44.000000 pami-2024.5.28.1/PAMI/extras/syntheticDataGenerator/TransactionalDatabase.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/extras/syntheticDataGenerator/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     2325 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialTemporal.py
--rw-r--r--   0 vanithak   (502) staff       (20)     2254 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialTransactions.py
--rw-r--r--   0 vanithak   (502) staff       (20)     2539 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialUncertainTransaction.py
--rw-r--r--   0 vanithak   (502) staff       (20)     1880 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/extras/syntheticDataGenerator/createSyntheticTemporal.py
--rw-r--r--   0 vanithak   (502) staff       (20)     1843 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/extras/syntheticDataGenerator/createSyntheticTransactions.py
--rw-r--r--   0 vanithak   (502) staff       (20)     2117 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/extras/syntheticDataGenerator/createSyntheticUncertainTemporal.py
--rw-r--r--   0 vanithak   (502) staff       (20)     2066 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/extras/syntheticDataGenerator/createSyntheticUncertainTransactions.py
--rw-r--r--   0 vanithak   (502) staff       (20)     2262 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/extras/syntheticDataGenerator/createSyntheticUtility.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/extras/syntheticDataGenerator/fuzzyDatabase.py
--rw-r--r--   0 vanithak   (502) staff       (20)     1121 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/extras/syntheticDataGenerator/generateTemporal.py
--rw-r--r--   0 vanithak   (502) staff       (20)     1111 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/extras/syntheticDataGenerator/generateTransactional.py
--rw-r--r--   0 vanithak   (502) staff       (20)     1625 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/extras/syntheticDataGenerator/generateUncertainTemporal.py
--rw-r--r--   0 vanithak   (502) staff       (20)     1610 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/extras/syntheticDataGenerator/generateUncertainTransactional.py
--rw-r--r--   0 vanithak   (502) staff       (20)     3613 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/extras/syntheticDataGenerator/generateUtilityTemporal.py
--rw-r--r--   0 vanithak   (502) staff       (20)     3603 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/extras/syntheticDataGenerator/generateUtilityTransactional.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/extras/syntheticDataGenerator/georeferencedTemporalDatabase.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/extras/syntheticDataGenerator/georeferencedTransactionalDatabase.py
--rw-r--r--   0 vanithak   (502) staff       (20)     4324 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/extras/syntheticDataGenerator/syntheticUtilityDatabase.py
--rw-r--r--   0 vanithak   (502) staff       (20)     3283 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/extras/syntheticDataGenerator/temporalDatabaseGen.py
--rw-r--r--   0 vanithak   (502) staff       (20)     4842 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/extras/syntheticDataGenerator/utilityDatabase.py
--rw-r--r--   0 vanithak   (502) staff       (20)     3240 2024-04-10 08:52:08.000000 pami-2024.5.28.1/PAMI/extras/topKPatterns.py
--rw-r--r--   0 vanithak   (502) staff       (20)     2322 2024-04-10 08:52:08.000000 pami-2024.5.28.1/PAMI/extras/uncertaindb_convert.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.416454 pami-2024.5.28.1/PAMI/extras/visualize/
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/extras/visualize/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     1897 2024-03-29 21:11:29.000000 pami-2024.5.28.1/PAMI/extras/visualize/graphs.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.418498 pami-2024.5.28.1/PAMI/faultTolerantFrequentPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/faultTolerantFrequentPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.430735 pami-2024.5.28.1/PAMI/faultTolerantFrequentPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    14510 2024-05-01 06:38:00.000000 pami-2024.5.28.1/PAMI/faultTolerantFrequentPattern/basic/FTApriori.py
--rw-r--r--   0 vanithak   (502) staff       (20)    23166 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/faultTolerantFrequentPattern/basic/FTFPGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/faultTolerantFrequentPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6856 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/faultTolerantFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.432167 pami-2024.5.28.1/PAMI/frequentPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/frequentPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.448086 pami-2024.5.28.1/PAMI/frequentPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    14126 2024-05-28 08:47:44.000000 pami-2024.5.28.1/PAMI/frequentPattern/basic/Apriori.py
--rw-r--r--   0 vanithak   (502) staff       (20)    13933 2024-05-27 02:58:37.000000 pami-2024.5.28.1/PAMI/frequentPattern/basic/Aprioribitset.py
--rw-r--r--   0 vanithak   (502) staff       (20)    13733 2024-05-27 02:58:37.000000 pami-2024.5.28.1/PAMI/frequentPattern/basic/ECLAT.py
--rw-r--r--   0 vanithak   (502) staff       (20)    13727 2024-05-27 02:58:37.000000 pami-2024.5.28.1/PAMI/frequentPattern/basic/ECLATDiffset.py
--rw-r--r--   0 vanithak   (502) staff       (20)    13834 2024-05-27 02:58:37.000000 pami-2024.5.28.1/PAMI/frequentPattern/basic/ECLATbitset.py
--rw-r--r--   0 vanithak   (502) staff       (20)    19664 2024-05-27 02:58:37.000000 pami-2024.5.28.1/PAMI/frequentPattern/basic/FPGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)    15219 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/frequentPattern/basic/_Apriori.py
--rw-r--r--   0 vanithak   (502) staff       (20)    14033 2024-05-27 02:58:37.000000 pami-2024.5.28.1/PAMI/frequentPattern/basic/_ECLATDiffset.py
--rw-r--r--   0 vanithak   (502) staff       (20)    22703 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/frequentPattern/basic/_FPGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/frequentPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6818 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/frequentPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.451803 pami-2024.5.28.1/PAMI/frequentPattern/closed/
--rw-r--r--   0 vanithak   (502) staff       (20)    20505 2024-05-27 02:58:37.000000 pami-2024.5.28.1/PAMI/frequentPattern/closed/CHARM.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/frequentPattern/closed/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6580 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/frequentPattern/closed/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.465458 pami-2024.5.28.1/PAMI/frequentPattern/cuda/
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/frequentPattern/cuda/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     5980 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/frequentPattern/cuda/abstract.py
--rw-r--r--   0 vanithak   (502) staff       (20)    13664 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/frequentPattern/cuda/cuApriori.py
--rw-r--r--   0 vanithak   (502) staff       (20)    14418 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/frequentPattern/cuda/cuAprioriBit.py
--rw-r--r--   0 vanithak   (502) staff       (20)    13015 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/frequentPattern/cuda/cuEclat.py
--rw-r--r--   0 vanithak   (502) staff       (20)    14583 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/frequentPattern/cuda/cuEclatBit.py
--rw-r--r--   0 vanithak   (502) staff       (20)    14499 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/frequentPattern/cuda/cudaAprioriGCT.py
--rw-r--r--   0 vanithak   (502) staff       (20)    17118 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/frequentPattern/cuda/cudaAprioriTID.py
--rw-r--r--   0 vanithak   (502) staff       (20)    14178 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/frequentPattern/cuda/cudaEclatGCT.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.469230 pami-2024.5.28.1/PAMI/frequentPattern/maximal/
--rw-r--r--   0 vanithak   (502) staff       (20)    26260 2024-05-27 02:58:37.000000 pami-2024.5.28.1/PAMI/frequentPattern/maximal/MaxFPGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/frequentPattern/maximal/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6561 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/frequentPattern/maximal/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.476471 pami-2024.5.28.1/PAMI/frequentPattern/pyspark/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/frequentPattern/pyspark/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     5573 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/frequentPattern/pyspark/abstract.py
--rw-r--r--   0 vanithak   (502) staff       (20)    15452 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/frequentPattern/pyspark/parallelApriori.py
--rw-r--r--   0 vanithak   (502) staff       (20)    12947 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/frequentPattern/pyspark/parallelECLAT.py
--rw-r--r--   0 vanithak   (502) staff       (20)    17438 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/frequentPattern/pyspark/parallelFPGrowth.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.481643 pami-2024.5.28.1/PAMI/frequentPattern/topk/
--rw-r--r--   0 vanithak   (502) staff       (20)    15734 2024-05-27 02:58:37.000000 pami-2024.5.28.1/PAMI/frequentPattern/topk/FAE.py
--rw-r--r--   0 vanithak   (502) staff       (20)    15424 2024-05-27 02:58:37.000000 pami-2024.5.28.1/PAMI/frequentPattern/topk/_FAE.py
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/frequentPattern/topk/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     4575 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/frequentPattern/topk/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.482683 pami-2024.5.28.1/PAMI/fuzzyCorrelatedPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/fuzzyCorrelatedPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.486328 pami-2024.5.28.1/PAMI/fuzzyCorrelatedPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    28229 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/fuzzyCorrelatedPattern/basic/FCPGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/fuzzyCorrelatedPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6652 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/fuzzyCorrelatedPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.487344 pami-2024.5.28.1/PAMI/fuzzyFrequentPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/fuzzyFrequentPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.494190 pami-2024.5.28.1/PAMI/fuzzyFrequentPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    22973 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/fuzzyFrequentPattern/basic/FFIMiner.py
--rw-r--r--   0 vanithak   (502) staff       (20)    28621 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/fuzzyFrequentPattern/basic/FFIMiner_old.py
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/fuzzyFrequentPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6442 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/fuzzyFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.495326 pami-2024.5.28.1/PAMI/fuzzyGeoreferencedFrequentPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/fuzzyGeoreferencedFrequentPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.500099 pami-2024.5.28.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    26162 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/FFSPMiner.py
--rw-r--r--   0 vanithak   (502) staff       (20)    28607 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/FFSPMiner_old.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6730 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.501203 pami-2024.5.28.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.508572 pami-2024.5.28.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    28521 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/FGPFPMiner.py
--rw-r--r--   0 vanithak   (502) staff       (20)    33585 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/FGPFPMiner_old.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6623 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.509795 pami-2024.5.28.1/PAMI/fuzzyPartialPeriodicPatterns/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/fuzzyPartialPeriodicPatterns/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.514381 pami-2024.5.28.1/PAMI/fuzzyPartialPeriodicPatterns/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    22562 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/fuzzyPartialPeriodicPatterns/basic/F3PMiner.py
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/fuzzyPartialPeriodicPatterns/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6469 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/fuzzyPartialPeriodicPatterns/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.515541 pami-2024.5.28.1/PAMI/fuzzyPeriodicFrequentPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/fuzzyPeriodicFrequentPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.520508 pami-2024.5.28.1/PAMI/fuzzyPeriodicFrequentPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    25786 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/fuzzyPeriodicFrequentPattern/basic/FPFPMiner.py
--rw-r--r--   0 vanithak   (502) staff       (20)    27472 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/fuzzyPeriodicFrequentPattern/basic/FPFPMiner_old.py
--rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/fuzzyPeriodicFrequentPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6683 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/fuzzyPeriodicFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.522231 pami-2024.5.28.1/PAMI/geoReferencedPeriodicFrequentPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/geoReferencedPeriodicFrequentPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.527082 pami-2024.5.28.1/PAMI/geoReferencedPeriodicFrequentPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    22107 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/geoReferencedPeriodicFrequentPattern/basic/GPFPMiner.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/geoReferencedPeriodicFrequentPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6791 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/geoReferencedPeriodicFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.528420 pami-2024.5.28.1/PAMI/georeferencedFrequentPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/georeferencedFrequentPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.533969 pami-2024.5.28.1/PAMI/georeferencedFrequentPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    22965 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/georeferencedFrequentPattern/basic/FSPGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)    21152 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/georeferencedFrequentPattern/basic/SpatialECLAT.py
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/georeferencedFrequentPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6697 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/georeferencedFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.535523 pami-2024.5.28.1/PAMI/georeferencedFrequentSequencePattern/
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/georeferencedFrequentSequencePattern/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6696 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/georeferencedFrequentSequencePattern/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.536721 pami-2024.5.28.1/PAMI/georeferencedPartialPeriodicPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/georeferencedPartialPeriodicPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.543446 pami-2024.5.28.1/PAMI/georeferencedPartialPeriodicPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    21718 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/georeferencedPartialPeriodicPattern/basic/STEclat.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/georeferencedPartialPeriodicPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6178 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/georeferencedPartialPeriodicPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.544828 pami-2024.5.28.1/PAMI/highUtilityFrequentPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/highUtilityFrequentPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.548352 pami-2024.5.28.1/PAMI/highUtilityFrequentPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    38524 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/highUtilityFrequentPattern/basic/HUFIM.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/highUtilityFrequentPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6179 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/highUtilityFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.549665 pami-2024.5.28.1/PAMI/highUtilityGeoreferencedFrequentPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/highUtilityGeoreferencedFrequentPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.552855 pami-2024.5.28.1/PAMI/highUtilityGeoreferencedFrequentPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    42772 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/highUtilityGeoreferencedFrequentPattern/basic/SHUFIM.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/highUtilityGeoreferencedFrequentPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6307 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/highUtilityGeoreferencedFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.554046 pami-2024.5.28.1/PAMI/highUtilityPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/highUtilityPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.563232 pami-2024.5.28.1/PAMI/highUtilityPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    35224 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/highUtilityPattern/basic/EFIM.py
--rw-r--r--   0 vanithak   (502) staff       (20)    26511 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/highUtilityPattern/basic/HMiner.py
--rw-r--r--   0 vanithak   (502) staff       (20)    28921 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/highUtilityPattern/basic/UPGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/highUtilityPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     5166 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/highUtilityPattern/basic/abstract.py
--rw-r--r--   0 vanithak   (502) staff       (20)    19909 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/highUtilityPattern/basic/efimParallel.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.567956 pami-2024.5.28.1/PAMI/highUtilityPattern/parallel/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/highUtilityPattern/parallel/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     5166 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/highUtilityPattern/parallel/abstract.py
--rw-r--r--   0 vanithak   (502) staff       (20)    17831 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/highUtilityPattern/parallel/efimparallel.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.574190 pami-2024.5.28.1/PAMI/highUtilityPatternsInStreams/
--rw-r--r--   0 vanithak   (502) staff       (20)    30022 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/highUtilityPatternsInStreams/HUPMS.py
--rw-r--r--   0 vanithak   (502) staff       (20)    32848 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/highUtilityPatternsInStreams/SHUGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/highUtilityPatternsInStreams/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     5193 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/highUtilityPatternsInStreams/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.576599 pami-2024.5.28.1/PAMI/highUtilitySpatialPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/highUtilitySpatialPattern/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6716 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/highUtilitySpatialPattern/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.581875 pami-2024.5.28.1/PAMI/highUtilitySpatialPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    29850 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/highUtilitySpatialPattern/basic/HDSHUIM.py
--rw-r--r--   0 vanithak   (502) staff       (20)    37379 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/highUtilitySpatialPattern/basic/SHUIM.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/highUtilitySpatialPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     5934 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/highUtilitySpatialPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.586318 pami-2024.5.28.1/PAMI/highUtilitySpatialPattern/topk/
--rw-r--r--   0 vanithak   (502) staff       (20)    37519 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/highUtilitySpatialPattern/topk/TKSHUIM.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/highUtilitySpatialPattern/topk/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6618 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/highUtilitySpatialPattern/topk/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.596185 pami-2024.5.28.1/PAMI/localPeriodicPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/localPeriodicPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.604617 pami-2024.5.28.1/PAMI/localPeriodicPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    35370 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/localPeriodicPattern/basic/LPPGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)    24681 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/localPeriodicPattern/basic/LPPMBreadth.py
--rw-r--r--   0 vanithak   (502) staff       (20)    23709 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/localPeriodicPattern/basic/LPPMDepth.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/localPeriodicPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     8385 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/localPeriodicPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.605779 pami-2024.5.28.1/PAMI/multipleMinimumSupportBasedFrequentPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/multipleMinimumSupportBasedFrequentPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.611000 pami-2024.5.28.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    24308 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/CFPGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)    22008 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/CFPGrowthPlus.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     5921 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.612108 pami-2024.5.28.1/PAMI/partialPeriodicFrequentPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/partialPeriodicFrequentPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.615836 pami-2024.5.28.1/PAMI/partialPeriodicFrequentPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    28270 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/partialPeriodicFrequentPattern/basic/GPFgrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)    21954 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/partialPeriodicFrequentPattern/basic/PPF_DFS.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/partialPeriodicFrequentPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     5398 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/partialPeriodicFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.616922 pami-2024.5.28.1/PAMI/partialPeriodicPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/partialPeriodicPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.628709 pami-2024.5.28.1/PAMI/partialPeriodicPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    26420 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/partialPeriodicPattern/basic/GThreePGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)     4329 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/partialPeriodicPattern/basic/Gabstract.py
--rw-r--r--   0 vanithak   (502) staff       (20)    25525 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/partialPeriodicPattern/basic/PPPGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)    19577 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/partialPeriodicPattern/basic/PPP_ECLAT.py
--rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/partialPeriodicPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     5520 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/partialPeriodicPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.635041 pami-2024.5.28.1/PAMI/partialPeriodicPattern/closed/
--rw-r--r--   0 vanithak   (502) staff       (20)    22070 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/partialPeriodicPattern/closed/PPPClose.py
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/partialPeriodicPattern/closed/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     5605 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/partialPeriodicPattern/closed/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.639025 pami-2024.5.28.1/PAMI/partialPeriodicPattern/maximal/
--rw-r--r--   0 vanithak   (502) staff       (20)    29144 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/partialPeriodicPattern/maximal/Max3PGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/partialPeriodicPattern/maximal/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     4278 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/partialPeriodicPattern/maximal/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.642147 pami-2024.5.28.1/PAMI/partialPeriodicPattern/pyspark/
--rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/partialPeriodicPattern/pyspark/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     5765 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/partialPeriodicPattern/pyspark/abstract.py
--rw-r--r--   0 vanithak   (502) staff       (20)    28554 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/partialPeriodicPattern/pyspark/parallel3PGrowth.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.647885 pami-2024.5.28.1/PAMI/partialPeriodicPattern/topk/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/partialPeriodicPattern/topk/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6441 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/partialPeriodicPattern/topk/abstract.py
--rw-r--r--   0 vanithak   (502) staff       (20)    19588 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/partialPeriodicPattern/topk/k3PMiner.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.703459 pami-2024.5.28.1/PAMI/partialPeriodicPatternInMultipleTimeSeries/
--rw-r--r--   0 vanithak   (502) staff       (20)    28160 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/partialPeriodicPatternInMultipleTimeSeries/PPGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/partialPeriodicPatternInMultipleTimeSeries/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6350 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/partialPeriodicPatternInMultipleTimeSeries/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.713824 pami-2024.5.28.1/PAMI/periodicCorrelatedPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/periodicCorrelatedPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.727930 pami-2024.5.28.1/PAMI/periodicCorrelatedPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    27559 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/periodicCorrelatedPattern/basic/EPCPGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/periodicCorrelatedPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6691 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/periodicCorrelatedPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.731189 pami-2024.5.28.1/PAMI/periodicFrequentPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/periodicFrequentPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.748679 pami-2024.5.28.1/PAMI/periodicFrequentPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    16948 2024-05-28 08:47:44.000000 pami-2024.5.28.1/PAMI/periodicFrequentPattern/basic/PFECLAT.py
--rw-r--r--   0 vanithak   (502) staff       (20)    23964 2024-05-28 08:47:44.000000 pami-2024.5.28.1/PAMI/periodicFrequentPattern/basic/PFPGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)    26383 2024-03-29 21:11:29.000000 pami-2024.5.28.1/PAMI/periodicFrequentPattern/basic/PFPGrowthPlus.py
--rw-r--r--   0 vanithak   (502) staff       (20)    18106 2024-03-29 21:11:29.000000 pami-2024.5.28.1/PAMI/periodicFrequentPattern/basic/PFPMC.py
--rw-r--r--   0 vanithak   (502) staff       (20)    36300 2024-03-29 21:11:29.000000 pami-2024.5.28.1/PAMI/periodicFrequentPattern/basic/PSGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)    17904 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/periodicFrequentPattern/basic/_PFECLAT.py
--rw-r--r--   0 vanithak   (502) staff       (20)    28583 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/periodicFrequentPattern/basic/_PFPGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)      726 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/periodicFrequentPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6549 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/periodicFrequentPattern/basic/abstract.py
--rw-r--r--   0 vanithak   (502) staff       (20)    26643 2024-03-29 21:11:29.000000 pami-2024.5.28.1/PAMI/periodicFrequentPattern/basic/parallelPFPGrowth.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.753033 pami-2024.5.28.1/PAMI/periodicFrequentPattern/closed/
--rw-r--r--   0 vanithak   (502) staff       (20)    24417 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/periodicFrequentPattern/closed/CPFPMiner.py
--rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/periodicFrequentPattern/closed/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6539 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/periodicFrequentPattern/closed/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.759819 pami-2024.5.28.1/PAMI/periodicFrequentPattern/cuda/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/periodicFrequentPattern/cuda/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6568 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/periodicFrequentPattern/cuda/abstract.py
--rw-r--r--   0 vanithak   (502) staff       (20)    23867 2024-03-29 21:11:29.000000 pami-2024.5.28.1/PAMI/periodicFrequentPattern/cuda/cuGPFMiner.py
--rw-r--r--   0 vanithak   (502) staff       (20)    18982 2024-03-29 21:11:29.000000 pami-2024.5.28.1/PAMI/periodicFrequentPattern/cuda/gPFMinerBit.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.764092 pami-2024.5.28.1/PAMI/periodicFrequentPattern/maximal/
--rw-r--r--   0 vanithak   (502) staff       (20)    31832 2024-03-29 21:11:29.000000 pami-2024.5.28.1/PAMI/periodicFrequentPattern/maximal/MaxPFGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/periodicFrequentPattern/maximal/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     7869 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/periodicFrequentPattern/maximal/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.766921 pami-2024.5.28.1/PAMI/periodicFrequentPattern/pyspark/
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/periodicFrequentPattern/pyspark/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     5219 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/periodicFrequentPattern/pyspark/abstract.py
--rw-r--r--   0 vanithak   (502) staff       (20)    26749 2024-03-29 21:11:29.000000 pami-2024.5.28.1/PAMI/periodicFrequentPattern/pyspark/parallelPFPGrowth.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.768689 pami-2024.5.28.1/PAMI/periodicFrequentPattern/topk/
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.774700 pami-2024.5.28.1/PAMI/periodicFrequentPattern/topk/TopkPFP/
--rw-r--r--   0 vanithak   (502) staff       (20)    19951 2024-04-09 02:02:26.000000 pami-2024.5.28.1/PAMI/periodicFrequentPattern/topk/TopkPFP/TopkPFP.py
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/periodicFrequentPattern/topk/TopkPFP/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6862 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/periodicFrequentPattern/topk/TopkPFP/abstract.py
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/periodicFrequentPattern/topk/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.785587 pami-2024.5.28.1/PAMI/periodicFrequentPattern/topk/kPFPMiner/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/periodicFrequentPattern/topk/kPFPMiner/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     4589 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/periodicFrequentPattern/topk/kPFPMiner/abstract.py
--rw-r--r--   0 vanithak   (502) staff       (20)    17514 2024-03-29 21:11:29.000000 pami-2024.5.28.1/PAMI/periodicFrequentPattern/topk/kPFPMiner/kPFPMiner.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.787103 pami-2024.5.28.1/PAMI/recurringPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/recurringPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.792131 pami-2024.5.28.1/PAMI/recurringPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    29135 2024-03-29 21:11:29.000000 pami-2024.5.28.1/PAMI/recurringPattern/basic/RPGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/recurringPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6637 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/recurringPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.793155 pami-2024.5.28.1/PAMI/relativeFrequentPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/relativeFrequentPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.797037 pami-2024.5.28.1/PAMI/relativeFrequentPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    30349 2024-03-29 21:11:29.000000 pami-2024.5.28.1/PAMI/relativeFrequentPattern/basic/RSFPGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/relativeFrequentPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     4261 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/relativeFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.798121 pami-2024.5.28.1/PAMI/relativeHighUtilityPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/relativeHighUtilityPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.800902 pami-2024.5.28.1/PAMI/relativeHighUtilityPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    35406 2024-04-09 02:02:26.000000 pami-2024.5.28.1/PAMI/relativeHighUtilityPattern/basic/RHUIM.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/relativeHighUtilityPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6052 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/relativeHighUtilityPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.801923 pami-2024.5.28.1/PAMI/sequence/
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/sequence/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.802448 pami-2024.5.28.1/PAMI/sequentialPatternMining/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/sequentialPatternMining/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.810853 pami-2024.5.28.1/PAMI/sequentialPatternMining/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    42265 2024-04-09 02:02:26.000000 pami-2024.5.28.1/PAMI/sequentialPatternMining/basic/SPADE.py
--rw-r--r--   0 vanithak   (502) staff       (20)    19986 2024-04-09 02:02:26.000000 pami-2024.5.28.1/PAMI/sequentialPatternMining/basic/SPAM.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/sequentialPatternMining/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6569 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/sequentialPatternMining/basic/abstract.py
--rw-r--r--   0 vanithak   (502) staff       (20)    24786 2024-04-09 02:02:26.000000 pami-2024.5.28.1/PAMI/sequentialPatternMining/basic/prefixSpan.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.814988 pami-2024.5.28.1/PAMI/sequentialPatternMining/closed/
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/sequentialPatternMining/closed/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6285 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/sequentialPatternMining/closed/abstract.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/sequentialPatternMining/closed/bide.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.815503 pami-2024.5.28.1/PAMI/stablePeriodicFrequentPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/stablePeriodicFrequentPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.826703 pami-2024.5.28.1/PAMI/stablePeriodicFrequentPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    18431 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/stablePeriodicFrequentPattern/basic/SPPEclat.py
--rw-r--r--   0 vanithak   (502) staff       (20)    26770 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/stablePeriodicFrequentPattern/basic/SPPGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)    19807 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/stablePeriodicFrequentPattern/basic/SPPGrowthDump.py
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/stablePeriodicFrequentPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     7271 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/stablePeriodicFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.830193 pami-2024.5.28.1/PAMI/stablePeriodicFrequentPattern/topK/
--rw-r--r--   0 vanithak   (502) staff       (20)    27859 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/stablePeriodicFrequentPattern/topK/TSPIN.py
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/stablePeriodicFrequentPattern/topK/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     7173 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/stablePeriodicFrequentPattern/topK/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.831216 pami-2024.5.28.1/PAMI/subgraphMining/
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/subgraphMining/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.847443 pami-2024.5.28.1/PAMI/subgraphMining/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/subgraphMining/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     1236 2024-05-27 02:58:37.000000 pami-2024.5.28.1/PAMI/subgraphMining/basic/abstract.py
--rw-r--r--   0 vanithak   (502) staff       (20)     2396 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/subgraphMining/basic/dfsCode.py
--rw-r--r--   0 vanithak   (502) staff       (20)      772 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/subgraphMining/basic/edge.py
--rw-r--r--   0 vanithak   (502) staff       (20)     2616 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/subgraphMining/basic/extendedEdge.py
--rw-r--r--   0 vanithak   (502) staff       (20)      670 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/subgraphMining/basic/frequentSubgraph.py
--rw-r--r--   0 vanithak   (502) staff       (20)     4943 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/subgraphMining/basic/graph.py
--rw-r--r--   0 vanithak   (502) staff       (20)    29464 2024-05-28 08:47:44.000000 pami-2024.5.28.1/PAMI/subgraphMining/basic/gspan.py
--rw-r--r--   0 vanithak   (502) staff       (20)     1748 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/subgraphMining/basic/sparseTriangularMatrix.py
--rw-r--r--   0 vanithak   (502) staff       (20)      826 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/subgraphMining/basic/vertex.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.857953 pami-2024.5.28.1/PAMI/subgraphMining/topK/
--rw-r--r--   0 vanithak   (502) staff       (20)     1949 2024-03-29 21:11:29.000000 pami-2024.5.28.1/PAMI/subgraphMining/topK/DFSCode.py
--rw-r--r--   0 vanithak   (502) staff       (20)      593 2024-03-29 21:11:29.000000 pami-2024.5.28.1/PAMI/subgraphMining/topK/DFSThread.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-29 21:11:29.000000 pami-2024.5.28.1/PAMI/subgraphMining/topK/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     1309 2024-05-27 02:58:37.000000 pami-2024.5.28.1/PAMI/subgraphMining/topK/abstract.py
--rw-r--r--   0 vanithak   (502) staff       (20)      772 2024-03-29 21:11:29.000000 pami-2024.5.28.1/PAMI/subgraphMining/topK/edge.py
--rw-r--r--   0 vanithak   (502) staff       (20)     2613 2024-03-29 21:11:29.000000 pami-2024.5.28.1/PAMI/subgraphMining/topK/extendedEdge.py
--rw-r--r--   0 vanithak   (502) staff       (20)      674 2024-03-29 21:11:29.000000 pami-2024.5.28.1/PAMI/subgraphMining/topK/frequentSubgraph.py
--rw-r--r--   0 vanithak   (502) staff       (20)     4285 2024-05-27 02:58:37.000000 pami-2024.5.28.1/PAMI/subgraphMining/topK/graph.py
--rw-r--r--   0 vanithak   (502) staff       (20)     1486 2024-03-29 21:11:29.000000 pami-2024.5.28.1/PAMI/subgraphMining/topK/sparseTriangularMatrix.py
--rw-r--r--   0 vanithak   (502) staff       (20)    21196 2024-05-27 02:58:37.000000 pami-2024.5.28.1/PAMI/subgraphMining/topK/tkg.py
--rw-r--r--   0 vanithak   (502) staff       (20)      818 2024-03-29 21:11:29.000000 pami-2024.5.28.1/PAMI/subgraphMining/topK/vertex.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.860914 pami-2024.5.28.1/PAMI/uncertainFaultTolerantFrequentPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)    17694 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/uncertainFaultTolerantFrequentPattern/VBFTMine.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/uncertainFaultTolerantFrequentPattern/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6756 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/uncertainFaultTolerantFrequentPattern/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.862211 pami-2024.5.28.1/PAMI/uncertainFrequentPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/uncertainFrequentPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.880169 pami-2024.5.28.1/PAMI/uncertainFrequentPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    28251 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/uncertainFrequentPattern/basic/CUFPTree.py
--rw-r--r--   0 vanithak   (502) staff       (20)    27430 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/uncertainFrequentPattern/basic/PUFGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)    20311 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/uncertainFrequentPattern/basic/TUFP.py
--rw-r--r--   0 vanithak   (502) staff       (20)    20112 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/uncertainFrequentPattern/basic/TubeP.py
--rw-r--r--   0 vanithak   (502) staff       (20)    28047 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/uncertainFrequentPattern/basic/TubeS.py
--rw-r--r--   0 vanithak   (502) staff       (20)    25675 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/uncertainFrequentPattern/basic/UFGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)    19871 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/uncertainFrequentPattern/basic/UVECLAT.py
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/uncertainFrequentPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     4945 2024-04-09 02:02:26.000000 pami-2024.5.28.1/PAMI/uncertainFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.881194 pami-2024.5.28.1/PAMI/uncertainGeoreferencedFrequentPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/uncertainGeoreferencedFrequentPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.885161 pami-2024.5.28.1/PAMI/uncertainGeoreferencedFrequentPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    30577 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/uncertainGeoreferencedFrequentPattern/basic/GFPGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/uncertainGeoreferencedFrequentPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     4986 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/uncertainGeoreferencedFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.886293 pami-2024.5.28.1/PAMI/uncertainPeriodicFrequentPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/uncertainPeriodicFrequentPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.892415 pami-2024.5.28.1/PAMI/uncertainPeriodicFrequentPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    33173 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/uncertainPeriodicFrequentPattern/basic/UPFPGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)    33178 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/uncertainPeriodicFrequentPattern/basic/UPFPGrowthPlus.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/uncertainPeriodicFrequentPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6536 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/uncertainPeriodicFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.893472 pami-2024.5.28.1/PAMI/weightedFrequentNeighbourhoodPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/weightedFrequentNeighbourhoodPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.897127 pami-2024.5.28.1/PAMI/weightedFrequentNeighbourhoodPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    29414 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/weightedFrequentNeighbourhoodPattern/basic/SWFPGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/weightedFrequentNeighbourhoodPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6603 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/weightedFrequentNeighbourhoodPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.898486 pami-2024.5.28.1/PAMI/weightedFrequentPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/weightedFrequentPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.901865 pami-2024.5.28.1/PAMI/weightedFrequentPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    25844 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/weightedFrequentPattern/basic/WFIM.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/weightedFrequentPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6659 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/weightedFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.902867 pami-2024.5.28.1/PAMI/weightedFrequentRegularPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/weightedFrequentRegularPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.906384 pami-2024.5.28.1/PAMI/weightedFrequentRegularPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    29035 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/weightedFrequentRegularPattern/basic/WFRIMiner.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/weightedFrequentRegularPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     7495 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/weightedFrequentRegularPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.907756 pami-2024.5.28.1/PAMI/weightedUncertainFrequentPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/weightedUncertainFrequentPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.912331 pami-2024.5.28.1/PAMI/weightedUncertainFrequentPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    31245 2024-05-01 06:38:01.000000 pami-2024.5.28.1/PAMI/weightedUncertainFrequentPattern/basic/WUFIM.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/weightedUncertainFrequentPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     4771 2024-03-12 04:33:29.000000 pami-2024.5.28.1/PAMI/weightedUncertainFrequentPattern/basic/abstract.py
--rw-r--r--   0 vanithak   (502) staff       (20)    75626 2024-05-28 08:48:12.919580 pami-2024.5.28.1/PKG-INFO
--rw-r--r--   0 vanithak   (502) staff       (20)    74134 2024-05-27 02:58:37.000000 pami-2024.5.28.1/README.md
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-28 08:48:12.916058 pami-2024.5.28.1/pami.egg-info/
--rw-r--r--   0 vanithak   (502) staff       (20)    75626 2024-05-28 08:48:11.000000 pami-2024.5.28.1/pami.egg-info/PKG-INFO
--rw-r--r--   0 vanithak   (502) staff       (20)    18468 2024-05-28 08:48:12.000000 pami-2024.5.28.1/pami.egg-info/SOURCES.txt
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-05-28 08:48:11.000000 pami-2024.5.28.1/pami.egg-info/dependency_links.txt
--rw-r--r--   0 vanithak   (502) staff       (20)      255 2024-05-28 08:48:11.000000 pami-2024.5.28.1/pami.egg-info/requires.txt
--rw-r--r--   0 vanithak   (502) staff       (20)        5 2024-05-28 08:48:11.000000 pami-2024.5.28.1/pami.egg-info/top_level.txt
--rw-r--r--   0 vanithak   (502) staff       (20)       38 2024-05-28 08:48:12.920727 pami-2024.5.28.1/setup.cfg
--rw-r--r--   0 vanithak   (502) staff       (20)     1536 2024-05-28 08:48:04.000000 pami-2024.5.28.1/setup.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:53:15.934216 pami-2024.5.7.1/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    35149 2024-02-19 12:10:44.000000 pami-2024.5.7.1/LICENSE
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:51:04.077991 pami-2024.5.7.1/PAMI/
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:51:06.209582 pami-2024.5.7.1/PAMI/AssociationRules/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        1 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/AssociationRules/__init__.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:51:06.217030 pami-2024.5.7.1/PAMI/AssociationRules/basic/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    14350 2024-05-05 10:33:44.000000 pami-2024.5.7.1/PAMI/AssociationRules/basic/_ARWithLeverage.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    14263 2024-05-05 10:33:44.000000 pami-2024.5.7.1/PAMI/AssociationRules/basic/_ARWithLift.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    19416 2024-05-05 10:33:44.000000 pami-2024.5.7.1/PAMI/AssociationRules/basic/_RuleMiner.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/AssociationRules/basic/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     6613 2024-05-05 10:33:44.000000 pami-2024.5.7.1/PAMI/AssociationRules/basic/abstract.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    12724 2024-05-05 10:33:44.000000 pami-2024.5.7.1/PAMI/AssociationRules/basic/confidence.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    12638 2024-05-05 10:33:44.000000 pami-2024.5.7.1/PAMI/AssociationRules/basic/leverage.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    12430 2024-05-05 10:33:44.000000 pami-2024.5.7.1/PAMI/AssociationRules/basic/lift.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)      139 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/__init__.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:51:06.217762 pami-2024.5.7.1/PAMI/correlatedPattern/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)      727 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/correlatedPattern/__init__.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:51:07.302034 pami-2024.5.7.1/PAMI/correlatedPattern/basic/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    26443 2024-05-05 10:33:44.000000 pami-2024.5.7.1/PAMI/correlatedPattern/basic/CoMine.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    27988 2024-05-05 10:33:44.000000 pami-2024.5.7.1/PAMI/correlatedPattern/basic/CoMinePlus.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)      727 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/correlatedPattern/basic/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     6208 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/correlatedPattern/basic/abstract.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:51:07.761518 pami-2024.5.7.1/PAMI/coveragePattern/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        1 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/coveragePattern/__init__.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:51:09.787669 pami-2024.5.7.1/PAMI/coveragePattern/basic/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    14643 2024-04-24 02:44:32.000000 pami-2024.5.7.1/PAMI/coveragePattern/basic/CMine.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    17200 2024-04-19 05:15:04.000000 pami-2024.5.7.1/PAMI/coveragePattern/basic/CPPG.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        1 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/coveragePattern/basic/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     7155 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/coveragePattern/basic/abstract.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:51:14.515242 pami-2024.5.7.1/PAMI/extras/
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:51:19.571452 pami-2024.5.7.1/PAMI/extras/DF2DB/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     4360 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/DF2DB/DF2DB.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     4287 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/DF2DB/DF2DBPlus.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    10331 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/DF2DB/DenseFormatDF.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     5413 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/DF2DB/SparseFormatDF.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/extras/DF2DB/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     3103 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/DF2DB/createTDB.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     6948 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/DF2DB/denseDF2DBPlus.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    11940 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/DF2DB/denseDF2DB_dump.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     5336 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/DF2DB/sparseDF2DBPlus.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/extras/__init__.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:51:20.571618 pami-2024.5.7.1/PAMI/extras/calculateMISValues/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/extras/calculateMISValues/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     6468 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/calculateMISValues/usingBeta.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     6499 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/calculateMISValues/usingSD.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     7345 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/convertMultiTSIntoFuzzy.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:51:25.517301 pami-2024.5.7.1/PAMI/extras/dbStats/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    14951 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/dbStats/FuzzyDatabase.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    13796 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/dbStats/MultipleTimeSeriesFuzzyDatabaseStats.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    16034 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/dbStats/SequentialDatabase.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    16883 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/dbStats/TemporalDatabase.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    12839 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/dbStats/TransactionalDatabase.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    15120 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/dbStats/UncertainTemporalDatabase.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    11953 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/dbStats/UncertainTransactionalDatabase.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    12679 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/dbStats/UtilityDatabase.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/extras/dbStats/__init__.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:51:26.881603 pami-2024.5.7.1/PAMI/extras/fuzzyTransformation/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/extras/fuzzyTransformation/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     5238 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/extras/fuzzyTransformation/abstract.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     8594 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/fuzzyTransformation/temporalToFuzzy.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     8792 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/fuzzyTransformation/transactionalToFuzzy.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     8313 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/fuzzyTransformation/utilityToFuzzy.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:51:28.353069 pami-2024.5.7.1/PAMI/extras/generateDatabase/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/extras/generateDatabase/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     5685 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/generateDatabase/generateSpatioTemporalDatabase.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     9558 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/generateDatabase/generateTemporalDatabase.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     5971 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/generateDatabase/generateTransactionalDatabase.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     5157 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/generateLatexGraphFile.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:51:31.609764 pami-2024.5.7.1/PAMI/extras/graph/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     3223 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/graph/DF2Fig.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     3577 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/graph/DF2Tex.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/extras/graph/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     2750 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/graph/plotLineGraphFromDictionary.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     3599 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/graph/plotLineGraphsFromDataFrame.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     4465 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/graph/visualizeFuzzyPatterns.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     4240 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/graph/visualizePatterns.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:51:32.114052 pami-2024.5.7.1/PAMI/extras/image2Database/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/extras/image2Database/__init__.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:51:32.512117 pami-2024.5.7.1/PAMI/extras/imageProcessing/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        1 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/extras/imageProcessing/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     6488 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/imageProcessing/imagery2Databases.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:51:33.664654 pami-2024.5.7.1/PAMI/extras/messaging/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/messaging/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)      533 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/messaging/discord.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     1575 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/messaging/gmail.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:51:35.152172 pami-2024.5.7.1/PAMI/extras/neighbours/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/extras/neighbours/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     4784 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/neighbours/findNeighborsUsingEuclideanDistanceforPointInfo.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     4410 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/neighbours/findNeighboursUsingEuclidean.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     4305 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/neighbours/findNeighboursUsingGeodesic.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     5013 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/plotPointOnMap.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     5183 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/plotPointOnMap_dump.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:51:35.733803 pami-2024.5.7.1/PAMI/extras/sampleDatasets/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        1 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/extras/sampleDatasets/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     4024 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/scatterPlotSpatialPoints.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:51:38.888297 pami-2024.5.7.1/PAMI/extras/stats/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    12724 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/stats/TransactionalDatabase.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/stats/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     4144 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/stats/graphDatabase.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    15998 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/stats/sequentialDatabase.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    16926 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/stats/temporalDatabase.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    12692 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/stats/utilityDatabase.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:51:47.358784 pami-2024.5.7.1/PAMI/extras/syntheticDataGenerator/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     7276 2024-04-19 05:15:04.000000 pami-2024.5.7.1/PAMI/extras/syntheticDataGenerator/TemporalDatabase.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     6930 2024-05-07 07:49:33.000000 pami-2024.5.7.1/PAMI/extras/syntheticDataGenerator/TransactionalDatabase.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/extras/syntheticDataGenerator/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     2325 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialTemporal.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     2254 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialTransactions.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     2539 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialUncertainTransaction.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     1880 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/extras/syntheticDataGenerator/createSyntheticTemporal.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     1843 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/extras/syntheticDataGenerator/createSyntheticTransactions.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     2117 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/extras/syntheticDataGenerator/createSyntheticUncertainTemporal.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     2066 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/extras/syntheticDataGenerator/createSyntheticUncertainTransactions.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     2262 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/extras/syntheticDataGenerator/createSyntheticUtility.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/extras/syntheticDataGenerator/fuzzyDatabase.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     1121 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/extras/syntheticDataGenerator/generateTemporal.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     1111 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/extras/syntheticDataGenerator/generateTransactional.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     1625 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/extras/syntheticDataGenerator/generateUncertainTemporal.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     1610 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/extras/syntheticDataGenerator/generateUncertainTransactional.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     3613 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/extras/syntheticDataGenerator/generateUtilityTemporal.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     3603 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/extras/syntheticDataGenerator/generateUtilityTransactional.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/extras/syntheticDataGenerator/georeferencedTemporalDatabase.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/extras/syntheticDataGenerator/georeferencedTransactionalDatabase.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     4324 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/syntheticDataGenerator/syntheticUtilityDatabase.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     3283 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/syntheticDataGenerator/temporalDatabaseGen.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     4842 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/extras/syntheticDataGenerator/utilityDatabase.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     3240 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/topKPatterns.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     2322 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/uncertaindb_convert.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:51:47.809100 pami-2024.5.7.1/PAMI/extras/visualize/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/visualize/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     1897 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/extras/visualize/graphs.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:51:48.251818 pami-2024.5.7.1/PAMI/faultTolerantFrequentPattern/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        1 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/faultTolerantFrequentPattern/__init__.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:51:50.689479 pami-2024.5.7.1/PAMI/faultTolerantFrequentPattern/basic/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    14510 2024-04-19 05:15:04.000000 pami-2024.5.7.1/PAMI/faultTolerantFrequentPattern/basic/FTApriori.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    23166 2024-04-19 05:15:04.000000 pami-2024.5.7.1/PAMI/faultTolerantFrequentPattern/basic/FTFPGrowth.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        1 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/faultTolerantFrequentPattern/basic/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     6856 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/faultTolerantFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:51:51.036520 pami-2024.5.7.1/PAMI/frequentPattern/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)      727 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/frequentPattern/__init__.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:51:52.796321 pami-2024.5.7.1/PAMI/frequentPattern/basic/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    13840 2024-05-01 07:34:25.000000 pami-2024.5.7.1/PAMI/frequentPattern/basic/Apriori.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    13544 2024-04-30 09:23:37.000000 pami-2024.5.7.1/PAMI/frequentPattern/basic/Aprioribitset.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    13631 2024-05-05 10:33:44.000000 pami-2024.5.7.1/PAMI/frequentPattern/basic/ECLAT.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    14033 2024-05-05 10:33:44.000000 pami-2024.5.7.1/PAMI/frequentPattern/basic/ECLATDiffset.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    13440 2024-05-05 10:33:44.000000 pami-2024.5.7.1/PAMI/frequentPattern/basic/ECLATbitset.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    19498 2024-05-01 07:34:25.000000 pami-2024.5.7.1/PAMI/frequentPattern/basic/FPGrowth.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    15219 2024-04-18 05:41:27.000000 pami-2024.5.7.1/PAMI/frequentPattern/basic/_Apriori.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    22703 2024-04-18 05:41:27.000000 pami-2024.5.7.1/PAMI/frequentPattern/basic/_FPGrowth.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/frequentPattern/basic/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     6818 2024-04-30 08:51:40.000000 pami-2024.5.7.1/PAMI/frequentPattern/basic/abstract.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:51:52.798085 pami-2024.5.7.1/PAMI/frequentPattern/closed/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    20178 2024-05-01 07:34:25.000000 pami-2024.5.7.1/PAMI/frequentPattern/closed/CHARM.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/frequentPattern/closed/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     6580 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/frequentPattern/closed/abstract.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:51:56.613931 pami-2024.5.7.1/PAMI/frequentPattern/cuda/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/frequentPattern/cuda/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     5980 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/frequentPattern/cuda/abstract.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    13664 2024-04-19 05:15:04.000000 pami-2024.5.7.1/PAMI/frequentPattern/cuda/cuApriori.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    14418 2024-04-19 05:15:04.000000 pami-2024.5.7.1/PAMI/frequentPattern/cuda/cuAprioriBit.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    13015 2024-04-19 05:15:04.000000 pami-2024.5.7.1/PAMI/frequentPattern/cuda/cuEclat.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    14583 2024-04-19 05:15:04.000000 pami-2024.5.7.1/PAMI/frequentPattern/cuda/cuEclatBit.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    14499 2024-04-19 05:15:04.000000 pami-2024.5.7.1/PAMI/frequentPattern/cuda/cudaAprioriGCT.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    17118 2024-04-19 05:15:04.000000 pami-2024.5.7.1/PAMI/frequentPattern/cuda/cudaAprioriTID.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    14178 2024-04-19 05:15:04.000000 pami-2024.5.7.1/PAMI/frequentPattern/cuda/cudaEclatGCT.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:51:58.077779 pami-2024.5.7.1/PAMI/frequentPattern/maximal/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    26092 2024-04-19 05:15:04.000000 pami-2024.5.7.1/PAMI/frequentPattern/maximal/MaxFPGrowth.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)      727 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/frequentPattern/maximal/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     6561 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/frequentPattern/maximal/abstract.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:00.749285 pami-2024.5.7.1/PAMI/frequentPattern/pyspark/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        1 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/frequentPattern/pyspark/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     5573 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/frequentPattern/pyspark/abstract.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    15452 2024-04-19 05:15:04.000000 pami-2024.5.7.1/PAMI/frequentPattern/pyspark/parallelApriori.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    12947 2024-04-19 05:15:04.000000 pami-2024.5.7.1/PAMI/frequentPattern/pyspark/parallelECLAT.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    17438 2024-04-19 05:15:04.000000 pami-2024.5.7.1/PAMI/frequentPattern/pyspark/parallelFPGrowth.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:02.338817 pami-2024.5.7.1/PAMI/frequentPattern/topk/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    15426 2024-04-19 05:15:04.000000 pami-2024.5.7.1/PAMI/frequentPattern/topk/FAE.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        1 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/frequentPattern/topk/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     4575 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/frequentPattern/topk/abstract.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:02.878077 pami-2024.5.7.1/PAMI/fuzzyCorrelatedPattern/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)      727 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/fuzzyCorrelatedPattern/__init__.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:03.813240 pami-2024.5.7.1/PAMI/fuzzyCorrelatedPattern/basic/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    28229 2024-04-24 02:44:32.000000 pami-2024.5.7.1/PAMI/fuzzyCorrelatedPattern/basic/FCPGrowth.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)      727 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/fuzzyCorrelatedPattern/basic/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     6652 2024-04-24 02:44:32.000000 pami-2024.5.7.1/PAMI/fuzzyCorrelatedPattern/basic/abstract.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:03.813617 pami-2024.5.7.1/PAMI/fuzzyFrequentPattern/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)      727 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/fuzzyFrequentPattern/__init__.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:04.752458 pami-2024.5.7.1/PAMI/fuzzyFrequentPattern/basic/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    22973 2024-04-24 02:44:32.000000 pami-2024.5.7.1/PAMI/fuzzyFrequentPattern/basic/FFIMiner.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    28621 2024-04-24 02:44:32.000000 pami-2024.5.7.1/PAMI/fuzzyFrequentPattern/basic/FFIMiner_old.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        1 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/fuzzyFrequentPattern/basic/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     6442 2024-04-24 02:44:32.000000 pami-2024.5.7.1/PAMI/fuzzyFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:04.753915 pami-2024.5.7.1/PAMI/fuzzyGeoreferencedFrequentPattern/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)      727 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/fuzzyGeoreferencedFrequentPattern/__init__.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:05.144229 pami-2024.5.7.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    26162 2024-04-24 02:44:32.000000 pami-2024.5.7.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/FFSPMiner.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    28607 2024-04-25 00:26:02.000000 pami-2024.5.7.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/FFSPMiner_old.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     6730 2024-04-24 02:44:32.000000 pami-2024.5.7.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:05.145081 pami-2024.5.7.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        1 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/__init__.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:05.541550 pami-2024.5.7.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    28521 2024-04-24 02:44:32.000000 pami-2024.5.7.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/FGPFPMiner.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    33585 2024-04-24 02:44:32.000000 pami-2024.5.7.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/FGPFPMiner_old.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     6623 2024-04-24 02:44:32.000000 pami-2024.5.7.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:05.542454 pami-2024.5.7.1/PAMI/fuzzyPartialPeriodicPatterns/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        1 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/fuzzyPartialPeriodicPatterns/__init__.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:06.954523 pami-2024.5.7.1/PAMI/fuzzyPartialPeriodicPatterns/basic/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    22562 2024-04-24 02:44:32.000000 pami-2024.5.7.1/PAMI/fuzzyPartialPeriodicPatterns/basic/F3PMiner.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        1 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/fuzzyPartialPeriodicPatterns/basic/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     6469 2024-04-24 02:44:32.000000 pami-2024.5.7.1/PAMI/fuzzyPartialPeriodicPatterns/basic/abstract.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:06.956333 pami-2024.5.7.1/PAMI/fuzzyPeriodicFrequentPattern/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)      727 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/fuzzyPeriodicFrequentPattern/__init__.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:08.240652 pami-2024.5.7.1/PAMI/fuzzyPeriodicFrequentPattern/basic/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    25786 2024-04-24 02:44:32.000000 pami-2024.5.7.1/PAMI/fuzzyPeriodicFrequentPattern/basic/FPFPMiner.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    27472 2024-04-24 02:44:32.000000 pami-2024.5.7.1/PAMI/fuzzyPeriodicFrequentPattern/basic/FPFPMiner_old.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)      727 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/fuzzyPeriodicFrequentPattern/basic/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     6683 2024-04-24 02:44:32.000000 pami-2024.5.7.1/PAMI/fuzzyPeriodicFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:08.242241 pami-2024.5.7.1/PAMI/geoReferencedPeriodicFrequentPattern/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        1 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/geoReferencedPeriodicFrequentPattern/__init__.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:08.699162 pami-2024.5.7.1/PAMI/geoReferencedPeriodicFrequentPattern/basic/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    22107 2024-04-24 02:44:32.000000 pami-2024.5.7.1/PAMI/geoReferencedPeriodicFrequentPattern/basic/GPFPMiner.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/geoReferencedPeriodicFrequentPattern/basic/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     6791 2024-04-24 02:44:32.000000 pami-2024.5.7.1/PAMI/geoReferencedPeriodicFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:08.700057 pami-2024.5.7.1/PAMI/georeferencedFrequentPattern/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)      727 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/georeferencedFrequentPattern/__init__.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:10.108287 pami-2024.5.7.1/PAMI/georeferencedFrequentPattern/basic/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    22965 2024-04-19 05:15:04.000000 pami-2024.5.7.1/PAMI/georeferencedFrequentPattern/basic/FSPGrowth.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    21152 2024-04-24 02:44:32.000000 pami-2024.5.7.1/PAMI/georeferencedFrequentPattern/basic/SpatialECLAT.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        1 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/georeferencedFrequentPattern/basic/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     6697 2024-04-24 02:44:32.000000 pami-2024.5.7.1/PAMI/georeferencedFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:10.109569 pami-2024.5.7.1/PAMI/georeferencedFrequentSequencePattern/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/georeferencedFrequentSequencePattern/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     6696 2024-04-24 02:44:32.000000 pami-2024.5.7.1/PAMI/georeferencedFrequentSequencePattern/abstract.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:10.110437 pami-2024.5.7.1/PAMI/georeferencedPartialPeriodicPattern/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        1 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/georeferencedPartialPeriodicPattern/__init__.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:10.547357 pami-2024.5.7.1/PAMI/georeferencedPartialPeriodicPattern/basic/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    21718 2024-04-24 02:44:32.000000 pami-2024.5.7.1/PAMI/georeferencedPartialPeriodicPattern/basic/STEclat.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/georeferencedPartialPeriodicPattern/basic/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     6178 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/georeferencedPartialPeriodicPattern/basic/abstract.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:10.948842 pami-2024.5.7.1/PAMI/highUtilityFrequentPattern/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/highUtilityFrequentPattern/__init__.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:10.951914 pami-2024.5.7.1/PAMI/highUtilityFrequentPattern/basic/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    38524 2024-04-24 02:44:32.000000 pami-2024.5.7.1/PAMI/highUtilityFrequentPattern/basic/HUFIM.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/highUtilityFrequentPattern/basic/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     6179 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/highUtilityFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:11.451342 pami-2024.5.7.1/PAMI/highUtilityGeoreferencedFrequentPattern/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)      727 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/highUtilityGeoreferencedFrequentPattern/__init__.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:12.341965 pami-2024.5.7.1/PAMI/highUtilityGeoreferencedFrequentPattern/basic/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    42772 2024-04-19 05:15:04.000000 pami-2024.5.7.1/PAMI/highUtilityGeoreferencedFrequentPattern/basic/SHUFIM.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/highUtilityGeoreferencedFrequentPattern/basic/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     6307 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/highUtilityGeoreferencedFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:12.762762 pami-2024.5.7.1/PAMI/highUtilityPattern/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/highUtilityPattern/__init__.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:14.583966 pami-2024.5.7.1/PAMI/highUtilityPattern/basic/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    35224 2024-04-19 05:15:04.000000 pami-2024.5.7.1/PAMI/highUtilityPattern/basic/EFIM.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    26511 2024-04-19 05:15:04.000000 pami-2024.5.7.1/PAMI/highUtilityPattern/basic/HMiner.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    28921 2024-04-19 05:15:04.000000 pami-2024.5.7.1/PAMI/highUtilityPattern/basic/UPGrowth.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/highUtilityPattern/basic/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     5166 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/highUtilityPattern/basic/abstract.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    19909 2024-04-19 05:15:04.000000 pami-2024.5.7.1/PAMI/highUtilityPattern/basic/efimParallel.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:16.199955 pami-2024.5.7.1/PAMI/highUtilityPattern/parallel/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        1 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/highUtilityPattern/parallel/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     5166 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/highUtilityPattern/parallel/abstract.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    17831 2024-04-19 05:15:04.000000 pami-2024.5.7.1/PAMI/highUtilityPattern/parallel/efimparallel.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:17.924365 pami-2024.5.7.1/PAMI/highUtilityPatternsInStreams/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    30022 2024-04-19 05:15:04.000000 pami-2024.5.7.1/PAMI/highUtilityPatternsInStreams/HUPMS.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    32848 2024-04-19 05:15:04.000000 pami-2024.5.7.1/PAMI/highUtilityPatternsInStreams/SHUGrowth.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/highUtilityPatternsInStreams/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     5193 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/highUtilityPatternsInStreams/abstract.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:18.778353 pami-2024.5.7.1/PAMI/highUtilitySpatialPattern/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)      727 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/highUtilitySpatialPattern/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     6716 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/highUtilitySpatialPattern/abstract.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:20.160402 pami-2024.5.7.1/PAMI/highUtilitySpatialPattern/basic/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    29850 2024-04-19 05:15:04.000000 pami-2024.5.7.1/PAMI/highUtilitySpatialPattern/basic/HDSHUIM.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    37379 2024-04-19 05:15:04.000000 pami-2024.5.7.1/PAMI/highUtilitySpatialPattern/basic/SHUIM.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/highUtilitySpatialPattern/basic/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     5934 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/highUtilitySpatialPattern/basic/abstract.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:21.405131 pami-2024.5.7.1/PAMI/highUtilitySpatialPattern/topk/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    37519 2024-04-19 05:15:04.000000 pami-2024.5.7.1/PAMI/highUtilitySpatialPattern/topk/TKSHUIM.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/highUtilitySpatialPattern/topk/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     6618 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/highUtilitySpatialPattern/topk/abstract.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:21.911432 pami-2024.5.7.1/PAMI/localPeriodicPattern/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/localPeriodicPattern/__init__.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:23.124520 pami-2024.5.7.1/PAMI/localPeriodicPattern/basic/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    35370 2024-04-19 05:15:04.000000 pami-2024.5.7.1/PAMI/localPeriodicPattern/basic/LPPGrowth.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    24681 2024-04-19 05:15:04.000000 pami-2024.5.7.1/PAMI/localPeriodicPattern/basic/LPPMBreadth.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    23709 2024-04-19 05:15:04.000000 pami-2024.5.7.1/PAMI/localPeriodicPattern/basic/LPPMDepth.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/localPeriodicPattern/basic/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     8385 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/localPeriodicPattern/basic/abstract.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:23.527273 pami-2024.5.7.1/PAMI/multipleMinimumSupportBasedFrequentPattern/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        1 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/multipleMinimumSupportBasedFrequentPattern/__init__.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:24.843594 pami-2024.5.7.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    24308 2024-04-19 05:15:04.000000 pami-2024.5.7.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/CFPGrowth.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    22008 2024-04-19 05:15:04.000000 pami-2024.5.7.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/CFPGrowthPlus.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     5921 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:25.299834 pami-2024.5.7.1/PAMI/partialPeriodicFrequentPattern/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/partialPeriodicFrequentPattern/__init__.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:25.303207 pami-2024.5.7.1/PAMI/partialPeriodicFrequentPattern/basic/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    28270 2024-04-25 00:26:02.000000 pami-2024.5.7.1/PAMI/partialPeriodicFrequentPattern/basic/GPFgrowth.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    21954 2024-04-25 00:26:02.000000 pami-2024.5.7.1/PAMI/partialPeriodicFrequentPattern/basic/PPF_DFS.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/partialPeriodicFrequentPattern/basic/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     5398 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/partialPeriodicFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:25.752184 pami-2024.5.7.1/PAMI/partialPeriodicPattern/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)      727 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/partialPeriodicPattern/__init__.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:27.923986 pami-2024.5.7.1/PAMI/partialPeriodicPattern/basic/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    26420 2024-04-25 00:26:02.000000 pami-2024.5.7.1/PAMI/partialPeriodicPattern/basic/GThreePGrowth.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     4329 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/partialPeriodicPattern/basic/Gabstract.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    25525 2024-04-19 05:15:04.000000 pami-2024.5.7.1/PAMI/partialPeriodicPattern/basic/PPPGrowth.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    19577 2024-04-19 05:15:04.000000 pami-2024.5.7.1/PAMI/partialPeriodicPattern/basic/PPP_ECLAT.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)      727 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/partialPeriodicPattern/basic/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     5520 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/partialPeriodicPattern/basic/abstract.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:29.234435 pami-2024.5.7.1/PAMI/partialPeriodicPattern/closed/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    22070 2024-04-19 05:15:04.000000 pami-2024.5.7.1/PAMI/partialPeriodicPattern/closed/PPPClose.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        1 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/partialPeriodicPattern/closed/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     5605 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/partialPeriodicPattern/closed/abstract.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:30.449126 pami-2024.5.7.1/PAMI/partialPeriodicPattern/maximal/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    29144 2024-04-25 00:26:02.000000 pami-2024.5.7.1/PAMI/partialPeriodicPattern/maximal/Max3PGrowth.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)      727 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/partialPeriodicPattern/maximal/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     4278 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/partialPeriodicPattern/maximal/abstract.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:31.916874 pami-2024.5.7.1/PAMI/partialPeriodicPattern/pyspark/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)      727 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/partialPeriodicPattern/pyspark/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     5765 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/partialPeriodicPattern/pyspark/abstract.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    28554 2024-04-19 05:15:04.000000 pami-2024.5.7.1/PAMI/partialPeriodicPattern/pyspark/parallel3PGrowth.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:33.441602 pami-2024.5.7.1/PAMI/partialPeriodicPattern/topk/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        1 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/partialPeriodicPattern/topk/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     6441 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/partialPeriodicPattern/topk/abstract.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    19588 2024-04-19 05:15:04.000000 pami-2024.5.7.1/PAMI/partialPeriodicPattern/topk/k3PMiner.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:34.222296 pami-2024.5.7.1/PAMI/partialPeriodicPatternInMultipleTimeSeries/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    28160 2024-04-24 02:44:32.000000 pami-2024.5.7.1/PAMI/partialPeriodicPatternInMultipleTimeSeries/PPGrowth.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        1 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/partialPeriodicPatternInMultipleTimeSeries/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     6350 2024-04-24 02:44:32.000000 pami-2024.5.7.1/PAMI/partialPeriodicPatternInMultipleTimeSeries/abstract.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:34.223413 pami-2024.5.7.1/PAMI/periodicCorrelatedPattern/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        1 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/periodicCorrelatedPattern/__init__.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:34.656140 pami-2024.5.7.1/PAMI/periodicCorrelatedPattern/basic/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    27559 2024-04-24 02:44:32.000000 pami-2024.5.7.1/PAMI/periodicCorrelatedPattern/basic/EPCPGrowth.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/periodicCorrelatedPattern/basic/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     6691 2024-04-24 02:44:32.000000 pami-2024.5.7.1/PAMI/periodicCorrelatedPattern/basic/abstract.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:34.657070 pami-2024.5.7.1/PAMI/periodicFrequentPattern/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)      727 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/periodicFrequentPattern/__init__.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:39.231540 pami-2024.5.7.1/PAMI/periodicFrequentPattern/basic/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    17272 2024-04-19 05:15:04.000000 pami-2024.5.7.1/PAMI/periodicFrequentPattern/basic/PFECLAT.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    21302 2024-04-19 05:15:04.000000 pami-2024.5.7.1/PAMI/periodicFrequentPattern/basic/PFPGrowth.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    26383 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/periodicFrequentPattern/basic/PFPGrowthPlus.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    18106 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/periodicFrequentPattern/basic/PFPMC.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    36300 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/periodicFrequentPattern/basic/PSGrowth.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    17904 2024-04-18 05:41:27.000000 pami-2024.5.7.1/PAMI/periodicFrequentPattern/basic/_PFECLAT.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    28583 2024-04-18 05:41:27.000000 pami-2024.5.7.1/PAMI/periodicFrequentPattern/basic/_PFPGrowth.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)      726 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/periodicFrequentPattern/basic/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     6549 2024-04-24 02:44:32.000000 pami-2024.5.7.1/PAMI/periodicFrequentPattern/basic/abstract.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    26643 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/periodicFrequentPattern/basic/parallelPFPGrowth.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:40.438175 pami-2024.5.7.1/PAMI/periodicFrequentPattern/closed/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    24417 2024-04-24 02:44:32.000000 pami-2024.5.7.1/PAMI/periodicFrequentPattern/closed/CPFPMiner.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)      727 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/periodicFrequentPattern/closed/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     6539 2024-04-24 02:44:32.000000 pami-2024.5.7.1/PAMI/periodicFrequentPattern/closed/abstract.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:41.859397 pami-2024.5.7.1/PAMI/periodicFrequentPattern/cuda/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        1 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/periodicFrequentPattern/cuda/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     6568 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/periodicFrequentPattern/cuda/abstract.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    23867 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/periodicFrequentPattern/cuda/cuGPFMiner.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    18982 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/periodicFrequentPattern/cuda/gPFMinerBit.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:43.027722 pami-2024.5.7.1/PAMI/periodicFrequentPattern/maximal/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    31832 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/periodicFrequentPattern/maximal/MaxPFGrowth.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)      727 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/periodicFrequentPattern/maximal/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     7869 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/periodicFrequentPattern/maximal/abstract.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:43.925963 pami-2024.5.7.1/PAMI/periodicFrequentPattern/pyspark/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/periodicFrequentPattern/pyspark/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     5219 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/periodicFrequentPattern/pyspark/abstract.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    26749 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/periodicFrequentPattern/pyspark/parallelPFPGrowth.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:44.612639 pami-2024.5.7.1/PAMI/periodicFrequentPattern/topk/
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:45.939605 pami-2024.5.7.1/PAMI/periodicFrequentPattern/topk/TopkPFP/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    19951 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/periodicFrequentPattern/topk/TopkPFP/TopkPFP.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        1 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/periodicFrequentPattern/topk/TopkPFP/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     6862 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/periodicFrequentPattern/topk/TopkPFP/abstract.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        1 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/periodicFrequentPattern/topk/__init__.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:47.404188 pami-2024.5.7.1/PAMI/periodicFrequentPattern/topk/kPFPMiner/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        1 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/periodicFrequentPattern/topk/kPFPMiner/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     4589 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/periodicFrequentPattern/topk/kPFPMiner/abstract.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    17514 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/periodicFrequentPattern/topk/kPFPMiner/kPFPMiner.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:48.051389 pami-2024.5.7.1/PAMI/recurringPattern/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        1 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/recurringPattern/__init__.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:48.930217 pami-2024.5.7.1/PAMI/recurringPattern/basic/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    29135 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/recurringPattern/basic/RPGrowth.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/recurringPattern/basic/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     6637 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/recurringPattern/basic/abstract.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:49.717874 pami-2024.5.7.1/PAMI/relativeFrequentPattern/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        1 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/relativeFrequentPattern/__init__.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:51.079349 pami-2024.5.7.1/PAMI/relativeFrequentPattern/basic/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    30349 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/relativeFrequentPattern/basic/RSFPGrowth.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        1 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/relativeFrequentPattern/basic/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     4261 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/relativeFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:51.573335 pami-2024.5.7.1/PAMI/relativeHighUtilityPattern/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/relativeHighUtilityPattern/__init__.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:52.003649 pami-2024.5.7.1/PAMI/relativeHighUtilityPattern/basic/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    35406 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/relativeHighUtilityPattern/basic/RHUIM.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/relativeHighUtilityPattern/basic/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     6052 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/relativeHighUtilityPattern/basic/abstract.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:52.451514 pami-2024.5.7.1/PAMI/sequence/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/sequence/__init__.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:52.452042 pami-2024.5.7.1/PAMI/sequentialPatternMining/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        1 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/sequentialPatternMining/__init__.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:54.437135 pami-2024.5.7.1/PAMI/sequentialPatternMining/basic/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    42265 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/sequentialPatternMining/basic/SPADE.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    19986 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/sequentialPatternMining/basic/SPAM.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/sequentialPatternMining/basic/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     6569 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/sequentialPatternMining/basic/abstract.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    24786 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/sequentialPatternMining/basic/prefixSpan.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:55.397868 pami-2024.5.7.1/PAMI/sequentialPatternMining/closed/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/sequentialPatternMining/closed/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     6285 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/sequentialPatternMining/closed/abstract.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/sequentialPatternMining/closed/bide.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:55.398301 pami-2024.5.7.1/PAMI/stablePeriodicFrequentPattern/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/stablePeriodicFrequentPattern/__init__.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:57.271849 pami-2024.5.7.1/PAMI/stablePeriodicFrequentPattern/basic/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    18431 2024-04-19 08:53:38.000000 pami-2024.5.7.1/PAMI/stablePeriodicFrequentPattern/basic/SPPEclat.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    26770 2024-04-25 00:26:02.000000 pami-2024.5.7.1/PAMI/stablePeriodicFrequentPattern/basic/SPPGrowth.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    19807 2024-04-19 08:53:38.000000 pami-2024.5.7.1/PAMI/stablePeriodicFrequentPattern/basic/SPPGrowthDump.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        1 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/stablePeriodicFrequentPattern/basic/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     7271 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/stablePeriodicFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:58.473503 pami-2024.5.7.1/PAMI/stablePeriodicFrequentPattern/topK/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    27859 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/stablePeriodicFrequentPattern/topK/TSPIN.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        1 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/stablePeriodicFrequentPattern/topK/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     7173 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/stablePeriodicFrequentPattern/topK/abstract.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:52:58.970602 pami-2024.5.7.1/PAMI/subgraphMining/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/subgraphMining/__init__.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:53:03.198155 pami-2024.5.7.1/PAMI/subgraphMining/basic/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/subgraphMining/basic/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     1241 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/subgraphMining/basic/abstract.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     2396 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/subgraphMining/basic/dfsCode.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)      772 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/subgraphMining/basic/edge.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     2616 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/subgraphMining/basic/extendedEdge.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)      670 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/subgraphMining/basic/frequentSubgraph.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     4943 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/subgraphMining/basic/graph.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    28244 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/subgraphMining/basic/gspan.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     1748 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/subgraphMining/basic/sparseTriangularMatrix.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)      826 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/subgraphMining/basic/vertex.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:53:08.142214 pami-2024.5.7.1/PAMI/subgraphMining/topK/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     1949 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/subgraphMining/topK/DFSCode.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)      593 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/subgraphMining/topK/DFSThread.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/subgraphMining/topK/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     1316 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/subgraphMining/topK/abstract.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)      772 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/subgraphMining/topK/edge.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     2613 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/subgraphMining/topK/extendedEdge.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)      674 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/subgraphMining/topK/frequentSubgraph.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     4295 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/subgraphMining/topK/graph.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     1486 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/subgraphMining/topK/sparseTriangularMatrix.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    20979 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/subgraphMining/topK/tkg.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)      818 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/subgraphMining/topK/vertex.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:53:08.561951 pami-2024.5.7.1/PAMI/uncertainFaultTolerantFrequentPattern/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    17694 2024-04-24 02:44:32.000000 pami-2024.5.7.1/PAMI/uncertainFaultTolerantFrequentPattern/VBFTMine.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/uncertainFaultTolerantFrequentPattern/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     6756 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/uncertainFaultTolerantFrequentPattern/abstract.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:53:08.936691 pami-2024.5.7.1/PAMI/uncertainFrequentPattern/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)      727 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/uncertainFrequentPattern/__init__.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:53:09.885566 pami-2024.5.7.1/PAMI/uncertainFrequentPattern/basic/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    28251 2024-04-24 02:44:32.000000 pami-2024.5.7.1/PAMI/uncertainFrequentPattern/basic/CUFPTree.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    27430 2024-04-25 00:26:02.000000 pami-2024.5.7.1/PAMI/uncertainFrequentPattern/basic/PUFGrowth.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    20311 2024-04-25 00:26:02.000000 pami-2024.5.7.1/PAMI/uncertainFrequentPattern/basic/TUFP.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    20112 2024-04-25 00:26:02.000000 pami-2024.5.7.1/PAMI/uncertainFrequentPattern/basic/TubeP.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    28047 2024-04-25 00:26:02.000000 pami-2024.5.7.1/PAMI/uncertainFrequentPattern/basic/TubeS.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    25675 2024-04-25 00:26:02.000000 pami-2024.5.7.1/PAMI/uncertainFrequentPattern/basic/UFGrowth.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    19871 2024-04-25 00:26:02.000000 pami-2024.5.7.1/PAMI/uncertainFrequentPattern/basic/UVECLAT.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        1 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/uncertainFrequentPattern/basic/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     4945 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/uncertainFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:53:10.430157 pami-2024.5.7.1/PAMI/uncertainGeoreferencedFrequentPattern/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        1 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/uncertainGeoreferencedFrequentPattern/__init__.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:53:11.213712 pami-2024.5.7.1/PAMI/uncertainGeoreferencedFrequentPattern/basic/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    30577 2024-04-24 02:44:32.000000 pami-2024.5.7.1/PAMI/uncertainGeoreferencedFrequentPattern/basic/GFPGrowth.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        1 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/uncertainGeoreferencedFrequentPattern/basic/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     4986 2024-04-17 07:22:09.000000 pami-2024.5.7.1/PAMI/uncertainGeoreferencedFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:53:11.600435 pami-2024.5.7.1/PAMI/uncertainPeriodicFrequentPattern/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)      727 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/uncertainPeriodicFrequentPattern/__init__.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:53:12.049758 pami-2024.5.7.1/PAMI/uncertainPeriodicFrequentPattern/basic/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    33173 2024-04-24 02:44:32.000000 pami-2024.5.7.1/PAMI/uncertainPeriodicFrequentPattern/basic/UPFPGrowth.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    33178 2024-04-24 02:44:32.000000 pami-2024.5.7.1/PAMI/uncertainPeriodicFrequentPattern/basic/UPFPGrowthPlus.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/uncertainPeriodicFrequentPattern/basic/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     6536 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/uncertainPeriodicFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:53:12.550226 pami-2024.5.7.1/PAMI/weightedFrequentNeighbourhoodPattern/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        1 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/weightedFrequentNeighbourhoodPattern/__init__.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:53:12.996215 pami-2024.5.7.1/PAMI/weightedFrequentNeighbourhoodPattern/basic/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    29414 2024-04-24 02:44:32.000000 pami-2024.5.7.1/PAMI/weightedFrequentNeighbourhoodPattern/basic/SWFPGrowth.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/weightedFrequentNeighbourhoodPattern/basic/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     6603 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/weightedFrequentNeighbourhoodPattern/basic/abstract.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:53:13.469807 pami-2024.5.7.1/PAMI/weightedFrequentPattern/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        1 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/weightedFrequentPattern/__init__.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:53:14.009251 pami-2024.5.7.1/PAMI/weightedFrequentPattern/basic/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    25844 2024-04-24 02:44:32.000000 pami-2024.5.7.1/PAMI/weightedFrequentPattern/basic/WFIM.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/weightedFrequentPattern/basic/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     6659 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/weightedFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:53:14.479639 pami-2024.5.7.1/PAMI/weightedFrequentRegularPattern/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/weightedFrequentRegularPattern/__init__.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:53:14.481543 pami-2024.5.7.1/PAMI/weightedFrequentRegularPattern/basic/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    29035 2024-04-24 02:44:32.000000 pami-2024.5.7.1/PAMI/weightedFrequentRegularPattern/basic/WFRIMiner.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/weightedFrequentRegularPattern/basic/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     7495 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/weightedFrequentRegularPattern/basic/abstract.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:53:14.912905 pami-2024.5.7.1/PAMI/weightedUncertainFrequentPattern/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        1 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/weightedUncertainFrequentPattern/__init__.py
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:53:15.367209 pami-2024.5.7.1/PAMI/weightedUncertainFrequentPattern/basic/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    31245 2024-04-24 02:44:32.000000 pami-2024.5.7.1/PAMI/weightedUncertainFrequentPattern/basic/WUFIM.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        0 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/weightedUncertainFrequentPattern/basic/__init__.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     4771 2024-02-19 12:10:44.000000 pami-2024.5.7.1/PAMI/weightedUncertainFrequentPattern/basic/abstract.py
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    72970 2024-05-07 07:53:15.932895 pami-2024.5.7.1/PKG-INFO
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    71479 2024-05-05 10:56:29.000000 pami-2024.5.7.1/README.md
+drwxr-xr-x   0 udaykiranrage   (501) staff       (20)        0 2024-05-07 07:53:15.916416 pami-2024.5.7.1/pami.egg-info/
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    72970 2024-05-07 07:51:02.000000 pami-2024.5.7.1/pami.egg-info/PKG-INFO
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)    18350 2024-05-07 07:51:02.000000 pami-2024.5.7.1/pami.egg-info/SOURCES.txt
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        1 2024-05-07 07:51:02.000000 pami-2024.5.7.1/pami.egg-info/dependency_links.txt
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)      255 2024-05-07 07:51:02.000000 pami-2024.5.7.1/pami.egg-info/requires.txt
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)        5 2024-05-07 07:51:02.000000 pami-2024.5.7.1/pami.egg-info/top_level.txt
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)       38 2024-05-07 07:53:15.934379 pami-2024.5.7.1/setup.cfg
+-rw-r--r--   0 udaykiranrage   (501) staff       (20)     1535 2024-05-07 07:50:57.000000 pami-2024.5.7.1/setup.py
```

### Comparing `pami-2024.5.28.1/LICENSE` & `pami-2024.5.7.1/LICENSE`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/AssociationRules/basic/_ARWithLeverage.py` & `pami-2024.5.7.1/PAMI/AssociationRules/basic/_ARWithLeverage.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/AssociationRules/basic/_ARWithLift.py` & `pami-2024.5.7.1/PAMI/AssociationRules/basic/_ARWithLift.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/AssociationRules/basic/_RuleMiner.py` & `pami-2024.5.7.1/PAMI/AssociationRules/basic/_RuleMiner.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/AssociationRules/basic/abstract.py` & `pami-2024.5.7.1/PAMI/AssociationRules/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/AssociationRules/basic/confidence.py` & `pami-2024.5.7.1/PAMI/AssociationRules/basic/confidence.py`

 * *Files 2% similar despite different names*

```diff
@@ -182,23 +182,21 @@
                     pattern = self._iFile[col].tolist()
                     # print("Using column: ", col, "for pattern")
                 if 'support' in col.lower():
                     support = self._iFile[col].tolist()
                     # print("Using column: ", col, "for support")
             for i in range(len(pattern)):
                 # if pattern[i] != tuple(): exit()
-                if type(pattern[i]) != str:
+                if type(pattern[i]) != tuple:
                     raise ValueError("Pattern should be a tuple. PAMI is going through a major revision.\
                                       Please raise an issue in the github repository regarding this error and provide information regarding input and algorithm.\
                                       In the meanwhile try saving the patterns to a file using (alg).save() and use the file as input. \
                                       If that doesn't work, please raise an issue in the github repository.\
                                       Got pattern: ", pattern[i], "at index: ", i, "in the dataframe, type: ", type(pattern[i]))
-                # s = tuple(sorted(pattern[i]))
-                s = pattern[i].split(self._sep)
-                s = tuple(sorted(s))
+                s = tuple(sorted(pattern[i]))
                 self._associationRules[s] = support[i]
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 f = _ab._urlopen(self._iFile)
                 for line in f:
                     line = line.strip()
                     line = line.split(':')
@@ -299,17 +297,15 @@
         # data = []
         # for a, b in self._finalPatterns.items():
         #     data.append([a.replace('\t', ' '), b])
         #     dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         # # dataFrame = dataFrame.replace(r'\r+|\n+|\t+',' ', regex=True)
         # return dataFrame
 
-        # dataFrame = _ab._pd.DataFrame(list(self._associationRules.items()), columns=['Patterns', 'Support'])
-        # dataFrame = _ab._pd.DataFrame(list([[" ".join(x), y] for x,y in self._finalPatterns.items()]), columns=['Patterns', 'Support'])
-        dataFrame = _ab._pd.DataFrame(list([[" ".join(x), y] for x, y in self._associationRules.items()]), columns=['Patterns', 'Support'])
+        dataFrame = _ab._pd.DataFrame(list(self._associationRules.items()), columns=['Patterns', 'Support'])
         return dataFrame
 
     def save(self, outFile: str) -> None:
         """
 
         Complete set of frequent patterns will be loaded in to an output file
```

### Comparing `pami-2024.5.28.1/PAMI/AssociationRules/basic/leverage.py` & `pami-2024.5.7.1/PAMI/AssociationRules/basic/leverage.py`

 * *Files 3% similar despite different names*

```diff
@@ -175,25 +175,19 @@
                     pattern = self._iFile[col].tolist()
                     # print("Using column: ", col, "for pattern")
                 if 'support' in col.lower():
                     support = self._iFile[col].tolist()
                     # print("Using column: ", col, "for support")
             for i in range(len(pattern)):
                 # if pattern[i] != tuple(): exit()
-                if type(pattern[i]) != str:
-                    raise ValueError("Pattern should be a tuple. PAMI is going through a major revision.\
-                                      Please raise an issue in the github repository regarding this error and provide information regarding input and algorithm.\
-                                      In the meanwhile try saving the patterns to a file using (alg).save() and use the file as input. \
-                                      If that doesn't work, please raise an issue in the github repository.\
-                                      Got pattern: ", pattern[i], "at index: ", i, "in the dataframe, type: ", type(pattern[i]))
-                # s = tuple(sorted(pattern[i]))
-                s = pattern[i].split(self._sep)
-                s = tuple(sorted(s))
+                if type(pattern[i]) != tuple:
+                    raise ValueError("Pattern should be a tuple. PAMI is going through a major revision. Please raise an issue in the github repository regarding this error and provide information regarding input and algorithm.\
+                                     In the meanwhile try saving the patterns to a file using (alg).save() and use the file as input. If that doesn't work, please raise an issue in the github repository.")
+                s = tuple(sorted(pattern[i]))
                 self._associationRules[s] = support[i] / self._maxTS
-                
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 f = _ab._urlopen(self._iFile)
                 for line in f:
                     line = line.strip()
                     line = line.split(':')
                     s = line[0].split(self._sep)
@@ -296,16 +290,15 @@
         # data = []
         # for a, b in self._finalPatterns.items():
         #     data.append([a.replace('\t', ' '), b])
         #     dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         # # dataFrame = dataFrame.replace(r'\r+|\n+|\t+',' ', regex=True)
         # return dataFrame
 
-        # dataFrame = _ab._pd.DataFrame(list(self._associationRules.items()), columns=['Patterns', 'Support'])
-        dataFrame = _ab._pd.DataFrame(list([[" ".join(x), y] for x, y in self._associationRules.items()]), columns=['Patterns', 'Support'])
+        dataFrame = _ab._pd.DataFrame(list(self._associationRules.items()), columns=['Patterns', 'Support'])
         return dataFrame
 
     def save(self, outFile: str) -> None:
         """
 
         Complete set of frequent patterns will be loaded in to an output file
```

### Comparing `pami-2024.5.28.1/PAMI/AssociationRules/basic/lift.py` & `pami-2024.5.7.1/PAMI/AssociationRules/basic/lift.py`

 * *Files 4% similar despite different names*

```diff
@@ -174,23 +174,18 @@
                     pattern = self._iFile[col].tolist()
                     # print("Using column: ", col, "for pattern")
                 if 'support' in col.lower():
                     support = self._iFile[col].tolist()
                     # print("Using column: ", col, "for support")
             for i in range(len(pattern)):
                 # if pattern[i] != tuple(): exit()
-                if type(pattern[i]) != str:
-                    raise ValueError("Pattern should be a tuple. PAMI is going through a major revision.\
-                                      Please raise an issue in the github repository regarding this error and provide information regarding input and algorithm.\
-                                      In the meanwhile try saving the patterns to a file using (alg).save() and use the file as input. \
-                                      If that doesn't work, please raise an issue in the github repository.\
-                                      Got pattern: ", pattern[i], "at index: ", i, "in the dataframe, type: ", type(pattern[i]))
-                # s = tuple(sorted(pattern[i]))
-                s = pattern[i].split(self._sep)
-                s = tuple(sorted(s))
+                if type(pattern[i]) != tuple:
+                    raise ValueError("Pattern should be a tuple. PAMI is going through a major revision. Please raise an issue in the github repository regarding this error and provide information regarding input and algorithm.\
+                                     In the meanwhile try saving the patterns to a file using (alg).save() and use the file as input. If that doesn't work, please raise an issue in the github repository.")
+                s = tuple(sorted(pattern[i]))
                 self._associationRules[s] = support[i]
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 f = _ab._urlopen(self._iFile)
                 for line in f:
                     line = line.strip()
                     line = line.split(':')
@@ -277,15 +272,15 @@
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
-    def getAssociationRulesAsDataFrame(self):
+    def getPatternsAsDataFrame(self):
         """
         Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
@@ -293,16 +288,15 @@
         # data = []
         # for a, b in self._finalPatterns.items():
         #     data.append([a.replace('\t', ' '), b])
         #     dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         # # dataFrame = dataFrame.replace(r'\r+|\n+|\t+',' ', regex=True)
         # return dataFrame
 
-        # dataFrame = _ab._pd.DataFrame(list(self._associationRules.items()), columns=['Patterns', 'Support'])
-        dataFrame = _ab._pd.DataFrame(list([[" ".join(x), y] for x, y in self._associationRules.items()]), columns=['Patterns', 'Support'])
+        dataFrame = _ab._pd.DataFrame(list(self._associationRules.items()), columns=['Patterns', 'Support'])
         return dataFrame
 
     def save(self, outFile: str) -> None:
         """
 
         Complete set of frequent patterns will be loaded in to an output file
```

### Comparing `pami-2024.5.28.1/PAMI/correlatedPattern/__init__.py` & `pami-2024.5.7.1/PAMI/correlatedPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/correlatedPattern/basic/CoMine.py` & `pami-2024.5.7.1/PAMI/correlatedPattern/basic/CoMine.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,24 +1,25 @@
 # CoMine is one of the fundamental algorithm to discover correlated patterns in a transactional database.
 #
 # **Importing this algorithm into a python program**
+# --------------------------------------------------------
 #
 #             from PAMI.correlatedPattern.basic import CoMine as alg
 #
 #             iFile = 'sampleTDB.txt'
 #
 #             minSup = 0.25 # can be specified between 0 and 1
 #
 #             minAllConf = 0.2 # can  be specified between 0 and 1
 #
 #             obj = alg.CoMine(iFile, minSup, minAllConf, sep)
 #
 #             obj.mine()
 #
-#             Patterns = obj.getPatterns()
+#             Rules = obj.getPatterns()
 #
 #             print("Total number of  Patterns:", len(Patterns))
 #
 #             obj.save(oFile)
 #
 #             Df = obj.getPatternsAsDataFrame()
 #
@@ -66,16 +67,17 @@
 
     :**Attributes**:    **itemId** (*int*) -- **storing item of a node**
                         **counter** (*int*) -- **To maintain the support of node**
                         **parent** (*node*) -- **To maintain the parent of every node**
                         **child** (*list*) -- **To maintain the children of node**
                         **nodeLink** (*node*) -- **Points to the node with same itemId**
 
-    :**Methods**:    **getChild (itemName)** -- **this method returns the node with same itemName from correlatedPatternTree**
-
+    :**Methods**:
+            getChild(itemName)
+                returns the node with same itemName from correlatedPatternTree
     """
 
     def __init__(self) -> None:
         self.itemId = -1
         self.counter = 1
         self.parent = None
         self.child = []
@@ -98,19 +100,26 @@
     A class used to represent the correlatedPatternGrowth tree structure
 
     :**Attributes**:    **headerList** (*list*) -- **storing the list of items in tree sorted in ascending of their supports**
                         **mapItemNodes** (*dictionary*) -- **storing the nodes with same item name**
                         **mapItemLastNodes** (*dictionary*) -- **representing the map that indicates the last node for each item**
                         **root** (*Node*) -- **representing the root Node in a tree**
 
-    :**Methods**:    **createHeaderList(items,minSup)** -- **takes items only which are greater than minSup and sort the items in ascending order**
-                     **addTransaction(transaction)**-- **creating transaction as a branch in correlatedPatternTree**
-                     **fixNodeLinks(item,newNode)** -- **To create the link for nodes with same item**
-                     **printTree(Node)** -- **gives the details of node in correlatedPatternGrowth tree**
-                     **addPrefixPath(prefix,port,minSup)** -- **It takes the items in prefix pattern whose support is >=minSup and construct a subtree**
+    :**Methods**:
+
+        createHeaderList(items,minSup)
+            takes items only which are greater than minSup and sort the items in ascending order
+        addTransaction(transaction)
+            creating transaction as a branch in correlatedPatternTree
+        fixNodeLinks(item,newNode)
+            To create the link for nodes with same item
+        printTree(Node)
+            gives the details of node in correlatedPatternGrowth tree
+        addPrefixPath(prefix,port,minSup)
+           It takes the items in prefix pattern whose support is >=minSup and construct a subtree
     """
 
     def __init__(self) -> None:
         self.headerList = []
         self.mapItemNodes = {}
         self.mapItemLastNodes = {}
         self.root = _Node()
@@ -303,15 +312,15 @@
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
     Credits
     =======
 
-    The complete program was written by B.Sai Chitra and revised by Tarun Sreepada under the supervision of Professor Rage Uday Kiran.
+             The complete program was written by B.Sai Chitra under the supervision of Professor Rage Uday Kiran.
 
     """
 
     _startTime = float()
     _endTime = float()
     _minSup = float()
     _finalPatterns = {}
@@ -404,23 +413,25 @@
                 if j not in self._mapSupport:
                     self._mapSupport[j] = 1
                 else:
                     self._mapSupport[j] += 1
 
     def _saveItemSet(self, prefix, prefixLength, support) -> None:
         """
-        To save the correlated patterns mined form correlatedPatternTree and patterns were stored in a global variable finalPatterns.
+        To save the correlated patterns mined form correlatedPatternTree
 
         :param prefix: the correlated pattern
         :type prefix: list
         :param prefixLength : the length of a correlated pattern
         :type prefixLength : int
         :param support: the support of a pattern
         :type support :  int
         :return: None
+
+        The correlated patterns were stored in a global variable finalPatterns
         """
         all_conf = self._getRatio(prefix, prefixLength, support)
         if all_conf < self._minAllConf:
             return
         l = []
         for i in range(prefixLength):
             l.append(prefix[i])
```

### Comparing `pami-2024.5.28.1/PAMI/correlatedPattern/basic/CoMinePlus.py` & `pami-2024.5.7.1/PAMI/correlatedPattern/basic/CoMinePlus.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/correlatedPattern/basic/_CoMine.py` & `pami-2024.5.7.1/PAMI/uncertainFrequentPattern/basic/CUFPTree.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,27 +1,24 @@
-# CoMine is one of the fundamental algorithm to discover correlated patterns in a transactional database.
+# CUFPTree is one of the fundamental algorithm to discover frequent patterns in a uncertain transactional database using CUFP-Tree
 #
 # **Importing this algorithm into a python program**
-# --------------------------------------------------------
 #
-#             from PAMI.correlatedPattern.basic import CoMine as alg
+#             from PAMI.uncertainFrequentPattern.basic import CUFPTree as alg
 #
-#             iFile = 'sampleTDB.txt'
+#             obj = alg.CUFPTree(iFile, minSup,oFile,sep)
 #
-#             minSup = 0.25 # can be specified between 0 and 1
+#             iFile = 'sampleDB.txt'
 #
-#             minAllConf = 0.2 # can  be specified between 0 and 1
-#
-#             obj = alg.CoMine(iFile, minSup, minAllConf, sep)
+#             minSup = 10  # can also be specified between 0 and 1
 #
 #             obj.mine()
 #
-#             Rules = obj.getPatterns()
+#             frequentPatterns = obj.getPatterns()
 #
-#             print("Total number of  Patterns:", len(Patterns))
+#             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
 #             obj.save(oFile)
 #
 #             Df = obj.getPatternsAsDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
@@ -34,276 +31,443 @@
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
 __copyright__ = """
-Copyright (C)  2021 Rage Uday Kiran
+ Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
-     Copyright (C)  2021 Rage Uday Kiran
-
 """
 
-from PAMI.correlatedPattern.basic import abstract as _ab
-import pandas as _pd
-from typing import List, Dict, Tuple, Union
+import pandas as pd
 from deprecated import deprecated
 
-class _Node:
+from PAMI.uncertainFrequentPattern.basic import abstract as _ab
+from typing import List, Tuple
+
+
+_minSup = str()
+_ab._sys.setrecursionlimit(20000)
+_finalPatterns = {}
+
+
+class _Item:
     """
-    A class used to represent the node of correlatedPatternTree
+    A class used to represent the item with probability in transaction of dataset
 
-    :**Attributes**:    **itemId** (*int*) -- **storing item of a node**
-                        **counter** (*int*) -- **To maintain the support of node**
-                        **parent** (*node*) -- **To maintain the parent of every node**
-                        **child** (*list*) -- **To maintain the children of node**
-                        **nodeLink** (*node*) -- **Points to the node with same itemId**
-
-    :**Methods**:
-            getChild(itemName)
-                returns the node with same itemName from correlatedPatternTree
+    :Attributes:
+
+        item : int or word
+            Represents the name of the item
+
+        probability : float
+            Represent the existential probability(likelihood presence) of an item
     """
 
-    def __init__(self) -> None:
-        self.itemId = -1
-        self.counter = 1
+    def __init__(self, item, probability) -> None:
+        self.item = item
+        self.probability = probability
+
+
+class _Node(object):
+    """
+    A class used to represent the node of frequentPatternTree
+
+    :Attributes:
+
+        item : int
+            storing item of a node
+
+        probability : int
+            To maintain the expected support of node
+
+        parent : node
+            To maintain the parent of every node
+
+        children : list
+            To maintain the children of node
+
+    :Methods:
+
+        addChild(itemName)
+            storing the children to their respective parent nodes
+    """
+
+    def __init__(self, item, children) -> None:
+        self.item = item
+        self.probability = 1
+        self.children = children
         self.parent = None
-        self.child = []
-        self.nodeLink = None
 
-    def getChild(self, id1) -> Union[None, '_Node']:
+    def addChild(self, node) -> None:
         """
-        :param id1: give item id as input
-        :type id1: int
-        :return: the node with same itemId
-        :rtype: _Node
-        """
-        for i in self.child:
-            if i.itemId == id1:
-                return i
-        return None
+        This method adds a child node to the current node in the frequent pattern tree. It updates the children
+        dictionary of the current node with the new child node and sets the parent of the child node to the current node.
 
-class _Tree:
+        :param node: The child node to be added.
+        :type node: _Node
+        :return: None
+        """
+        self.children[node.item] = node
+        node.parent = self
+
+
+class _Tree(object):
     """
-    A class used to represent the correlatedPatternGrowth tree structure
+    A class used to represent the frequentPatternGrowth tree structure
+
+    :Attributes:
 
-    :**Attributes**:    **headerList** (*list*) -- **storing the list of items in tree sorted in ascending of their supports**
-                        **mapItemNodes** (*dictionary*) -- **storing the nodes with same item name**
-                        **mapItemLastNodes** (*dictionary*) -- **representing the map that indicates the last node for each item**
-                        **root** (*Node*) -- **representing the root Node in a tree**
+        root : Node
+            Represents the root node of the tree
 
-    :**Methods**:
+        summaries : dictionary
+            storing the nodes with same item name
+
+        info : dictionary
+            stores the support of items
+
+    :Methods:
 
-        createHeaderList(items,minSup)
-            takes items only which are greater than minSup and sort the items in ascending order
         addTransaction(transaction)
-            creating transaction as a branch in correlatedPatternTree
-        fixNodeLinks(item,newNode)
-            To create the link for nodes with same item
-        printTree(Node)
-            gives the details of node in correlatedPatternGrowth tree
-        addPrefixPath(prefix,port,minSup)
-           It takes the items in prefix pattern whose support is >=minSup and construct a subtree
+            creating transaction as a branch in frequentPatternTree
+        addConditionalPattern(prefixPaths, supportOfItems)
+            construct the conditional tree for prefix paths
+        conditionalPatterns(Node)
+            generates the conditional patterns from tree for specific node
+        conditionalTransactions(prefixPaths,Support)
+            takes the prefixPath of a node and support at child of the path and extract the frequent items from
+            prefixPaths and generates prefixPaths with items which are frequent
+        remove(Node)
+            removes the node from tree once after generating all the patterns respective to the node
+        generatePatterns(Node)
+            starts from the root node of the tree and mines the frequent patterns
     """
 
     def __init__(self) -> None:
-        self.headerList = []
-        self.mapItemNodes = {}
-        self.mapItemLastNodes = {}
-        self.root = _Node()
+        self.root = _Node(None, {})
+        self.summaries = {}
+        self.info = {}
+
+    def addTransaction(self, transaction) -> None:
+        """
+        adding transaction into tree
+
+        :param transaction : it represents the one self.Database in database
+        :type transaction : list
+        :return: None
+        """
+
+        currentNode = self.root
+        for i in range(len(transaction)):
+            if transaction[i].item not in currentNode.children:
+                newNode = _Node(transaction[i].item, {})
+                l1 = i - 1
+                lp = []
+                while l1 >= 0:
+                    lp.append(transaction[l1].probability)
+                    l1 -= 1
+                if len(lp) == 0:
+                    newNode.probability = transaction[i].probability
+                else:
+                    newNode.probability = max(lp) * transaction[i].probability
+                currentNode.addChild(newNode)
+                if transaction[i].item in self.summaries:
+                    self.summaries[transaction[i].item].append(newNode)
+                else:
+                    self.summaries[transaction[i].item] = [newNode]
+                currentNode = newNode
+            else:
+                currentNode = currentNode.children[transaction[i].item]
+                l1 = i - 1
+                lp = []
+                while l1 >= 0:
+                    lp.append(transaction[l1].probability)
+                    l1 -= 1
+                if len(lp) == 0:
+                    currentNode.probability += transaction[i].probability
+                else:
+                    currentNode.probability += max(lp) * transaction[i].probability
 
-    def addTransaction(self, transaction: List[int]) -> None:
+    def addConditionalPattern(self, transaction, sup) -> None:
         """
-        Adding transaction into tree
+        constructing conditional tree from prefixPaths
 
-        :param transaction: it represents a single transaction in a database
-        :type transaction: list
+        :param transaction : it represents the one self.Database in database
+        :type transaction : list
+        :param sup : support of prefixPath taken at last child of the path
+        :type sup : int
         :return: None
         """
 
-        current = self.root
-        for i in transaction:
-            child = current.getChild(i)
-            if child is None:
-                newNode = _Node()
-                newNode.itemId = i
-                newNode.parent = current
-                current.child.append(newNode)
-                self.fixNodeLinks(i, newNode)
-                current = newNode
+        # This method takes transaction, support and constructs the conditional tree
+        currentNode = self.root
+        for i in range(len(transaction)):
+            if transaction[i] not in currentNode.children:
+                newNode = _Node(transaction[i], {})
+                newNode.probability = sup
+                currentNode.addChild(newNode)
+                if transaction[i] in self.summaries:
+                    self.summaries[transaction[i]].append(newNode)
+                else:
+                    self.summaries[transaction[i]] = [newNode]
+                currentNode = newNode
             else:
-                child.counter += 1
-                current = child
+                currentNode = currentNode.children[transaction[i]]
+                currentNode.probability += sup
 
-    def fixNodeLinks(self, item: int, newNode: '_Node') -> None:
+    def conditionalPatterns(self, alpha) -> Tuple[List, List, dict]:
         """
-        Fixing node link for the newNode that inserted into correlatedPatternTree
+        generates all the conditional patterns of respective node
 
-        :param item: it represents the item of newNode
-        :type item: int
-        :param newNode: it represents the newNode that inserted in correlatedPatternTree
-        :type newNode: Node
-        :return: None
+        :param alpha : it represents the Node in tree
+        :type alpha : _Node
+        :return: Tuple
         """
-        if item in self.mapItemLastNodes.keys():
-            lastNode = self.mapItemLastNodes[item]
-            lastNode.nodeLink = newNode
-        self.mapItemLastNodes[item] = newNode
-        if item not in self.mapItemNodes.keys():
-            self.mapItemNodes[item] = newNode
 
-    def printTree(self, root: '_Node') -> None:
+        # This method generates conditional patterns of node by traversing the tree
+        finalPatterns = []
+        sup = []
+        for i in self.summaries[alpha]:
+            s = i.probability
+            set2 = []
+            while i.parent.item is not None:
+                set2.append(i.parent.item)
+                i = i.parent
+            if len(set2) > 0:
+                set2.reverse()
+                finalPatterns.append(set2)
+                sup.append(s)
+        finalPatterns, support, info = self.conditionalTransactions(finalPatterns, sup)
+        return finalPatterns, support, info
+
+    def removeNode(self, nodeValue) -> None:
         """
-        This method is to find the details of parent, children, and support of a Node
+        removing the node from tree
 
-        :param root: it represents the Node in correlatedPatternTree
-        :type root: Node
+        :param nodeValue : it represents the node in tree
+        :type nodeValue : node
         :return: None
         """
 
-        if root.child is None:
-            return
-        else:
-            for i in root.child:
-                print(i.itemId, i.counter, i.parent.itemId)
-                self.printTree(i)
-
-    def createHeaderList(self, mapSupport: Dict[int, int], minSup: int) -> None:
-        """
-        To create the headerList
-
-        :param mapSupport : it represents the items with their supports
-        :type mapSupport : dictionary
-        :param minSup : it represents the minSup
-        :param minSup : float
-        :return: None
+        for i in self.summaries[nodeValue]:
+            del i.parent.children[nodeValue]
+
+    def conditionalTransactions(self, condPatterns, support) -> Tuple[List, List, dict]:
         """
-        
-        t1 = []
-        for x, y in mapSupport.items():
-            if y >= minSup:
-                t1.append(x)
-        itemSetBuffer = [k for k, v in sorted(mapSupport.items(), key=lambda x: x[1], reverse=True)]
-        self.headerList = [i for i in t1 if i in itemSetBuffer]
+        It generates the conditional patterns with frequent items
 
-    def addPrefixPath(self, prefix: List['_Node'], mapSupportBeta, minSup) -> None:
+        :param condPatterns : conditionalPatterns generated from conditionalPattern method for respective node
+        :type condPatterns : list
+        :support : the support of conditional pattern in tree
+        :support : int
+        :return: Tuple consist of patterns,support and updated Dictionary
+        :rtype: Tuple
         """
-        To construct the conditional tree with prefix paths of a node in correlatedPatternTree
 
-        :param prefix : it represents the prefix items of a Node
+        global minSup
+        pat = []
+        sup = []
+        count = {}
+        for i in range(len(condPatterns)):
+            for j in condPatterns[i]:
+                if j in count:
+                    count[j] += support[i]
+                else:
+                    count[j] = support[i]
+        updatedDict = {}
+        updatedDict = {k: v for k, v in count.items() if v >= minSup}
+        count = 0
+        for p in condPatterns:
+            p1 = [v for v in p if v in updatedDict]
+            trans = sorted(p1, key=lambda x: updatedDict[x], reverse=True)
+            if len(trans) > 0:
+                pat.append(trans)
+                sup.append(support[count])
+                count += 1
+        return pat, sup, updatedDict
+
+    def generatePatterns(self, prefix) -> None:
+        """
+        Generates the patterns
+
+        :param prefix : forms the combination of items
         :type prefix : list
-        :param mapSupportBeta : it represents the items with their supports
-        :param mapSupportBeta : dictionary
-        :param minSup : to check the item meets with minSup
-        :param minSup : float
         :return: None
         """
-        pathCount = prefix[0].counter
-        current = self.root
-        prefix.reverse()
-        for i in range(0, len(prefix) - 1):
-            pathItem = prefix[i]
-            if mapSupportBeta.get(pathItem.itemId) >= minSup:
-                child = current.getChild(pathItem.itemId)
-                if child is None:
-                    newNode = _Node()
-                    newNode.itemId = pathItem.itemId
-                    newNode.parent = current
-                    newNode.counter = pathCount
-                    current.child.append(newNode)
-                    current = newNode
-                    self.fixNodeLinks(pathItem.itemId, newNode)
-                else:
-                    child.counter += pathCount
-                    current = child
 
+        global _finalPatterns, minSup
+        for i in sorted(self.summaries, key=lambda x: (self.info.get(x))):
+            pattern = prefix[:]
+            pattern.append(i)
+            s = 0
+            for x in self.summaries[i]:
+                s += x.probability
+            _finalPatterns[tuple(pattern)] = self.info[i]
+            if s >= minSup:
+                patterns, support, info = self.conditionalPatterns(i)
+                conditionalTree = _Tree()
+                conditionalTree.info = info.copy()
+                for pat in range(len(patterns)):
+                    conditionalTree.addConditionalPattern(patterns[pat], support[pat])
+                if len(patterns) > 0:
+                    conditionalTree.generatePatterns(pattern)
+            self.removeNode(i)
 
-class CoMine(_ab._correlatedPatterns):
+class CUFPTree(_ab._frequentPatterns):
+  
     """
     About this algorithm
     ====================
 
-    :**Description**: CoMine is one of the fundamental algorithm to discover correlated  patterns in a transactional database. It is based on the traditional FP-Growth algorithm. This algorithm uses depth-first search technique to find all correlated patterns in a transactional database.
+    :Description: It is one of the fundamental algorithm to discover frequent patterns in a uncertain transactional database using CUFP-Tree.
 
-    :**Reference**: Lee, Y.K., Kim, W.Y., Cao, D., Han, J. (2003). CoMine: efficient mining of correlated patterns. In ICDM (pp. 581584).
-
-    :**parameters**:    **iFile** (*str*) -- **Name of the Input file to mine complete set of correlated patterns**
-                        **oFile** (*str*) -- **Name of the output file to store complete set of correlated patterns**
-                        **minSup** (*int or float or str*) -- **The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.**
-                        **minAllConf** (*float*) -- **The user can specify minAllConf values within the range (0, 1).**
-                        **sep** (*str*) -- **This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.**
-
-    :**Attributes**:    **memoryUSS** (*float*) -- **To store the total amount of USS memory consumed by the program**
-                        **memoryRSS** (*float*) -- **To store the total amount of RSS memory consumed by the program**
-                        **startTime** (*float*) -- **To record the start time of the mining process**
-                        **endTime** (*float*) -- **To record the completion time of the mining process**
-                        **minSup** (*int*) -- **The user given minSup**
-                        **minAllConf** (*float*) -- **The user given minimum all confidence Ratio(should be in range of 0 to 1)**
-                        **Database** (*list*) -- **To store the transactions of a database in list**
-                        **mapSupport** (*Dictionary*) -- **To maintain the information of item and their frequency**
-                        **lno** (*int*) -- **it represents the total no of transactions**
-                        **tree** (*class*) -- **it represents the Tree class**
-                        **itemSetCount** (*int*) -- **it represents the total no of patterns**
-                        **finalPatterns** (*dict*) -- **it represents to store the patterns**
-                        **itemSetBuffer** (*list*) -- **it represents the store the items in mining**
-                        **maxPatternLength** (*int*) -- **it represents the constraint for pattern length**
+    :Reference: Chun-Wei Lin Tzung-PeiHong, 'new mining approach for uncertain databases using CUFP trees',
+                Expert Systems with Applications, Volume 39, Issue 4, March 2012, Pages 4084-4093, https://doi.org/10.1016/j.eswa.2011.09.087
+    
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of Uncertain Frequent Patterns
+    :param  oFile: str :
+                   Name of the output file to store complete set of Uncertain frequent patterns
+    :param  minSup: int or float or str :
+                   minimum support thresholds were tuned to find the appropriate ranges in the limited memory
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+
+
+    :Attributes:
+
+        iFile : file
+            Name of the Input file or path of the input file
+
+        oFile : file
+            Name of the output file or path of the output file
+
+        minSup: float or int or str
+            The user can specify minSup either in count or proportion of database size.
+            If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+
+        sep : str
+            This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
+            However, the users can override their default separator.
+
+        memoryUSS : float
+            To store the total amount of USS memory consumed by the program
+
+        memoryRSS : float
+            To store the total amount of RSS memory consumed by the program
+
+        startTime:float
+            To record the start time of the mining process
+
+        endTime:float
+            To record the completion time of the mining process
+
+        Database : list
+            To store the transactions of a database in list
+
+        mapSupport : Dictionary
+            To maintain the information of item and their frequency
+
+        lno : int
+            To represent the total no of transaction
+
+        tree : class
+            To represents the Tree class
+
+        itemSetCount : int
+            To represents the total no of patterns
+
+        finalPatterns : dict
+            To store the complete patterns
+
+    :Methods:
+
+        mine()
+            Mining process will start from here
+        getPatterns()
+            Complete set of patterns will be retrieved with this function
+        save(oFile)
+            Complete set of frequent patterns will be loaded in to a output file
+        getPatternsAsDataFrame()
+            Complete set of frequent patterns will be loaded in to a dataframe
+        getMemoryUSS()
+            Total amount of USS memory consumed by the mining process will be retrieved from this function
+        getMemoryRSS()
+            Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        getRuntime()
+            Total amount of runtime taken by the mining process will be retrieved from this function
+        creatingItemSets(fileName)
+            Scans the dataset and stores in a list format
+        frequentOneItem()
+            Extracts the one-length frequent patterns from database
+        updateTransactions()
+            Update the transactions by removing non-frequent items and sort the Database by item decreased support
+        buildTree()
+            After updating the Database, remaining items will be added into the tree by setting root node as null
+        convert()
+            to convert the user specified value
+        startMine()
+            Mining process will start from this function
 
     Execution methods
     =================
 
+
     **Terminal command**
 
+
     .. code-block:: console
 
-      Format:
+       Format:
 
-      (.venv) $ python3 CoMine.py <inputFile> <outputFile> <minSup> <minAllConf> <sep>
+       (.venv) $ python3 CUFPTree.py <inputFile> <outputFile> <minSup>
 
-      Example Usage:
+       Example Usage:
 
-      (.venv) $ python3 CoMine.py sampleTDB.txt output.txt 0.25 0.2
+       (.venv) $ python3 CUFPTree.py sampleTDB.txt patterns.txt 3
 
-    .. note:: minSup can be specified in support count or a value between 0 and 1.
+    .. note:: minSup  will be considered in support count or frequency
 
     **Calling from a python program**
 
     .. code-block:: python
 
-            from PAMI.correlatedPattern.basic import CoMine as alg
-
-            iFile = 'sampleTDB.txt'
+            from PAMI.uncertainFrequentPattern.basic import CUFPTree as alg
 
-            minSup = 0.25 # can be specified between 0 and 1
+            iFile = 'sampleDB.txt'
 
-            minAllConf = 0.2 # can  be specified between 0 and 1
+            minSup = 10  # can also be specified between 0 and 1
 
-            obj = alg.CoMine(iFile, minSup, minAllConf,sep)
+            obj = alg.CUFPTree(iFile, minSup)
 
             obj.mine()
 
-            patterns = obj.getPatterns()
+            frequentPatterns = obj.getPatterns()
 
-            print("Total number of  Patterns:", len(patterns))
+            print("Total number of Frequent Patterns:", len(frequentPatterns))
 
-            obj.savePatterns(oFile)
+            obj.save(oFile)
 
-            df = obj.getPatternsAsDataFrame()
+            Df = obj.getPatternsAsDataFrame()
 
             memUSS = obj.getMemoryUSS()
 
             print("Total Memory in USS:", memUSS)
 
             memRSS = obj.getMemoryRSS()
 
@@ -312,383 +476,353 @@
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
     Credits
     =======
 
-             The complete program was written by B.Sai Chitra under the supervision of Professor Rage Uday Kiran.
 
+            The complete program was written by  P.Likhitha  under the supervision of Professor Rage Uday Kiran.
     """
 
     _startTime = float()
     _endTime = float()
-    _minSup = float()
+    _minSup = str()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
+    _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
-    _minAllConf = 0.0
     _Database = []
-    _mapSupport = {}
-    _lno = 0
-    _tree = str()
-    _itemSetBuffer = None
-    _fpNodeTempBuffer = []
-    _itemSetCount = 0
-    _maxPatternLength = 1000
-    _sep = "\t"
-
-    def __init__(self, iFile: Union[str, _pd.DataFrame], minSup: Union[int, float, str], minAllConf: float, sep: str="\t") ->None:
-        """
-        param iFile: give the input file
-        type iFile: str or DataFrame or url
-        param minSup: minimum support
-        type minSup:   int or float
-        param sep: Delimiter of input file
-        type sep: str
-        """
+    _rank = {}
 
-        super().__init__(iFile, minSup, minAllConf, sep)
+    def __init__(self, iFile, minSup, sep='\t') -> None:
+        super().__init__(iFile, minSup, sep)
 
     def _creatingItemSets(self) -> None:
         """
-        Storing the complete transactions of the database/input file in a database variable
+        Scans the uncertain transactional dataset
+
+        :return: None
         """
         self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
+            uncertain, data = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
                 self._Database = self._iFile['Transactions'].tolist()
+            if 'uncertain' in i:
+                uncertain = self._iFile['uncertain'].tolist()
+            for k in range(len(data)):
+                tr = []
+                for j in range(len(data[k])):
+                    product = _Item(data[k][j], uncertain[k][j])
+                    tr.append(product)
+                self._Database.append(tr)
+
+            # print(self.Database)
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
+                    tr = []
+                    for i in temp:
+                        i1 = i.index('(')
+                        i2 = i.index(')')
+                        item = i[0:i1]
+                        probability = float(i[i1 + 1:i2])
+                        product = _Item(item, probability)
+                        tr.append(product)
                     self._Database.append(temp)
             else:
                 try:
-                    with open(self._iFile, 'r', encoding='utf-8') as f:
+                    with open(self._iFile, 'r') as f:
                         for line in f:
-                            line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
-                            self._Database.append(temp)
+                            tr = []
+                            for i in temp:
+                                i1 = i.index('(')
+                                i2 = i.index(')')
+                                item = i[0:i1]
+                                probability = float(i[i1 + 1:i2])
+                                product = _Item(item, probability)
+                                tr.append(product)
+                            self._Database.append(tr)
                 except IOError:
                     print("File Not Found")
-                    quit()
 
-    def _getRatio(self, prefix: List[int], prefixLength: int, s: int) -> float:
+    def _frequentOneItem(self) -> Tuple[dict, List]:
         """
-        A Function to get itemSet Ratio
+        Takes the self.Database and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
 
-        :param prefix:the path
-        :type prefix: list
-        :param prefixLength: length
-        :type prefixLength:int
-        :param s:current ratio
-        :type s:float
-        :return: minAllConf of prefix
-        :rtype: float
+        :param self.Database : it represents the one self.Database in database
+        :type self.Database : list
+        :return: tuple
         """
-        maximums = 0
-        for ele in range(prefixLength):
-            i = prefix[ele]
-            if maximums < self._mapSupport.get(i):
-                maximums = self._mapSupport.get(i)
-        return s / maximums
 
-    def _correlatedOneItem(self) -> None:
-        """
-        Generating One correlated item
-        """
-        self._mapSupport = {}
+        mapSupport = {}
         for i in self._Database:
             for j in i:
-                if j not in self._mapSupport:
-                    self._mapSupport[j] = 1
+                if j.item not in mapSupport:
+                    mapSupport[j.item] = j.probability
                 else:
-                    self._mapSupport[j] += 1
-
-    def _saveItemSet(self, prefix, prefixLength, support) -> None:
-        """
-        To save the correlated patterns mined form correlatedPatternTree
+                    mapSupport[j.item] += j.probability
+        mapSupport = {k: v for k, v in mapSupport.items() if v >= self._minSup}
+        plist = [k for k, v in sorted(mapSupport.items(), key=lambda x: x[1], reverse=True)]
+        self.rank = dict([(index, item) for (item, index) in enumerate(plist)])
+        return mapSupport, plist
+
+    @staticmethod
+    def _buildTree(data, info) -> '_Tree':
+        """
+        It takes the self.Database and support of each item and construct the main tree with setting root node as null
+
+        :param data : it represents the one self.Database in database
+        :type data : list
+        :param info : it represents the support of each item
+        :type info : dict
+        :return: Dictionary
+        """
+
+        rootNode = _Tree()
+        rootNode.info = info.copy()
+        for i in range(len(data)):
+            rootNode.addTransaction(data[i])
+        return rootNode
+
+    def _updateTransactions(self, dict1) -> List:
+        """
+        Remove the items which are not frequent from self.Database and updates the self.Database with rank of items
+
+        :param dict1 : frequent items with support
+        :type dict1 : dictionary
+        :return: list
+        """
+
+        list1 = []
+        for tr in self._Database:
+            list2 = []
+            for i in range(0, len(tr)):
+                if tr[i].item in dict1:
+                    list2.append(tr[i])
+            if len(list2) >= 2:
+                basket = list2
+                basket.sort(key=lambda val: self.rank[val.item])
+                list2 = basket
+                list1.append(list2)
+        return list1
+
+    @staticmethod
+    def _check(i, x) -> int:
+        """
+        To check the presence of item or pattern in transaction
+
+        :param x: it represents the pattern
+        :type x : list
+        :param i : represents the uncertain self.Database
+        :type i : list
+        :return: int
+        """
+
+        # This method taken a transaction as input and returns the tree
+        for m in x:
+            k = 0
+            for n in i:
+                if m == n.item:
+                    k += 1
+            if k == 0:
+                return 0
+        return 1
 
-        :param prefix: the correlated pattern
-        :type prefix: list
-        :param prefixLength : the length of a correlated pattern
-        :type prefixLength : int
-        :param support: the support of a pattern
-        :type support :  int
-        :return: None
-
-        The correlated patterns were stored in a global variable finalPatterns
-        """
-        all_conf = self._getRatio(prefix, prefixLength, support)
-        if all_conf < self._minAllConf:
-            return
-        l = []
-        for i in range(prefixLength):
-            l.append(prefix[i])
-        self._itemSetCount += 1
-        self._finalPatterns[tuple(l)] = [support, all_conf]
-    
-    def _convert(self, value: Union[int, float, str]) -> None:
+    def _convert(self, value) -> float:
         """
         To convert the type of user specified minSup value
 
         :param value: user specified minSup value
-        :type value: int or float or str
-        :return: None
+        :return: converted type minSup value
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self._Database) * value)
         if type(value) is str:
             if '.' in value:
-                value = float(value)
                 value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
-    def _saveAllCombinations(self, tempBuffer, s, position, prefix, prefixLength) -> None:
+    def _removeFalsePositives(self) -> None:
         """
-        Generating all the combinations for items in single branch in correlatedPatternTree
+        To remove the false positive patterns generated in frequent patterns.
 
-        :param tempBuffer: items in a single branch
-        :type tempBuffer: list
-        :param s: support at leaf node of a branch
-        :param position: the length of a tempBuffer
-        :type position: int
-        :param prefix: it represents the list of leaf node
-        :type prefix: list
-        :param prefixLength: the length of prefix
-        :type prefixLength: int
-        :return: None
-        """
-        max1 = 1 << position
-        for i in range(1, max1):
-            newPrefixLength = prefixLength
-            for j in range(position):
-                isSet = i & (1 << j)
-                if isSet > 0:
-                    prefix.insert(newPrefixLength, tempBuffer[j].itemId)
-                    newPrefixLength += 1
-            self._saveItemSet(prefix, newPrefixLength, s)
-
-    def _correlatedPatternGrowthGenerate(self, correlatedPatternTree, prefix, prefixLength, mapSupport) -> None:
-        """
-        Mining the fp tree
-
-        :param correlatedPatternTree: it represents the correlatedPatternTree
-        :type correlatedPatternTree: class Tree
-        :param prefix: it represents an empty list and store the patterns that are mined
-        :type prefix: list
-        :param prefixLength: the length of prefix
-        :type prefixLength: int
-        :param mapSupport: it represents the support of item
-        :type mapSupport: dictionary
-        :return: None
+        :return: patterns with accurate probability
         """
+        global _finalPatterns
+        periods = {}
+        for i in self._Database:
+            for x, y in _finalPatterns.items():
+                if len(x) == 1:
+                    periods[x] = y
+                else:
+                    s = 1
+                    check = self._check(i, x)
+                    if check == 1:
+                        for j in i:
+                            if j.item in x:
+                                s *= j.probability
+                        if x in periods:
+                            periods[x] += s
+                        else:
+                            periods[x] = s
+        for x, y in periods.items():
+            if y >= self._minSup:
+                sample = str()
+                for i in x:
+                    sample = sample + i + "\t"
+                self._finalPatterns[sample] = y
 
-        singlePath = True
-        position = 0
-        s = 0
-        if len(correlatedPatternTree.root.child) > 1:
-            singlePath = False
-        else:
-            currentNode = correlatedPatternTree.root.child[0]
-            while True:
-                if len(currentNode.child) > 1:
-                    singlePath = False
-                    break
-                self._fpNodeTempBuffer.insert(position, currentNode)
-                s = currentNode.counter
-                position += 1
-                if len(currentNode.child) == 0:
-                    break
-                currentNode = currentNode.child[0]
-        if singlePath is True:
-            self._saveAllCombinations(self._fpNodeTempBuffer, s, position, prefix, prefixLength)
-        else:
-            for i in reversed(correlatedPatternTree.headerList):
-                item = i
-                support = mapSupport[i]
-                betaSupport = support
-                prefix.insert(prefixLength, item)
-                self._saveItemSet(prefix, prefixLength + 1, betaSupport)
-                if prefixLength + 1 < self._maxPatternLength:
-                    prefixPaths = []
-                    path = correlatedPatternTree.mapItemNodes.get(item)
-                    mapSupportBeta = {}
-                    while path is not None:
-                        if path.parent.itemId != -1:
-                            prefixPath = []
-                            prefixPath.append(path)
-                            pathCount = path.counter
-                            parent1 = path.parent
-                            while parent1.itemId != -1:
-                                prefixPath.append(parent1)
-                                if mapSupportBeta.get(parent1.itemId) is None:
-                                    mapSupportBeta[parent1.itemId] = pathCount
-                                else:
-                                    mapSupportBeta[parent1.itemId] = mapSupportBeta[parent1.itemId] + pathCount
-                                parent1 = parent1.parent
-                            prefixPaths.append(prefixPath)
-                        path = path.nodeLink
-                    treeBeta = _Tree()
-                    for k in prefixPaths:
-                        treeBeta.addPrefixPath(k, mapSupportBeta, self._minSup)
-                    if len(treeBeta.root.child) > 0:
-                        treeBeta.createHeaderList(mapSupportBeta, self._minSup)
-                        self._correlatedPatternGrowthGenerate(treeBeta, prefix, prefixLength + 1, mapSupportBeta)
-
-    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
+    @deprecated(
+        "It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
-        main method to start
+        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns.
+
+        :return: None
         """
         self.mine()
 
     def mine(self) -> None:
         """
-        main method to start
+        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns.
+
+        :return: None
         """
+        global minSup
         self._startTime = _ab._time.time()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
         self._creatingItemSets()
         self._minSup = self._convert(self._minSup)
-        self._tree = _Tree()
+        minSup = self._minSup
         self._finalPatterns = {}
-        self._correlatedOneItem()
-        self._mapSupport = {k: v for k, v in self._mapSupport.items() if v >= self._minSup}
-        _itemSetBuffer = [k for k, v in sorted(self._mapSupport.items(), key=lambda x: x[1], reverse=True)]
-        for i in self._Database:
-            _transaction = []
-            for j in i:
-                if j in _itemSetBuffer:
-                    _transaction.append(j)
-            _transaction.sort(key=lambda val: self._mapSupport[val], reverse=True)
-            self._tree.addTransaction(_transaction)
-        self._tree.createHeaderList(self._mapSupport, self._minSup)
-        if len(self._tree.headerList) > 0:
-            self._itemSetBuffer = []
-            self._correlatedPatternGrowthGenerate(self._tree, self._itemSetBuffer, 0, self._mapSupport)
-        print("Correlated patterns were generated successfully using CoMine algorithm")
+        mapSupport, plist = self._frequentOneItem()
+        self.Database1 = self._updateTransactions(mapSupport)
+        info = {k: v for k, v in mapSupport.items()}
+        Tree1 = self._buildTree(self.Database1, info)
+        Tree1.generatePatterns([])
+        self._removeFalsePositives()
+        print("Uncertain Frequent patterns were successfully generated using CUFPTree algorithm")
         self._endTime = _ab._time.time()
-        self._memoryUSS = float()
-        self._memoryRSS = float()
         process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self.memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
+        self.memoryRSS = process.memory_info().rss
+
 
     def getMemoryUSS(self) -> float:
         """
+
         Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self) -> float:
         """
+
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
-        return self._memoryRSS
+        return self.memoryRSS
 
     def getRuntime(self) -> float:
         """
+
         Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
-    def getPatternsAsDataFrame(self) -> _pd.DataFrame:
+    def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
         """
-        Storing final correlated patterns in a dataframe
 
-        :return: returning correlated patterns in a dataframe
+        Storing final frequent patterns in a dataframe
+
+        :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataframe = {}
         data = []
         for a, b in self._finalPatterns.items():
-            pat = " "
-            for i in a:
-                pat += str(i) + " "
-            data.append([pat, b[0], b[1]])
-            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Confidence'])
+            data.append([a.replace('\t', ' '), b])
+            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataframe
 
-    def save(self, outFile) -> None:
+    def save(self, outFile: str) -> None:
         """
-        Complete set of correlated patterns will be saved into an output file
 
-        :param outFile: name of the outputfile
-        :type outFile: file
+        Complete set of frequent patterns will be loaded in to an output file
+
+        :param outFile: name of the output file
+        :type outFile: csv file
         :return: None
         """
-        self._oFile = outFile
-        writer = open(self._oFile, 'w+')
+        self.oFile = outFile
+        writer = open(self.oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            pat = ""
-            for i in x:
-                pat += str(i) + "\t"
-            patternsAndSupport = pat.strip() + ":" + str(y[0]) + ":" + str(y[1])
-            writer.write("%s \n" % patternsAndSupport)
+            s1 = x.strip() + ":" + str(y)
+            writer.write("%s \n" % s1)
 
-    def getPatterns(self) -> Dict[Tuple[int], List[Union[int, float]]]:
+    def getPatterns(self) -> dict:
         """
-        Function to send the set of correlated patterns after completion of the mining process
 
-        :return: returning correlated patterns
+        Function to send the set of frequent patterns after completion of the mining process
+
+        :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self) -> None:
         """
-        function to print the result after completing the process
-
-        :return: None
+        This function is used to print the results
         """
-        print("Total number of Correlated Patterns:", len(self.getPatterns()))
+        print("Total number of  Uncertain Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:",  self.getRuntime())
 
+
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
-        if len(_ab._sys.argv) == 6:
-            _ap = CoMine(_ab._sys.argv[1], _ab._sys.argv[3], float(_ab._sys.argv[4]), _ab._sys.argv[5])
+    if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
-            _ap = CoMine(_ab._sys.argv[1], _ab._sys.argv[3], float(_ab._sys.argv[4]))
+            _ap = CUFPTree(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+        if len(_ab._sys.argv) == 4:
+            _ap = CUFPTree(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
         _ap.mine()
-        print("Total number of Correlated-Frequent Patterns:", len(_ap.getPatterns()))
+        print("Total number of Uncertain Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in seconds:", _ap.getRuntime())
+        print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
-
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `pami-2024.5.28.1/PAMI/correlatedPattern/basic/__init__.py` & `pami-2024.5.7.1/PAMI/correlatedPattern/basic/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/correlatedPattern/basic/abstract.py` & `pami-2024.5.7.1/PAMI/correlatedPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/coveragePattern/basic/CMine.py` & `pami-2024.5.7.1/PAMI/coveragePattern/basic/CMine.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/coveragePattern/basic/CPPG.py` & `pami-2024.5.7.1/PAMI/coveragePattern/basic/CPPG.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/coveragePattern/basic/abstract.py` & `pami-2024.5.7.1/PAMI/coveragePattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/DF2DB/DF2DB.py` & `pami-2024.5.7.1/PAMI/extras/DF2DB/DF2DB.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/DF2DB/DF2DBPlus.py` & `pami-2024.5.7.1/PAMI/extras/DF2DB/DF2DBPlus.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/DF2DB/DenseFormatDF.py` & `pami-2024.5.7.1/PAMI/extras/DF2DB/DenseFormatDF.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/DF2DB/SparseFormatDF.py` & `pami-2024.5.7.1/PAMI/extras/DF2DB/SparseFormatDF.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/DF2DB/createTDB.py` & `pami-2024.5.7.1/PAMI/extras/DF2DB/createTDB.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/DF2DB/denseDF2DBPlus.py` & `pami-2024.5.7.1/PAMI/extras/DF2DB/denseDF2DBPlus.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/DF2DB/denseDF2DB_dump.py` & `pami-2024.5.7.1/PAMI/extras/DF2DB/denseDF2DB_dump.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/DF2DB/sparseDF2DBPlus.py` & `pami-2024.5.7.1/PAMI/extras/DF2DB/sparseDF2DBPlus.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/calculateMISValues/usingBeta.py` & `pami-2024.5.7.1/PAMI/extras/calculateMISValues/usingBeta.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/calculateMISValues/usingSD.py` & `pami-2024.5.7.1/PAMI/extras/calculateMISValues/usingSD.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/convertMultiTSIntoFuzzy.py` & `pami-2024.5.7.1/PAMI/extras/convertMultiTSIntoFuzzy.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/dbStats/FuzzyDatabase.py` & `pami-2024.5.7.1/PAMI/extras/dbStats/FuzzyDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/dbStats/MultipleTimeSeriesFuzzyDatabaseStats.py` & `pami-2024.5.7.1/PAMI/extras/dbStats/MultipleTimeSeriesFuzzyDatabaseStats.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/dbStats/SequentialDatabase.py` & `pami-2024.5.7.1/PAMI/extras/dbStats/SequentialDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/dbStats/TemporalDatabase.py` & `pami-2024.5.7.1/PAMI/extras/dbStats/TemporalDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/dbStats/TransactionalDatabase.py` & `pami-2024.5.7.1/PAMI/extras/dbStats/TransactionalDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/dbStats/UncertainTemporalDatabase.py` & `pami-2024.5.7.1/PAMI/extras/dbStats/UncertainTemporalDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/dbStats/UncertainTransactionalDatabase.py` & `pami-2024.5.7.1/PAMI/extras/dbStats/UncertainTransactionalDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/dbStats/UtilityDatabase.py` & `pami-2024.5.7.1/PAMI/extras/dbStats/UtilityDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/fuzzyTransformation/abstract.py` & `pami-2024.5.7.1/PAMI/extras/fuzzyTransformation/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/fuzzyTransformation/temporalToFuzzy.py` & `pami-2024.5.7.1/PAMI/extras/fuzzyTransformation/temporalToFuzzy.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/fuzzyTransformation/transactionalToFuzzy.py` & `pami-2024.5.7.1/PAMI/extras/fuzzyTransformation/transactionalToFuzzy.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/fuzzyTransformation/utilityToFuzzy.py` & `pami-2024.5.7.1/PAMI/extras/fuzzyTransformation/utilityToFuzzy.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/generateDatabase/generateSpatioTemporalDatabase.py` & `pami-2024.5.7.1/PAMI/extras/generateDatabase/generateSpatioTemporalDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/generateDatabase/generateTemporalDatabase.py` & `pami-2024.5.7.1/PAMI/extras/generateDatabase/generateTemporalDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/generateDatabase/generateTransactionalDatabase.py` & `pami-2024.5.7.1/PAMI/extras/generateDatabase/generateTransactionalDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/generateLatexGraphFile.py` & `pami-2024.5.7.1/PAMI/extras/generateLatexGraphFile.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/graph/DF2Fig.py` & `pami-2024.5.7.1/PAMI/extras/graph/DF2Fig.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/graph/DF2Tex.py` & `pami-2024.5.7.1/PAMI/extras/graph/DF2Tex.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/graph/plotLineGraphFromDictionary.py` & `pami-2024.5.7.1/PAMI/extras/graph/plotLineGraphFromDictionary.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/graph/plotLineGraphsFromDataFrame.py` & `pami-2024.5.7.1/PAMI/extras/graph/plotLineGraphsFromDataFrame.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/graph/visualizeFuzzyPatterns.py` & `pami-2024.5.7.1/PAMI/extras/graph/visualizeFuzzyPatterns.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/graph/visualizePatterns.py` & `pami-2024.5.7.1/PAMI/extras/graph/visualizePatterns.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/imageProcessing/imagery2Databases.py` & `pami-2024.5.7.1/PAMI/extras/imageProcessing/imagery2Databases.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/messaging/discord.py` & `pami-2024.5.7.1/PAMI/extras/messaging/discord.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/messaging/gmail.py` & `pami-2024.5.7.1/PAMI/extras/messaging/gmail.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/neighbours/findNeighborsUsingEuclideanDistanceforPointInfo.py` & `pami-2024.5.7.1/PAMI/extras/neighbours/findNeighborsUsingEuclideanDistanceforPointInfo.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/neighbours/findNeighboursUsingEuclidean.py` & `pami-2024.5.7.1/PAMI/extras/neighbours/findNeighboursUsingEuclidean.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/neighbours/findNeighboursUsingGeodesic.py` & `pami-2024.5.7.1/PAMI/extras/neighbours/findNeighboursUsingGeodesic.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/plotPointOnMap.py` & `pami-2024.5.7.1/PAMI/extras/plotPointOnMap.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/plotPointOnMap_dump.py` & `pami-2024.5.7.1/PAMI/extras/plotPointOnMap_dump.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/scatterPlotSpatialPoints.py` & `pami-2024.5.7.1/PAMI/extras/scatterPlotSpatialPoints.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/stats/TransactionalDatabase.py` & `pami-2024.5.7.1/PAMI/extras/stats/TransactionalDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/stats/graphDatabase.py` & `pami-2024.5.7.1/PAMI/extras/stats/graphDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/stats/sequentialDatabase.py` & `pami-2024.5.7.1/PAMI/extras/stats/sequentialDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/stats/temporalDatabase.py` & `pami-2024.5.7.1/PAMI/extras/stats/temporalDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/stats/utilityDatabase.py` & `pami-2024.5.7.1/PAMI/extras/stats/utilityDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/syntheticDataGenerator/TemporalDatabase.py` & `pami-2024.5.7.1/PAMI/extras/syntheticDataGenerator/TemporalDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialTemporal.py` & `pami-2024.5.7.1/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialTemporal.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialTransactions.py` & `pami-2024.5.7.1/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialTransactions.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialUncertainTransaction.py` & `pami-2024.5.7.1/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialUncertainTransaction.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/syntheticDataGenerator/createSyntheticTemporal.py` & `pami-2024.5.7.1/PAMI/extras/syntheticDataGenerator/createSyntheticTemporal.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/syntheticDataGenerator/createSyntheticTransactions.py` & `pami-2024.5.7.1/PAMI/extras/syntheticDataGenerator/createSyntheticTransactions.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/syntheticDataGenerator/createSyntheticUncertainTemporal.py` & `pami-2024.5.7.1/PAMI/extras/syntheticDataGenerator/createSyntheticUncertainTemporal.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/syntheticDataGenerator/createSyntheticUncertainTransactions.py` & `pami-2024.5.7.1/PAMI/extras/syntheticDataGenerator/createSyntheticUncertainTransactions.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/syntheticDataGenerator/createSyntheticUtility.py` & `pami-2024.5.7.1/PAMI/extras/syntheticDataGenerator/createSyntheticUtility.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/syntheticDataGenerator/generateTemporal.py` & `pami-2024.5.7.1/PAMI/extras/syntheticDataGenerator/generateTemporal.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/syntheticDataGenerator/generateTransactional.py` & `pami-2024.5.7.1/PAMI/extras/syntheticDataGenerator/generateTransactional.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/syntheticDataGenerator/generateUncertainTemporal.py` & `pami-2024.5.7.1/PAMI/extras/syntheticDataGenerator/generateUncertainTemporal.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/syntheticDataGenerator/generateUncertainTransactional.py` & `pami-2024.5.7.1/PAMI/extras/syntheticDataGenerator/generateUncertainTransactional.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/syntheticDataGenerator/generateUtilityTemporal.py` & `pami-2024.5.7.1/PAMI/extras/syntheticDataGenerator/generateUtilityTemporal.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/syntheticDataGenerator/generateUtilityTransactional.py` & `pami-2024.5.7.1/PAMI/extras/syntheticDataGenerator/generateUtilityTransactional.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/syntheticDataGenerator/syntheticUtilityDatabase.py` & `pami-2024.5.7.1/PAMI/extras/syntheticDataGenerator/syntheticUtilityDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/syntheticDataGenerator/temporalDatabaseGen.py` & `pami-2024.5.7.1/PAMI/extras/syntheticDataGenerator/temporalDatabaseGen.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/syntheticDataGenerator/utilityDatabase.py` & `pami-2024.5.7.1/PAMI/extras/syntheticDataGenerator/utilityDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/topKPatterns.py` & `pami-2024.5.7.1/PAMI/extras/topKPatterns.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/uncertaindb_convert.py` & `pami-2024.5.7.1/PAMI/extras/uncertaindb_convert.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/extras/visualize/graphs.py` & `pami-2024.5.7.1/PAMI/extras/visualize/graphs.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/faultTolerantFrequentPattern/basic/FTApriori.py` & `pami-2024.5.7.1/PAMI/faultTolerantFrequentPattern/basic/FTApriori.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/faultTolerantFrequentPattern/basic/FTFPGrowth.py` & `pami-2024.5.7.1/PAMI/faultTolerantFrequentPattern/basic/FTFPGrowth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/faultTolerantFrequentPattern/basic/abstract.py` & `pami-2024.5.7.1/PAMI/faultTolerantFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/frequentPattern/__init__.py` & `pami-2024.5.7.1/PAMI/frequentPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/frequentPattern/basic/Apriori.py` & `pami-2024.5.7.1/PAMI/frequentPattern/basic/Apriori.py`

 * *Files 2% similar despite different names*

```diff
@@ -203,15 +203,16 @@
             if '.' in value:
                 value = float(value)
                 value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
-    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
+    @deprecated(
+        "It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         Frequent pattern mining process will start from here
         """
         self.mine()
 
     def mine(self) -> None:
@@ -323,37 +324,35 @@
         # data = []
         # for a, b in self._finalPatterns.items():
         #     # data.append([a.replace('\t', ' '), b])
         #     data.append([" ".join(a), b])
         #     dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         # print("Time taken to convert the frequent patterns into DataFrame is: ", _ab._time.time() - time)
 
-        dataFrame = _ab._pd.DataFrame(list([[" ".join(x), y] for x,y in self._finalPatterns.items()]), columns=['Patterns', 'Support'])
+
+        dataFrame = _ab._pd.DataFrame(list(self._finalPatterns.items()), columns=['Patterns', 'Support'])
 
         return dataFrame
 
-    def save(self, outFile: str, seperator = "\t" ) -> None:
+    def save(self, outFile: str) -> None:
         """
 
         Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
+
         :type outFile: csvfile
+
         :return: None
-        """
 
-        # self._oFile = outFile
-        # writer = open(self._oFile, 'w+')
-        # for x, y in self._finalPatterns.items():
-        #     patternsAndSupport = x.strip() + ":" + str(y[0])
-        #     writer.write("%s \n" % patternsAndSupport)
+        """
         with open(outFile, 'w') as f:
             for x, y in self._finalPatterns.items():
-                x = seperator.join(x)
-                f.write(f"{x}:{y}\n")
+                x = self._sep.join(x)
+                f.write(f"{x} : {y}\n")
 
     def getPatterns(self) -> Dict[str, int]:
         """
 
         Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
@@ -383,9 +382,7 @@
         print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ap._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
-
-
```

### Comparing `pami-2024.5.28.1/PAMI/frequentPattern/basic/Aprioribitset.py` & `pami-2024.5.7.1/PAMI/frequentPattern/basic/Aprioribitset.py`

 * *Files 2% similar despite different names*

```diff
@@ -48,15 +48,15 @@
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
 from PAMI.frequentPattern.basic import abstract as _ab
 from deprecated import deprecated
 
 
-class Aprioribitset(_ab._frequentPatterns):
+class AprioriBitset(_ab._frequentPatterns):
     """
     :Description:  AprioriBitset is one of the fundamental algorithm to discover frequent patterns in a transactional database.
 
     :Reference:  Mohammed Javeed Zaki: Scalable Algorithms for Association Mining. IEEE Trans. Knowl. Data Eng. 12(3):
             372-390 (2000), https://ieeexplore.ieee.org/document/846291
 
     :param  iFile: str :
@@ -344,38 +344,35 @@
         # data = []
         # for a, b in self._finalPatterns.items():
         #     # data.append([a.replace('\t', ' '), b])
         #     data.append([" ".join(a), b])
         #     dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         # print("Time taken to convert the frequent patterns into DataFrame is: ", _ab._time.time() - time)
 
-        dataFrame = _ab._pd.DataFrame(list([[" ".join(x), y] for x,y in self._finalPatterns.items()]), columns=['Patterns', 'Support'])
-        # dataFrame = _ab._pd.DataFrame(list(self._finalPatterns.items()), columns=['Patterns', 'Support'])
+
+        dataFrame = _ab._pd.DataFrame(list(self._finalPatterns.items()), columns=['Patterns', 'Support'])
 
         return dataFrame
 
-    def save(self, outFile: str, seperator = "\t" ) -> None:
+    def save(self, outFile: str) -> None:
         """
 
         Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
+
         :type outFile: csvfile
+
         :return: None
-        """
 
-        # self._oFile = outFile
-        # writer = open(self._oFile, 'w+')
-        # for x, y in self._finalPatterns.items():
-        #     patternsAndSupport = x.strip() + ":" + str(y[0])
-        #     writer.write("%s \n" % patternsAndSupport)
+        """
         with open(outFile, 'w') as f:
             for x, y in self._finalPatterns.items():
-                x = seperator.join(x)
-                f.write(f"{x}:{y}\n")
+                x = self._sep.join(x)
+                f.write(f"{x} : {y}\n")
 
     def getPatterns(self):
         """
         Function to send the set of frequent patterns after completion of the mining process
         :return: returning frequent patterns
         :rtype: dict
         """
@@ -391,20 +388,21 @@
         print("Total ExecutionTime in ms:", self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
-            _ap = Aprioribitset(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = AprioriBitset(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
-            _ap = Aprioribitset(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap = AprioriBitset(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
         _ap.mine()
         print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
 
+
```

### Comparing `pami-2024.5.28.1/PAMI/frequentPattern/basic/ECLAT.py` & `pami-2024.5.7.1/PAMI/frequentPattern/basic/ECLATbitset.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,18 +1,18 @@
-# ECLAT is one of the fundamental algorithm to discover frequent patterns in a transactional database.
+# ECLATbitset is one of the fundamental algorithm to discover frequent patterns in a transactional database.
 #
 # **Importing this algorithm into a python program**
 #
-#             import PAMI.frequentPattern.basic.ECLAT as alg
+#             import PAMI.frequentPattern.basic.ECLATbitset as alg
 #
 #             iFile = 'sampleDB.txt'
 #
 #             minSup = 10  # can also be specified between 0 and 1
 #
-#             obj = alg.ECLAT(iFile, minSup)
+#             obj = alg.ECLATbitset(iFile, minSup)
 #
 #             obj.mine()
 #
 #             frequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
@@ -50,65 +50,61 @@
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
 from PAMI.frequentPattern.basic import abstract as _ab
 from deprecated import deprecated
 
-class ECLAT(_ab._frequentPatterns):
-    """
-    About this algorithm
-    ====================
 
-    :**Description**: ECLAT is one of the fundamental algorithm to discover frequent patterns in a transactional database.
+class ECLATbitset(_ab._frequentPatterns):
+    """
+    :*Description*:  ECLATbitset is one of the fundamental algorithm to discover frequent patterns in a transactional database.
 
-    :**Reference**:  Mohammed Javeed Zaki: Scalable Algorithms for Association Mining. IEEE Trans. Knowl. Data Eng. 12(3):
-            372-390 (2000), https://ieeexplore.ieee.org/document/846291
+    :*Reference*:  Mohammed Javeed Zaki: Scalable Algorithms for Association Mining. IEEE Trans. Knowl. Data Eng. 12(3):
+                   372-390 (2000), https://ieeexplore.ieee.org/document/846291
 
     :**Parameters**:    - **iFile** (*str or URL or dataFrame*) -- *Name of the Input file to mine complete set of frequent patterns.*
-                        - **oFile** (*str*) -- *Name of the output file to store complete set of frequent patterns.*
-                        - **minSup** (*int or float or str*) -- *The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.*
-                        - **sep** (*str*) -- *This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.*
+                        - **oFile** (*str*) -- *Name of the output file to store complete set of frequent patterns*
+                        - **minSup** (*int or float or str*) -- *The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.*
+                        - **sep** (*str*) -- **This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.**
 
     :**Attributes**:    - **startTime** (*float*) -- *To record the start time of the mining process.*
-                        - **endTime** (*float*) -- *To record the completion time of the mining process.*
+                        - **endTime** (*float*) -- *To record the end time of the mining process.*
                         - **finalPatterns** (*dict*) -- *Storing the complete set of patterns in a dictionary variable.*
                         - **memoryUSS** (*float*) -- *To store the total amount of USS memory consumed by the program.*
-                        - **memoryRSS** (*float*) -- *To store the total amount of RSS memory consumed by the program.*
+                        - **memoryRSS** *(float*) -- *To store the total amount of RSS memory consumed by the program.*
                         - **Database** (*list*) -- *To store the transactions of a database in list.*
 
     Execution methods
     =================
 
     **Terminal command**
 
-    .. code-block:: console
-
       Format:
 
-      (.venv) $ python3 ECLAT.py <inputFile> <outputFile> <minSup>
+      (.venv) $ python3 ECLATbitset.py <inputFile> <outputFile> <minSup>
 
       Example Usage:
 
-      (.venv) $ python3 ECLAT.py sampleDB.txt patterns.txt 10.0
+      (.venv) $ python3 ECLATbitset.py sampleDB.txt patterns.txt 10.0
 
     .. note:: minSup can be specified  in support count or a value between 0 and 1.
 
 
     **Calling from a python program**
 
     .. code-block:: python
 
-            import PAMI.frequentPattern.basic.ECLAT as alg
+            import PAMI.frequentPattern.basic.ECLATbitset as alg
 
             iFile = 'sampleDB.txt'
 
             minSup = 10  # can also be specified between 0 and 1
 
-            obj = alg.ECLAT(iFile, minSup)
+            obj = alg.ECLATbitset(iFile, minSup)
 
             obj.mine()
 
             frequentPatterns = obj.getPatterns()
 
             print("Total number of Frequent Patterns:", len(frequentPatterns))
 
@@ -124,269 +120,268 @@
 
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
-
     Credits:
     ========
 
-    The complete program was written by Kundai and revised by Tarun Sreepada under the supervision of Professor Rage Uday Kiran.
+    The complete program was written by Yudai Masu and revised by Tarun Sreepada under the supervision of Professor Rage Uday Kiran.
 
     """
 
-    _minSup = float()
     _startTime = float()
     _endTime = float()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
+    _minSup = str()
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
+    _mapSupport = {}
+    _lno = 0
 
-    def _creatingItemSets(self) -> float:
+    def _convert(self, value):
         """
 
-        Storing the complete transactions of the database/input file in a database variable
+        To convert the user specified minSup value
 
-        :return: the complete transactions of the database/input file in a database variable
-        :rtype: float
+        :param value: user specified minSup value
+        :type value: int
+        :return: converted type
+        :rtype: int or float or string
+        """
+        if type(value) is int:
+            value = int(value)
+        if type(value) is float:
+            value = (len(self._Database) * value)
+        if type(value) is str:
+            if '.' in value:
+                value = float(value)
+                value = (len(self._Database) * value)
+            else:
+                value = int(value)
+        return value
+
+    def _creatingItemSets(self):
+        """
+        Storing the complete transactions of the database/input file in a database variable
         """
         self._Database = []
+        self._mapSupport = {}
         if isinstance(self._iFile, _ab._pd.DataFrame):
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
                 self._Database = self._iFile['Transactions'].tolist()
+
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
                     self._Database.append(temp)
             else:
                 try:
-                    with open(self._iFile, 'r', encoding='utf-8') as f:
+                    with open(self._iFile, 'r') as f:
                         for line in f:
-                            line.strip()
-                            temp = [i.rstrip() for i in line.split(self._sep)]
-                            temp = [x for x in temp if x]
-                            self._Database.append(temp)
+                            self._lno += 1
+                            splitter = [i.rstrip() for i in line.split(self._sep)]
+                            splitter = [x for x in splitter if x]
+                            self._Database.append(splitter)
                 except IOError:
                     print("File Not Found")
-                    quit()
-
-    def _convert(self, value) -> float:
-        """
-
-        To convert the user specified minSup value
-
-        :param value: user specified minSup value
-        :return: converted type
-        :rtype: float
-        """
-        if type(value) is int:
-            value = int(value)
-        if type(value) is float:
-            value = (len(self._Database) * value)
-        if type(value) is str:
-            if '.' in value:
-                value = float(value)
-                value = (len(self._Database) * value)
-            else:
-                value = int(value)
-        return value
+        self._minSup = self._convert(self._minSup)
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
-    def startMine(self) -> None:
+    def startMine(self):
         """
         Frequent pattern mining process will start from here
-        """
 
+        We start with the scanning the itemSets and store the bitsets respectively.
+        We form the combinations of single items and  check with minSup condition to check the frequency of patterns
+        """
         self.mine()
 
-    def __recursive(self, items, cands):
+    def _bitPacker(self, data, maxIndex):
         """
 
-        This function generates new candidates by taking input as original candidates.
+        It takes the data and maxIndex as input and generates integer as output value.
 
-        :param items: A dictionary containing items and their corresponding support values.
-        :type items: dict
-        :param cands: A list of candidate itemsets.
-        :type cands: list
-        :return: None
+        :param data: it takes data as input.
+        :type data: int or float
+        :param maxIndex: It converts the data into bits By taking the maxIndex value as condition.
+        :type maxIndex: int
         """
+        packed_bits = 0
+        for i in data:
+            packed_bits |= 1 << (maxIndex - i)
 
-        for i in range(len(cands)):
-            newCands = []
-            for j in range(i + 1, len(cands)):
-                intersection = items[cands[i]].intersection(items[cands[j]])
-                if len(intersection) >= self._minSup:
-                    newCand = tuple(cands[i] + tuple([cands[j][-1]]))
-                    newCands.append(newCand)
-                    items[newCand] = intersection
-                    self._finalPatterns[newCand] = len(intersection)
-            if len(newCands) > 1:
-                self.__recursive(items, newCands)
+        return packed_bits
 
     def mine(self) -> None:
         """
         Frequent pattern mining process will start from here
+        # Bitset implementation
         """
-
         self._startTime = _ab._time.time()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        if self._minSup is None:
-            raise Exception("Please enter the Minimum Support")
-        self._creatingItemSets()
 
-        self._minSup = self._convert(self._minSup)
+        self._Database = []
+
+        self._creatingItemSets()
 
-    
         items = {}
         index = 0
         for line in self._Database:
             for item in line:
-                if item not in items:
-                    items[item] = []
-                items[item].append(index)
+                if tuple([item]) in items:
+                    items[tuple([item])].append(index)
+                else:
+                    items[tuple([item])] = [index]
             index += 1
-        
-        items = {tuple([k]): set(v) for k, v in items.items() if len(v) >= self._minSup}
-        items = {k: v for k, v in sorted(items.items(), key=lambda item: len(item[1]), reverse=False)}
-        for k, v in items.items():
-            self._finalPatterns[k] = len(v)
 
-        cands = list(items.keys())
+        # sort by length in descending order
+        items = dict(sorted(items.items(), key=lambda x: len(x[1]), reverse=True))
+        cands = []
+        for key in items:
+            if len(items[key]) >= self._minSup:
+                self._finalPatterns["\t".join(key)] = len(items[key])
+                cands.append(key)
+                items[key] = self._bitPacker(items[key], index)
+                # print(key, items[key])
+            else:
+                break
 
-        self.__recursive(items, cands)
+        while cands:
+            newCands = []
+            for i in range(len(cands)):
+                for j in range(i + 1, len(cands)):
+                    if cands[i][:-1] == cands[j][:-1]:
+                        newCand = tuple(cands[i] + tuple([cands[j][-1]]))
+                        intersection = items[tuple([newCand[0]])]
+                        for k in range(1, len(newCand)):
+                            intersection &= items[tuple([newCand[k]])]
+                        count = int.bit_count(intersection)
+                        if count >= self._minSup:
+                            newCands.append(newCand)
+                            newCand = "\t".join(newCand)
+                            self._finalPatterns[newCand] = count
+                    else:
+                        break
 
+            cands = newCands
 
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
-        print("Frequent patterns were generated successfully using ECLAT algorithm")
+        print("Frequent patterns were generated successfully using Apriori algorithm ")
 
-    def getMemoryUSS(self) -> float:
+    def getMemoryUSS(self):
         """
 
         Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
-    def getMemoryRSS(self) -> float:
+    def getMemoryRSS(self):
         """
 
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
-    def getRuntime(self) -> float:
+    def getRuntime(self):
         """
+
         Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
-    def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
+    def getPatternsAsDataFrame(self):
         """
 
         Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
-        # time = _ab._time.time()
-        # dataFrame = {}
-        # data = []
-        # for a, b in self._finalPatterns.items():
-        #     # data.append([a.replace('\t', ' '), b])
-        #     data.append([" ".join(a), b])
-        #     dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
-        # print("Time taken to convert the frequent patterns into DataFrame is: ", _ab._time.time() - time)
-
-        dataFrame = _ab._pd.DataFrame(list([[" ".join(x), y] for x,y in self._finalPatterns.items()]), columns=['Patterns', 'Support'])
-        # dataFrame = _ab._pd.DataFrame(list(self._finalPatterns.items()), columns=['Patterns', 'Support'])
-
+        dataFrame = {}
+        data = []
+        for a, b in self._finalPatterns.items():
+            data.append([a.replace('\t', ' '), b])
+            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataFrame
 
-    def save(self, outFile: str, seperator = "\t" ) -> None:
+    def save(self, outFile):
         """
 
         Complete set of frequent patterns will be loaded in to an output file
 
-        :param outFile: name of the output file
-        :type outFile: csvfile
-        :return: None
-        """
-
-        # self._oFile = outFile
-        # writer = open(self._oFile, 'w+')
-        # for x, y in self._finalPatterns.items():
-        #     patternsAndSupport = x.strip() + ":" + str(y[0])
-        #     writer.write("%s \n" % patternsAndSupport)
-        with open(outFile, 'w') as f:
-            for x, y in self._finalPatterns.items():
-                x = seperator.join(x)
-                f.write(f"{x}:{y}\n")
+        :param outFile: name of the outputfile
+        :type outFile: file
+        """
+        self._oFile = outFile
+        writer = open(self._oFile, 'w+')
+        for x, y in self._finalPatterns.items():
+            patternsAndSupport = x.strip() + ":" + str(y)
+            writer.write("%s \n" % patternsAndSupport)
 
-    def getPatterns(self) -> dict:
+    def getPatterns(self):
         """
+
         Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
-    def printResults(self) -> None:
+    def printResults(self):
         """
-        Function used to print the results
+        This function is used to print the result
         """
         print("Total number of Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:",  self.getRuntime())
+        print("Total ExecutionTime in ms:", self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
-            _ap = ECLAT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = ECLATbitset(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
-            _ap = ECLAT(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap = ECLATbitset(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
         _ap.mine()
         print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
-        print(_ap.getPatternsAsDataFrame())
-        print("Total Memory in USS:",  _ap.getMemoryUSS())
+        print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
-        print("Error! The number of input parameters do not match the total number of parameters provided")
-
+        print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.5.28.1/PAMI/frequentPattern/basic/ECLATDiffset.py` & `pami-2024.5.7.1/PAMI/frequentPattern/basic/ECLATDiffset.py`

 * *Files 6% similar despite different names*

```diff
@@ -48,21 +48,22 @@
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
 
+# from abstract import *
+
 from PAMI.frequentPattern.basic import abstract as _ab
 from deprecated import deprecated
 
 
 class ECLATDiffset(_ab._frequentPatterns):
     """
-
     :**Description**:   ECLATDiffset uses diffset to extract the frequent patterns in a transactional database.
 
     :**Reference**:  KDD '03: Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining
                      August 2003 Pages 326335 https://doi.org/10.1145/956750.956788
             
     :**Parameters**:    - **iFile** (*str or URL or dataFrame*) -- *Name of the Input file to mine complete set of frequent patterns.*
                         - **oFile** (*str*) -- *Name of the output file to store complete set of frequent patterns*
@@ -198,46 +199,80 @@
             if '.' in value:
                 value = float(value)
                 value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
+    def _getUniqueItemList(self):
+
+        # tidSets will store all the initial tids
+        tidSets = {}
+        # uniqueItem will store all frequent 1 items
+        uniqueItem = []
+        for line in self._Database:
+                transNum = 0
+                # Database = [set([i.rstrip() for i in transaction.split('\t')]) for transaction in f]
+                for transaction in self._Database:
+                    transNum += 1
+                    self._trans_set.add(transNum)
+                    for item in transaction:
+                        if item in tidSets:
+                            tidSets[item].add(transNum)
+                        else:
+                            tidSets[item] = {transNum}
+        for key, value in tidSets.items():
+            supp = len(value)
+            if supp >= self._minSup:
+                self._diffSets[key] = [supp, self._trans_set.difference(value)]
+                uniqueItem.append(key)
+        # for x, y in self._diffSets.items():
+        #     print(x, y)
+        uniqueItem.sort()
+        # print()
+        return uniqueItem
+
+    def _runDeclat(self, candidateList):
+        """
+
+        It will generate the combinations of frequent items
+
+        :param candidateList :it represents the items with their respective transaction identifiers
+        :type candidateList: list
+        :return: returning transaction dictionary
+        :rtype: dict
+        """
+
+        newList = []
+        for i in range(0, len(candidateList)):
+            item1 = candidateList[i]
+            iList = item1.split()
+            for j in range(i + 1, len(candidateList)):
+                item2 = candidateList[j]
+                jList = item2.split()
+                if iList[:-1] == jList[:-1]:
+                    unionDiffSet = self._diffSets[item2][1].difference(self._diffSets[item1][1])
+                    unionSup = self._diffSets[item1][0] - len(unionDiffSet)
+                    if unionSup >= self._minSup:
+                        newKey = item1 + "\t" + jList[-1]
+                        self._diffSets[newKey] = [unionSup, unionDiffSet]
+                        newList.append(newKey)
+                    else: 
+                        break
+
+        if len(newList) > 0:
+            self._runDeclat(newList)
+
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Frequent pattern mining process will start from here
         """
         self.mine()
 
-    def __recursive(self, items, cands):
-        """
-
-        This function generates new candidates by taking input as original candidates.
-
-        :param items: A dictionary containing items and their corresponding support values.
-        :type items: dict
-        :param cands: A list of candidate itemsets.
-        :type cands: list
-        :return: None
-        """
-
-        for i in range(len(cands)):
-            newCands = []
-            for j in range(i + 1, len(cands)):
-                intersection = items[cands[i]] | items[cands[j]]
-                supp = len(self._db - intersection)
-                if supp >= self._minSup:
-                    newCand = tuple(cands[i] + tuple([cands[j][-1]]))
-                    newCands.append(newCand)
-                    items[newCand] = intersection
-                    self._finalPatterns[newCand] = supp
-            if len(newCands) > 1:
-                self.__recursive(items, newCands)
-
     def mine(self):
         """
         Frequent pattern mining process will start from here
         """
 
         self._startTime = _ab._time.time()
         self._Database = []
@@ -247,41 +282,19 @@
         if self._iFile is None:
             raise Exception("Please enter the file path or file name:")
         if self._minSup is None:
             raise Exception("Please enter the Minimum Support")
         self._creatingItemSets()
         #print(len(self._Database))
         self._minSup = self._convert(self._minSup)
-
-        items = {}
-        db = set([i for i in range(len(self._Database))])
-        for i in range(len(self._Database)):
-            for item in self._Database[i]:
-                if tuple([item]) in items:
-                    items[tuple([item])].append(i)
-                else:
-                    items[tuple([item])] = [i]
-        
-        items = dict(sorted(items.items(), key=lambda x: len(x[1]), reverse=True))
-
-        keys = []
-        for item in list(items.keys()):
-            if len(items[item]) < self._minSup:
-                del items[item]
-                continue
-            self._finalPatterns[item] = len(items[item])
-            # print(item, len(items[item]))
-            items[item] = db - set(items[item])
-            # print(item, len(items[item]))
-            keys.append(item)
-
-        self._db = db
-
-        self.__recursive(items, keys)
-
+        uniqueItemList = []
+        uniqueItemList = self._getUniqueItemList()
+        self._runDeclat(uniqueItemList)
+        self._finalPatterns = self._diffSets
+        #print(len(self._finalPatterns), len(uniqueItemList))
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
         print("Frequent patterns were generated successfully using ECLAT Diffset algorithm")
@@ -318,63 +331,54 @@
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self):
         """
 
-        Storing final frequent patterns in a dataframe.
+        Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
-        # dataFrame = {}
-        # data = []
-        # for a, b in self._finalPatterns.items():
-        #     data.append([a.replace('\t', ' '), b[0]])
-        #     dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
-
-        dataFrame = _ab._pd.DataFrame(list([[" ".join(x), y] for x,y in self._finalPatterns.items()]), columns=['Patterns', 'Support'])
-        
+        dataFrame = {}
+        data = []
+        for a, b in self._finalPatterns.items():
+            data.append([a.replace('\t', ' '), b[0]])
+            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataFrame
 
-    def save(self, outFile: str, seperator = "\t" ) -> None:
+    def save(self, outFile):
         """
 
-        Complete set of frequent patterns will be loaded in to an output csv file.
+        Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: csvfile
-        :return: None
         """
-
-        # self._oFile = outFile
-        # writer = open(self._oFile, 'w+')
-        # for x, y in self._finalPatterns.items():
-        #     patternsAndSupport = x.strip() + ":" + str(y[0])
-        #     writer.write("%s \n" % patternsAndSupport)
-        with open(outFile, 'w') as f:
-            for x, y in self._finalPatterns.items():
-                x = seperator.join(x)
-                f.write(f"{x}:{y}\n")
+        self._oFile = outFile
+        writer = open(self._oFile, 'w+')
+        for x, y in self._finalPatterns.items():
+            patternsAndSupport = x.strip() + ":" + str(y[0])
+            writer.write("%s \n" % patternsAndSupport)
 
     def getPatterns(self):
         """
 
-        This function returns the frequent patterns after completion of the mining process
+        Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self):
         """
-        This function is used to print the results.
+        This function is used to print the results
         """
         print("Total number of Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:",  self.getRuntime())
```

### Comparing `pami-2024.5.28.1/PAMI/frequentPattern/basic/ECLATbitset.py` & `pami-2024.5.7.1/PAMI/frequentPattern/cuda/cudaAprioriGCT.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,18 +1,16 @@
-# ECLATbitset is one of the fundamental algorithm to discover frequent patterns in a transactional database.
+# cudaAprioriGCT is one of the fundamental algorithm to discover frequent patterns using CUDA in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
 #
-# **Importing this algorithm into a python program**
-#
-#             import PAMI.frequentPattern.basic.ECLATbitset as alg
 #
-#             iFile = 'sampleDB.txt'
+# **Importing this algorithm into a python program**
+# ----------------------------------------------------
 #
-#             minSup = 10  # can also be specified between 0 and 1
+#             import PAMI.frequentPattern.cuda.cudaAprioriGCT as alg
 #
-#             obj = alg.ECLATbitset(iFile, minSup)
+#             obj = alg.cuAprioriGCT(iFile, minSup)
 #
 #             obj.mine()
 #
 #             frequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
@@ -30,14 +28,16 @@
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
+
+
 __copyright__ = """
 Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
@@ -47,64 +47,87 @@
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
-from PAMI.frequentPattern.basic import abstract as _ab
 from deprecated import deprecated
+from PAMI.frequentPattern.basic import abstract as _ab
+# import abstract as _ab
 
+import os
+import time
+import numpy as np
+import pycuda.gpuarray as gpuarray
+import psutil
 
-class ECLATbitset(_ab._frequentPatterns):
+
+class cudaAprioriGCT(_ab._frequentPatterns):
     """
-    :*Description*:  ECLATbitset is one of the fundamental algorithm to discover frequent patterns in a transactional database.
+    :Description: cudaAprioriGCT is one of the fundamental algorithm to discover frequent patterns in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
+
+    :Reference:  Agrawal, R., Imieli nski, T., Swami, A.: Mining association rules between sets of items in large databases.
+                In: SIGMOD. pp. 207216 (1993), https://doi.org/10.1145/170035.170072
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of frequent patterns
+    :param  oFile: str :
+                   Name of the output file to store complete set of frequent patterns
+    :param  minSup: int :
+                    The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+
+    :Attributes:
+
+        startTime : float
+              To record the start time of the mining process
+
+        endTime : float
+              To record the completion time of the mining process
+
+        finalPatterns : dict
+              Storing the complete set of patterns in a dictionary variable
 
-    :*Reference*:  Mohammed Javeed Zaki: Scalable Algorithms for Association Mining. IEEE Trans. Knowl. Data Eng. 12(3):
-                   372-390 (2000), https://ieeexplore.ieee.org/document/846291
+        memoryUSS : float
+              To store the total amount of USS memory consumed by the program
 
-    :**Parameters**:    - **iFile** (*str or URL or dataFrame*) -- *Name of the Input file to mine complete set of frequent patterns.*
-                        - **oFile** (*str*) -- *Name of the output file to store complete set of frequent patterns*
-                        - **minSup** (*int or float or str*) -- *The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.*
-                        - **sep** (*str*) -- **This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.**
-
-    :**Attributes**:    - **startTime** (*float*) -- *To record the start time of the mining process.*
-                        - **endTime** (*float*) -- *To record the end time of the mining process.*
-                        - **finalPatterns** (*dict*) -- *Storing the complete set of patterns in a dictionary variable.*
-                        - **memoryUSS** (*float*) -- *To store the total amount of USS memory consumed by the program.*
-                        - **memoryRSS** *(float*) -- *To store the total amount of RSS memory consumed by the program.*
-                        - **Database** (*list*) -- *To store the transactions of a database in list.*
+        memoryRSS : float
+              To store the total amount of RSS memory consumed by the program
 
-    Execution methods
-    =================
+        Database : list
+              To store the transactions of a database in list
 
-    **Terminal command**
+
+
+    **Methods to execute code on terminal**
+    ----------------------------------------------------
+
+    .. code-block:: console
 
       Format:
 
-      (.venv) $ python3 ECLATbitset.py <inputFile> <outputFile> <minSup>
+      (.venv) $ python3 cudaAprioriGCT.py <inputFile> <outputFile> <minSup>
 
       Example Usage:
 
-      (.venv) $ python3 ECLATbitset.py sampleDB.txt patterns.txt 10.0
+      (.venv) $ python3 cudaAprioriGCT.py sampleDB.txt patterns.txt 10.0
 
-    .. note:: minSup can be specified  in support count or a value between 0 and 1.
+    .. note:: minSup will be considered in percentage of database transactions
 
 
-    **Calling from a python program**
+    **Importing this algorithm into a python program**
+    ----------------------------------------------------
 
     .. code-block:: python
 
-            import PAMI.frequentPattern.basic.ECLATbitset as alg
-
-            iFile = 'sampleDB.txt'
+            import PAMI.frequentPattern.cuda.cuAprioriGCT as alg
 
-            minSup = 10  # can also be specified between 0 and 1
-
-            obj = alg.ECLATbitset(iFile, minSup)
+            obj = alg.cuAprioriGCT(iFile, minSup)
 
             obj.mine()
 
             frequentPatterns = obj.getPatterns()
 
             print("Total number of Frequent Patterns:", len(frequentPatterns))
 
@@ -120,276 +143,272 @@
 
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
-    Credits:
-    ========
 
-    The complete program was written by Yudai Masu and revised by Tarun Sreepada under the supervision of Professor Rage Uday Kiran.
+    **Credits:**
+    -------------
+
+                The complete program was written by Tarun Sreepada under the supervision of Professor Rage Uday Kiran.
 
     """
 
-    _startTime = float()
-    _endTime = float()
+    __time = 0
+    __memRSS = 0
+    __memUSS = 0
+    __GPU_MEM = 0
+    _minSup = 0
     _finalPatterns = {}
-    _iFile = " "
-    _oFile = " "
-    _sep = " "
-    _minSup = str()
-    _memoryUSS = float()
-    _memoryRSS = float()
-    _Database = []
-    _mapSupport = {}
-    _lno = 0
 
-    def _convert(self, value):
-        """
-
-        To convert the user specified minSup value
-
-        :param value: user specified minSup value
-        :type value: int
-        :return: converted type
-        :rtype: int or float or string
-        """
-        if type(value) is int:
-            value = int(value)
-        if type(value) is float:
-            value = (len(self._Database) * value)
-        if type(value) is str:
-            if '.' in value:
-                value = float(value)
-                value = (len(self._Database) * value)
-            else:
-                value = int(value)
-        return value
+    def __init__(self, filePath, minSup, sep):
+        self._iFile = filePath
+        self._sep = sep
+        self._minSup = minSup
+        self.__time = 0
+        self.__memRSS = 0
+        self.__memUSS = 0
 
-    def _creatingItemSets(self):
+    def __creatingItemSets(self):
         """
         Storing the complete transactions of the database/input file in a database variable
         """
-        self._Database = []
-        self._mapSupport = {}
+        self.__Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
-                self._Database = self._iFile['Transactions'].tolist()
+                self.__Database = self._iFile['Transactions'].tolist()
 
+            # print(self.Database)
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    self._Database.append(temp)
+                    self.__Database.append(temp)
             else:
                 try:
-                    with open(self._iFile, 'r') as f:
+                    with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
-                            self._lno += 1
-                            splitter = [i.rstrip() for i in line.split(self._sep)]
-                            splitter = [x for x in splitter if x]
-                            self._Database.append(splitter)
+                            line = line.strip()
+                            temp = [i.rstrip() for i in line.split(self._sep)]
+                            temp = [x for x in temp if x]
+                            self.__Database.append(temp)
                 except IOError:
                     print("File Not Found")
-        self._minSup = self._convert(self._minSup)
+                    quit()
 
-    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
-    def startMine(self):
+    def __convert(self, value):
         """
-        Frequent pattern mining process will start from here
 
-        We start with the scanning the itemSets and store the bitsets respectively.
-        We form the combinations of single items and  check with minSup condition to check the frequency of patterns
-        """
-        self.mine()
+        To convert the type of user specified minSup value
 
-    def _bitPacker(self, data, maxIndex):
-        """
-
-        It takes the data and maxIndex as input and generates integer as output value.
+        :param value: user specified minSup value
 
-        :param data: it takes data as input.
-        :type data: int or float
-        :param maxIndex: It converts the data into bits By taking the maxIndex value as condition.
-        :type maxIndex: int
-        """
-        packed_bits = 0
-        for i in data:
-            packed_bits |= 1 << (maxIndex - i)
+        :type value: int or float or str
 
-        return packed_bits
+        :return: converted type
 
-    def mine(self) -> None:
         """
-        Frequent pattern mining process will start from here
-        # Bitset implementation
-        """
-        self._startTime = _ab._time.time()
-
-        self._Database = []
-
-        self._creatingItemSets()
-
-        items = {}
-        index = 0
-        for line in self._Database:
-            for item in line:
-                if tuple([item]) in items:
-                    items[tuple([item])].append(index)
-                else:
-                    items[tuple([item])] = [index]
-            index += 1
-
-        # sort by length in descending order
-        items = dict(sorted(items.items(), key=lambda x: len(x[1]), reverse=True))
-        cands = []
-        for key in items:
-            if len(items[key]) >= self._minSup:
-                self._finalPatterns["\t".join(key)] = len(items[key])
-                cands.append(key)
-                items[key] = self._bitPacker(items[key], index)
-                # print(key, items[key])
+        if type(value) is int:
+            value = int(value)
+        if type(value) is float:
+            value = (len(self.__Database) * value)
+        if type(value) is str:
+            if '.' in value:
+                value = float(value)
+                value = (len(self.__Database) * value)
             else:
-                break
-
-        while cands:
-            newCands = []
-            for i in range(len(cands)):
-                for j in range(i + 1, len(cands)):
-                    if cands[i][:-1] == cands[j][:-1]:
-                        newCand = tuple(cands[i] + tuple([cands[j][-1]]))
-                        intersection = items[tuple([newCand[0]])]
-                        for k in range(1, len(newCand)):
-                            intersection &= items[tuple([newCand[k]])]
-                        count = int.bit_count(intersection)
-                        if count >= self._minSup:
-                            newCands.append(newCand)
-                            newCand = "\t".join(newCand)
-                            self._finalPatterns[newCand] = count
-                    else:
-                        break
-
-            cands = newCands
-
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("Frequent patterns were generated successfully using ECLAT algorithm ")
+                value = int(value)
+        return value
 
-    def getMemoryUSS(self):
+    def compute_vertical_bitvector_data(self):
         """
+        Converting database into bit vector
+        """
+        # ---build item to idx mapping---#
+        idx = 0
+        item2idx = {}
+        for transaction in self.__Database:
+            for item in transaction:
+                if not item in item2idx:
+                    item2idx[item] = idx
+                    idx += 1
+        idx2item = {idx: str(int(item)) for item, idx in item2idx.items()}
+        # ---build vertical data---#
+        vb_data = np.zeros((len(item2idx), len(self.__Database)), dtype=np.uint16)
+        for trans_id, transaction in enumerate(self.__Database):
+            for item in transaction:
+                vb_data[item2idx[item], trans_id] = 1
+        vb_data = gpuarray.to_gpu(vb_data.astype(np.uint16))
+        return vb_data, idx2item
 
-        Total amount of USS memory consumed by the mining process will be retrieved from this function
-
-        :return: returning USS memory consumed by the mining process
+    def getRuntime(self):
+        """
+        Calculating the total amount of time taken by the mining process
+        :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
-
-        return self._memoryUSS
+        return self.__time
 
     def getMemoryRSS(self):
         """
-
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
-
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
-        return self._memoryRSS
+        return self.__memRSS
 
-    def getRuntime(self):
+    def getMemoryUSS(self):
+        """
+        Total amount of USS memory consumed by the mining process will be retrieved from this function
+        :return: returning USS memory consumed by the mining process
+        :rtype: float
+        """
+        return self.__memUSS
+
+    def getGPUMemory(self):
+        """
+        To calculate the total memory consumed by GPU
+        :return: return GPU memory
+        :rtype: int
         """
 
-        Calculating the total amount of runtime taken by the mining process
+        return self.__GPU_MEM
 
-        :return: returning total amount of runtime taken by the mining process
-        :rtype: float
+    def getPatterns(self):
         """
+        Function to send the set of frequent patterns after completion of the mining process
+        :return: returning frequent patterns
+        :rtype: dict
+        """
+        return self._finalPatterns
 
-        return self._endTime - self._startTime
+    def get_numberOfPatterns(self):
+        return len(self._finalPatterns)
 
     def getPatternsAsDataFrame(self):
         """
-
         Storing final frequent patterns in a dataframe
-
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
-        # dataFrame = {}
-        # data = []
-        # for a, b in self._finalPatterns.items():
-        #     data.append([a.replace('\t', ' '), b])
-        #     dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
-
-        dataFrame = _ab._pd.DataFrame(list([[x.replace("\t", " "), y] for x,y in self._finalPatterns.items()]), columns=['Patterns', 'Support'])
+        dataFrame = {}
+        data = []
+        for a, b in self._finalPatterns.items():
+            data.append([a, b])
+            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataFrame
 
-    def save(self, outFile: str, seperator = "\t" ) -> None:
+    def save(self, outFile):
         """
-
         Complete set of frequent patterns will be loaded in to an output file
-
         :param outFile: name of the output file
         :type outFile: csvfile
-        :return: None
         """
+        self._oFile = outFile
+        writer = open(self._oFile, 'w+')
+        for x, y in self._finalPatterns.items():
+            if type(x) == tuple:
+                pattern = ""
+                for item in x:
+                    pattern = pattern + str(item) + " "
+                s1 = pattern + ":" + str(y)
+            else:
+                s1 = str(x) + ":" + str(y)
+            writer.write("%s \n" % s1)
 
-        # self._oFile = outFile
-        # writer = open(self._oFile, 'w+')
-        # for x, y in self._finalPatterns.items():
-        #     patternsAndSupport = x.strip() + ":" + str(y[0])
-        #     writer.write("%s \n" % patternsAndSupport)
-        with open(outFile, 'w') as f:
-            for x, y in self._finalPatterns.items():
-                x = seperator.join(x)
-                f.write(f"{x}:{y}\n")
-
-    def getPatterns(self):
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
+    def startMine(self):
         """
+        Frequent pattern mining process will start from here
+        """
+        self.mine()
 
-        Function to send the set of frequent patterns after completion of the mining process
-
-        :return: returning frequent patterns
-        :rtype: dict
+    def mine(self):
         """
-        return self._finalPatterns
+        Frequent pattern mining process will start from here
+        """
+        startTime = time.time()
+        basePattern = {}
+        final = {}
+
+        self.__creatingItemSets()
+        self._minSup = self.__convert(self._minSup)
+        minSup = self._minSup
+        vb_data, idx2item = self.compute_vertical_bitvector_data()
+
+        for i in range(len(vb_data)):
+            if gpuarray.sum(vb_data[i]).get() >= self._minSup:
+                basePattern[idx2item[i]] = [i]
+                final[idx2item[i]] = gpuarray.sum(vb_data[i]).get()
+
+        while len(basePattern) > 0:
+            temp = {}
+            keysList = list(basePattern.keys())
+            valuesList = list(basePattern.values())
+            for i in range(len(basePattern) - 1):
+                keyI = keysList[i].split(" ")
+                keyI = [int(x) for x in keyI]
+
+                for j in range(i + 1, len(basePattern)):
+                    keyJ = keysList[j].split(" ")
+                    keyJ = [int(x) for x in keyJ]
+                    values = set(valuesList[i])
+                    for val in valuesList[j]:
+                        values.add(val)
+                    values = list(sorted(values))
+                    totalArray = vb_data[values[0]]
+                    for k in range(1, len(values)):
+                        totalArray = totalArray.__mul__(vb_data[values[k]])
+                    support = gpuarray.sum(totalArray).get()
+                    if support >= self._minSup:
+                        combinedKey = " ".join(
+                            str(x) for x in sorted(set(keyI) | set(keyJ)))
+                        temp[combinedKey] = values
+                        final[str(combinedKey)] = support
+            basePattern = temp
+
+        self.__time = time.time() - startTime
+        self.__memRSS = psutil.Process(os.getpid()).memory_info().rss
+        self.__memUSS = psutil.Process(os.getpid()).memory_full_info().uss
+        self._finalPatterns = final
+        self.__GPU_MEM = vb_data.nbytes
 
     def printResults(self):
         """
-        This function is used to print the result
+        This function is used to print the results
         """
-        print("Total number of Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of Coverage Patterns:", len(self.getPatterns()))
+        print("GPU MEM: ", _ap.getGPUMemory())
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:", self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
-            _ap = ECLATbitset(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = cudaAprioriGCT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
-            _ap = ECLATbitset(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap = cudaAprioriGCT(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
         _ap.mine()
         print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
+        print("GPU MEM: ", _ap.getGPUMemory())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
+
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `pami-2024.5.28.1/PAMI/frequentPattern/basic/FPGrowth.py` & `pami-2024.5.7.1/PAMI/frequentPattern/basic/FPGrowth.py`

 * *Files 1% similar despite different names*

```diff
@@ -474,37 +474,35 @@
 
         Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
-        # # dataframe = {}
-        # # data = []
-        # # for a, b in self.__finalPatterns.items():
-        # #     data.append([a.replace('\t', ' '), b])
-        # #     dataframe = _fp._pd.DataFrame(data, columns=['Patterns', 'Support'])
-        # dataFrame = _fp._pd.DataFrame(list(self._finalPatterns.items()), columns=['Patterns', 'Support'])
-        dataFrame = _fp._pd.DataFrame(list([[" ".join(x), y] for x,y in self._finalPatterns.items()]), columns=['Patterns', 'Support'])
-
+        # dataframe = {}
+        # data = []
+        # for a, b in self.__finalPatterns.items():
+        #     data.append([a.replace('\t', ' '), b])
+        #     dataframe = _fp._pd.DataFrame(data, columns=['Patterns', 'Support'])
+        dataFrame = _fp._pd.DataFrame(list(self._finalPatterns.items()), columns=['Patterns', 'Support'])
         return dataFrame
 
-    def save(self, outFile: str, seperator = "\t" ) -> None:
+    def save(self, outFile: str) -> None:
         """
 
         Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: csvfile
         :return: None
         """
         with open(outFile, 'w') as f:
             for x, y in self._finalPatterns.items():
-                x = seperator.join(x)
-                f.write(f"{x}:{y}\n")
+                x = self._sep.join(x)
+                f.write(f"{x} : {y}\n")
 
     def getPatterns(self) -> Dict[str, int]:
         """
 
         Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
```

### Comparing `pami-2024.5.28.1/PAMI/frequentPattern/basic/_Apriori.py` & `pami-2024.5.7.1/PAMI/frequentPattern/basic/_Apriori.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/frequentPattern/basic/_ECLATDiffset.py` & `pami-2024.5.7.1/PAMI/frequentPattern/cuda/cudaEclatGCT.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,26 +1,23 @@
-# ECLATDiffest uses diffset to extract the frequent patterns in a transactional database.
+# cudaEclatGCT is one of the fundamental algorithm to discover frequent patterns in a transactional database. It stores the database in compressed fp-tree decreasing the memory usage and extracts the patterns from tree.It  employs downward closure property to  reduce the search space effectively.
 #
 # **Importing this algorithm into a python program**
+# --------------------------------------------------------
 #
-#             import PAMI.frequentPattern.basic.ECLATDiffset as alg
+#             from PAMI.frequentPattern.cuda.cudaEclatGCT as alg
 #
-#             iFile = 'sampleDB.txt'
-#
-#             minSup = 10  # can also be specified between 0 and 1
-#
-#             obj = alg.ECLATDiffset(iFile, minSup)
+#             obj = alg.FPGrowth(iFile, minSup)
 #
 #             obj.mine()
 #
 #             frequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
-#             obj.savePatterns(oFile)
+#             obj.save(oFile)
 #
 #             Df = obj.getPatternInDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
 #             print("Total Memory in USS:", memUSS)
 #
@@ -30,14 +27,17 @@
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
+
+
+
 __copyright__ = """
 Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
@@ -47,78 +47,98 @@
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
+from deprecated import deprecated
+from PAMI.frequentPattern.basic import abstract as _ab
 
-# from abstract import *
+minSup = str()
+_ab._sys.setrecursionlimit(20000)
 
-from PAMI.frequentPattern.basic import abstract as _ab
-from deprecated import deprecated
+import os
+import csv
+import time
+import numpy as np
+import pycuda.gpuarray as _gpuarray
+import pycuda.autoinit
+import psutil
 
 
-class ECLATDiffset(_ab._frequentPatterns):
+class cudaEclatGCT:
     """
-    :**Description**:   ECLATDiffset uses diffset to extract the frequent patterns in a transactional database.
+    :Description: Apriori is one of the fundamental algorithm to discover frequent patterns in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
+
+    :Reference:  Agrawal, R., Imieli nski, T., Swami, A.: Mining association rules between sets of items in large databases.
+                In: SIGMOD. pp. 207216 (1993), https://doi.org/10.1145/170035.170072
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of frequent patterns
+    :param  oFile: str :
+                   Name of the output file to store complete set of frequent patterns
+    :param  minSup: int :
+                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+
+    :Attributes:
 
-    :**Reference**:  KDD '03: Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining
-                     August 2003 Pages 326335 https://doi.org/10.1145/956750.956788
-            
-    :**Parameters**:    - **iFile** (*str or URL or dataFrame*) -- *Name of the Input file to mine complete set of frequent patterns.*
-                        - **oFile** (*str*) -- *Name of the output file to store complete set of frequent patterns*
-                        - **minSup** (*int or float or str*) -- *The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.*
-                        - **sep** (*str*) -- **This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.**
-
-    :**Attributes**:    - **startTime** (*float*) -- *To record the start time of the mining process.*
-                        - **endTime** (*float*) -- *To record the end time of the mining process.*
-                        - **finalPatterns** (*dict*) -- *Storing the complete set of patterns in a dictionary variable.*
-                        - **memoryUSS** (*float*) -- *To store the total amount of USS memory consumed by the program.*
-                        - **memoryRSS** *(float*) -- *To store the total amount of RSS memory consumed by the program.*
-                        - **Database** (*list*) -- *To store the transactions of a database in list.*
-          
-        
-    Execution methods
-    =================
+        startTime : float
+            To record the start time of the mining process
 
-    **Terminal command**
+        endTime : float
+            To record the completion time of the mining process
+
+        finalPatterns : dict
+            Storing the complete set of patterns in a dictionary variable
+
+        memoryUSS : float
+            To store the total amount of USS memory consumed by the program
+
+        memoryRSS : float
+            To store the total amount of RSS memory consumed by the program
+
+        Database : list
+            To store the transactions of a database in list
+
+
+
+    **Methods to execute code on terminal**
+    ----------------------------------------------------
 
     .. code-block:: console
 
       Format:
 
-      (.venv) $ python3 ECLATDiffset.py <inputFile> <outputFile> <minSup>
+      (.venv) $ python3 cudaEclatGCT.py <inputFile> <outputFile> <minSup>
 
       Example Usage:
 
-      (.venv) $ python3 ECLATDiffset.py sampleDB.txt patterns.txt 10.0
+      (.venv) $ python3 cudaEclatGCT.py sampleDB.txt patterns.txt 10.0
 
-    .. note:: minSup can be specified  in support count or a value between 0 and 1.
+    .. note:: minSup will be considered in percentage of database transactions
 
-    
-     **Calling from a python program**
 
+    **Importing this algorithm into a python program**
+    ----------------------------------------------------
     .. code-block:: python
 
-            import PAMI.frequentPattern.basic.ECLATDiffset as alg
-
-            iFile = 'sampleDB.txt'
-
-            minSup = 10  # can also be specified between 0 and 1
+            import PAMI.frequentPattern.cuda.cuAprioriBit as alg
 
-            obj = alg.ECLATDiffset(iFile, minSup)
+            obj = alg.cuAprioriBit(iFile, minSup)
 
             obj.mine()
 
             frequentPatterns = obj.getPatterns()
 
             print("Total number of Frequent Patterns:", len(frequentPatterns))
 
-            obj.savePatterns(oFile)
+            obj.save(oFile)
 
             Df = obj.getPatternInDataFrame()
 
             memUSS = obj.getMemoryUSS()
 
             print("Total Memory in USS:", memUSS)
 
@@ -127,276 +147,254 @@
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
 
-    Credits:
-    ========
-
-    The complete program was written by Kundai and revised by Tarun Sreepada under the supervision of Professor Rage Uday Kiran.
-
+    **Credits:**
+    -------------
+             The complete program was written by Tarun Sreepada under the supervision of Professor Rage Uday Kiran.
     """
 
-    _minSup = float()
-    _startTime = float()
-    _endTime = float()
+    __time = 0
+    __memRSS = 0
+    __memUSS = 0
+    __GPU_MEM = 0
+    _minSup = 0
     _finalPatterns = {}
-    _iFile = " "
-    _oFile = " "
-    _sep = " "
-    _memoryUSS = float()
-    _memoryRSS = float()
-    _Database = []
-    _diffSets = {}
-    _trans_set = set()
 
-    def _creatingItemSets(self):
+    def __init__(self, filePath, minSup, sep):
+        self._iFile = filePath
+        self._sep = sep
+        self._minSup = minSup
+        self.__time = 0
+        self.__memRSS = 0
+        self.__memUSS = 0
+
+    """def read_data(self, data_path, sep):
+
+
+        data = []
+        if not os.path.isfile(data_path):
+            raise ValueError('Invalid data path.' + data_path)
+        with open(data_path, 'r') as f:
+            file = csv.reader(f, delimiter=sep, quotechar='\r')
+            lineNo = 1
+            for row in file:
+                data.append([str(item) for item in row if item != ''])
+                lineNo += 1
+        return data, lineNo"""
+
+    def __creatingItemSets(self):
         """
         Storing the complete transactions of the database/input file in a database variable
         """
-        self._Database = []
+        self.__Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
-                self._Database = self._iFile['Transactions'].tolist()
+                self.__Database = self._iFile['Transactions'].tolist()
+
+            # print(self.Database)
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    self._Database.append(temp)
+                    self.__Database.append(temp)
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
-                            line.strip()
+                            line = line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
-                            self._Database.append(temp)
+                            self.__Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
 
-    def _convert(self, value):
+    def __convert(self, value):
         """
 
-        To convert the user specified minSup value
+        To convert the type of user specified minSup value
 
         :param value: user specified minSup value
+
+        :type value: int or float or str
+
         :return: converted type
+
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
-            value = (len(self._Database) * value)
+            value = (len(self.__Database) * value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
-                value = (len(self._Database) * value)
+                value = (len(self.__Database) * value)
             else:
                 value = int(value)
         return value
 
-    def _getUniqueItemList(self):
-
-        # tidSets will store all the initial tids
-        tidSets = {}
-        # uniqueItem will store all frequent 1 items
-        uniqueItem = []
-        for line in self._Database:
-                transNum = 0
-                # Database = [set([i.rstrip() for i in transaction.split('\t')]) for transaction in f]
-                for transaction in self._Database:
-                    transNum += 1
-                    self._trans_set.add(transNum)
-                    for item in transaction:
-                        if item in tidSets:
-                            tidSets[item].add(transNum)
-                        else:
-                            tidSets[item] = {transNum}
-        for key, value in tidSets.items():
-            supp = len(value)
-            if supp >= self._minSup:
-                self._diffSets[key] = [supp, self._trans_set.difference(value)]
-                uniqueItem.append(key)
-        # for x, y in self._diffSets.items():
-        #     print(x, y)
-        uniqueItem.sort()
-        # print()
-        return uniqueItem
-
-    def _runDeclat(self, candidateList):
-        """
-
-        It will generate the combinations of frequent items
-
-        :param candidateList :it represents the items with their respective transaction identifiers
-        :type candidateList: list
-        :return: returning transaction dictionary
-        :rtype: dict
-        """
-
-        newList = []
-        for i in range(0, len(candidateList)):
-            item1 = candidateList[i]
-            iList = item1.split()
-            for j in range(i + 1, len(candidateList)):
-                item2 = candidateList[j]
-                jList = item2.split()
-                if iList[:-1] == jList[:-1]:
-                    unionDiffSet = self._diffSets[item2][1].difference(self._diffSets[item1][1])
-                    unionSup = self._diffSets[item1][0] - len(unionDiffSet)
-                    if unionSup >= self._minSup:
-                        newKey = item1 + "\t" + jList[-1]
-                        self._diffSets[newKey] = [unionSup, unionDiffSet]
-                        newList.append(newKey)
-                    else: 
-                        break
-
-        if len(newList) > 0:
-            self._runDeclat(newList)
-
-    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
-    def startMine(self):
-        """
-        Frequent pattern mining process will start from here
-        """
-        self.mine()
-
-    def mine(self):
+    def compute_vertical_bitvector_data(self):
         """
-        Frequent pattern mining process will start from here
+        Converting  database into bit vector
         """
+        # ---build item to idx mapping---#
+        idx = 0
+        item2idx = {}
+        for transaction in self.__Database:
+            for item in transaction:
+                if not item in item2idx:
+                    item2idx[item] = idx
+                    idx += 1
+        idx2item = {idx: str(int(item)) for item, idx in item2idx.items()}
+        # ---build vertical data---#
+        vb_data = np.zeros((len(item2idx), len(self.__Database)), dtype=np.uint16)
+        for trans_id, transaction in enumerate(self.__Database):
+            for item in transaction:
+                vb_data[item2idx[item], trans_id] = 1
+        vb_data = _gpuarray.to_gpu(vb_data.astype(np.uint16))
+        return vb_data, idx2item
 
-        self._startTime = _ab._time.time()
-        self._Database = []
-        self._finalPatterns = {}
-        self._diffSets = {}
-        self._trans_set = set()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        if self._minSup is None:
-            raise Exception("Please enter the Minimum Support")
-        self._creatingItemSets()
-        #print(len(self._Database))
-        self._minSup = self._convert(self._minSup)
-        uniqueItemList = []
-        uniqueItemList = self._getUniqueItemList()
-        self._runDeclat(uniqueItemList)
-        self._finalPatterns = self._diffSets
-        #print(len(self._finalPatterns), len(uniqueItemList))
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("Frequent patterns were generated successfully using ECLAT Diffset algorithm")
-
-    def getMemoryUSS(self):
+    def getRuntime(self):
         """
-
-        Total amount of USS memory consumed by the mining process will be retrieved from this function
-
-        :return: returning USS memory consumed by the mining process
+        Calculating the total amount of time taken by the mining process
+        :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
-
-        return self._memoryUSS
+        return self.__time
 
     def getMemoryRSS(self):
         """
-
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
-
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
+        return self.__memRSS
 
-        return self._memoryRSS
-
-    def getRuntime(self):
+    def getMemoryUSS(self):
         """
-
-        Calculating the total amount of runtime taken by the mining process
-
-        :return: returning total amount of runtime taken by the mining process
+        Total amount of USS memory consumed by the mining process will be retrieved from this function
+        :return: returning USS memory consumed by the mining process
         :rtype: float
         """
+        return self.__memUSS
 
-        return self._endTime - self._startTime
-
-    def getPatternsAsDataFrame(self):
+    def getGPUMemory(self):
         """
-
-        Storing final frequent patterns in a dataframe
-
-        :return: returning frequent patterns in a dataframe
-        :rtype: pd.DataFrame
+        To calculate the total memory consumed by GPU
+        :return: return GPU memory
+        :rtype: int
         """
 
-        dataFrame = {}
-        data = []
-        for a, b in self._finalPatterns.items():
-            data.append([a.replace('\t', ' '), b[0]])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
-        return dataFrame
+        return self.__GPU_MEM
 
-    def save(self, outFile):
+    def getPatterns(self):
         """
+        Function to send the set of frequent patterns after completion of the mining process
+        :return: returning frequent patterns
+        :rtype: dict
+        """
+        return self._finalPatterns
 
-        Complete set of frequent patterns will be loaded in to an output file
+    def get_numberOfPatterns(self):
 
-        :param outFile: name of the output file
-        :type outFile: csvfile
-        """
-        self._oFile = outFile
-        writer = open(self._oFile, 'w+')
-        for x, y in self._finalPatterns.items():
-            patternsAndSupport = x.strip() + ":" + str(y[0])
-            writer.write("%s \n" % patternsAndSupport)
+        return len(self._finalPatterns)
 
-    def getPatterns(self):
+    def eclat(self, basePattern, final, vb_data, idx2item, item2idx):
         """
+        param basePattern: base pattern used for the mining process after completion of the mining process
+        type basePattern:
+        param final: final pattern used for the mining process after completion of the mining process
+        type final:
+        param vb_data: vb_data used for the mining process after completion of the mining process
+        type vb_data:
+        param idx2item: idx2item used for the mining process after completion of the mining process
+        type idx2item:
+        param item2idx: item2idx used for the mining process after completion of the mining process
+        type item2idx:
+        """
+        newBasePattern = []
+        for i in range(0, len(basePattern)):
+            item1 = basePattern[i]
+            i1_list = item1.split()
+            for j in range(i + 1, len(basePattern)):
+                item2 = basePattern[j]
+                i2_list = item2.split()
+                if i1_list[:-1] == i2_list[:-1]:
+                    unionOfKey = list(set(i1_list) | set(i2_list))
+                    unionOfKey.sort()
+                    valueList = []
+                    for key in unionOfKey:
+                        valueList.append(item2idx[key])
+                    total = vb_data[valueList[0]]
+                    for k in range(1, len(valueList)):
+                        total = total.__mul__(vb_data[valueList[k]])
+                    support = _gpuarray.sum(total).get()
+                    if support >= self._minSup:
+                        newBasePattern.append(" ".join(unionOfKey))
+                        final[" ".join(unionOfKey)] = support
 
-        Function to send the set of frequent patterns after completion of the mining process
+        if len(newBasePattern) > 0:
+            self.eclat(newBasePattern, final, vb_data, idx2item, item2idx)
 
-        :return: returning frequent patterns
-        :rtype: dict
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
+    def startMine(self):
         """
-        return self._finalPatterns
+        Frequent pattern mining process will start from here
+        """
+        self.mine()
 
-    def printResults(self):
+    def mine(self):
         """
-        This function is used to print the results
+        Frequent pattern mining process will start from here
         """
-        print("Total number of Frequent Patterns:", len(self.getPatterns()))
-        print("Total Memory in USS:", self.getMemoryUSS())
-        print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:",  self.getRuntime())
+        startTime = time.time()
+        basePattern = []
+        final = {}
+
+        self.__creatingItemSets()
+        self._minSup = self.__convert(self._minSup)
+        minSup = self._minSup
+        vb_data, idx2item = self.compute_vertical_bitvector_data()
+
+        for i in range(len(vb_data)):
+            if _gpuarray.sum(vb_data[i]).get() >= self._minSup:
+                basePattern.append(idx2item[i])
+                final[idx2item[i]] = _gpuarray.sum(vb_data[i]).get()
+
+        # reverse idx2item
+        item2idx = {idx2item[i]: i for i in idx2item}
+        self.eclat(basePattern, final, vb_data, idx2item, item2idx)
+        self.__time = time.time() - startTime
+        self.__memRSS = psutil.Process(os.getpid()).memory_info().rss
+        self.__memUSS = psutil.Process(os.getpid()).memory_full_info().uss
+        self._finalPatterns = final
+        self.__GPU_MEM = vb_data.nbytes
 
 
 if __name__ == "__main__":
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
-            _ap = ECLATDiffset(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = cudaEclatGCT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
-            _ap = ECLATDiffset(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap = cudaEclatGCT(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
         _ap.mine()
         print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
-        _ap.save(_ab._sys.argv[2])
-        print(_ap.getPatternsAsDataFrame())
+        _ap.save(_ap._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
+        print("GPU MEM: ", _ap.getGPUMemory())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
-
```

### Comparing `pami-2024.5.28.1/PAMI/frequentPattern/basic/_FPGrowth.py` & `pami-2024.5.7.1/PAMI/frequentPattern/basic/_FPGrowth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/frequentPattern/basic/abstract.py` & `pami-2024.5.7.1/PAMI/frequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/frequentPattern/closed/CHARM.py` & `pami-2024.5.7.1/PAMI/frequentPattern/closed/CHARM.py`

 * *Files 2% similar despite different names*

```diff
@@ -466,22 +466,19 @@
 
         Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
-        # dataframe = {}
-        # data = []
-        # for a, b in self._finalPatterns.items():
-        #     data.append([a.replace('\t', ' '), b])
-        #     dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
-
-        dataframe = _ab._pd.DataFrame(list([[x.replace('\t', ' '), y] for x,y in self._finalPatterns.items()]), columns=['Patterns', 'Support'])
-
+        dataframe = {}
+        data = []
+        for a, b in self._finalPatterns.items():
+            data.append([a.replace('\t', ' '), b])
+            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataframe
 
     def save(self, outFile):
         """
 
         Complete set of frequent patterns will be loaded in to an output file
 
@@ -528,12 +525,7 @@
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         _memRSS = _ap.getMemoryRSS()
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
-
-    obj = CHARM("/Users/tarunsreepada/Downloads/Transactional_T10I4D100K.csv", 0.01)
-    obj.mine()
-    print(obj.getPatternsAsDataFrame())
-    print(obj.printResults())
```

### Comparing `pami-2024.5.28.1/PAMI/frequentPattern/closed/abstract.py` & `pami-2024.5.7.1/PAMI/frequentPattern/closed/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/frequentPattern/cuda/abstract.py` & `pami-2024.5.7.1/PAMI/frequentPattern/cuda/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/frequentPattern/cuda/cuApriori.py` & `pami-2024.5.7.1/PAMI/frequentPattern/cuda/cuApriori.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/frequentPattern/cuda/cuAprioriBit.py` & `pami-2024.5.7.1/PAMI/frequentPattern/cuda/cuAprioriBit.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/frequentPattern/cuda/cuEclat.py` & `pami-2024.5.7.1/PAMI/frequentPattern/cuda/cuEclat.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/frequentPattern/cuda/cuEclatBit.py` & `pami-2024.5.7.1/PAMI/frequentPattern/cuda/cuEclatBit.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/frequentPattern/cuda/cudaAprioriGCT.py` & `pami-2024.5.7.1/PAMI/frequentPattern/pyspark/parallelECLAT.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,16 +1,16 @@
-# cudaAprioriGCT is one of the fundamental algorithm to discover frequent patterns using CUDA in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
-#
+# ParallelEclat is an algorithm to discover frequent patterns in a transactional database. This program employs parallel apriori property (or downward closure property) to  reduce the search space effectively.
 #
 # **Importing this algorithm into a python program**
-# ----------------------------------------------------
+#  ----------------------------------------------------
+#
 #
-#             import PAMI.frequentPattern.cuda.cudaAprioriGCT as alg
+#             import PAMI.frequentPattern.pyspark.parallelECLAT as alg
 #
-#             obj = alg.cuAprioriGCT(iFile, minSup)
+#             obj = alg.parallelECLAT(iFile, minSup, numWorkers)
 #
 #             obj.mine()
 #
 #             frequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
@@ -30,14 +30,15 @@
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
 
 
+
 __copyright__ = """
 Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
@@ -47,87 +48,85 @@
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
-from deprecated import deprecated
-from PAMI.frequentPattern.basic import abstract as _ab
+from pyspark import SparkConf, SparkContext
 # import abstract as _ab
-
-import os
-import time
-import numpy as np
-import pycuda.gpuarray as gpuarray
-import psutil
+from PAMI.frequentPattern.pyspark import abstract as _ab
+from abc import ABC as _ABC, abstractmethod as _abstractmethod
+from deprecated import deprecated
 
 
-class cudaAprioriGCT(_ab._frequentPatterns):
+class parallelECLAT(_ab._frequentPatterns):
     """
-    :Description: cudaAprioriGCT is one of the fundamental algorithm to discover frequent patterns in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
+    :Description: ParallelEclat is an algorithm to discover frequent patterns in a transactional database.
+     This program employs parallel apriori property (or downward closure property) to  reduce the search space effectively.
 
-    :Reference:  Agrawal, R., Imieli nski, T., Swami, A.: Mining association rules between sets of items in large databases.
-                In: SIGMOD. pp. 207216 (1993), https://doi.org/10.1145/170035.170072
+    :Reference:
 
     :param  iFile: str :
                    Name of the Input file to mine complete set of frequent patterns
     :param  oFile: str :
                    Name of the output file to store complete set of frequent patterns
     :param  minSup: int :
-                    The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
+                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+    :param  numPartitions: int :
+                   The number of partitions. On each worker node, an executor process is started and this process performs processing.The processing unit of worker node is partition
+
 
     :Attributes:
 
         startTime : float
-              To record the start time of the mining process
+            To record the start time of the mining process
 
         endTime : float
-              To record the completion time of the mining process
+            To record the completion time of the mining process
 
         finalPatterns : dict
-              Storing the complete set of patterns in a dictionary variable
+            Storing the complete set of patterns in a dictionary variable
 
         memoryUSS : float
-              To store the total amount of USS memory consumed by the program
+            To store the total amount of USS memory consumed by the program
 
         memoryRSS : float
-              To store the total amount of RSS memory consumed by the program
-
-        Database : list
-              To store the transactions of a database in list
+            To store the total amount of RSS memory consumed by the program
 
+        lno : int
+            the number of transactions
 
 
     **Methods to execute code on terminal**
     ----------------------------------------------------
 
     .. code-block:: console
 
       Format:
 
-      (.venv) $ python3 cudaAprioriGCT.py <inputFile> <outputFile> <minSup>
+      (.venv) $ python3 parallelECLAT.py <inputFile> <outputFile> <minSup> <numWorkers>
 
       Example Usage:
 
-      (.venv) $ python3 cudaAprioriGCT.py sampleDB.txt patterns.txt 10.0
+      (.venv) $ python3 parallelECLAT.py sampleDB.txt patterns.txt 10.0 3
 
     .. note:: minSup will be considered in percentage of database transactions
 
 
+
     **Importing this algorithm into a python program**
     ----------------------------------------------------
-
     .. code-block:: python
 
-            import PAMI.frequentPattern.cuda.cuAprioriGCT as alg
+            import PAMI.frequentPattern.pyspark.parallelECLAT as alg
 
-            obj = alg.cuAprioriGCT(iFile, minSup)
+            obj = alg.parallelECLAT(iFile, minSup, numWorkers)
 
             obj.mine()
 
             frequentPatterns = obj.getPatterns()
 
             print("Total number of Frequent Patterns:", len(frequentPatterns))
 
@@ -145,158 +144,60 @@
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
 
     **Credits:**
-    -------------
-
-                The complete program was written by Tarun Sreepada under the supervision of Professor Rage Uday Kiran.
+    ----------------------------------------------------
+             The complete program was written by Yudai Masu under the supervision of Professor Rage Uday Kiran.
 
     """
 
-    __time = 0
-    __memRSS = 0
-    __memUSS = 0
-    __GPU_MEM = 0
-    _minSup = 0
+    _minSup = float()
+    _numPartitions = int()
+    _startTime = float()
+    _endTime = float()
     _finalPatterns = {}
+    _iFile = " "
+    _oFile = " "
+    _sep = " "
+    _memoryUSS = float()
+    _memoryRSS = float()
+    _lno = int()
 
-    def __init__(self, filePath, minSup, sep):
-        self._iFile = filePath
-        self._sep = sep
-        self._minSup = minSup
-        self.__time = 0
-        self.__memRSS = 0
-        self.__memUSS = 0
-
-    def __creatingItemSets(self):
-        """
-        Storing the complete transactions of the database/input file in a database variable
-        """
-        self.__Database = []
-        if isinstance(self._iFile, _ab._pd.DataFrame):
-            if self._iFile.empty:
-                print("its empty..")
-            i = self._iFile.columns.values.tolist()
-            if 'Transactions' in i:
-                self.__Database = self._iFile['Transactions'].tolist()
-
-            # print(self.Database)
-        if isinstance(self._iFile, str):
-            if _ab._validators.url(self._iFile):
-                data = _ab._urlopen(self._iFile)
-                for line in data:
-                    line.strip()
-                    line = line.decode("utf-8")
-                    temp = [i.rstrip() for i in line.split(self._sep)]
-                    temp = [x for x in temp if x]
-                    self.__Database.append(temp)
-            else:
-                try:
-                    with open(self._iFile, 'r', encoding='utf-8') as f:
-                        for line in f:
-                            line = line.strip()
-                            temp = [i.rstrip() for i in line.split(self._sep)]
-                            temp = [x for x in temp if x]
-                            self.__Database.append(temp)
-                except IOError:
-                    print("File Not Found")
-                    quit()
-
-    def __convert(self, value):
-        """
-
-        To convert the type of user specified minSup value
-
-        :param value: user specified minSup value
-
-        :type value: int or float or str
-
-        :return: converted type
-
-        """
-        if type(value) is int:
-            value = int(value)
-        if type(value) is float:
-            value = (len(self.__Database) * value)
-        if type(value) is str:
-            if '.' in value:
-                value = float(value)
-                value = (len(self.__Database) * value)
-            else:
-                value = int(value)
-        return value
-
-    def compute_vertical_bitvector_data(self):
-        """
-        Converting database into bit vector
-        """
-        # ---build item to idx mapping---#
-        idx = 0
-        item2idx = {}
-        for transaction in self.__Database:
-            for item in transaction:
-                if not item in item2idx:
-                    item2idx[item] = idx
-                    idx += 1
-        idx2item = {idx: str(int(item)) for item, idx in item2idx.items()}
-        # ---build vertical data---#
-        vb_data = np.zeros((len(item2idx), len(self.__Database)), dtype=np.uint16)
-        for trans_id, transaction in enumerate(self.__Database):
-            for item in transaction:
-                vb_data[item2idx[item], trans_id] = 1
-        vb_data = gpuarray.to_gpu(vb_data.astype(np.uint16))
-        return vb_data, idx2item
+    def __init__(self, iFile, minSup, numWorkers, sep="\t"):
+        super().__init__(iFile, minSup, int(numWorkers), sep)
 
-    def getRuntime(self):
+    def getMemoryUSS(self):
         """
-        Calculating the total amount of time taken by the mining process
-        :return: returning total amount of runtime taken by the mining process
+        Total amount of USS memory consumed by the mining process will be retrieved from this function
+        :return: returning USS memory consumed by the mining process
         :rtype: float
         """
-        return self.__time
+
+        return self._memoryUSS
 
     def getMemoryRSS(self):
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
-        return self.__memRSS
+        return self._memoryRSS
 
-    def getMemoryUSS(self):
+    def getRuntime(self):
         """
-        Total amount of USS memory consumed by the mining process will be retrieved from this function
-        :return: returning USS memory consumed by the mining process
+        Calculating the total amount of runtime taken by the mining process
+        :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
-        return self.__memUSS
-
-    def getGPUMemory(self):
-        """
-        To calculate the total memory consumed by GPU
-        :return: return GPU memory
-        :rtype: int
-        """
 
-        return self.__GPU_MEM
-
-    def getPatterns(self):
-        """
-        Function to send the set of frequent patterns after completion of the mining process
-        :return: returning frequent patterns
-        :rtype: dict
-        """
-        return self._finalPatterns
-
-    def get_numberOfPatterns(self):
-        return len(self._finalPatterns)
+        return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self):
         """
         Storing final frequent patterns in a dataframe
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
@@ -313,102 +214,162 @@
         Complete set of frequent patterns will be loaded in to an output file
         :param outFile: name of the output file
         :type outFile: csvfile
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            if type(x) == tuple:
-                pattern = ""
-                for item in x:
-                    pattern = pattern + str(item) + " "
-                s1 = pattern + ":" + str(y)
-            else:
-                s1 = str(x) + ":" + str(y)
+            s1 = x + ":" + str(y)
             writer.write("%s \n" % s1)
+            
+    def printResults(self):
+        """
+        This method prints all the stats
+        """
+        print("Total number of Frequent Patterns:", len(self.getPatterns()))
+        print("Total Memory in USS:", self.getMemoryUSS())
+        print("Total Memory in RSS", self.getMemoryRSS())
+        print("Total ExecutionTime in ms:", self.getRuntime())
 
-    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
-    def startMine(self):
+    def getPatterns(self):
         """
-        Frequent pattern mining process will start from here
+        Function to send the set of frequent patterns after completion of the mining process
+        :return: returning frequent patterns
+        :rtype: dict
         """
-        self.mine()
+        return self._finalPatterns
 
-    def mine(self):
+    def _genPatterns(self, suffix, pattern, data):
         """
-        Frequent pattern mining process will start from here
+        This function is used to generate patterns
+        :param suffix: the suffix of the generated ptterns
+
+        :type suffix: str
+
+        :param pattern: the pattern of the generated ptterns
+
+        :type pattern: str
+
+        :param data: the data of the generated ptterns after completion of the mining process
+
+        :type data: str
         """
-        startTime = time.time()
-        basePattern = {}
-        final = {}
-
-        self.__creatingItemSets()
-        self._minSup = self.__convert(self._minSup)
-        minSup = self._minSup
-        vb_data, idx2item = self.compute_vertical_bitvector_data()
-
-        for i in range(len(vb_data)):
-            if gpuarray.sum(vb_data[i]).get() >= self._minSup:
-                basePattern[idx2item[i]] = [i]
-                final[idx2item[i]] = gpuarray.sum(vb_data[i]).get()
-
-        while len(basePattern) > 0:
-            temp = {}
-            keysList = list(basePattern.keys())
-            valuesList = list(basePattern.values())
-            for i in range(len(basePattern) - 1):
-                keyI = keysList[i].split(" ")
-                keyI = [int(x) for x in keyI]
-
-                for j in range(i + 1, len(basePattern)):
-                    keyJ = keysList[j].split(" ")
-                    keyJ = [int(x) for x in keyJ]
-                    values = set(valuesList[i])
-                    for val in valuesList[j]:
-                        values.add(val)
-                    values = list(sorted(values))
-                    totalArray = vb_data[values[0]]
-                    for k in range(1, len(values)):
-                        totalArray = totalArray.__mul__(vb_data[values[k]])
-                    support = gpuarray.sum(totalArray).get()
-                    if support >= self._minSup:
-                        combinedKey = " ".join(
-                            str(x) for x in sorted(set(keyI) | set(keyJ)))
-                        temp[combinedKey] = values
-                        final[str(combinedKey)] = support
-            basePattern = temp
-
-        self.__time = time.time() - startTime
-        self.__memRSS = psutil.Process(os.getpid()).memory_info().rss
-        self.__memUSS = psutil.Process(os.getpid()).memory_full_info().uss
-        self._finalPatterns = final
-        self.__GPU_MEM = vb_data.nbytes
+        freqPatterns = {}
+        index = data.index(suffix)
+        for i in range(index + 1, len(data)):
+            tid = pattern[1].intersection(data[i][1])
+            if len(tid) >= self._minSup:
+                freqPattern = pattern[0] + ' ' + data[i][0]
+                freqPatterns[freqPattern] = len(tid)
+                freqPatterns.update(self._genPatterns(data[i], (freqPattern, tid), data))
+        return freqPatterns
 
     def printResults(self):
         """
         This function is used to print the results
         """
-        print("Total number of Coverage Patterns:", len(self.getPatterns()))
-        print("GPU MEM: ", _ap.getGPUMemory())
+        print("Total number of Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:", self.getRuntime())
 
+    def _convert(self, value):
+        """
+        To convert the user specified minSup value
+        :param value: user specified minSup value
+        :type value: int or float or str
+        :return: converted type
+        """
+        print(value, type(value))
+        if type(value) is int:
+            value = int(value)
+        elif type(value) is float:
+            value = (self._lno * value)
+        elif type(value) is str:
+            if '.' in value:
+                value = float(value)
+                value = (self._lno * value)
+            else:
+                value = int(value)
+        else:
+            print("None")
+        print(type(value), value)
+        return value
+
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
+    def startMine(self):
+        """
+        Frequent pattern mining process will start from here
+        """
+        self.mine()
+
+    def mine(self):
+        """
+        Frequent pattern mining process will start from here
+        """
+
+        self._startTime = _ab._time.time()
+        conf = SparkConf().setAppName("Parallel ECLAT").setMaster("local[*]")
+        sc = SparkContext(conf=conf)
+
+        data = sc.textFile(self._iFile, self._numPartitions) \
+            .map(lambda line: [int(y) for y in line.rstrip().split(self._sep)]).persist()
+        self._lno = data.count()
+        self._minSup = self._convert(self._minSup)
+
+        frequentItems = None
+        frequentItems = data.zipWithIndex() \
+            .flatMap(lambda x: [(str(item), x[1]) for item in x[0]]) \
+            .groupByKey() \
+            .filter(lambda x: len(x[1]) >= self._minSup) \
+            .sortBy(lambda x: len(x[1])) \
+            .mapValues(set) \
+            .persist()
+        data.unpersist()
+        # elif 'temporal' in self._iFile:
+        #     frequentItems = data.flatMap(lambda trans: [(str(item), trans[0]) for item in trans[1:]]) \
+        #         .groupByKey() \
+        #         .filter(lambda x: len(x[1]) >= self._minSup) \
+        #         .mapValues(set) \
+        #         .persist()
+        #     data.unpersist()
+        # else:
+        #     pass
+        #     # print("may be not able to process the input file")
+
+        freqItems = dict(frequentItems.collect())
+        # print(len(freqItems))
+        self._finalPatterns = {k: len(v) for k, v in freqItems.items()}
+
+        freqPatterns = list(frequentItems.map(lambda x: self._genPatterns(x, x, list(freqItems.items())))
+                            .filter(lambda x: len(x) != 0).collect())
+        for value in freqPatterns:
+            self._finalPatterns.update(value)
+
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Frequent patterns were generated successfully using Parallel ECLAT algorithm")
+        sc.stop()
+
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
+    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
+        if len(_ab._sys.argv) == 6:
+            _ap = parallelECLAT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
         if len(_ab._sys.argv) == 5:
-            _ap = cudaAprioriGCT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
-        if len(_ab._sys.argv) == 4:
-            _ap = cudaAprioriGCT(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap = parallelECLAT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         _ap.startMine()
         _ap.mine()
-        print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
+        _finalPatterns = _ap.getPatterns()
+        print("Total number of Frequent Patterns:", len(_finalPatterns))
         _ap.save(_ab._sys.argv[2])
-        print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("GPU MEM: ", _ap.getGPUMemory())
-        print("Total ExecutionTime in ms:", _ap.getRuntime())
+        _memUSS = _ap.getMemoryUSS()
+        print("Total Memory in USS:", _memUSS)
+        _memRSS = _ap.getMemoryRSS()
+        print("Total Memory in RSS", _memRSS)
+        _run = _ap.getRuntime()
+        print("Total ExecutionTime in ms:", _run)
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
-
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `pami-2024.5.28.1/PAMI/frequentPattern/cuda/cudaAprioriTID.py` & `pami-2024.5.7.1/PAMI/frequentPattern/cuda/cudaAprioriTID.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/frequentPattern/cuda/cudaEclatGCT.py` & `pami-2024.5.7.1/PAMI/frequentPattern/pyspark/parallelApriori.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,15 +1,16 @@
-# cudaEclatGCT is one of the fundamental algorithm to discover frequent patterns in a transactional database. It stores the database in compressed fp-tree decreasing the memory usage and extracts the patterns from tree.It  employs downward closure property to  reduce the search space effectively.
+# Parallel Apriori is an algorithm to discover frequent patterns in a transactional database. This program employs parallel apriori property (or downward closure property) to  reduce the search space effectively.
 #
-# **Importing this algorithm into a python program**
-# --------------------------------------------------------
 #
-#             from PAMI.frequentPattern.cuda.cudaEclatGCT as alg
+#  **Importing this algorithm into a python program**
+#  ---------------------------------------------------
 #
-#             obj = alg.FPGrowth(iFile, minSup)
+#             import PAMI.frequentPattern.pyspark.parallelApriori as alg
+#
+#             obj = alg.parallelApriori(iFile, minSup, numWorkers)
 #
 #             obj.mine()
 #
 #             frequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
@@ -47,354 +48,373 @@
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
+from PAMI.frequentPattern.pyspark import abstract as _ab
 from deprecated import deprecated
-from PAMI.frequentPattern.basic import abstract as _ab
-
-minSup = str()
-_ab._sys.setrecursionlimit(20000)
-
-import os
-import csv
-import time
-import numpy as np
-import pycuda.gpuarray as _gpuarray
-import pycuda.autoinit
-import psutil
 
 
-class cudaEclatGCT:
+class parallelApriori(_ab._frequentPatterns):
     """
-    :Description: Apriori is one of the fundamental algorithm to discover frequent patterns in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
 
-    :Reference:  Agrawal, R., Imieli nski, T., Swami, A.: Mining association rules between sets of items in large databases.
-                In: SIGMOD. pp. 207216 (1993), https://doi.org/10.1145/170035.170072
+    :Description: Parallel Apriori is an algorithm to discover frequent patterns in a transactional database. This program employs parallel apriori property (or downward closure property) to  reduce the search space effectively.
+
+    :Reference: N. Li, L. Zeng, Q. He and Z. Shi, "Parallel Implementation of Apriori Algorithm Based on MapReduce,"
+                2012 13th ACIS International Conference on Software Engineering, Artificial Intelligence,
+                Networking and Parallel/Distributed Computing, Kyoto, Japan, 2012, pp. 236-241, doi: 10.1109/SNPD.2012.31.
 
     :param  iFile: str :
                    Name of the Input file to mine complete set of frequent patterns
     :param  oFile: str :
                    Name of the output file to store complete set of frequent patterns
     :param  minSup: int :
                    The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+    :param  numPartitions: int :
+                   The number of partitions. On each worker node, an executor process is started and this process performs processing.The processing unit of worker node is partition
+
+
 
     :Attributes:
 
         startTime : float
-            To record the start time of the mining process
+          To record the start time of the mining process
 
         endTime : float
-            To record the completion time of the mining process
+          To record the completion time of the mining process
 
         finalPatterns : dict
-            Storing the complete set of patterns in a dictionary variable
+          Storing the complete set of patterns in a dictionary variable
 
         memoryUSS : float
-            To store the total amount of USS memory consumed by the program
+          To store the total amount of USS memory consumed by the program
 
         memoryRSS : float
-            To store the total amount of RSS memory consumed by the program
-
-        Database : list
-            To store the transactions of a database in list
+          To store the total amount of RSS memory consumed by the program
 
+        lno : int
+                the number of transactions
 
+    
 
     **Methods to execute code on terminal**
-    ----------------------------------------------------
+    -------------------------------------------
 
     .. code-block:: console
 
       Format:
 
-      (.venv) $ python3 cudaEclatGCT.py <inputFile> <outputFile> <minSup>
+      (.venv) $ python3 parallelApriori.py <inputFile> <outputFile> <minSup> <numWorkers>
 
       Example Usage:
 
-      (.venv) $ python3 cudaEclatGCT.py sampleDB.txt patterns.txt 10.0
+      (.venv) $ python3 parallelApriori.py sampleDB.txt patterns.txt 10.0 3
 
     .. note:: minSup will be considered in percentage of database transactions
 
-
+    
+    
     **Importing this algorithm into a python program**
-    ----------------------------------------------------
+    ----------------------------------------------------------------------------------
     .. code-block:: python
-
-            import PAMI.frequentPattern.cuda.cuAprioriBit as alg
-
-            obj = alg.cuAprioriBit(iFile, minSup)
-
-            obj.mine()
-
-            frequentPatterns = obj.getPatterns()
-
-            print("Total number of Frequent Patterns:", len(frequentPatterns))
-
-            obj.save(oFile)
-
-            Df = obj.getPatternInDataFrame()
-
-            memUSS = obj.getMemoryUSS()
-
-            print("Total Memory in USS:", memUSS)
-
-            memRSS = obj.getMemoryRSS()
-
-            print("Total Memory in RSS", memRSS)
-
-            run = obj.getRuntime()
-
-            print("Total ExecutionTime in seconds:", run)
+    
+                import PAMI.frequentPattern.pyspark.parallelApriori as alg
+    
+                obj = alg.parallelApriori(iFile, minSup, numWorkers)
+    
+                obj.mine()
+    
+                frequentPatterns = obj.getPatterns()
+    
+                print("Total number of Frequent Patterns:", len(frequentPatterns))
+    
+                obj.save(oFile)
+    
+                Df = obj.getPatternInDataFrame()
+    
+                memUSS = obj.getMemoryUSS()
+    
+                print("Total Memory in USS:", memUSS)
+    
+                memRSS = obj.getMemoryRSS()
+    
+                print("Total Memory in RSS", memRSS)
+    
+                run = obj.getRuntime()
+    
+                print("Total ExecutionTime in seconds:", run)
+    
+    
+    **Credits:**
+    -----------------------------------------
+            The complete program was written by Yudai Masu  under the supervision of Professor Rage Uday Kiran.
 
 
-    **Credits:**
-    -------------
-             The complete program was written by Tarun Sreepada under the supervision of Professor Rage Uday Kiran.
     """
 
-    __time = 0
-    __memRSS = 0
-    __memUSS = 0
-    __GPU_MEM = 0
-    _minSup = 0
+    _minSup = float()
+    _startTime = float()
+    _endTime = float()
     _finalPatterns = {}
+    _iFile = " "
+    _oFile = " "
+    _sep = " "
+    _memoryUSS = float()
+    _memoryRSS = float()
+    _numPartitions = int()
+    _lno = int()
 
-    def __init__(self, filePath, minSup, sep):
-        self._iFile = filePath
-        self._sep = sep
-        self._minSup = minSup
-        self.__time = 0
-        self.__memRSS = 0
-        self.__memUSS = 0
-
-    """def read_data(self, data_path, sep):
-
+    def __init__(self, iFile, minSup, numWorkers, sep='\t'):
+        super().__init__(iFile, minSup, int(numWorkers), sep)
 
-        data = []
-        if not os.path.isfile(data_path):
-            raise ValueError('Invalid data path.' + data_path)
-        with open(data_path, 'r') as f:
-            file = csv.reader(f, delimiter=sep, quotechar='\r')
-            lineNo = 1
-            for row in file:
-                data.append([str(item) for item in row if item != ''])
-                lineNo += 1
-        return data, lineNo"""
-
-    def __creatingItemSets(self):
+    def _creatingItemSets(self):
         """
         Storing the complete transactions of the database/input file in a database variable
         """
-        self.__Database = []
+        self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
-                self.__Database = self._iFile['Transactions'].tolist()
-
-            # print(self.Database)
+                self._Database = self._iFile['Transactions'].tolist()
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    self.__Database.append(temp)
+                    self._Database.append(temp)
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
-                            line = line.strip()
+                            line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
-                            self.__Database.append(temp)
+                            #print(line)
+                            self._Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
-
-    def __convert(self, value):
-        """
-
-        To convert the type of user specified minSup value
-
-        :param value: user specified minSup value
-
-        :type value: int or float or str
-
-        :return: converted type
-
-        """
-        if type(value) is int:
-            value = int(value)
-        if type(value) is float:
-            value = (len(self.__Database) * value)
-        if type(value) is str:
-            if '.' in value:
-                value = float(value)
-                value = (len(self.__Database) * value)
-            else:
-                value = int(value)
-        return value
-
-    def compute_vertical_bitvector_data(self):
-        """
-        Converting  database into bit vector
-        """
-        # ---build item to idx mapping---#
-        idx = 0
-        item2idx = {}
-        for transaction in self.__Database:
-            for item in transaction:
-                if not item in item2idx:
-                    item2idx[item] = idx
-                    idx += 1
-        idx2item = {idx: str(int(item)) for item, idx in item2idx.items()}
-        # ---build vertical data---#
-        vb_data = np.zeros((len(item2idx), len(self.__Database)), dtype=np.uint16)
-        for trans_id, transaction in enumerate(self.__Database):
-            for item in transaction:
-                vb_data[item2idx[item], trans_id] = 1
-        vb_data = _gpuarray.to_gpu(vb_data.astype(np.uint16))
-        return vb_data, idx2item
-
-    def getRuntime(self):
+    def getMemoryUSS(self):
         """
-        Calculating the total amount of time taken by the mining process
-        :return: returning total amount of runtime taken by the mining process
+        Total amount of USS memory consumed by the mining process will be retrieved from this function
+        :return: returning USS memory consumed by the mining process
         :rtype: float
         """
-        return self.__time
+
+        return self._memoryUSS
 
     def getMemoryRSS(self):
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
-        return self.__memRSS
 
-    def getMemoryUSS(self):
+        return self._memoryRSS
+
+    def getRuntime(self):
         """
-        Total amount of USS memory consumed by the mining process will be retrieved from this function
-        :return: returning USS memory consumed by the mining process
+        Calculating the total amount of runtime taken by the mining process
+        :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
-        return self.__memUSS
 
-    def getGPUMemory(self):
+        return self._endTime - self._startTime
+
+    def getPatternsAsDataFrame(self):
         """
-        To calculate the total memory consumed by GPU
-        :return: return GPU memory
-        :rtype: int
+        Storing final frequent patterns in a dataframe
+        :return: returning frequent patterns in a dataframe
+        :rtype: pd.DataFrame
         """
 
-        return self.__GPU_MEM
+        dataFrame = {}
+        data = []
+        for a, b in self._finalPatterns.items():
+            data.append([a, b])
+            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
+        return dataFrame
+
+    def save(self, outFile):
+        """
+        Complete set of frequent patterns will be loaded in to an output file
+        :param outFile: name of the output file
+        :type outFile: csvfile
+        """
+        self._oFile = outFile
+        writer = open(self._oFile, 'w+')
+        for x, y in self._finalPatterns.items():
+            s1 = str(x) + " : " + str(y)
+            writer.write("%s \n" % s1)
 
     def getPatterns(self):
         """
         Function to send the set of frequent patterns after completion of the mining process
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
-
-    def get_numberOfPatterns(self):
-
-        return len(self._finalPatterns)
-
-    def eclat(self, basePattern, final, vb_data, idx2item, item2idx):
+    
+    def printResults(self):
+        """
+        This method prints all the stats
+        """
+        print("Total number of Frequent Patterns:", len(self.getPatterns()))
+        print("Total Memory in USS:", self.getMemoryUSS())
+        print("Total Memory in RSS", self.getMemoryRSS())
+        print("Total ExecutionTime in ms:", self.getRuntime())
+
+    @staticmethod
+    def _Mapper(transaction, candidateItemsets):
+        """
+        Map each candidate itemset of candidateItemsets to (itemset,1) if a candidate itemset is in transaction
+        :param transaction: a transaction of database
+        :type transaction: set
+        :param candidateItemsets: candidate item sets
+        :type candidateItemsets: list
+        :return:set
+        """
+
+        candidates = set()
+        for itemset in candidateItemsets:
+            if set(itemset).issubset(transaction):
+                candidates.add((itemset, 1))
+        return candidates
+
+    @staticmethod
+    def _genCandidateItemsets(frequentPatterns, length):
+        """
+        Generate candidate itemsets from frequentPatterns
+        :param frequentPatterns: set of all frequent patterns to generate candidate patterns of each of size is length
+        :type frequentPatterns: list
+        :param length: size of each candidate patterns to be generated
+        :type length: int
+        :return: list of candidate patterns
+        :rtype: list
+        """
+        candidates = list(_ab._c(frequentPatterns, 2))
+        candidates = set([tuple(set(item[0]).union(set(item[1]))) for item in [x for x in candidates]])
+        candidates = list({item for item in candidates if len(item) == length})
+        return candidates
+
+    def _genFrequentItems(self, database):
+        """
+        Get frequent items which length is 1
+        :param database: database to get frequent items
+        :return: frequent items which length is 1
+        :rtype: dict
         """
-        param basePattern: base pattern used for the mining process after completion of the mining process
-        type basePattern:
-        param final: final pattern used for the mining process after completion of the mining process
-        type final:
-        param vb_data: vb_data used for the mining process after completion of the mining process
-        type vb_data:
-        param idx2item: idx2item used for the mining process after completion of the mining process
-        type idx2item:
-        param item2idx: item2idx used for the mining process after completion of the mining process
-        type item2idx:
-        """
-        newBasePattern = []
-        for i in range(0, len(basePattern)):
-            item1 = basePattern[i]
-            i1_list = item1.split()
-            for j in range(i + 1, len(basePattern)):
-                item2 = basePattern[j]
-                i2_list = item2.split()
-                if i1_list[:-1] == i2_list[:-1]:
-                    unionOfKey = list(set(i1_list) | set(i2_list))
-                    unionOfKey.sort()
-                    valueList = []
-                    for key in unionOfKey:
-                        valueList.append(item2idx[key])
-                    total = vb_data[valueList[0]]
-                    for k in range(1, len(valueList)):
-                        total = total.__mul__(vb_data[valueList[k]])
-                    support = _gpuarray.sum(total).get()
-                    if support >= self._minSup:
-                        newBasePattern.append(" ".join(unionOfKey))
-                        final[" ".join(unionOfKey)] = support
+        frequentItems = dict(database.flatMap(lambda x: [(item, 1) for item in x])
+                             .reduceByKey(lambda x, y: x + y)
+                             .filter(lambda c: c[1] >= self._minSup)
+                             .collect())
+        return frequentItems
+
+    def _getAllFrequentPatterns(self, database, frequentItems):
+        """
+        Get all frequent patterns and save them to self.oFile
+        :param database: database
+        :type : RDD
+        :param frequentItems: dict
+        :type frequentItems: dict
+        """
+
+        # Get candidate patterns that length is 2
+        candidates = list(_ab._c(frequentItems.keys(), 2))
+        length = 3
+        while len(candidates) != 0:
+            # if each itemset of candidates is in each transaction, then create (itemset,1)
+            mappedDatabase = database.flatMap(lambda transaction: self._Mapper(transaction, candidates))
+
+            # aggregate the values by key by reduceByKey() method
+            frequentPatterns = dict(
+                mappedDatabase.reduceByKey(lambda x, y: x + y).filter(lambda c: c[1] >= self._minSup).collect())
+            self._finalPatterns.update(frequentPatterns)
+            candidates = self._genCandidateItemsets(list(frequentPatterns.keys()), length)
+            length += 1
 
-        if len(newBasePattern) > 0:
-            self.eclat(newBasePattern, final, vb_data, idx2item, item2idx)
+    def _convert(self, value):
+        """
+        To convert the user specified minSup value
+        :param value: user specified minSup value
+        :type value: int or float or str
+        :return: converted type
+        """
+        if type(value) is int:
+            value = int(value)
+        if type(value) is float:
+            value = (self._lno * value)
+        if type(value) is str:
+            if '.' in value:
+                value = float(value)
+                value = (self._lno * value)
+            else:
+                value = int(value)
+        return value
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Frequent pattern mining process will start from here
         """
         self.mine()
 
     def mine(self):
         """
         Frequent pattern mining process will start from here
         """
-        startTime = time.time()
-        basePattern = []
-        final = {}
-
-        self.__creatingItemSets()
-        self._minSup = self.__convert(self._minSup)
-        minSup = self._minSup
-        vb_data, idx2item = self.compute_vertical_bitvector_data()
-
-        for i in range(len(vb_data)):
-            if _gpuarray.sum(vb_data[i]).get() >= self._minSup:
-                basePattern.append(idx2item[i])
-                final[idx2item[i]] = _gpuarray.sum(vb_data[i]).get()
-
-        # reverse idx2item
-        item2idx = {idx2item[i]: i for i in idx2item}
-        self.eclat(basePattern, final, vb_data, idx2item, item2idx)
-        self.__time = time.time() - startTime
-        self.__memRSS = psutil.Process(os.getpid()).memory_info().rss
-        self.__memUSS = psutil.Process(os.getpid()).memory_full_info().uss
-        self._finalPatterns = final
-        self.__GPU_MEM = vb_data.nbytes
+        self._startTime = _ab._time.time()
+
+        # setting SparkConf and SparkContext to process in parallel
+        conf = _ab._SparkConf().setAppName("parallelApriori").setMaster("local[*]")
+        sc = _ab._SparkContext(conf=conf)
+        # sc.addFile("file:///home/hadoopuser/Spark_code/abstract.py")
+
+        # read database from iFile
+        database = sc.textFile(self._iFile, self._numPartitions).map(
+            lambda x: {int(y) for y in x.rstrip().split(self._sep)})
+        self._lno = database.count()
+        # Calculating minSup as a percentage
+        self._minSup = self._convert(self._minSup)
+
+        oneFrequentItems = self._genFrequentItems(database)
+        self._finalPatterns = oneFrequentItems
+        self._getAllFrequentPatterns(database, oneFrequentItems)
+
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Frequent patterns were generated successfully using Parallel Apriori algorithm")
+        sc.stop()
 
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
+    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
+        if len(_ab._sys.argv) == 6:
+            _ap = parallelApriori(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
         if len(_ab._sys.argv) == 5:
-            _ap = cudaEclatGCT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
-        if len(_ab._sys.argv) == 4:
-            _ap = cudaEclatGCT(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap = parallelApriori(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         _ap.startMine()
         _ap.mine()
-        print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
-        _ap.save(_ap._sys.argv[2])
-        print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("GPU MEM: ", _ap.getGPUMemory())
-        print("Total ExecutionTime in ms:", _ap.getRuntime())
+        _finalPatterns = _ap.getPatterns()
+        print("Total number of Frequent Patterns:", len(_finalPatterns))
+        _ap.savePatterns(_ab._sys.argv[2])
+        _memUSS = _ap.getMemoryUSS()
+        print("Total Memory in USS:", _memUSS)
+        _memRSS = _ap.getMemoryRSS()
+        print("Total Memory in RSS", _memRSS)
+        _run = _ap.getRuntime()
+        print("Total ExecutionTime in ms:", _run)
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
+
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `pami-2024.5.28.1/PAMI/frequentPattern/maximal/MaxFPGrowth.py` & `pami-2024.5.7.1/PAMI/frequentPattern/maximal/MaxFPGrowth.py`

 * *Files 1% similar despite different names*

```diff
@@ -714,36 +714,29 @@
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self):
         """
-
         Storing final frequent patterns in a dataframe
-
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
-        # dataframe = {}
-        # data = []
-        # for a, b in self._finalPatterns.items():
-        #     data.append([a.replace('\t', ' '), b])
-        #     dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
-
-        dataframe = _ab._pd.DataFrame(list([[x.replace('\t', ' '), y] for x,y in self._finalPatterns.items()]), columns=['Patterns', 'Support'])
-
-        return dataframe
+        dataFrame = {}
+        data = []
+        for a, b in self._finalPatterns.items():
+            data.append([a.replace('\t', ' '), b])
+            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
+        return dataFrame
 
     def save(self, outFile):
         """
-
-        Complete set of frequent patterns will be loaded in to an output file
-
+        Complete set of frequent patterns will be loaded in to a output file
         :param outFile: name of the output file
         :type outFile: csvfile
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
             s1 = x.strip() + ":" + str(y)
@@ -757,15 +750,15 @@
         """
         return self._finalPatterns
     
     def printResults(self):
         """
         This functon is used to print the results
         """
-        print('Total number of Maximal Frequent Patterns: ' + str(len(self.getPatterns())))
+        print('Total number of Maximal Frequent Patterns: ' + str(self.getPatterns()))
         print('Runtime: ' + str(self.getRuntime()))
         print('Memory (RSS): ' + str(self.getMemoryRSS()))
         print('Memory (USS): ' + str(self.getMemoryUSS()))
 
 
 if __name__ == "__main__":
     _ap = str()
@@ -779,8 +772,7 @@
         _ap.save(_ab._sys.argv[2])
         print("Total number of Maximal Frequent Patterns:", len(_ap.getPatterns()))
         print("Total Memory in USS:",  _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
-
```

### Comparing `pami-2024.5.28.1/PAMI/frequentPattern/maximal/__init__.py` & `pami-2024.5.7.1/PAMI/frequentPattern/maximal/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/frequentPattern/maximal/abstract.py` & `pami-2024.5.7.1/PAMI/frequentPattern/maximal/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/frequentPattern/pyspark/abstract.py` & `pami-2024.5.7.1/PAMI/frequentPattern/pyspark/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/frequentPattern/pyspark/parallelApriori.py` & `pami-2024.5.7.1/PAMI/frequentPattern/topk/FAE.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,22 +1,21 @@
-# Parallel Apriori is an algorithm to discover frequent patterns in a transactional database. This program employs parallel apriori property (or downward closure property) to  reduce the search space effectively.
+# Top - K is and algorithm to discover top frequent patterns in a transactional database.
 #
+# **Importing this algorithm into a python program**
+# ---------------------------------------------------------
 #
-#  **Importing this algorithm into a python program**
-#  ---------------------------------------------------
+#             import PAMI.frequentPattern.topK.FAE as alg
 #
-#             import PAMI.frequentPattern.pyspark.parallelApriori as alg
-#
-#             obj = alg.parallelApriori(iFile, minSup, numWorkers)
+#             obj = alg.FAE(iFile, K)
 #
 #             obj.mine()
 #
-#             frequentPatterns = obj.getPatterns()
+#             topKFrequentPatterns = obj.getPatterns()
 #
-#             print("Total number of Frequent Patterns:", len(frequentPatterns))
+#             print("Total number of Frequent Patterns:", len(topKFrequentPatterns))
 #
 #             obj.save(oFile)
 #
 #             Df = obj.getPatternInDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
@@ -48,37 +47,37 @@
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
-from PAMI.frequentPattern.pyspark import abstract as _ab
+from PAMI.frequentPattern.topk import abstract as _ab
 from deprecated import deprecated
 
 
-class parallelApriori(_ab._frequentPatterns):
+class FAE(_ab._frequentPatterns):
     """
+    :Description: Top - K is and algorithm to discover top frequent patterns in a transactional database.
 
-    :Description: Parallel Apriori is an algorithm to discover frequent patterns in a transactional database. This program employs parallel apriori property (or downward closure property) to  reduce the search space effectively.
 
-    :Reference: N. Li, L. Zeng, Q. He and Z. Shi, "Parallel Implementation of Apriori Algorithm Based on MapReduce,"
-                2012 13th ACIS International Conference on Software Engineering, Artificial Intelligence,
-                Networking and Parallel/Distributed Computing, Kyoto, Japan, 2012, pp. 236-241, doi: 10.1109/SNPD.2012.31.
+    :Reference:   Zhi-Hong Deng, Guo-Dong Fang: Mining Top-Rank-K Frequent Patterns: DOI: 10.1109/ICMLC.2007.4370261  Source: IEEE Xplore
+                  https://ieeexplore.ieee.org/document/4370261
 
     :param  iFile: str :
                    Name of the Input file to mine complete set of frequent patterns
     :param  oFile: str :
                    Name of the output file to store complete set of frequent patterns
-    :param  minSup: int :
-                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
+    :param  k: int :
+                    User specified count of top frequent patterns
+    :param minimum: int :
+                    Minimum number of frequent patterns to consider in analysis
+
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
-    :param  numPartitions: int :
-                   The number of partitions. On each worker node, an executor process is started and this process performs processing.The processing unit of worker node is partition
 
 
 
     :Attributes:
 
         startTime : float
           To record the start time of the mining process
@@ -91,100 +90,99 @@
 
         memoryUSS : float
           To store the total amount of USS memory consumed by the program
 
         memoryRSS : float
           To store the total amount of RSS memory consumed by the program
 
-        lno : int
-                the number of transactions
+        finalPatterns : dict
+            it represents to store the patterns
 
-    
 
     **Methods to execute code on terminal**
     -------------------------------------------
 
     .. code-block:: console
 
       Format:
 
-      (.venv) $ python3 parallelApriori.py <inputFile> <outputFile> <minSup> <numWorkers>
+      (.venv) $ python3 FAE.py <inputFile> <outputFile> <K>
 
       Example Usage:
 
-      (.venv) $ python3 parallelApriori.py sampleDB.txt patterns.txt 10.0 3
+      (.venv) $ python3 FAE.py sampleDB.txt patterns.txt 10
+
+    .. note:: k will be considered as count of top frequent patterns to consider in analysis
+
 
-    .. note:: minSup will be considered in percentage of database transactions
 
-    
-    
     **Importing this algorithm into a python program**
-    ----------------------------------------------------------------------------------
+    ---------------------------------------------------------
     .. code-block:: python
-    
-                import PAMI.frequentPattern.pyspark.parallelApriori as alg
-    
-                obj = alg.parallelApriori(iFile, minSup, numWorkers)
-    
-                obj.mine()
-    
-                frequentPatterns = obj.getPatterns()
-    
-                print("Total number of Frequent Patterns:", len(frequentPatterns))
-    
-                obj.save(oFile)
-    
-                Df = obj.getPatternInDataFrame()
-    
-                memUSS = obj.getMemoryUSS()
-    
-                print("Total Memory in USS:", memUSS)
-    
-                memRSS = obj.getMemoryRSS()
-    
-                print("Total Memory in RSS", memRSS)
-    
-                run = obj.getRuntime()
-    
-                print("Total ExecutionTime in seconds:", run)
-    
-    
-    **Credits:**
-    -----------------------------------------
-            The complete program was written by Yudai Masu  under the supervision of Professor Rage Uday Kiran.
 
+        import PAMI.frequentPattern.topK.FAE as alg
+
+        obj = alg.FAE(iFile, K)
+
+        obj.mine()
+
+        topKFrequentPatterns = obj.getPatterns()
+
+        print("Total number of Frequent Patterns:", len(topKFrequentPatterns))
+
+        obj.save(oFile)
+
+        Df = obj.getPatternInDataFrame()
+
+        memUSS = obj.getMemoryUSS()
+
+        print("Total Memory in USS:", memUSS)
+
+        memRSS = obj.getMemoryRSS()
+
+        print("Total Memory in RSS", memRSS)
+
+        run = obj.getRuntime()
+
+        print("Total ExecutionTime in seconds:", run)
+
+    Credits:
+    --------
+        The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
 
     """
 
-    _minSup = float()
     _startTime = float()
     _endTime = float()
+    _k = int()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
-    _numPartitions = int()
-    _lno = int()
-
-    def __init__(self, iFile, minSup, numWorkers, sep='\t'):
-        super().__init__(iFile, minSup, int(numWorkers), sep)
+    _Database = []
+    _tidList = {}
+    _minimum = int()
 
     def _creatingItemSets(self):
         """
-        Storing the complete transactions of the database/input file in a database variable
+            Storing the complete transactions of the database/input file in a database variable
+
         """
+
         self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
                 self._Database = self._iFile['Transactions'].tolist()
+
+            # print(self.Database)
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
@@ -193,228 +191,270 @@
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
                             line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
-                            #print(line)
                             self._Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
+
+    def _frequentOneItem(self):
+        """
+        Generating one frequent patterns
+        """
+        candidate = {}
+        self._tidList = {}
+        for i in range(len(self._Database)):
+            for j in self._Database[i]:
+                if j not in candidate:
+                    candidate[j] = 1
+                    self._tidList[j] = [i]
+                else:
+                    candidate[j] += 1
+                    self._tidList[j].append(i)
+        self._finalPatterns = {}
+        plist = [key for key, value in sorted(candidate.items(), key=lambda x: x[1], reverse=True)]
+        for i in plist:
+            if len(self._finalPatterns) >= self._k:
+                break
+            else:
+                self._finalPatterns[i] = candidate[i]
+        self._minimum = min([self._finalPatterns[i] for i in self._finalPatterns.keys()])
+        plist = list(self._finalPatterns.keys())
+        return plist
+
+    def _save(self, prefix, suffix, tidSetI):
+        """Saves the patterns that satisfy the periodic frequent property.
+
+            :param prefix: the prefix of a pattern
+            :type prefix: list
+            :param suffix: the suffix of a patterns
+            :type suffix: list
+            :param tidSetI: the timestamp of a patterns
+            :type tidSetI: list
+        """
+
+        if prefix is None:
+            prefix = suffix
+        else:
+            prefix = prefix + suffix
+        val = len(tidSetI)
+        sample = str()
+        for i in prefix:
+            sample = sample + i + "\t"
+        if len(self._finalPatterns) < self._k:
+            if val > self._minimum:
+                self._finalPatterns[sample] = val
+                self._finalPatterns = {k: v for k, v in sorted(self._finalPatterns.items(), key=lambda item: item[1], reverse=True)}
+                self._minimum = min([i for i in self._finalPatterns.values()])
+        else:
+            for x, y in sorted(self._finalPatterns.items(), key=lambda x: x[1]):
+                if val > y:
+                    del self._finalPatterns[x]
+                    self._finalPatterns[sample] = val
+                    self._finalPatterns = {k: v for k, v in
+                                              sorted(self._finalPatterns.items(), key=lambda item: item[1],
+                                                     reverse=True)}
+                    self._minimum = min([i for i in self._finalPatterns.values()])
+                    return
+
+    def _Generation(self, prefix, itemSets, tidSets):
+        """Equivalence class is followed  and checks for the patterns generated for periodic-frequent patterns.
+
+            :param prefix:  main equivalence prefix
+            :type prefix: periodic-frequent item or pattern
+            :param itemSets: patterns which are items combined with prefix and satisfying the periodicity
+                            and frequent with their timestamps
+            :type itemSets: list
+            :param tidSets: timestamps of the items in the argument itemSets
+            :type tidSets: list
+
+
+                    """
+        if len(itemSets) == 1:
+            i = itemSets[0]
+            tidI = tidSets[0]
+            self._save(prefix, [i], tidI)
+            return
+        for i in range(len(itemSets)):
+            itemI = itemSets[i]
+            if itemI is None:
+                continue
+            tidSetI = tidSets[i]
+            classItemSets = []
+            classTidSets = []
+            itemSetX = [itemI]
+            for j in range(i + 1, len(itemSets)):
+                itemJ = itemSets[j]
+                tidSetJ = tidSets[j]
+                y = list(set(tidSetI).intersection(tidSetJ))
+                if len(y) >= self._minimum:
+                    classItemSets.append(itemJ)
+                    classTidSets.append(y)
+            newPrefix = list(set(itemSetX)) + prefix
+            self._Generation(newPrefix, classItemSets, classTidSets)
+            self._save(prefix, list(set(itemSetX)), tidSetI)
+
+    def _convert(self, value):
+        """
+        to convert the type of user specified minSup value
+        :param value: user specified minSup value
+        :type value: int or float or str
+        :return: converted type
+        """
+        if type(value) is int:
+            value = int(value)
+        if type(value) is float:
+            value = (len(self._Database) * value)
+        if type(value) is str:
+            if '.' in value:
+                value = float(value)
+                value = ((len(self._Database)) * value)
+            else:
+                value = int(value)
+        return value
+
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
+    def startMine(self):
+        """
+            Main function of the program
+        """
+        self.mine()
+
+    def mine(self):
+        """
+            Main function of the program
+        """
+        self._startTime = _ab._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._k is None:
+            raise Exception("Please enter the Minimum Support")
+        self._creatingItemSets()
+        self._k = self._convert(self._k)
+        plist = self._frequentOneItem()
+        for i in range(len(plist)):
+            itemI = plist[i]
+            tidSetI = self._tidList[itemI]
+            itemSetX = [itemI]
+            itemSets = []
+            tidSets = []
+            for j in range(i + 1, len(plist)):
+                itemJ = plist[j]
+                tidSetJ = self._tidList[itemJ]
+                y1 = list(set(tidSetI).intersection(tidSetJ))
+                if len(y1) >= self._minimum:
+                    itemSets.append(itemJ)
+                    tidSets.append(y1)
+            self._Generation(itemSetX, itemSets, tidSets)
+        print(" TopK frequent patterns were successfully generated using FAE algorithm.")
+        self._endTime = _ab._time.time()
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+
     def getMemoryUSS(self):
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
+
         :return: returning USS memory consumed by the mining process
+
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self):
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
+
         :return: returning RSS memory consumed by the mining process
+
         :rtype: float
         """
 
         return self._memoryRSS
 
     def getRuntime(self):
         """
         Calculating the total amount of runtime taken by the mining process
+
         :return: returning total amount of runtime taken by the mining process
+
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self):
         """
         Storing final frequent patterns in a dataframe
+
         :return: returning frequent patterns in a dataframe
+
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a, b])
+            data.append([a.replace('\t', ' '), b])
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataFrame
 
     def save(self, outFile):
         """
         Complete set of frequent patterns will be loaded in to an output file
+
         :param outFile: name of the output file
-        :type outFile: csvfile
+
+        :type outFile: file
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            s1 = str(x) + " : " + str(y)
-            writer.write("%s \n" % s1)
+            patternsAndSupport = x.strip() + ":" + str(y)
+            writer.write("%s \n" % patternsAndSupport)
 
     def getPatterns(self):
         """
         Function to send the set of frequent patterns after completion of the mining process
+
         :return: returning frequent patterns
+
         :rtype: dict
         """
         return self._finalPatterns
-    
-    def printResults(self):
+
+    def printTOPK(self):
         """
-        This method prints all the stats
+        This function is used to print the results
         """
-        print("Total number of Frequent Patterns:", len(self.getPatterns()))
+        print("Top K Frequent  Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:", self.getRuntime())
-
-    @staticmethod
-    def _Mapper(transaction, candidateItemsets):
-        """
-        Map each candidate itemset of candidateItemsets to (itemset,1) if a candidate itemset is in transaction
-        :param transaction: a transaction of database
-        :type transaction: set
-        :param candidateItemsets: candidate item sets
-        :type candidateItemsets: list
-        :return:set
-        """
-
-        candidates = set()
-        for itemset in candidateItemsets:
-            if set(itemset).issubset(transaction):
-                candidates.add((itemset, 1))
-        return candidates
-
-    @staticmethod
-    def _genCandidateItemsets(frequentPatterns, length):
-        """
-        Generate candidate itemsets from frequentPatterns
-        :param frequentPatterns: set of all frequent patterns to generate candidate patterns of each of size is length
-        :type frequentPatterns: list
-        :param length: size of each candidate patterns to be generated
-        :type length: int
-        :return: list of candidate patterns
-        :rtype: list
-        """
-        candidates = list(_ab._c(frequentPatterns, 2))
-        candidates = set([tuple(set(item[0]).union(set(item[1]))) for item in [x for x in candidates]])
-        candidates = list({item for item in candidates if len(item) == length})
-        return candidates
-
-    def _genFrequentItems(self, database):
-        """
-        Get frequent items which length is 1
-        :param database: database to get frequent items
-        :return: frequent items which length is 1
-        :rtype: dict
-        """
-        frequentItems = dict(database.flatMap(lambda x: [(item, 1) for item in x])
-                             .reduceByKey(lambda x, y: x + y)
-                             .filter(lambda c: c[1] >= self._minSup)
-                             .collect())
-        return frequentItems
-
-    def _getAllFrequentPatterns(self, database, frequentItems):
-        """
-        Get all frequent patterns and save them to self.oFile
-        :param database: database
-        :type : RDD
-        :param frequentItems: dict
-        :type frequentItems: dict
-        """
-
-        # Get candidate patterns that length is 2
-        candidates = list(_ab._c(frequentItems.keys(), 2))
-        length = 3
-        while len(candidates) != 0:
-            # if each itemset of candidates is in each transaction, then create (itemset,1)
-            mappedDatabase = database.flatMap(lambda transaction: self._Mapper(transaction, candidates))
-
-            # aggregate the values by key by reduceByKey() method
-            frequentPatterns = dict(
-                mappedDatabase.reduceByKey(lambda x, y: x + y).filter(lambda c: c[1] >= self._minSup).collect())
-            self._finalPatterns.update(frequentPatterns)
-            candidates = self._genCandidateItemsets(list(frequentPatterns.keys()), length)
-            length += 1
-
-    def _convert(self, value):
-        """
-        To convert the user specified minSup value
-        :param value: user specified minSup value
-        :type value: int or float or str
-        :return: converted type
-        """
-        if type(value) is int:
-            value = int(value)
-        if type(value) is float:
-            value = (self._lno * value)
-        if type(value) is str:
-            if '.' in value:
-                value = float(value)
-                value = (self._lno * value)
-            else:
-                value = int(value)
-        return value
-
-    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
-    def startMine(self):
-        """
-        Frequent pattern mining process will start from here
-        """
-        self.mine()
-
-    def mine(self):
-        """
-        Frequent pattern mining process will start from here
-        """
-        self._startTime = _ab._time.time()
-
-        # setting SparkConf and SparkContext to process in parallel
-        conf = _ab._SparkConf().setAppName("parallelApriori").setMaster("local[*]")
-        sc = _ab._SparkContext(conf=conf)
-        # sc.addFile("file:///home/hadoopuser/Spark_code/abstract.py")
-
-        # read database from iFile
-        database = sc.textFile(self._iFile, self._numPartitions).map(
-            lambda x: {int(y) for y in x.rstrip().split(self._sep)})
-        self._lno = database.count()
-        # Calculating minSup as a percentage
-        self._minSup = self._convert(self._minSup)
-
-        oneFrequentItems = self._genFrequentItems(database)
-        self._finalPatterns = oneFrequentItems
-        self._getAllFrequentPatterns(database, oneFrequentItems)
-
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("Frequent patterns were generated successfully using Parallel Apriori algorithm")
-        sc.stop()
+        print("Total ExecutionTime in ms:",  self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
-        if len(_ab._sys.argv) == 6:
-            _ap = parallelApriori(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
+    if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
-            _ap = parallelApriori(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = FAE(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+        if len(_ab._sys.argv) == 4:
+            _ap = FAE(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
         _ap.mine()
-        _finalPatterns = _ap.getPatterns()
-        print("Total number of Frequent Patterns:", len(_finalPatterns))
-        _ap.savePatterns(_ab._sys.argv[2])
-        _memUSS = _ap.getMemoryUSS()
-        print("Total Memory in USS:", _memUSS)
-        _memRSS = _ap.getMemoryRSS()
-        print("Total Memory in RSS", _memRSS)
-        _run = _ap.getRuntime()
-        print("Total ExecutionTime in ms:", _run)
+        print("Top K Frequent Patterns:", len(_ap.getPatterns()))
+        _ap.save(_ab._sys.argv[2])
+        print("Total Memory in USS:", _ap.getMemoryUSS())
+        print("Total Memory in RSS", _ap.getMemoryRSS())
+        print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
 
+
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `pami-2024.5.28.1/PAMI/frequentPattern/pyspark/parallelECLAT.py` & `pami-2024.5.7.1/PAMI/frequentPattern/basic/ECLAT.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,16 +1,18 @@
-# ParallelEclat is an algorithm to discover frequent patterns in a transactional database. This program employs parallel apriori property (or downward closure property) to  reduce the search space effectively.
+# ECLAT is one of the fundamental algorithm to discover frequent patterns in a transactional database.
 #
 # **Importing this algorithm into a python program**
-#  ----------------------------------------------------
 #
+#             import PAMI.frequentPattern.basic.ECLAT as alg
 #
-#             import PAMI.frequentPattern.pyspark.parallelECLAT as alg
+#             iFile = 'sampleDB.txt'
 #
-#             obj = alg.parallelECLAT(iFile, minSup, numWorkers)
+#             minSup = 10  # can also be specified between 0 and 1
+#
+#             obj = alg.ECLAT(iFile, minSup)
 #
 #             obj.mine()
 #
 #             frequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
@@ -28,17 +30,14 @@
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
-
-
-
 __copyright__ = """
 Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
@@ -48,85 +47,68 @@
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
-from pyspark import SparkConf, SparkContext
-# import abstract as _ab
-from PAMI.frequentPattern.pyspark import abstract as _ab
-from abc import ABC as _ABC, abstractmethod as _abstractmethod
+from PAMI.frequentPattern.basic import abstract as _ab
 from deprecated import deprecated
 
-
-class parallelECLAT(_ab._frequentPatterns):
+class ECLAT(_ab._frequentPatterns):
     """
-    :Description: ParallelEclat is an algorithm to discover frequent patterns in a transactional database.
-     This program employs parallel apriori property (or downward closure property) to  reduce the search space effectively.
-
-    :Reference:
-
-    :param  iFile: str :
-                   Name of the Input file to mine complete set of frequent patterns
-    :param  oFile: str :
-                   Name of the output file to store complete set of frequent patterns
-    :param  minSup: int :
-                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
-    :param  sep: str :
-                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
-    :param  numPartitions: int :
-                   The number of partitions. On each worker node, an executor process is started and this process performs processing.The processing unit of worker node is partition
-
+    About this algorithm
+    ====================
 
-    :Attributes:
+    :**Description**: *ECLAT is one of the fundamental algorithm to discover frequent patterns in a transactional database.*
 
-        startTime : float
-            To record the start time of the mining process
+    :**Reference**:  Mohammed Javeed Zaki: Scalable Algorithms for Association Mining. IEEE Trans. Knowl. Data Eng. 12(3):
+                     372-390 (2000), https://ieeexplore.ieee.org/document/846291
 
-        endTime : float
-            To record the completion time of the mining process
+    :**Parameters**:    - **iFile** (*str or URL or dataFrame*) -- *Name of the Input file to mine complete set of frequent patterns.*
+                        - **oFile** (*str*) -- *Name of the Output file to store the frequent patterns.*
+                        - **minSup** (*int or float or str*) -- The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+                        - **sep** (*str*) -- This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+
+    :**Attributes**:    - **startTime** (*float*) -- *To record the start time of the mining process.*
+                        - **endTime** (*float*) -- *To record the end time of the mining process.*
+                        - **finalPatterns** (*dict*) -- *Storing the complete set of patterns in a dictionary variable.*
+                        - **memoryUSS** (*float*) -- *To store the total amount of USS memory consumed by the program.*
+                        - **memoryRSS** *(float*) -- *To store the total amount of RSS memory consumed by the program.*
+                        - **Database** (*list*) -- *To store the transactions of a database in list.*
 
-        finalPatterns : dict
-            Storing the complete set of patterns in a dictionary variable
+    Execution methods
+    =================
 
-        memoryUSS : float
-            To store the total amount of USS memory consumed by the program
-
-        memoryRSS : float
-            To store the total amount of RSS memory consumed by the program
-
-        lno : int
-            the number of transactions
-
-
-    **Methods to execute code on terminal**
-    ----------------------------------------------------
+    **Terminal command**
 
     .. code-block:: console
 
       Format:
 
-      (.venv) $ python3 parallelECLAT.py <inputFile> <outputFile> <minSup> <numWorkers>
+      (.venv) $ python3 ECLAT.py <inputFile> <outputFile> <minSup>
 
       Example Usage:
 
-      (.venv) $ python3 parallelECLAT.py sampleDB.txt patterns.txt 10.0 3
+      (.venv) $ python3 ECLAT.py sampleDB.txt patterns.txt 10.0
 
-    .. note:: minSup will be considered in percentage of database transactions
+    .. note:: minSup can be specified  in support count or a value between 0 and 1.
 
 
+    **Calling from a python program**
 
-    **Importing this algorithm into a python program**
-    ----------------------------------------------------
     .. code-block:: python
 
-            import PAMI.frequentPattern.pyspark.parallelECLAT as alg
+            import PAMI.frequentPattern.basic.ECLAT as alg
 
-            obj = alg.parallelECLAT(iFile, minSup, numWorkers)
+            iFile = 'sampleDB.txt'
+
+            minSup = 10  # can also be specified between 0 and 1
+
+            obj = alg.ECLAT(iFile, minSup)
 
             obj.mine()
 
             frequentPatterns = obj.getPatterns()
 
             print("Total number of Frequent Patterns:", len(frequentPatterns))
 
@@ -143,233 +125,266 @@
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
 
-    **Credits:**
-    ----------------------------------------------------
-             The complete program was written by Yudai Masu under the supervision of Professor Rage Uday Kiran.
+    Credits:
+    ========
+
+    The complete program was written by Kundai and revised by Tarun Sreepada under the supervision of Professor Rage Uday Kiran.
 
     """
 
     _minSup = float()
-    _numPartitions = int()
     _startTime = float()
     _endTime = float()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
-    _lno = int()
+    _Database = []
 
-    def __init__(self, iFile, minSup, numWorkers, sep="\t"):
-        super().__init__(iFile, minSup, int(numWorkers), sep)
+    def _creatingItemSets(self) -> float:
+        """
+
+        Storing the complete transactions of the database/input file in a database variable
+
+        :return: the complete transactions of the database/input file in a database variable
+        :rtype: float
+        """
+        self._Database = []
+        if isinstance(self._iFile, _ab._pd.DataFrame):
+            if self._iFile.empty:
+                print("its empty..")
+            i = self._iFile.columns.values.tolist()
+            if 'Transactions' in i:
+                self._Database = self._iFile['Transactions'].tolist()
+        if isinstance(self._iFile, str):
+            if _ab._validators.url(self._iFile):
+                data = _ab._urlopen(self._iFile)
+                for line in data:
+                    line.strip()
+                    line = line.decode("utf-8")
+                    temp = [i.rstrip() for i in line.split(self._sep)]
+                    temp = [x for x in temp if x]
+                    self._Database.append(temp)
+            else:
+                try:
+                    with open(self._iFile, 'r', encoding='utf-8') as f:
+                        for line in f:
+                            line.strip()
+                            temp = [i.rstrip() for i in line.split(self._sep)]
+                            temp = [x for x in temp if x]
+                            self._Database.append(temp)
+                except IOError:
+                    print("File Not Found")
+                    quit()
+
+    def _getUniqueItemList(self) -> list:
+        """
+
+        Generating one frequent patterns
+
+        :return: list of unique patterns
+        :rtype: list
+        """
+        self._finalPatterns = {}
+        candidate = {}
+        uniqueItem = []
+        for i in range(len(self._Database)):
+            for j in range(len(self._Database[i])):
+                if self._Database[i][j] not in candidate:
+                    candidate[self._Database[i][j]] = {i}
+                else:
+                    candidate[self._Database[i][j]].add(i)
+        for key, value in candidate.items():
+            supp = len(value)
+            if supp >= self._minSup:
+                self._finalPatterns[key] = [value]
+                uniqueItem.append(key)
+        uniqueItem.sort()
+        return uniqueItem
+
+    def _generateFrequentPatterns(self, candidateFrequent: list) -> None:
+        """
+
+        It will generate the combinations of frequent items
+
+        :param candidateFrequent :it represents the items with their respective transaction identifiers
+        :type candidateFrequent: list
+        :return: None
+        """
+        new_freqList = []
+        for i in range(0, len(candidateFrequent)):
+            item1 = candidateFrequent[i]
+            i1_list = item1.split()
+            for j in range(i + 1, len(candidateFrequent)):
+                item2 = candidateFrequent[j]
+                i2_list = item2.split()
+                if i1_list[:-1] == i2_list[:-1]:
+                    interSet = self._finalPatterns[item1][0].intersection(self._finalPatterns[item2][0])
+                    if len(interSet) >= self._minSup:
+                        newKey = item1 + "\t" + i2_list[-1]
+                        self._finalPatterns[newKey] = [interSet]
+                        new_freqList.append(newKey)
+                else: break
+
+        if len(new_freqList) > 0:
+                self._generateFrequentPatterns(new_freqList)
+
+    def _convert(self, value) -> float:
+        """
+
+        To convert the user specified minSup value
+
+        :param value: user specified minSup value
+        :return: converted type
+        :rtype: float
+        """
+        if type(value) is int:
+            value = int(value)
+        if type(value) is float:
+            value = (len(self._Database) * value)
+        if type(value) is str:
+            if '.' in value:
+                value = float(value)
+                value = (len(self._Database) * value)
+            else:
+                value = int(value)
+        return value
+
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
+    def startMine(self) -> None:
+        """
+        Frequent pattern mining process will start from here
+        """
+
+        self.mine()
+
+    def mine(self) -> None:
+        """
+        Frequent pattern mining process will start from here
+        """
+
+        self._startTime = _ab._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._minSup is None:
+            raise Exception("Please enter the Minimum Support")
+        self._creatingItemSets()
+        self._minSup = self._convert(self._minSup)
+        uniqueItemList = self._getUniqueItemList()
+        self._generateFrequentPatterns(uniqueItemList)
+        for x, y in self._finalPatterns.items():
+            self._finalPatterns[x] = len(y[0])
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Frequent patterns were generated successfully using ECLAT algorithm")
 
-    def getMemoryUSS(self):
+    def getMemoryUSS(self) -> float:
         """
+
         Total amount of USS memory consumed by the mining process will be retrieved from this function
+
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
-    def getMemoryRSS(self):
+    def getMemoryRSS(self) -> float:
         """
+
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
+
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
-    def getRuntime(self):
+    def getRuntime(self) -> float:
         """
         Calculating the total amount of runtime taken by the mining process
+
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
-    def getPatternsAsDataFrame(self):
+    def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
         """
+
         Storing final frequent patterns in a dataframe
+
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a, b])
+            data.append([a.replace('\t', ' '), b])
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataFrame
 
-    def save(self, outFile):
+    def save(self, outFile: str) -> None:
         """
+
         Complete set of frequent patterns will be loaded in to an output file
+
         :param outFile: name of the output file
         :type outFile: csvfile
+        :return: None
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            s1 = x + ":" + str(y)
-            writer.write("%s \n" % s1)
-            
-    def printResults(self):
-        """
-        This method prints all the stats
-        """
-        print("Total number of Frequent Patterns:", len(self.getPatterns()))
-        print("Total Memory in USS:", self.getMemoryUSS())
-        print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:", self.getRuntime())
+            patternsAndSupport = x.strip() + ":" + str(y)
+            writer.write("%s \n" % patternsAndSupport)
 
-    def getPatterns(self):
+    def getPatterns(self) -> dict:
         """
+
         Function to send the set of frequent patterns after completion of the mining process
+
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
-    def _genPatterns(self, suffix, pattern, data):
-        """
-        This function is used to generate patterns
-        :param suffix: the suffix of the generated ptterns
-
-        :type suffix: str
-
-        :param pattern: the pattern of the generated ptterns
-
-        :type pattern: str
-
-        :param data: the data of the generated ptterns after completion of the mining process
-
-        :type data: str
+    def printResults(self) -> None:
         """
-        freqPatterns = {}
-        index = data.index(suffix)
-        for i in range(index + 1, len(data)):
-            tid = pattern[1].intersection(data[i][1])
-            if len(tid) >= self._minSup:
-                freqPattern = pattern[0] + ' ' + data[i][0]
-                freqPatterns[freqPattern] = len(tid)
-                freqPatterns.update(self._genPatterns(data[i], (freqPattern, tid), data))
-        return freqPatterns
-
-    def printResults(self):
-        """
-        This function is used to print the results
+        Function used to print the results
         """
         print("Total number of Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:", self.getRuntime())
-
-    def _convert(self, value):
-        """
-        To convert the user specified minSup value
-        :param value: user specified minSup value
-        :type value: int or float or str
-        :return: converted type
-        """
-        print(value, type(value))
-        if type(value) is int:
-            value = int(value)
-        elif type(value) is float:
-            value = (self._lno * value)
-        elif type(value) is str:
-            if '.' in value:
-                value = float(value)
-                value = (self._lno * value)
-            else:
-                value = int(value)
-        else:
-            print("None")
-        print(type(value), value)
-        return value
-
-    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
-    def startMine(self):
-        """
-        Frequent pattern mining process will start from here
-        """
-        self.mine()
-
-    def mine(self):
-        """
-        Frequent pattern mining process will start from here
-        """
-
-        self._startTime = _ab._time.time()
-        conf = SparkConf().setAppName("Parallel ECLAT").setMaster("local[*]")
-        sc = SparkContext(conf=conf)
-
-        data = sc.textFile(self._iFile, self._numPartitions) \
-            .map(lambda line: [int(y) for y in line.rstrip().split(self._sep)]).persist()
-        self._lno = data.count()
-        self._minSup = self._convert(self._minSup)
-
-        frequentItems = None
-        frequentItems = data.zipWithIndex() \
-            .flatMap(lambda x: [(str(item), x[1]) for item in x[0]]) \
-            .groupByKey() \
-            .filter(lambda x: len(x[1]) >= self._minSup) \
-            .sortBy(lambda x: len(x[1])) \
-            .mapValues(set) \
-            .persist()
-        data.unpersist()
-        # elif 'temporal' in self._iFile:
-        #     frequentItems = data.flatMap(lambda trans: [(str(item), trans[0]) for item in trans[1:]]) \
-        #         .groupByKey() \
-        #         .filter(lambda x: len(x[1]) >= self._minSup) \
-        #         .mapValues(set) \
-        #         .persist()
-        #     data.unpersist()
-        # else:
-        #     pass
-        #     # print("may be not able to process the input file")
-
-        freqItems = dict(frequentItems.collect())
-        # print(len(freqItems))
-        self._finalPatterns = {k: len(v) for k, v in freqItems.items()}
-
-        freqPatterns = list(frequentItems.map(lambda x: self._genPatterns(x, x, list(freqItems.items())))
-                            .filter(lambda x: len(x) != 0).collect())
-        for value in freqPatterns:
-            self._finalPatterns.update(value)
-
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("Frequent patterns were generated successfully using Parallel ECLAT algorithm")
-        sc.stop()
+        print("Total ExecutionTime in ms:",  self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
-        if len(_ab._sys.argv) == 6:
-            _ap = parallelECLAT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
+    if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
-            _ap = parallelECLAT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = ECLAT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+        if len(_ab._sys.argv) == 4:
+            _ap = ECLAT(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
         _ap.mine()
-        _finalPatterns = _ap.getPatterns()
-        print("Total number of Frequent Patterns:", len(_finalPatterns))
+        print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
-        _memUSS = _ap.getMemoryUSS()
-        print("Total Memory in USS:", _memUSS)
-        _memRSS = _ap.getMemoryRSS()
-        print("Total Memory in RSS", _memRSS)
-        _run = _ap.getRuntime()
-        print("Total ExecutionTime in ms:", _run)
+        print(_ap.getPatternsAsDataFrame())
+        print("Total Memory in USS:",  _ap.getMemoryUSS())
+        print("Total Memory in RSS", _ap.getMemoryRSS())
+        print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.5.28.1/PAMI/frequentPattern/pyspark/parallelFPGrowth.py` & `pami-2024.5.7.1/PAMI/frequentPattern/pyspark/parallelFPGrowth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/frequentPattern/topk/FAE.py` & `pami-2024.5.7.1/PAMI/periodicFrequentPattern/topk/kPFPMiner/kPFPMiner.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,24 +1,20 @@
-# Top - K is and algorithm to discover top frequent patterns in a transactional database.
-#
 # **Importing this algorithm into a python program**
+# --------------------------------------------------------
 #
-#             import PAMI.frequentPattern.topK.FAE as alg
-#
-#             iFile = 'sampleDB.txt'
-#
-#             K = 2
+
+#             import PAMI.periodicFrequentPattern.kPFPMiner as alg
 #
-#             obj = alg.FAE(iFile, K)
+#             obj = alg.kPFPMiner(iFile, k)
 #
-#             obj.mine()
+#             obj.startMine()
 #
-#             topKFrequentPatterns = obj.getPatterns()
+#             periodicFrequentPatterns = obj.getPatterns()
 #
-#             print("Total number of Frequent Patterns:", len(topKFrequentPatterns))
+#             print("Total number of top-k Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 #
 #             obj.save(oFile)
 #
 #             Df = obj.getPatternInDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
@@ -30,90 +26,132 @@
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
+
 __copyright__ = """
-Copyright (C)  2021 Rage Uday Kiran
+ Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
+     Copyright (C)  2021 Rage Uday Kiran
+
 """
 
-from PAMI.frequentPattern.topk import abstract as _ab
+from PAMI.periodicFrequentPattern.basic import abstract as _ab
+import pandas as pd
 from deprecated import deprecated
 
+from PAMI.periodicFrequentPattern.topk.kPFPMiner import abstract as _ab
 
-class FAE(_ab._frequentPatterns):
-    """
-    About this algorithm
-    ====================
-
-    :**Description**: Top - K is and algorithm to discover top frequent patterns in a transactional database.
-
-    :**Reference**:   Zhi-Hong Deng, Guo-Dong Fang: Mining Top-Rank-K Frequent Patterns: DOI: 10.1109/ICMLC.2007.4370261  Source: IEEE Xplore https://ieeexplore.ieee.org/document/4370261
-
-    :**Parameters**:    - **iFile** (*str or URL or dataFrame*) -- *Name of the Input file to mine complete set of frequent patterns.*
-                        - **oFile** (*str*) -- *Name of the output file to store complete set of frequent patterns.*
-                        - **k** (*int*) -- *User specified count of top frequent patterns.*
-                        **minimum** (*int*) -- *Minimum number of frequent patterns to consider in analysis.*
-                        **sep** (*str*) -- *This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.*
 
-    :**Attributes**:    - **startTime** (*float*) -- *To record the start time of the mining process.*
-                        - **endTime** (*float*) -- *To record the completion time of the mining process.*
-                        - **finalPatterns** (*dict*) -- *Storing the complete set of patterns in a dictionary variable.*
-                        - **memoryUSS** (*float*) -- *To store the total amount of USS memory consumed by the program.*
-                        - **memoryRSS** (*float*) -- *To store the total amount of RSS memory consumed by the program.*
-
-    Execution methods
-    =================
+class kPFPMiner(_ab._periodicFrequentPatterns):
+    """
+    :Description:   Top - K is and algorithm to discover top periodic-frequent patterns in a temporal database.
 
-    **Terminal command**
+    :Reference:   Likhitha, P., Ravikumar, P., Kiran, R.U., Watanobe, Y. (2022).
+                  Discovering Top-k Periodic-Frequent Patterns in Very Large Temporal Databases. Big Data Analytics.
+                 BDA 2022. Lecture Notes in Computer Science, vol 13773. Springer, Cham. https://doi.org/10.1007/978-3-031-24094-2_14
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of periodic frequent pattern's
+    :param  oFile: str :
+                   Name of the output file to store complete set of periodic frequent pattern's
+
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+
+    :Attributes:
+
+        iFile : str
+            Input file name or path of the input file
+        k: int
+            User specified counte of top-k periodic frequent patterns
+        sep : str
+            This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
+            However, the users can override their default separator.
+        oFile : str
+            Name of the output file or the path of the output file
+        startTime:float
+            To record the start time of the mining process
+        endTime:float
+            To record the completion time of the mining process
+        finalPatterns: dict
+            Storing the complete set of patterns in a dictionary variable
+        memoryUSS : float
+            To store the total amount of USS memory consumed by the program
+        memoryRSS : float
+            To store the total amount of RSS memory consumed by the program
+
+    :Methods:
+
+        startMine()
+            Mining process will start from here
+        getPatterns()
+            Complete set of patterns will be retrieved with this function
+        savePatterns(oFile)
+            Complete set of frequent patterns will be loaded in to a output file
+        getPatternsAsDataFrame()
+            Complete set of frequent patterns will be loaded in to a dataframe
+        getMemoryUSS()
+            Total amount of USS memory consumed by the mining process will be retrieved from this function
+        getMemoryRSS()
+            Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        getRuntime()
+            Total amount of runtime taken by the mining process will be retrieved from this function
+        creatingItemSets()
+            Scans the dataset or dataframes and stores in list format
+        frequentOneItem()
+            Generates one frequent patterns
+        eclatGeneration(candidateList)
+            It will generate the combinations of frequent items
+        generateFrequentPatterns(tidList)
+            It will generate the combinations of frequent items from a list of items
 
+    **Executing the code on terminal:**
+    ------------------------------------------
     .. code-block:: console
 
-      Format:
 
-      (.venv) $ python3 FAE.py <inputFile> <outputFile> <K>
+       Format:
 
-      Example Usage:
 
-      (.venv) $ python3 FAE.py sampleDB.txt patterns.txt 10.0
+       (.venv) $ python3 kPFPMiner.py <inputFile> <outputFile> <k>
 
-    .. note:: k will be considered as count of top frequent patterns to consider in analysis.
+       Examples :
 
-    **Calling from a python program**
+       (.venv) $  python3 kPFPMiner.py sampleDB.txt patterns.txt 10
 
-    .. code-block:: python
 
-            import PAMI.frequentPattern.topK.FAE as alg
-
-            iFile = 'sampleDB.txt'
+    **Sample run of the importing code:
+    --------------------------------------
+    .. code-block:: python
 
-            K = 2
+            import PAMI.periodicFrequentPattern.kPFPMiner as alg
 
-            obj = alg.FAE(iFile, K)
+            obj = alg.kPFPMiner(iFile, k)
 
-            obj.mine()
+            obj.startMine()
 
-            topKFrequentPatterns = obj.getPatterns()
+            periodicFrequentPatterns = obj.getPatterns()
 
-            print("Total number of Frequent Patterns:", len(topKFrequentPatterns))
+            print("Total number of top-k Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 
             obj.save(oFile)
 
             Df = obj.getPatternInDataFrame()
 
             memUSS = obj.getMemoryUSS()
 
@@ -123,34 +161,33 @@
 
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
-
-    Credits
-    =======
-
-    The complete program was written by P. Likhitha  and revised by Tarun Sreepada under the supervision of Professor Rage Uday Kiran.
+    **Credits:**
+    --------------
+            The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
 
     """
 
     _startTime = float()
     _endTime = float()
     _k = int()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
     _tidList = {}
-    _minimum = int()
+    lno = int()
+    _maximum = int()
 
     def _creatingItemSets(self):
         """
         Storing the complete transactions of the database/input file in a database variable
         """
 
         self._Database = []
@@ -178,88 +215,108 @@
                             line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
                             self._Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
+                    
+    def getPer_Sup(self, tids):
+        tids.sort()
+        cur=0
+        per=list()
+        sup=0
+        #print(tids)
+        for i in range(len(tids)-1):
+            j = i + 1
+            #if tids[j] - cur <= periodicity:
+                #return [0,0]
+            per.append(tids[j] - cur)
+            cur = tids[j]
+        per.append(self.lno - cur)
+        return max(per)
 
     def _frequentOneItem(self):
         """
         Generating one frequent patterns
         """
-        candidate = {}
+        self._mapSupport = {}
         self._tidList = {}
-        for i in range(len(self._Database)):
-            for j in self._Database[i]:
-                if j not in candidate:
-                    candidate[j] = 1
-                    self._tidList[j] = [i]
+        n = 0
+        for line in self._Database:
+            self.lno += 1
+            n = int(line[0])
+            for i in range(1, len(line)):
+                si = line[i]
+                if self._mapSupport.get(si) is None:
+                    self._mapSupport[si] = [1, abs(0 - n), n]
+                    self._tidList[si] = [n]
                 else:
-                    candidate[j] += 1
-                    self._tidList[j].append(i)
-        self._finalPatterns = {}
-        plist = [key for key, value in sorted(candidate.items(), key=lambda x: x[1], reverse=True)]
-        self._tidList = {k: frozenset(v) for k, v in self._tidList.items()}
+                    self._mapSupport[si][0] += 1
+                    self._mapSupport[si][1] = max(self._mapSupport[si][1], abs(n - self._mapSupport[si][2]))
+                    self._mapSupport[si][2] = n
+                    self._tidList[si].append(n)
+        for x, y in self._mapSupport.items():
+            self._mapSupport[x][1] = max(self._mapSupport[x][1], abs(n - self._mapSupport[x][2]))
+        plist = [key for key, value in sorted(self._mapSupport.items(), key=lambda x: x[1], reverse=True)]
         for i in plist:
             if len(self._finalPatterns) >= self._k:
                 break
             else:
-                self._finalPatterns[i] = candidate[i]
-        self._minimum = min([self._finalPatterns[i] for i in self._finalPatterns.keys()])
+                self._finalPatterns[i] = self._mapSupport[i][1]
+        self._maximum = max([self._finalPatterns[i] for i in self._finalPatterns.keys()])
         plist = list(self._finalPatterns.keys())
         return plist
 
+
     def _save(self, prefix, suffix, tidSetI):
-        """
-        Saves the patterns that satisfy the periodic frequent property.
+        """Saves the patterns that satisfy the periodic frequent property.
 
         :param prefix: the prefix of a pattern
         :type prefix: list
         :param suffix: the suffix of a patterns
         :type suffix: list
         :param tidSetI: the timestamp of a patterns
         :type tidSetI: list
         """
 
         if prefix is None:
             prefix = suffix
         else:
             prefix = prefix + suffix
-        val = len(tidSetI)
+        val = self.getPer_Sup(tidSetI)
         sample = str()
-        # for i in prefix:
-        #     sample = sample + i + "\t"
-        sample = "\t".join(prefix)
+        for i in prefix:
+            sample = sample + i + " "
         if len(self._finalPatterns) < self._k:
-            if val > self._minimum:
+            if val < self._maximum:
                 self._finalPatterns[sample] = val
                 self._finalPatterns = {k: v for k, v in sorted(self._finalPatterns.items(), key=lambda item: item[1], reverse=True)}
-                self._minimum = min([i for i in self._finalPatterns.values()])
+                self._maximum = max([i for i in self._finalPatterns.values()])
         else:
-            for x, y in sorted(self._finalPatterns.items(), key=lambda x: x[1]):
-                if val > y:
+            for x, y in sorted(self._finalPatterns.items(), key=lambda x: x[1], reverse=True):
+                if val < y:
                     del self._finalPatterns[x]
                     self._finalPatterns[sample] = val
                     self._finalPatterns = {k: v for k, v in
                                               sorted(self._finalPatterns.items(), key=lambda item: item[1],
                                                      reverse=True)}
-                    self._minimum = min([i for i in self._finalPatterns.values()])
+                    self._maximum = max([i for i in self._finalPatterns.values()])
                     return
 
     def _Generation(self, prefix, itemSets, tidSets):
-        """
-        Equivalence class is followed  and checks for the patterns generated for periodic-frequent patterns.
+        """Equivalence class is followed  and checks for the patterns generated for periodic-frequent patterns.
 
         :param prefix:  main equivalence prefix
         :type prefix: periodic-frequent item or pattern
         :param itemSets: patterns which are items combined with prefix and satisfying the periodicity and frequent with their timestamps
         :type itemSets: list
         :param tidSets: timestamps of the items in the argument itemSets
         :type tidSets: list
+
         """
         if len(itemSets) == 1:
             i = itemSets[0]
             tidI = tidSets[0]
             self._save(prefix, [i], tidI)
             return
         for i in range(len(itemSets)):
@@ -269,52 +326,45 @@
             tidSetI = tidSets[i]
             classItemSets = []
             classTidSets = []
             itemSetX = [itemI]
             for j in range(i + 1, len(itemSets)):
                 itemJ = itemSets[j]
                 tidSetJ = tidSets[j]
-                y = tidSetI.intersection(tidSetJ)
-                if len(y) >= self._minimum:
+                y = list(set(tidSetI).intersection(tidSetJ))
+                if self.getPer_Sup(y) <= self._maximum:
                     classItemSets.append(itemJ)
                     classTidSets.append(y)
             newPrefix = list(set(itemSetX)) + prefix
             self._Generation(newPrefix, classItemSets, classTidSets)
             self._save(prefix, list(set(itemSetX)), tidSetI)
 
     def _convert(self, value):
         """
         to convert the type of user specified minSup value
 
         :param value: user specified minSup value
-        :type value: int or float or str
         :return: converted type
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self._Database) * value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
                 value = ((len(self._Database)) * value)
             else:
                 value = int(value)
         return value
 
-    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
-        TopK Frequent pattern mining process will start from here
-        """
-        self.mine()
+        Main function of the program
 
-    def mine(self):
-        """
-        TopK Frequent pattern mining process will start from here
         """
         self._startTime = _ab._time.time()
         if self._iFile is None:
             raise Exception("Please enter the file path or file name:")
         if self._k is None:
             raise Exception("Please enter the Minimum Support")
         self._creatingItemSets()
@@ -325,121 +375,110 @@
             tidSetI = self._tidList[itemI]
             itemSetX = [itemI]
             itemSets = []
             tidSets = []
             for j in range(i + 1, len(plist)):
                 itemJ = plist[j]
                 tidSetJ = self._tidList[itemJ]
-                y1 = tidSetI.intersection(tidSetJ)
-                if len(y1) >= self._minimum:
+                y1 = list(set(tidSetI).intersection(tidSetJ))
+                if self.getPer_Sup(y1) <= self._maximum:
                     itemSets.append(itemJ)
                     tidSets.append(y1)
             self._Generation(itemSetX, itemSets, tidSets)
-        print(" TopK frequent patterns were successfully generated using FAE algorithm.")
+        print("kPFPMiner has successfully generated top-k frequent patterns")
         self._endTime = _ab._time.time()
         self._memoryUSS = float()
         self._memoryRSS = float()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
 
     def getMemoryUSS(self):
-        """
-        Total amount of USS memory consumed by the mining process will be retrieved from this function
+        """Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self):
-        """
-        Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
     def getRuntime(self):
-        """
-        Calculating the total amount of runtime taken by the mining process
+        """Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self):
-        """
-
-        Storing final frequent patterns in a dataframe
+        """Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
-        # dataframe = {}
-        # data = []
-        # for a, b in self._finalPatterns.items():
-        #     data.append([a.replace('\t', ' '), b])
-        #     dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
-
-        dataframe = _ab._pd.DataFrame(list([[x.replace('\t', ' '), y] for x,y in self._finalPatterns.items()]), columns=['Patterns', 'Support'])
-
-        return dataframe
+        dataFrame = {}
+        data = []
+        for a, b in self._finalPatterns.items():
+            data.append([a, b])
+            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'periodicity'])
+        return dataFrame
 
     def save(self, outFile):
-        """
-
-        Complete set of frequent patterns will be loaded in to an output file
+        """Complete set of frequent patterns will be loaded in to a output file
 
         :param outFile: name of the output file
 
-        :type outFile: csvfile
-
+        :type outFile: file
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            s1 = x.strip() + ":" + str(y)
-            writer.write("%s \n" % s1)
+            patternsAndSupport = x + ":" + str(y)
+            writer.write("%s \n" % patternsAndSupport)
 
     def getPatterns(self):
-        """
-        Function to send the set of frequent patterns after completion of the mining process
+        """ Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self):
-        """
-        This function is used to print the results
-        """
-        print("Top K Frequent  Patterns:", len(self.getPatterns()))
+        print("Total number of  Top-k Periodic Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:",  self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
-            _ap = FAE(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = kPFPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
-            _ap = FAE(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap = kPFPMiner(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
-        _ap.mine()
-        print("Top K Frequent Patterns:", len(_ap.getPatterns()))
+        _Patterns = _ap.getPatterns()
+        print("Total number of top-k periodic frequent patterns:", len(_Patterns))
         _ap.save(_ab._sys.argv[2])
-        print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in ms:", _ap.getRuntime())
+        _memUSS = _ap.getMemoryUSS()
+        print("Total Memory in USS:", _memUSS)
+        _memRSS = _ap.getMemoryRSS()
+        print("Total Memory in RSS", _memRSS)
+        _run = _ap.getRuntime()
+        print("Total ExecutionTime in ms:", _run)
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
 
+
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `pami-2024.5.28.1/PAMI/frequentPattern/topk/_FAE.py` & `pami-2024.5.7.1/PAMI/periodicFrequentPattern/basic/PFECLAT.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,25 +1,27 @@
-# Top - K is and algorithm to discover top frequent patterns in a transactional database.
+# PFECLAT is the fundamental approach to mine the periodic-frequent patterns.
+#
 #
 # **Importing this algorithm into a python program**
-# ---------------------------------------------------------
+# --------------------------------------------------------
+#
 #
-#             import PAMI.frequentPattern.topK.FAE as alg
+#             from PAMI.periodicFrequentPattern.basic import PFECLAT as alg
 #
-#             obj = alg.FAE(iFile, K)
+#             obj = alg.PFECLAT("../basic/sampleTDB.txt", "2", "5")
 #
-#             obj.mine()
+#             obj.startMine()
 #
-#             topKFrequentPatterns = obj.getPatterns()
+#             periodicFrequentPatterns = obj.getPatterns()
 #
-#             print("Total number of Frequent Patterns:", len(topKFrequentPatterns))
+#             print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 #
-#             obj.save(oFile)
+#             obj.save("patterns")
 #
-#             Df = obj.getPatternInDataFrame()
+#             Df = obj.getPatternsAsDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
 #             print("Total Memory in USS:", memUSS)
 #
 #             memRSS = obj.getMemoryRSS()
 #
@@ -29,160 +31,234 @@
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
 
 
-
 __copyright__ = """
-Copyright (C)  2021 Rage Uday Kiran
+ Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
+     Copyright (C)  2021 Rage Uday Kiran
+
 """
 
-from PAMI.frequentPattern.topk import abstract as _ab
+from PAMI.periodicFrequentPattern.basic import abstract as _ab
+import pandas as pd
 from deprecated import deprecated
+import numpy as np
 
+from PAMI.periodicFrequentPattern.basic import abstract as _ab
 
-class FAE(_ab._frequentPatterns):
-    """
-    :Description: Top - K is and algorithm to discover top frequent patterns in a transactional database.
 
+class PFECLAT(_ab._periodicFrequentPatterns):
+    """
+    :Description:   PFECLAT is the fundamental approach to mine the periodic-frequent patterns.
 
-    :Reference:   Zhi-Hong Deng, Guo-Dong Fang: Mining Top-Rank-K Frequent Patterns: DOI: 10.1109/ICMLC.2007.4370261  Source: IEEE Xplore
-                  https://ieeexplore.ieee.org/document/4370261
+    :Reference:   P. Ravikumar, P.Likhitha, R. Uday kiran, Y. Watanobe, and Koji Zettsu, "Towards efficient discovery of
+                  periodic-frequent patterns in columnar temporal databases", 2021 IEA/AIE.
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of frequent patterns
+                   Name of the Input file to mine complete set of periodic frequent pattern's
     :param  oFile: str :
-                   Name of the output file to store complete set of frequent patterns
-    :param  k: int :
-                    User specified count of top frequent patterns
-    :param minimum: int :
-                    Minimum number of frequent patterns to consider in analysis
-
+                   Name of the output file to store complete set of periodic frequent pattern's
+    :param  minSup: str:
+                   Controls the minimum number of transactions in which every item must appear in a database.
+    :param  maxPer: str:
+                   Controls the maximum number of transactions in which any two items within a pattern can reappear.
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
-
-
     :Attributes:
 
-        startTime : float
-          To record the start time of the mining process
-
-        endTime : float
-          To record the completion time of the mining process
-
-        finalPatterns : dict
-          Storing the complete set of patterns in a dictionary variable
-
+        iFile : file
+            Name of the Input file or path of the input file
+        oFile : file
+            Name of the output file or path of the output file
+        minSup : int or float or str
+            The user can specify minSup either in count or proportion of database size.
+            If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+        maxPer : int or float or str
+            The user can specify maxPer either in count or proportion of database size.
+            If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: maxPer=10 will be treated as integer, while maxPer=10.0 will be treated as float
+        sep : str
+            This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
+            However, the users can override their default separator.
         memoryUSS : float
-          To store the total amount of USS memory consumed by the program
-
+            To store the total amount of USS memory consumed by the program
         memoryRSS : float
-          To store the total amount of RSS memory consumed by the program
-
+            To store the total amount of RSS memory consumed by the program
+        startTime : float
+            To record the start time of the mining process
+        endTime : float
+            To record the completion time of the mining process
+        Database : list
+            To store the transactions of a database in list
+        mapSupport : Dictionary
+            To maintain the information of item and their frequency
+        lno : int
+            it represents the total no of transactions
+        tree : class
+            it represents the Tree class
+        itemSetCount : int
+            it represents the total no of patterns
         finalPatterns : dict
             it represents to store the patterns
-
+        tidList : dict
+            stores the timestamps of an item
+        hashing : dict
+            stores the patterns with their support to check for the closed property
+
+    :Methods:
+
+        startMine()
+            Mining process will start from here
+        getPatterns()
+            Complete set of patterns will be retrieved with this function
+        save(oFile)
+            Complete set of periodic-frequent patterns will be loaded in to a output file
+        getPatternsAsDataFrame()
+            Complete set of periodic-frequent patterns will be loaded in to a dataframe
+        getMemoryUSS()
+            Total amount of USS memory consumed by the mining process will be retrieved from this function
+        getMemoryRSS()
+            Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        getRuntime()
+            Total amount of runtime taken by the mining process will be retrieved from this function
+        creatingOneItemSets()
+            Scan the database and store the items with their timestamps which are periodic frequent 
+        getPeriodAndSupport()
+            Calculates the support and period for a list of timestamps.
+        Generation()
+            Used to implement prefix class equivalence method to generate the periodic patterns recursively
+            
 
     **Methods to execute code on terminal**
-    -------------------------------------------
-
+    ------------------------------------------
     .. code-block:: console
 
-      Format:
 
-      (.venv) $ python3 FAE.py <inputFile> <outputFile> <K>
+       Format:
+
+       (.venv) $ python3 PFECLAT.py <inputFile> <outputFile> <minSup>
+
+       Example usage:
 
-      Example Usage:
+       (.venv) $ python3 PFECLAT.py sampleDB.txt patterns.txt 10.0
 
-      (.venv) $ python3 FAE.py sampleDB.txt patterns.txt 10
 
-    .. note:: k will be considered as count of top frequent patterns to consider in analysis
 
+               .. note:: minSup will be considered in percentage of database transactions
 
 
     **Importing this algorithm into a python program**
-    ---------------------------------------------------------
+    --------------------------------------------------------
     .. code-block:: python
 
-        import PAMI.frequentPattern.topK.FAE as alg
+             from PAMI.periodicFrequentPattern.basic import PFECLAT as alg
 
-        obj = alg.FAE(iFile, K)
+                obj = alg.PFECLAT("../basic/sampleTDB.txt", "2", "5")
 
-        obj.mine()
+                obj.startMine()
 
-        topKFrequentPatterns = obj.getPatterns()
+                periodicFrequentPatterns = obj.getPatterns()
 
-        print("Total number of Frequent Patterns:", len(topKFrequentPatterns))
+                print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 
-        obj.save(oFile)
+                obj.save("patterns")
 
-        Df = obj.getPatternInDataFrame()
+                Df = obj.getPatternsAsDataFrame()
 
-        memUSS = obj.getMemoryUSS()
+                memUSS = obj.getMemoryUSS()
 
-        print("Total Memory in USS:", memUSS)
+                print("Total Memory in USS:", memUSS)
 
-        memRSS = obj.getMemoryRSS()
+                memRSS = obj.getMemoryRSS()
 
-        print("Total Memory in RSS", memRSS)
+                print("Total Memory in RSS", memRSS)
 
-        run = obj.getRuntime()
+                run = obj.getRuntime()
 
-        print("Total ExecutionTime in seconds:", run)
+                print("Total ExecutionTime in seconds:", run)
 
-    Credits:
-    --------
-        The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
+    **Credits:**
+    --------------
+             The complete program was written by  P.Likhitha   under the supervision of Professor Rage Uday Kiran.
 
     """
-
-    _startTime = float()
-    _endTime = float()
-    _k = int()
-    _finalPatterns = {}
+    
     _iFile = " "
     _oFile = " "
     _sep = " "
+    _dbSize = None
+    _Database = None
+    _minSup = str()
+    _maxPer = str()
+    _tidSet = set()
+    _finalPatterns = {}
+    _startTime = None
+    _endTime = None
     _memoryUSS = float()
     _memoryRSS = float()
-    _Database = []
-    _tidList = {}
-    _minimum = int()
 
-    def _creatingItemSets(self):
+    def _convert(self, value) -> float:
         """
-            Storing the complete transactions of the database/input file in a database variable
+        To convert the given user specified value
 
+        :param value: user specified value
+        :return: converted value
         """
+        if type(value) is int:
+            value = int(value)
+        if type(value) is float:
+            value = (self._dbSize * value)
+        if type(value) is str:
+            if '.' in value:
+                value = float(value)
+                value = (self._dbSize * value)
+            else:
+                value = int(value)
+        return value
 
+    def _creatingItemSets(self) -> None:
+        """
+            Storing the complete transactions of the database/input file in a database variable
+        :return: None
+        """
         self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
+            data, ts = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
+            if 'TS' in i:
+                ts = self._iFile['TS'].tolist()
             if 'Transactions' in i:
-                self._Database = self._iFile['Transactions'].tolist()
+                data = self._iFile['Transactions'].tolist()
+            for i in range(len(data)):
+                tr = [ts[i][0]]
+                tr = tr + data[i]
+                self._Database.append(tr)
 
-            # print(self.Database)
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
@@ -196,263 +272,196 @@
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
                             self._Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
 
-    def _frequentOneItem(self):
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
+    def startMine(self) -> None:
         """
-        Generating one frequent patterns
+        Mining process will start from this function
+        :return: None
         """
-        candidate = {}
-        self._tidList = {}
-        for i in range(len(self._Database)):
-            for j in self._Database[i]:
-                if j not in candidate:
-                    candidate[j] = 1
-                    self._tidList[j] = [i]
-                else:
-                    candidate[j] += 1
-                    self._tidList[j].append(i)
-        self._finalPatterns = {}
-        plist = [key for key, value in sorted(candidate.items(), key=lambda x: x[1], reverse=True)]
-        for i in plist:
-            if len(self._finalPatterns) >= self._k:
-                break
-            else:
-                self._finalPatterns[i] = candidate[i]
-        self._minimum = min([self._finalPatterns[i] for i in self._finalPatterns.keys()])
-        plist = list(self._finalPatterns.keys())
-        return plist
-
-    def _save(self, prefix, suffix, tidSetI):
-        """Saves the patterns that satisfy the periodic frequent property.
-
-            :param prefix: the prefix of a pattern
-            :type prefix: list
-            :param suffix: the suffix of a patterns
-            :type suffix: list
-            :param tidSetI: the timestamp of a patterns
-            :type tidSetI: list
-        """
-
-        if prefix is None:
-            prefix = suffix
-        else:
-            prefix = prefix + suffix
-        val = len(tidSetI)
-        sample = str()
-        for i in prefix:
-            sample = sample + i + "\t"
-        if len(self._finalPatterns) < self._k:
-            if val > self._minimum:
-                self._finalPatterns[sample] = val
-                self._finalPatterns = {k: v for k, v in sorted(self._finalPatterns.items(), key=lambda item: item[1], reverse=True)}
-                self._minimum = min([i for i in self._finalPatterns.values()])
-        else:
-            for x, y in sorted(self._finalPatterns.items(), key=lambda x: x[1]):
-                if val > y:
-                    del self._finalPatterns[x]
-                    self._finalPatterns[sample] = val
-                    self._finalPatterns = {k: v for k, v in
-                                              sorted(self._finalPatterns.items(), key=lambda item: item[1],
-                                                     reverse=True)}
-                    self._minimum = min([i for i in self._finalPatterns.values()])
-                    return
-
-    def _Generation(self, prefix, itemSets, tidSets):
-        """Equivalence class is followed  and checks for the patterns generated for periodic-frequent patterns.
-
-            :param prefix:  main equivalence prefix
-            :type prefix: periodic-frequent item or pattern
-            :param itemSets: patterns which are items combined with prefix and satisfying the periodicity
-                            and frequent with their timestamps
-            :type itemSets: list
-            :param tidSets: timestamps of the items in the argument itemSets
-            :type tidSets: list
-
-
-                    """
-        if len(itemSets) == 1:
-            i = itemSets[0]
-            tidI = tidSets[0]
-            self._save(prefix, [i], tidI)
-            return
-        for i in range(len(itemSets)):
-            itemI = itemSets[i]
-            if itemI is None:
-                continue
-            tidSetI = tidSets[i]
-            classItemSets = []
-            classTidSets = []
-            itemSetX = [itemI]
-            for j in range(i + 1, len(itemSets)):
-                itemJ = itemSets[j]
-                tidSetJ = tidSets[j]
-                y = list(set(tidSetI).intersection(tidSetJ))
-                if len(y) >= self._minimum:
-                    classItemSets.append(itemJ)
-                    classTidSets.append(y)
-            newPrefix = list(set(itemSetX)) + prefix
-            self._Generation(newPrefix, classItemSets, classTidSets)
-            self._save(prefix, list(set(itemSetX)), tidSetI)
-
-    def _convert(self, value):
-        """
-        to convert the type of user specified minSup value
-        :param value: user specified minSup value
-        :type value: int or float or str
-        :return: converted type
-        """
-        if type(value) is int:
-            value = int(value)
-        if type(value) is float:
-            value = (len(self._Database) * value)
-        if type(value) is str:
-            if '.' in value:
-                value = float(value)
-                value = ((len(self._Database)) * value)
-            else:
-                value = int(value)
-        return value
+        self.Mine()
+        # self._startTime = _ab._time.time()
+        # self._finalPatterns = {}
+        # frequentSets = self._creatingOneItemSets()
+        # self._generateEclat(frequentSets)
+        # self._endTime = _ab._time.time()
+        # process = _ab._psutil.Process(_ab._os.getpid())
+        # self._memoryRSS = float()
+        # self._memoryUSS = float()
+        # self._memoryUSS = process.memory_full_info().uss
+        # self._memoryRSS = process.memory_info().rss
+        # print("Periodic-Frequent patterns were generated successfully using PFECLAT algorithm ")
+
+    def _getMaxPer(self, arr, maxTS):
+        arr = np.append(list(arr), [0, maxTS])
+        arr = np.sort(arr)
+        arr = np.diff(arr)
 
-    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
-    def startMine(self):
-        """
-            Main function of the program
-        """
-        self.mine()
+        return np.max(arr)
 
-    def mine(self):
+    def Mine(self) -> None:
         """
-            Main function of the program
+        Mining process will start from this function
+        :return: None
         """
         self._startTime = _ab._time.time()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        if self._k is None:
-            raise Exception("Please enter the Minimum Support")
-        self._creatingItemSets()
-        self._k = self._convert(self._k)
-        plist = self._frequentOneItem()
-        for i in range(len(plist)):
-            itemI = plist[i]
-            tidSetI = self._tidList[itemI]
-            itemSetX = [itemI]
-            itemSets = []
-            tidSets = []
-            for j in range(i + 1, len(plist)):
-                itemJ = plist[j]
-                tidSetJ = self._tidList[itemJ]
-                y1 = list(set(tidSetI).intersection(tidSetJ))
-                if len(y1) >= self._minimum:
-                    itemSets.append(itemJ)
-                    tidSets.append(y1)
-            self._Generation(itemSetX, itemSets, tidSets)
-        print(" TopK frequent patterns were successfully generated using FAE algorithm.")
+        self._finalPatterns = {}
+        frequentSets = self._creatingItemSets()
+
+        items = {}
+        maxTS = 0
+        for line in self._Database:
+            index = int(line[0])
+            maxTS = max(maxTS, index)
+            for item in line[1:]:
+                if tuple([item]) not in items:
+                    items[tuple([item])] = set()
+                items[tuple([item])].add(index)
+
+        self._dbSize = maxTS
+
+        self._minSup = self._convert(self._minSup)
+        self._maxPer = self._convert(self._maxPer)
+        minSup = self._minSup
+        maxPer = self._maxPer
+
+
+        items = {k: v for k, v in items.items() if len(v) >= minSup}
+        items = {k: v for k, v in sorted(items.items(), key = lambda x: len(x[1]), reverse = True)}
+
+        keys = []
+        for item in list(items.keys()):
+            per = self._getMaxPer(items[item], maxTS)
+            if per <= maxPer:
+                keys.append(item)
+                self._finalPatterns[item] = [len(items[item]), per, set(items[item])]
+
+        while keys:
+            newKeys = []
+            for i in range(len(keys)):
+                for j in range(i + 1, len(keys)):
+                    if keys[i][:-1] == keys[j][:-1] and keys[i][-1] != keys[j][-1]:
+                        # print(keys[i], keys[j])
+                        newKey = tuple(keys[i] + (keys[j][-1],))
+                        intersect = items[keys[i]].intersection(items[keys[j]])
+                        per = self._getMaxPer(intersect, maxTS)
+                        sup = len(intersect)
+                        if sup >= minSup and per <= maxPer:
+                            items[newKey] = intersect
+                            newKeys.append(newKey)
+                            self._finalPatterns[newKey] = [sup, per, set(intersect)]
+                    else:
+                        break
+            keys = newKeys
+
+        newPattern = {}
+        for k, v in self._finalPatterns.items():
+            newPattern["\t".join([str(x) for x in k])] = v
+
+        self._finalPatterns = newPattern
+
+        # self._generateEclat(frequentSets)
         self._endTime = _ab._time.time()
-        self._memoryUSS = float()
-        self._memoryRSS = float()
         process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryRSS = float()
+        self._memoryUSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
+        print("Periodic-Frequent patterns were generated successfully using PFECLAT algorithm ")
 
-    def getMemoryUSS(self):
-        """
-        Total amount of USS memory consumed by the mining process will be retrieved from this function
+    def getMemoryUSS(self) -> float:
+        """Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
-
         :rtype: float
         """
 
         return self._memoryUSS
 
-    def getMemoryRSS(self):
-        """
-        Total amount of RSS memory consumed by the mining process will be retrieved from this function
+    def getMemoryRSS(self) -> float:
+        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
-
         :rtype: float
         """
 
         return self._memoryRSS
 
-    def getRuntime(self):
-        """
-        Calculating the total amount of runtime taken by the mining process
+    def getRuntime(self) -> float:
+        """Calculating the total amount of runtime taken by the mining process
 
-        :return: returning total amount of runtime taken by the mining process
 
+        :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
-    def getPatternsAsDataFrame(self):
+    def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
         """
-        Storing final frequent patterns in a dataframe
-
-        :return: returning frequent patterns in a dataframe
+        Storing final periodic-frequent patterns in a dataframe
 
+        :return: returning periodic-frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
-        dataFrame = {}
+        dataframe = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a.replace('\t', ' '), b])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
-        return dataFrame
+            data.append([a, b[0], b[1]])
+            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
+        return dataframe
 
-    def save(self, outFile):
+    def save(self, outFile: str) -> None:
         """
-        Complete set of frequent patterns will be loaded in to an output file
+        Complete set of periodic-frequent patterns will be loaded in to a output file
 
         :param outFile: name of the output file
-
-        :type outFile: file
+        :type outFile: csv file
+        :return: None
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            patternsAndSupport = x.strip() + ":" + str(y)
-            writer.write("%s \n" % patternsAndSupport)
+            s1 = x + ":" + str(y[0]) + ":" + str(y[1])
+            #s1 = x.replace(' ', '\t') + ":" + str(y[0]) + ":" + str(y[1])
+            writer.write("%s \n" % s1)
 
-    def getPatterns(self):
+    def getPatterns(self) -> dict:
         """
-        Function to send the set of frequent patterns after completion of the mining process
-
-        :return: returning frequent patterns
+        Function to send the set of periodic-frequent patterns after completion of the mining process
 
+        :return: returning periodic-frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
-    def printTOPK(self):
+    def printResults(self) -> None:
         """
         This function is used to print the results
+        :return: None
         """
-        print("Top K Frequent  Patterns:", len(self.getPatterns()))
+        print("Total number of Periodic Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:",  self.getRuntime())
-
+                    
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
+    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
+        if len(_ab._sys.argv) == 6:
+            _ap = PFECLAT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
         if len(_ab._sys.argv) == 5:
-            _ap = FAE(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
-        if len(_ab._sys.argv) == 4:
-            _ap = FAE(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap = PFECLAT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         _ap.startMine()
-        _ap.mine()
-        print("Top K Frequent Patterns:", len(_ap.getPatterns()))
+        print("Total number of Periodic-Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `pami-2024.5.28.1/PAMI/frequentPattern/topk/abstract.py` & `pami-2024.5.7.1/PAMI/frequentPattern/topk/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/fuzzyCorrelatedPattern/__init__.py` & `pami-2024.5.7.1/PAMI/fuzzyCorrelatedPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/fuzzyCorrelatedPattern/basic/FCPGrowth.py` & `pami-2024.5.7.1/PAMI/fuzzyCorrelatedPattern/basic/FCPGrowth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/fuzzyCorrelatedPattern/basic/__init__.py` & `pami-2024.5.7.1/PAMI/fuzzyCorrelatedPattern/basic/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/fuzzyCorrelatedPattern/basic/abstract.py` & `pami-2024.5.7.1/PAMI/fuzzyCorrelatedPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/fuzzyFrequentPattern/__init__.py` & `pami-2024.5.7.1/PAMI/fuzzyFrequentPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/fuzzyFrequentPattern/basic/FFIMiner.py` & `pami-2024.5.7.1/PAMI/fuzzyFrequentPattern/basic/FFIMiner.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/fuzzyFrequentPattern/basic/FFIMiner_old.py` & `pami-2024.5.7.1/PAMI/fuzzyFrequentPattern/basic/FFIMiner_old.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/fuzzyFrequentPattern/basic/abstract.py` & `pami-2024.5.7.1/PAMI/fuzzyFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/fuzzyGeoreferencedFrequentPattern/__init__.py` & `pami-2024.5.7.1/PAMI/fuzzyGeoreferencedFrequentPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/FFSPMiner.py` & `pami-2024.5.7.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/FFSPMiner.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/FFSPMiner_old.py` & `pami-2024.5.7.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/FFSPMiner_old.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/abstract.py` & `pami-2024.5.7.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/FGPFPMiner.py` & `pami-2024.5.7.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/FGPFPMiner.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/FGPFPMiner_old.py` & `pami-2024.5.7.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/FGPFPMiner_old.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/abstract.py` & `pami-2024.5.7.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/fuzzyPartialPeriodicPatterns/basic/F3PMiner.py` & `pami-2024.5.7.1/PAMI/fuzzyPartialPeriodicPatterns/basic/F3PMiner.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/fuzzyPartialPeriodicPatterns/basic/abstract.py` & `pami-2024.5.7.1/PAMI/fuzzyPartialPeriodicPatterns/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/fuzzyPeriodicFrequentPattern/__init__.py` & `pami-2024.5.7.1/PAMI/fuzzyPeriodicFrequentPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/fuzzyPeriodicFrequentPattern/basic/FPFPMiner.py` & `pami-2024.5.7.1/PAMI/fuzzyPeriodicFrequentPattern/basic/FPFPMiner.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/fuzzyPeriodicFrequentPattern/basic/FPFPMiner_old.py` & `pami-2024.5.7.1/PAMI/fuzzyPeriodicFrequentPattern/basic/FPFPMiner_old.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/fuzzyPeriodicFrequentPattern/basic/__init__.py` & `pami-2024.5.7.1/PAMI/fuzzyPeriodicFrequentPattern/basic/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/fuzzyPeriodicFrequentPattern/basic/abstract.py` & `pami-2024.5.7.1/PAMI/fuzzyPeriodicFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/geoReferencedPeriodicFrequentPattern/basic/GPFPMiner.py` & `pami-2024.5.7.1/PAMI/geoReferencedPeriodicFrequentPattern/basic/GPFPMiner.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/geoReferencedPeriodicFrequentPattern/basic/abstract.py` & `pami-2024.5.7.1/PAMI/geoReferencedPeriodicFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/georeferencedFrequentPattern/__init__.py` & `pami-2024.5.7.1/PAMI/georeferencedFrequentPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/georeferencedFrequentPattern/basic/FSPGrowth.py` & `pami-2024.5.7.1/PAMI/georeferencedFrequentPattern/basic/FSPGrowth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/georeferencedFrequentPattern/basic/SpatialECLAT.py` & `pami-2024.5.7.1/PAMI/georeferencedFrequentPattern/basic/SpatialECLAT.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/georeferencedFrequentPattern/basic/abstract.py` & `pami-2024.5.7.1/PAMI/georeferencedFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/georeferencedFrequentSequencePattern/abstract.py` & `pami-2024.5.7.1/PAMI/georeferencedFrequentSequencePattern/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/georeferencedPartialPeriodicPattern/basic/STEclat.py` & `pami-2024.5.7.1/PAMI/georeferencedPartialPeriodicPattern/basic/STEclat.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/georeferencedPartialPeriodicPattern/basic/abstract.py` & `pami-2024.5.7.1/PAMI/georeferencedPartialPeriodicPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/highUtilityFrequentPattern/basic/HUFIM.py` & `pami-2024.5.7.1/PAMI/highUtilityFrequentPattern/basic/HUFIM.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/highUtilityFrequentPattern/basic/abstract.py` & `pami-2024.5.7.1/PAMI/highUtilityFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/highUtilityGeoreferencedFrequentPattern/__init__.py` & `pami-2024.5.7.1/PAMI/highUtilityGeoreferencedFrequentPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/highUtilityGeoreferencedFrequentPattern/basic/SHUFIM.py` & `pami-2024.5.7.1/PAMI/highUtilityGeoreferencedFrequentPattern/basic/SHUFIM.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/highUtilityGeoreferencedFrequentPattern/basic/abstract.py` & `pami-2024.5.7.1/PAMI/highUtilityGeoreferencedFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/highUtilityPattern/basic/EFIM.py` & `pami-2024.5.7.1/PAMI/highUtilityPattern/basic/EFIM.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/highUtilityPattern/basic/HMiner.py` & `pami-2024.5.7.1/PAMI/highUtilityPattern/basic/HMiner.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/highUtilityPattern/basic/UPGrowth.py` & `pami-2024.5.7.1/PAMI/highUtilityPattern/basic/UPGrowth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/highUtilityPattern/basic/abstract.py` & `pami-2024.5.7.1/PAMI/highUtilityPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/highUtilityPattern/basic/efimParallel.py` & `pami-2024.5.7.1/PAMI/highUtilityPattern/basic/efimParallel.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/highUtilityPattern/parallel/abstract.py` & `pami-2024.5.7.1/PAMI/highUtilityPattern/parallel/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/highUtilityPattern/parallel/efimparallel.py` & `pami-2024.5.7.1/PAMI/highUtilityPattern/parallel/efimparallel.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/highUtilityPatternsInStreams/HUPMS.py` & `pami-2024.5.7.1/PAMI/highUtilityPatternsInStreams/HUPMS.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/highUtilityPatternsInStreams/SHUGrowth.py` & `pami-2024.5.7.1/PAMI/highUtilityPatternsInStreams/SHUGrowth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/highUtilityPatternsInStreams/abstract.py` & `pami-2024.5.7.1/PAMI/highUtilityPatternsInStreams/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/highUtilitySpatialPattern/__init__.py` & `pami-2024.5.7.1/PAMI/highUtilitySpatialPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/highUtilitySpatialPattern/abstract.py` & `pami-2024.5.7.1/PAMI/highUtilitySpatialPattern/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/highUtilitySpatialPattern/basic/HDSHUIM.py` & `pami-2024.5.7.1/PAMI/highUtilitySpatialPattern/basic/HDSHUIM.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/highUtilitySpatialPattern/basic/SHUIM.py` & `pami-2024.5.7.1/PAMI/highUtilitySpatialPattern/basic/SHUIM.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/highUtilitySpatialPattern/basic/abstract.py` & `pami-2024.5.7.1/PAMI/highUtilitySpatialPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/highUtilitySpatialPattern/topk/TKSHUIM.py` & `pami-2024.5.7.1/PAMI/highUtilitySpatialPattern/topk/TKSHUIM.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/highUtilitySpatialPattern/topk/abstract.py` & `pami-2024.5.7.1/PAMI/highUtilitySpatialPattern/topk/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/localPeriodicPattern/basic/LPPGrowth.py` & `pami-2024.5.7.1/PAMI/localPeriodicPattern/basic/LPPGrowth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/localPeriodicPattern/basic/LPPMBreadth.py` & `pami-2024.5.7.1/PAMI/localPeriodicPattern/basic/LPPMBreadth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/localPeriodicPattern/basic/LPPMDepth.py` & `pami-2024.5.7.1/PAMI/localPeriodicPattern/basic/LPPMDepth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/localPeriodicPattern/basic/abstract.py` & `pami-2024.5.7.1/PAMI/localPeriodicPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/CFPGrowth.py` & `pami-2024.5.7.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/CFPGrowth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/CFPGrowthPlus.py` & `pami-2024.5.7.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/CFPGrowthPlus.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/abstract.py` & `pami-2024.5.7.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/partialPeriodicFrequentPattern/basic/GPFgrowth.py` & `pami-2024.5.7.1/PAMI/partialPeriodicFrequentPattern/basic/GPFgrowth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/partialPeriodicFrequentPattern/basic/PPF_DFS.py` & `pami-2024.5.7.1/PAMI/partialPeriodicFrequentPattern/basic/PPF_DFS.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/partialPeriodicFrequentPattern/basic/abstract.py` & `pami-2024.5.7.1/PAMI/partialPeriodicFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/partialPeriodicPattern/__init__.py` & `pami-2024.5.7.1/PAMI/partialPeriodicPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/partialPeriodicPattern/basic/GThreePGrowth.py` & `pami-2024.5.7.1/PAMI/partialPeriodicPattern/basic/GThreePGrowth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/partialPeriodicPattern/basic/Gabstract.py` & `pami-2024.5.7.1/PAMI/partialPeriodicPattern/basic/Gabstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/partialPeriodicPattern/basic/PPPGrowth.py` & `pami-2024.5.7.1/PAMI/partialPeriodicPattern/basic/PPPGrowth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/partialPeriodicPattern/basic/PPP_ECLAT.py` & `pami-2024.5.7.1/PAMI/partialPeriodicPattern/basic/PPP_ECLAT.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/partialPeriodicPattern/basic/__init__.py` & `pami-2024.5.7.1/PAMI/partialPeriodicPattern/basic/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/partialPeriodicPattern/basic/abstract.py` & `pami-2024.5.7.1/PAMI/partialPeriodicPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/partialPeriodicPattern/closed/PPPClose.py` & `pami-2024.5.7.1/PAMI/partialPeriodicPattern/closed/PPPClose.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/partialPeriodicPattern/closed/abstract.py` & `pami-2024.5.7.1/PAMI/partialPeriodicPattern/closed/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/partialPeriodicPattern/maximal/Max3PGrowth.py` & `pami-2024.5.7.1/PAMI/partialPeriodicPattern/maximal/Max3PGrowth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/partialPeriodicPattern/maximal/__init__.py` & `pami-2024.5.7.1/PAMI/partialPeriodicPattern/maximal/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/partialPeriodicPattern/maximal/abstract.py` & `pami-2024.5.7.1/PAMI/partialPeriodicPattern/maximal/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/partialPeriodicPattern/pyspark/__init__.py` & `pami-2024.5.7.1/PAMI/partialPeriodicPattern/pyspark/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/partialPeriodicPattern/pyspark/abstract.py` & `pami-2024.5.7.1/PAMI/partialPeriodicPattern/pyspark/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/partialPeriodicPattern/pyspark/parallel3PGrowth.py` & `pami-2024.5.7.1/PAMI/partialPeriodicPattern/pyspark/parallel3PGrowth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/partialPeriodicPattern/topk/abstract.py` & `pami-2024.5.7.1/PAMI/partialPeriodicPattern/topk/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/partialPeriodicPattern/topk/k3PMiner.py` & `pami-2024.5.7.1/PAMI/partialPeriodicPattern/topk/k3PMiner.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/partialPeriodicPatternInMultipleTimeSeries/PPGrowth.py` & `pami-2024.5.7.1/PAMI/partialPeriodicPatternInMultipleTimeSeries/PPGrowth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/partialPeriodicPatternInMultipleTimeSeries/abstract.py` & `pami-2024.5.7.1/PAMI/partialPeriodicPatternInMultipleTimeSeries/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/periodicCorrelatedPattern/basic/EPCPGrowth.py` & `pami-2024.5.7.1/PAMI/periodicCorrelatedPattern/basic/EPCPGrowth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/periodicCorrelatedPattern/basic/abstract.py` & `pami-2024.5.7.1/PAMI/periodicCorrelatedPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/periodicFrequentPattern/__init__.py` & `pami-2024.5.7.1/PAMI/periodicFrequentPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/periodicFrequentPattern/basic/PFECLAT.py` & `pami-2024.5.7.1/PAMI/uncertainFaultTolerantFrequentPattern/VBFTMine.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,45 +1,48 @@
-# PFECLAT is the fundamental approach to mine the periodic-frequent patterns.
+# VBFTMine is one of the fundamental algorithm to discover fault-tolerant frequent patterns in an uncertain transactional database based on bitset representation.
 #
 # **Importing this algorithm into a python program**
 #
-#             from PAMI.periodicFrequentPattern.basic import PFECLAT as alg
+#             import PAMI.uncertainFaultTolerantFrequentPattern.basic.VBFTMine as alg
 #
 #             iFile = 'sampleDB.txt'
 #
 #             minSup = 10  # can also be specified between 0 and 1
 #
-#             maxPer = 20 # can also be specified between 0 and 1
+#             itemSup = 2  # can also be specified between 0 and 1
 #
-#             obj = alg.PFECLAT(iFile, minSup, maxPer)
+#             minLength = 3 # can also be specified between 0 and 1
+#
+#             faultTolerance = 2 # can also be specified between 0 and 1
+#
+#             obj = alg.VBFTMine(iFile, minSup, itemSup, minLength, faultTolerance)
 #
 #             obj.mine()
 #
-#             periodicFrequentPatterns = obj.getPatterns()
+#             faultTolerantFrequentPattern = obj.getPatterns()
 #
-#             print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
+#             print("Total number of Fault Tolerant Frequent Patterns:", len(faultTolerantFrequentPattern))
 #
-#             obj.save("periodicFrequentPatterns")
+#             obj.save(oFile)
 #
-#             Df = obj.getPatternsAsDataFrame()
+#             Df = obj.getPatternInDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
 #             print("Total Memory in USS:", memUSS)
 #
 #             memRSS = obj.getMemoryRSS()
 #
 #             print("Total Memory in RSS", memRSS)
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
-
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
@@ -47,106 +50,120 @@
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
-     Copyright (C)  2021 Rage Uday Kiran
-
 """
 
 import pandas as pd
 from deprecated import deprecated
-import numpy as np
-
-from PAMI.periodicFrequentPattern.basic import abstract as _ab
 
+import numpy as _np
+from PAMI.faultTolerantFrequentPattern.basic import abstract as _ab
 
-class PFECLAT(_ab._periodicFrequentPatterns):
+class VBFTMine(_ab._faultTolerantFrequentPatterns):
     """
     About this algorithm
     ====================
+    
+    :Description:  VBFTMine is one of the fundamental algorithm to discover fault tolerant frequent patterns in an uncertain transactional database based on
+                   bitset representation.
+                   This program employs apriori property (or downward closure property) to  reduce the search space effectively.
+
+    :Reference:   Koh, JL., Yo, PW. (2005). An Efficient Approach for Mining Fault-Tolerant Frequent Patterns Based on Bit Vector Representations.
+            In:   Zhou, L., Ooi, B.C., Meng, X. (eds) Database Systems for Advanced Applications. DASFAA 2005. Lecture Notes in Computer Science,
+                  vol 3453. Springer, Berlin, Heidelberg. https://doi.org/10.1007/11408079_51
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of uncertain Fault Tolerant FrequentFrequent Patterns
+    :param  oFile: str :
+                   Name of the output file to store complete set of uncertain Fault Tolerant FrequentFrequent Patterns
+    :param  minSup: float or int or str :
+                   The user can specify minSup either in count or proportion of database size.
+                   If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+                   Otherwise, it will be treated as float.
+                   Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+    :param  itemSup: int or float :
+                    Frequency of an item
+    :param minLength: int
+                    minimum length of a pattern
+    :param faultTolerance: int :
+                    The ability of a pattern mining algorithm to handle errors or inconsistencies in the data without completely failing or producing incorrect results.
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+
+    :Attributes:
+
+        startTime : float
+          To record the start time of the mining process
+
+        endTime : float
+          To record the completion time of the mining process
+
+        finalPatterns : dict
+          Storing the complete set of patterns in a dictionary variable
+
+        memoryUSS : float
+          To store the total amount of USS memory consumed by the program
 
-    :**Description**:   PFECLAT is the fundamental approach to mine the periodic-frequent patterns.
+        memoryRSS : float
+          To store the total amount of RSS memory consumed by the program
 
-    :**Reference**:   P. Ravikumar, P.Likhitha, R. Uday kiran, Y. Watanobe, and Koji Zettsu, "Towards efficient discovery of
-                      periodic-frequent patterns in columnar temporal databases", 2021 IEA/AIE.
+        Database : list
+          To store the transactions of a database in list
 
-    :**Parameters**:    - **iFile** (*str or URL or dataFrame*) -- *Name of the Input file to mine complete set of frequent patterns.*
-                        - **oFile** (*str*) -- *Name of the output file to store complete set of frequent patterns.*
-                        - **minSup** (*int or float or str*) -- *The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.*
-                        - **maxPer** (*int or float or str*) -- *The user can specify maxPer either in count or proportion of database size. It controls the maximum number of transactions in which any two items within a pattern can reappear.*
-                        - **sep** (*str*) -- *This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.*
-
-    :**Attributes**:    - **startTime** (*float*) -- *To record the start time of the mining process.*
-                        - **endTime** (*float*) -- *To record the completion time of the mining process.*
-                        - **finalPatterns** (*dict*) -- *Storing the complete set of patterns in a dictionary variable.*
-                        - **memoryUSS** (*float*) -- *To store the total amount of USS memory consumed by the program.*
-                        - **memoryRSS** (*float*) -- *To store the total amount of RSS memory consumed by the program.*
-                        - **Database** (*list*) -- *To store the transactions of a database in list.*
-                        - **mapSupport** (*Dictionary*) -- *To maintain the information of item and their frequency.*
-                        - **lno** (*int*) -- *It represents the total no of transactions*
-                        - **tree** (*class*) -- *it represents the Tree class.*
-                        - **itemSetCount** (*int*) -- *it represents the total no of patterns.*
-                        - **tidList** (*dict*) -- *stores the timestamps of an item.*
-                        - **hashing** (*dict*) -- *stores the patterns with their support to check for the closed property.*
-
-    :**Methods**:       - **startMine()** -- *Mining process will start from here.*
-                        - **getPatterns()** -- *Complete set of patterns will be retrieved with this function.*
-                        - **save(oFile)** -- *Complete set of periodic-frequent patterns will be loaded in to a output file.*
-                        - **getPatternsAsDataFrame()** -- *Complete set of periodic-frequent patterns will be loaded in to a dataframe.*
-                        - **getMemoryUSS()** -- *Total amount of USS memory consumed by the mining process will be retrieved from this function.*
-                        - **getMemoryRSS()** -- *Total amount of RSS memory consumed by the mining process will be retrieved from this function.*
-                        - **getRuntime()** -- *Total amount of runtime taken by the mining process will be retrieved from this function.*
-                        - **creatingOneItemSets()** -- *Scan the database and store the items with their timestamps which are periodic frequent.*
-                        - **getPeriodAndSupport()** -- * Calculates the support and period for a list of timestamps.*
-                        - **Generation()** -- *Used to implement prefix class equivalence method to generate the periodic patterns recursively*
 
     Execution methods
     =================
 
     **Terminal command**
 
+
     .. code-block:: console
 
        Format:
 
-       (.venv) $ python3 PFECLAT.py <inputFile> <outputFile> <minSup> <maxPer>
+       (.venv) $ python3 VBFTMine.py <inputFile> <outputFile> <minSup> <itemSup> <minLength> <faultTolerance>
 
-       Example usage:
+       Examples usage:
 
-       (.venv) $ python3 PFECLAT.py sampleDB.txt patterns.txt 10.0 20.0
+       (.venv) $ python3 VBFTMine.py sampleDB.txt patterns.txt 10.0 3.0 3 1
 
-    .. note:: minSup will be considered in percentage of database transactions
+    .. note:: minSup will be considered in times of minSup and count of database transactions
 
 
     **Calling from a python program**
 
     .. code-block:: python
-
-            from PAMI.periodicFrequentPattern.basic import PFECLAT as alg
+    
+            import PAMI.faultTolerantFrequentPattern.basic.VBFTMine as alg
 
             iFile = 'sampleDB.txt'
 
             minSup = 10  # can also be specified between 0 and 1
 
-            maxPer = 20 # can also be specified between 0 and 1
+            itemSup = 2  # can also be specified between 0 and 1
+
+            minLength = 3 # can also be specified between 0 and 1
+
+            faultTolerance = 2 # can also be specified between 0 and 1
 
-            obj = alg.PFECLAT(iFile, minSup, maxPer)
+            obj = alg.VBFTMine(iFile, minSup, itemSup, minLength, faultTolerance)
 
             obj.mine()
 
-            periodicFrequentPatterns = obj.getPatterns()
+            faultTolerantFrequentPattern = obj.getPatterns()
 
-            print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
+            print("Total number of Fault Tolerant Frequent Patterns:", len(faultTolerantFrequentPattern))
 
-            obj.save("periodicFrequentPatterns")
+            obj.save(oFile)
 
-            Df = obj.getPatternsAsDataFrame()
+            Df = obj.getPatternInDataFrame()
 
             memUSS = obj.getMemoryUSS()
 
             print("Total Memory in USS:", memUSS)
 
             memRSS = obj.getMemoryRSS()
 
@@ -155,277 +172,337 @@
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
     Credits
     =======
 
-    The complete program was written by P. Likhitha  and revised by Tarun Sreepada under the supervision of Professor Rage Uday Kiran.
+           The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
 
     """
-    
+
+    _minSup = float()
+    _itemSup = float()
+    _minLength = int()
+    _faultTolerance = int()
+    _startTime = float()
+    _endTime = float()
+    _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
-    _dbSize = None
-    _Database = None
-    _minSup = str()
-    _maxPer = str()
-    _tidSet = set()
-    _finalPatterns = {}
-    _startTime = None
-    _endTime = None
+    _plist = []
     _memoryUSS = float()
     _memoryRSS = float()
+    _Database = []
+    _mapSupport = {}
 
-    def _convert(self, value) -> float:
-        """
-        To convert the given user specified value
-
-        :param value: user specified value
-        :type value: int or float or str
-        :return: converted value
-        :rtype: int or float
+    def _creatingItemSets(self):
         """
-        if type(value) is int:
-            value = int(value)
-        if type(value) is float:
-            value = (self._dbSize * value)
-        if type(value) is str:
-            if '.' in value:
-                value = float(value)
-                value = (self._dbSize * value)
-            else:
-                value = int(value)
-        return value
-
-    def _creatingItemSets(self) -> None:
-        """
-
         Storing the complete transactions of the database/input file in a database variable
-
-        :return: None
         """
         self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
-            data, ts = [], []
+            temp = []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
-            if 'TS' in i:
-                ts = self._iFile['TS'].tolist()
             if 'Transactions' in i:
-                data = self._iFile['Transactions'].tolist()
-            for i in range(len(data)):
-                tr = [ts[i][0]]
-                tr = tr + data[i]
-                self._Database.append(tr)
+                temp = self._iFile['Transactions'].tolist()
 
+            for k in temp:
+                self._Database.append(set(k))
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    self._Database.append(temp)
+                    self._Database.append(set(temp))
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
                             line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
-                            self._Database.append(temp)
+                            for i in temp:
+                                if i not in self._plist:
+                                    self._plist.append(i)
+                            self._Database.append(set(temp))
                 except IOError:
                     print("File Not Found")
                     quit()
 
-    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
-    def startMine(self) -> None:
-        self.mine()
-        # self._startTime = _ab._time.time()
-        # self._finalPatterns = {}
-        # frequentSets = self._creatingOneItemSets()
-        # self._generateEclat(frequentSets)
-        # self._endTime = _ab._time.time()
-        # process = _ab._psutil.Process(_ab._os.getpid())
-        # self._memoryRSS = float()
-        # self._memoryUSS = float()
-        # self._memoryUSS = process.memory_full_info().uss
-        # self._memoryRSS = process.memory_info().rss
-        # print("Periodic-Frequent patterns were generated successfully using PFECLAT algorithm ")
-
-    def _getMaxPer(self, arr, maxTS):
-        arr = np.append(list(arr), [0, maxTS])
-        arr = np.sort(arr)
-        arr = np.diff(arr)
+    def _convert(self, value):
+        """
+        To convert the user specified minSup value
 
-        return np.max(arr)
+        :param value: user specified minSup value
+        :return: converted type
+        """
+        if type(value) is int:
+            value = int(value)
+        if type(value) is float:
+            value = (len(self._Database) * value)
+        if type(value) is str:
+            if '.' in value:
+                value = float(value)
+                value = (len(self._Database) * value)
+            else:
+                value = int(value)
+        return value
+
+    def _Count(self, tids):
+        """
+        Count the occurrences of 1s in the given list of transaction IDs.
 
-    def mine(self) -> None:
+        :param tids: List of transaction IDs.
+        :type tids: List[int]
+        :return: Count of occurrences of 1s in the list.
+        :rtype: int
+        """
+        count = 0
+        for i in tids:
+            if i == 1:
+                count += 1
+        return count
+
+    def _save(self, prefix, suffix, tidsetx):
+        """
+        Save the pattern with its support count if it meets the fault tolerance criteria.
+
+        :param prefix: Prefix part of the pattern.
+        :type prefix: list
+        :param suffix: Suffix part of the pattern.
+        :type suffix: list
+        :param tidsetx: Transaction IDs associated with the pattern.
+        :type tidsetx: list
+        :return: None
         """
-        Mining process will start from this function
+        if (prefix == None):
+            prefix = suffix
+        else:
+            prefix = prefix + suffix
+        prefix = list(set(prefix))
+        prefix.sort()
+        val = self._Count(tidsetx)
+        if len(prefix) > self._faultTolerance:
+            self._finalPatterns[tuple(prefix)] = val
+
+    def _processEquivalenceClass(self, prefix, itemsets, tidsets):
+        """
+        Process the equivalence class to generate frequent patterns.
+
+        :param prefix: Prefix part of the pattern.
+        :type prefix: list.
+        :param itemsets: List of itemsets in the equivalence class.
+        :type itemsets: list.
+        :param tidsets: List of transaction IDs associated with each itemset.
+        :type tidsets: list
         :return: None
         """
-        self._startTime = _ab._time.time()
-        self._finalPatterns = {}
-        frequentSets = self._creatingItemSets()
-
-        items = {}
-        maxTS = 0
-        for line in self._Database:
-            index = int(line[0])
-            maxTS = max(maxTS, index)
-            for item in line[1:]:
-                if tuple([item]) not in items:
-                    items[tuple([item])] = set()
-                items[tuple([item])].add(index)
-
-        self._dbSize = maxTS
+        if (len(itemsets) == 1):
+            i = itemsets[0]
+            tidi = tidsets[0]
+            self._save(prefix, [i], tidi)
+            return
+        for i in range(len(itemsets)):
+            itemx = itemsets[i]
+            if (itemx == None):
+                continue
+            tidsetx = tidsets[i]
+            classItemsets = []
+            classtidsets = []
+            itemsetx = [itemx]
+            for j in range(i + 1, len(itemsets)):
+                itemj = itemsets[j]
+                tidsetj = tidsets[j]
+                y = list(_np.array(tidsetx) & _np.array(tidsetj))
+                total = self._Count(y)
+                if total >= self._minSup:
+                    classItemsets.append(itemj)
+                    classtidsets.append(y)
+            if (len(classItemsets) > 0):
+                newprefix = list(set(itemsetx)) + prefix
+                self._processEquivalenceClass(newprefix, classItemsets, classtidsets)
+            self._save(prefix, list(set(itemsetx)), tidsetx)
+
+    def _oneLengthFrequentItems(self):
+        """
+        To calculate the one Length items
+        """
+        Vector = {}
+        items = []
+        for i in self._Database:
+            for j in self._plist:
+                count = 0
+                if j in i:
+                    count = 1
+                if j in Vector:
+                    Vector[j].append(count)
+                else:
+                    Vector[j] = [count]
+        for x, y in Vector.items():
+            v = self._Count(y)
+            if v >= self._itemSup:
+                items.append(x)
+        return Vector, items
+
+    @deprecated(
+        "It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
+    def startMine(self):
+        """
+        Frequent pattern mining process will start from here
+        """
+        self.mine()
 
+    def mine(self):
+        """
+        Frequent pattern mining process will start from here
+        """
+        self._Database = []
+        self._startTime = _ab._time.time()
+        self._creatingItemSets()
         self._minSup = self._convert(self._minSup)
-        self._maxPer = self._convert(self._maxPer)
-        minSup = self._minSup
-        maxPer = self._maxPer
-
-
-        items = {k: v for k, v in items.items() if len(v) >= minSup}
-        items = {k: v for k, v in sorted(items.items(), key = lambda x: len(x[1]), reverse = True)}
-
-        keys = []
-        for item in list(items.keys()):
-            per = self._getMaxPer(items[item], maxTS)
-            if per <= maxPer:
-                keys.append(item)
-                self._finalPatterns[item] = [len(items[item]), per, set(items[item])]
-
-        while keys:
-            newKeys = []
-            for i in range(len(keys)):
-                for j in range(i + 1, len(keys)):
-                    if keys[i][:-1] == keys[j][:-1] and keys[i][-1] != keys[j][-1]:
-                        # print(keys[i], keys[j])
-                        newKey = tuple(keys[i] + (keys[j][-1],))
-                        intersect = items[keys[i]].intersection(items[keys[j]])
-                        per = self._getMaxPer(intersect, maxTS)
-                        sup = len(intersect)
-                        if sup >= minSup and per <= maxPer:
-                            items[newKey] = intersect
-                            newKeys.append(newKey)
-                            self._finalPatterns[newKey] = [sup, per, set(intersect)]
-                    else:
-                        break
-            keys = newKeys
-
-        newPattern = {}
-        for k, v in self._finalPatterns.items():
-            newPattern["\t".join([str(x) for x in k])] = v
-
-        self._finalPatterns = newPattern
-
-        # self._generateEclat(frequentSets)
+        self._itemSup = self._convert(self._itemSup)
+        self._minLength = int(self._minLength)
+        self._faultTolerance = int(self._faultTolerance)
+        Vector, plist = self._oneLengthFrequentItems()
+        for i in range(len(plist)):
+            itemx = plist[i]
+            tidsetx = Vector[itemx]
+            itemsetx = [itemx]
+            itemsets = []
+            tidsets = []
+            for j in range(i + 1, len(plist)):
+                itemj = plist[j]
+                tidsetj = Vector[itemj]
+                y1 = list(_np.array(tidsetx) | _np.array(tidsetj))
+                total = self._Count(y1)
+                if total >= self._minSup:
+                    itemsets.append(itemj)
+                    tidsets.append(y1)
+            if (len(itemsets) > 0):
+                self._processEquivalenceClass(itemsetx, itemsets, tidsets)
+            self._save(None, itemsetx, tidsetx)
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryRSS = float()
         self._memoryUSS = float()
+        self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
-        print("Periodic-Frequent patterns were generated successfully using PFECLAT algorithm ")
+        print("Fault-Tolerant Frequent patterns were generated successfully using VBFTMine algorithm ")
 
-    def getMemoryUSS(self) -> float:
+    def getMemoryUSS(self):
         """
+
         Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
-    def getMemoryRSS(self) -> float:
+    def getMemoryRSS(self):
         """
+
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
-    def getRuntime(self) -> float:
+    def getRuntime(self):
         """
+
         Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
-    def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
+    def getPatternsAsDataFrame(self):
         """
-        Storing final periodic-frequent patterns in a dataframe
 
-        :return: returning periodic-frequent patterns in a dataframe
+        Storing final frequent patterns in a dataframe
+
+        :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
-        dataframe = {}
+        dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a, b[0], b[1]])
-            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
-        return dataframe
+            s = str()
+            for i in a:
+                s = s + i + ' '
+            data.append([s, b])
+            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
+        # dataFrame = dataFrame.replace(r'\r+|\n+|\t+',' ', regex=True)
+        return dataFrame
 
-    def save(self, outFile: str) -> None:
+    def save(self, outFile):
         """
-        Complete set of periodic-frequent patterns will be loaded in to a output file
+
+        Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
-        :type outFile: csv file
-        :return: None
+        :type outFile: file
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            s1 = x + ":" + str(y[0]) + ":" + str(y[1])
-            #s1 = x.replace(' ', '\t') + ":" + str(y[0]) + ":" + str(y[1])
+            s = str()
+            for i in x:
+                s = s + i + '\t'
+            s1 = s.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
-    def getPatterns(self) -> dict:
+    def getPatterns(self):
         """
-        Function to send the set of periodic-frequent patterns after completion of the mining process
 
-        :return: returning periodic-frequent patterns
+        Function to send the set of frequent patterns after completion of the mining process
+
+        :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
-    def printResults(self) -> None:
+    def printResults(self):
         """
         This function is used to print the results
-        :return: None
         """
-        print("Total number of Periodic Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:",  self.getRuntime())
-                    
+        print("Total ExecutionTime in ms:", self.getRuntime())
+
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
-        if len(_ab._sys.argv) == 6:
-            _ap = PFECLAT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
-        if len(_ab._sys.argv) == 5:
-            _ap = PFECLAT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+    if len(_ab._sys.argv) == 7 or len(_ab._sys.argv) == 8:
+        if len(_ab._sys.argv) == 8:
+            _ap = VBFTMine(_ab._sys.argv[1], _ab._sys.argv[3],  _ab._sys.argv[4],
+                            _ab._sys.argv[5], _ab._sys.argv[6], _ab._sys.argv[7],)
+        if len(_ab._sys.argv) == 7:
+            _ap = VBFTMine(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5], _ab._sys.argv[6])
         _ap.startMine()
         _ap.mine()
-        print("Total number of Periodic-Frequent Patterns:", len(_ap.getPatterns()))
+        print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
+        _ap = VBFTMine('/Users/Likhitha/Downloads/fault/sample4.txt', 5, 3, 2, 1, ' ')
+        _ap.startMine()
+        _ap.printResults()
+        print(_ap.getPatternsAsDataFrame())
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.5.28.1/PAMI/periodicFrequentPattern/basic/PFPGrowth.py` & `pami-2024.5.7.1/PAMI/uncertainFrequentPattern/basic/TubeS.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,209 +1,491 @@
-# PFPGrowth is one of the fundamental algorithm to discover periodic-frequent patterns in a transactional database.
+# TubeS is one of the fastest algorithm to discover frequent patterns in a uncertain transactional database.
 #
 # **Importing this algorithm into a python program**
 #
-#             from PAMI.periodicFrequentPattern.basic import PFPGrowth as alg
+#             from PAMI.uncertainFrequentPattern.basic import TubeS as alg
 #
 #             iFile = 'sampleDB.txt'
 #
 #             minSup = 10  # can also be specified between 0 and 1
 #
-#             maxPer = 20 # can also be specified between 0 and 1
-#
-#             obj = alg.PFPGrowth(iFile, minSup, maxPer)
+#             obj = alg.TubeS(iFile, minSup)
 #
 #             obj.mine()
 #
-#             periodicFrequentPatterns = obj.getPatterns()
+#             frequentPatterns = obj.getPatterns()
 #
-#             print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
+#             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
-#             obj.save("periodicFrequentPatterns")
+#             obj.save(oFile)
 #
 #             Df = obj.getPatternsAsDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
-#             print("Total Memory in USS:", memUSS)
+#              print("Total Memory in USS:", memUSS)
 #
-#             memRSS = obj.getMemoryRSS()
+#              memRSS = obj.getMemoryRSS()
 #
-#             print("Total Memory in RSS", memRSS)
+#              print("Total Memory in RSS", memRSS)
 #
-#             run = obj.getRuntime()
+#              run = obj.getRuntime()
 #
-#             print("Total ExecutionTime in seconds:", run)
+#              print("Total ExecutionTime in seconds:", run)
 #
 
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
-
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
-
+     
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
-
+     
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
-     Copyright (C)  2021 Rage Uday Kiran
-
 """
 
-from PAMI.periodicFrequentPattern.basic import abstract as _ab
-from typing import Dict, Tuple
-import pandas as pd
-from deprecated import deprecated
-import numpy as np
+from PAMI.uncertainFrequentPattern.basic import abstract as _fp
+import deprecated
 
-_maxPer = float()
 _minSup = float()
-_lno = int()
+_fp._sys.setrecursionlimit(20000)
+_finalPatterns = {}
+
+
+class _Item:
+    """
+    A class used to represent the item with probability in transaction of dataset
+
+    :Attributes:
+
+        item : int or word
+          Represents the name of the item
+
+        probability : float
+          Represent the existential probability(likelihood presence) of an item
+    """
+
+    def __init__(self, item, probability):
+        self.item = item
+        self.probability = probability
 
 
 class _Node(object):
     """
     A class used to represent the node of frequentPatternTree
 
-    :**Attributes**:    - **item** (*int or None*) -- *Storing item of a node.*
-                        - **timeStamps** (*list*) -- *To maintain the timestamps of a database at the end of the branch.*
-                        - **parent** (*list*) -- *To maintain the parent of every node.*
-                        - **children** (*list*) -- *To maintain the children of a node.*
+    :Attributes:
+
+        item : int
+            storing item of a node
+
+        probability : int
+            To maintain the expected support of node
+
+        parent : node
+            To maintain the parent of every node
+
+        children : list
+            To maintain the children of node
+
+    :Methods:
 
-    :**Methods**:    -**addChild(itemName)** -- *Storing the children to their respective parent nodes.*
+        addChild(itemName)
+             storing the children to their respective parent nodes
     """
 
-    def __init__(self, item, locations, parent=None):
+    def __init__(self, item, children):
         self.item = item
-        self.locations = locations
-        self.parent = parent
-        self.children = {}
-
-    def addChild(self, item, locations):
-        """
-        This method takes an item and locations as input, adds a new child node
-        if the item does not already exist among the current node's children, or
-        updates the locations of the existing child node if the item is already present.
-
-        :param item: Represents the distinct item to be added as a child node.
-        :type item: Any
-        :param locations: Represents the locations associated with the item.
-        :type locations: list
-        :return: The child node associated with the item.
-        :rtype: _Node
-        """
-        if item not in self.children:
-            self.children[item] = _Node(item, locations, self)
-        else:
-            self.children[item].locations = locations + self.children[item].locations
-            
-        return self.children[item]
-
-    def traverse(self):
-        """
-        This method constructs a transaction by traversing from the current node to the root node, collecting items along the way.
-
-        :return: A tuple containing the transaction and the locations associated with the current node.
-        :rtype: tuple(list, Any)
-        """
-        transaction = []
-        locs = self.locations
-        node = self.parent
-        while node.parent is not None:
-            transaction.append(node.item)
-            node = node.parent
-        return transaction[::-1], locs
+        self.probability = 1
+        self.secondProbability = 1
+        self.children = children
+        self.parent = None
+
+    def addChild(self, node):
+        """
+        This function is used to add child
+        """
+        self.children[node.item] = node
+        node.parent = self
+
+
+def Second(transaction, i):
+    """
+    To calculate the second probability of a node in transaction
+
+    :param transaction: transaction in a database
+    :param i: index of item in transaction
+    :return: second probability of a node
+    """
+    temp = []
+    for j in range(0, i):
+        temp.append(transaction[j].probability)
+    l1 = max(temp)
+    temp.remove(l1)
+    l2 = max(temp)
+    return l2 * l2
+
+
+def printTree(root):
+    """
+    To print the tree with root node through recursion
+
+    :param root: root node of  tree
+    :return: details of tree
+    """
+    for x, y in root.children.items():
+        print(x, y.item, y.probability, y.parent.item, y.tids, y.secondProbability)
+        printTree(y)
+
+
+class _Tree(object):
+    """
+    A class used to represent the frequentPatternGrowth tree structure
+
+    :Attributes:
+
+        root : Node
+          Represents the root node of the tree
 
+        summaries : dictionary
+          storing the nodes with same item name
+
+        info : dictionary
+          stores the support of items
+
+    :Methods:
+        addTransaction(transaction)
+            creating transaction as a branch in frequentPatternTree
+        addConditionalTransaction(prefixPaths, supportOfItems)
+            construct the conditional tree for prefix paths
+        conditionalPatterns(Node)
+            generates the conditional patterns from tree for specific node
+        conditionalTransactions(prefixPaths,Support)
+            takes the prefixPath of a node and support at child of the path and extract the frequent items from
+            prefixPaths and generates prefixPaths with items which are frequent
+        removeNode(Node)
+            removes the node from tree once after generating all the patterns respective to the node
+        generate_patterns(Node)
+            starts from the root node of the tree and mines the frequent patterns
+            """
+
+    def __init__(self):
+        self.root = _Node(None, {})
+        self.summaries = {}
+        self.info = {}
+
+    def addTransaction(self, transaction):
+        """
+        Adding transaction into tree
+
+        :param transaction : it represents the one transactions in database
+        :type transaction : list
+        """
+        currentNode = self.root
+        k = 0
+        for i in range(len(transaction)):
+            k += 1
+            if transaction[i].item not in currentNode.children:
+                newNode = _Node(transaction[i].item, {})
+                newNode.k = k
+                if k >= 3:
+                    newNode.secondProbability = Second(transaction, i)
+                l1 = i - 1
+                temp = []
+                while l1 >= 0:
+                    temp.append(transaction[l1].probability)
+                    l1 -= 1
+                if len(temp) == 0:
+                    newNode.probability = round(transaction[i].probability, 2)
+                else:
+                    newNode.probability = round(max(temp) * transaction[i].probability, 2)
+                currentNode.addChild(newNode)
+                if transaction[i].item in self.summaries:
+                    self.summaries[transaction[i].item].append(newNode)
+                else:
+                    self.summaries[transaction[i].item] = [newNode]
+                currentNode = newNode
+            else:
+                currentNode = currentNode.children[transaction[i].item]
+                if k >= 3:
+                    currentNode.secondProbability = max(transaction[i].probability, currentNode.secondProbability)
+                currentNode.k = k
+                l1 = i - 1
+                temp = []
+                while l1 >= 0:
+                    temp.append(transaction[l1].probability)
+                    l1 -= 1
+                if len(temp) == 0:
+                    currentNode.probability += round(transaction[i].probability, 2)
+                else:
+                    nn = max(temp) * transaction[i].probability
+                    currentNode.probability += round(nn, 2)
+
+    def addConditionalTransaction(self, transaction, sup, second):
+        """
+        Constructing conditional tree from prefixPaths
+
+        :param transaction : it represents the one transactions in database
+        :type transaction : list
+        :param sup : support of prefixPath taken at last child of the path
+        :type sup : int
+        :param second: second probability of the leaf node
+        :type second: float
+        """
+        currentNode = self.root
+        k = 0
+        for i in range(len(transaction)):
+            k += 1
+            if transaction[i] not in currentNode.children:
+                newNode = _Node(transaction[i], {})
+                newNode.k = k
+                newNode.secondProbability = second
+                newNode.probability = sup
+                currentNode.addChild(newNode)
+                if transaction[i] in self.summaries:
+                    self.summaries[transaction[i]].append(newNode)
+                else:
+                    self.summaries[transaction[i]] = [newNode]
+                currentNode = newNode
+            else:
+                currentNode = currentNode.children[transaction[i]]
+                currentNode.k = k
+                currentNode.secondProbability = max(currentNode.secondProbability, second)
+                currentNode.probability += sup
+
+    def conditionalPatterns(self, alpha):
+        """
+        Generates all the conditional patterns of respective node
+
+        :param alpha : it represents the Node in tree
+        :type alpha : _Node
+        """
+        finalPatterns = []
+        sup = []
+        second = []
+        for i in self.summaries[alpha]:
+            s = i.probability
+            s1 = i.secondProbability
+            set2 = []
+            while i.parent.item is not None:
+                set2.append(i.parent.item)
+                i = i.parent
+            if len(set2) > 0:
+                set2.reverse()
+                finalPatterns.append(set2)
+                second.append(s1)
+                sup.append(s)
+        finalPatterns, support, info = self.conditionalTransactions(finalPatterns, sup)
+        return finalPatterns, support, info, second
+
+    def conditionalTransactions(self, condPatterns, support):
+        """
+        It generates the conditional patterns with frequent items
+
+        :param condPatterns : conditional patterns generated from conditionalPatterns() method for respective node
+        :type condPatterns : list
+        :param support : the support of conditional pattern in tree
+        :type support : list
+        """
+        global _minSup
+        pat = []
+        sup = []
+        data1 = {}
+        for i in range(len(condPatterns)):
+            for j in condPatterns[i]:
+                if j in data1:
+                    data1[j] += support[i]
+                else:
+                    data1[j] = support[i]
+        updatedDict = {}
+        updatedDict = {k: v for k, v in data1.items() if v >= _minSup}
+        count = 0
+        for p in condPatterns:
+            p1 = [v for v in p if v in updatedDict]
+            trans = sorted(p1, key=lambda x: (updatedDict.get(x)), reverse=True)
+            if len(trans) > 0:
+                pat.append(trans)
+                sup.append(support[count])
+            count += 1
+        return pat, sup, updatedDict
+
+    def removeNode(self, nodeValue):
+        """
+        Removing the node from tree
+
+        :param nodeValue : it represents the node in tree
+        :type nodeValue : node
+        """
+        for i in self.summaries[nodeValue]:
+            del i.parent.children[nodeValue]
+
+    def generatePatterns(self, prefix):
+        """
+        Generates the patterns
+
+        :param prefix : forms the combination of items
+        :type prefix : list
+        """
+        global _finalPatterns, _minSup
+        for i in sorted(self.summaries, key=lambda x: (self.info.get(x))):
+            pattern = prefix[:]
+            pattern.append(i)
+            s = 0
+            for x in self.summaries[i]:
+                #if x.k <= 2:
+                    #s += x.probability
+                #elif x.k >= 3:
+                    #n = x.probability * pow(x.secondProbability, (x.k - 2))
+                    #s += n
+                if len(pattern) <= 2:
+                    s += x.probability
+                elif len(pattern) >= 3:
+                    n = x.probability * pow(x.secondProbability, (x.k - 2))
+                    s += n
+            _finalPatterns[tuple(pattern)] = self.info[i]
+            if s >= _minSup:
+                patterns, support, info, second = self.conditionalPatterns(i)
+                conditionalTree = _Tree()
+                conditionalTree.info = info.copy()
+                for pat in range(len(patterns)):
+                    conditionalTree.addConditionalTransaction(patterns[pat], support[pat], second[pat])
+                if len(patterns) > 0:
+                    conditionalTree.generatePatterns(pattern)
+            self.removeNode(i)
 
-class PFPGrowth(_ab._periodicFrequentPatterns):
+
+class TubeS(_fp._frequentPatterns):
     """
     About this algorithm
     ====================
 
-    :**Description**:   PFPGrowth is one of the fundamental algorithm to discover periodic-frequent patterns in a transactional database.
+    :Description: TubeS is one of the fastest algorithm to discover frequent patterns in a uncertain transactional database.
 
-    :**Reference**:   Syed Khairuzzaman Tanbeer, Chowdhury Farhan, Byeong-Soo Jeong, and Young-Koo Lee, "Discovering Periodic-Frequent
-                      Patterns in Transactional Databases", PAKDD 2009, https://doi.org/10.1007/978-3-642-01307-2_24
+    :Reference:  Carson Kai-Sang Leung and Richard Kyle MacKinnon. 2014. Fast Algorithms for Frequent Itemset Mining from Uncertain Data.
+                 In Proceedings of the 2014 IEEE International Conference on Data Mining (ICDM '14). IEEE Computer Society, USA, 893898. https://doi.org/10.1109/ICDM.2014.146
 
+    :Attributes:
 
-    :**Parameters**:    - **iFile** (*str or URL or dataFrame*) -- *Name of the Input file to mine complete set of frequent patterns.*
-                        - **oFile** (*str*) -- *Name of the output file to store complete set of frequent patterns.*
-                        - **minSup** (*int or float or str*) -- *The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.*
-                        - **maxPer** (*int or float or str*) -- *The user can specify maxPer either in count or proportion of database size. It controls the maximum number of transactions in which any two items within a pattern can reappear.*
-                        - **sep** (*str*) -- *This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.*
-
-    :**Attributes**:    - **startTime** (*float*) -- *To record the start time of the mining process.*
-                        - **endTime** (*float*) -- *To record the completion time of the mining process.*
-                        - **finalPatterns** (*dict*) -- *Storing the complete set of patterns in a dictionary variable.*
-                        - **memoryUSS** (*float*) -- *To store the total amount of USS memory consumed by the program.*
-                        - **memoryRSS** (*float*) -- *To store the total amount of RSS memory consumed by the program.*
-                        - **Database** (*list*) -- *To store the transactions of a database in list.*
-                        - **mapSupport** (*Dictionary*) -- *To maintain the information of item and their frequency.*
-                        - **lno** (*int*) -- *It represents the total no of transactions*
-                        - **tree** (*class*) -- *it represents the Tree class.*
-                        - **itemSetCount** (*int*) -- *it represents the total no of patterns.*
-
-    :**Methods**:       - **startMine()** -- *Mining process will start from here.*
-                        - **getPatterns()** -- *Complete set of patterns will be retrieved with this function.*
-                        - **save(oFile)** -- *Complete set of periodic-frequent patterns will be loaded in to a output file.*
-                        - **getPatternsAsDataFrame()** -- *Complete set of periodic-frequent patterns will be loaded in to a dataframe.*
-                        - **getMemoryUSS()** -- *Total amount of USS memory consumed by the mining process will be retrieved from this function.*
-                        - **getMemoryRSS()** -- *Total amount of RSS memory consumed by the mining process will be retrieved from this function.*
-                        - **getRuntime()** -- *Total amount of runtime taken by the mining process will be retrieved from this function.*
-                        - **creatingItemSets(fileName)** -- *Scans the dataset and stores in a list format.*
-                        - **PeriodicFrequentOneItem()** -- * Extracts the one-periodic-frequent patterns from database.*
-                        - **updateDatabases()** -- *Update the database by removing aperiodic items and sort the Database by item decreased support.*
-                        - **buildTree()** -- *After updating the Database, remaining items will be added into the tree by setting root node as null.*
-                        - **convert()** -- *This methos is used to convert the user specified value.*
+        iFile : file
+            Name of the Input file or path of the input file
+
+        oFile : file
+            Name of the output file or path of the output file
+
+        minSup : float or int or str
+            The user can specify minSup either in count or proportion of database size.
+            If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+
+        sep : str
+            This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
+            However, the users can override their default separator.
+
+        memoryUSS : float
+            To store the total amount of USS memory consumed by the program
+
+        memoryRSS : float
+            To store the total amount of RSS memory consumed by the program
+
+        startTime : float
+            To record the start time of the mining process
+
+        endTime : float
+            To record the completion time of the mining process
+
+        Database : list
+            To store the transactions of a database in list
+
+        mapSupport : Dictionary
+            To maintain the information of item and their frequency
+
+        lno : int
+            To represent the total no of transaction
+
+        tree : class
+            To represents the Tree class
+
+        itemSetCount : int
+            To represents the total no of patterns
+
+        finalPatterns : dict
+            To store the complete patterns
+
+    :Methods:
+        mine()
+            Mining process will start from here
+        getPatterns()
+            Complete set of patterns will be retrieved with this function
+        save(oFile)
+            Complete set of frequent patterns will be loaded in to a output file
+        getPatternsAsDataFrame()
+            Complete set of frequent patterns will be loaded in to a dataframe
+        getMemoryUSS()
+            Total amount of USS memory consumed by the mining process will be retrieved from this function
+        getMemoryRSS()
+            Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        getRuntime()
+            Total amount of runtime taken by the mining process will be retrieved from this function
+        creatingItemSets(fileName)
+            Scans the dataset and stores in a list format
+        frequentOneItem()
+            Extracts the one-length frequent patterns from database
+        updateTransactions()
+            Update the transactions by removing non-frequent items and sort the Database by item decreased support
+        buildTree()
+            After updating the Database, remaining items will be added into the tree by setting root node as null
+        convert()
+            to convert the user specified value
 
     Execution methods
     =================
 
+
     **Terminal command**
 
-    .. code-block:: console
 
-       Format:
+    .. code-block:: console
 
-       (.venv) $ python3 PFPGrowth.py <inputFile> <outputFile> <minSup> <maxPer>
+      Format:
 
-       Example usage:
+      (.venv) $ python3 TubeS.py <inputFile> <outputFile> <minSup>
 
-       (.venv) $ python3 PFPGrowth.py sampleDB.txt patterns.txt 10.0 20.0
+      Example Usage:
 
-    .. note:: minSup will be considered in percentage of database transactions
+      (.venv) $ python3 TubeS.py sampleDB.txt patterns.txt 10.0
 
+    .. note:: minSup can be specified  in support count or a value between 0 and 1.
 
     **Calling from a python program**
 
     .. code-block:: python
 
-            from PAMI.periodicFrequentPattern.basic import PFPGrowth as alg
+            from PAMI.uncertainFrequentPattern.basic import TubeS as alg
 
             iFile = 'sampleDB.txt'
 
             minSup = 10  # can also be specified between 0 and 1
 
-            maxPer = 20 # can also be specified between 0 and 1
-
-            obj = alg.PFPGrowth(iFile, minSup, maxPer)
+            obj = alg.TubeS(iFile, minSup)
 
             obj.mine()
 
-            periodicFrequentPatterns = obj.getPatterns()
+            frequentPatterns = obj.getPatterns()
 
-            print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
+            print("Total number of Frequent Patterns:", len(frequentPatterns))
 
-            obj.save("periodicFrequentPatterns")
+            obj.save(oFile)
 
             Df = obj.getPatternsAsDataFrame()
 
             memUSS = obj.getMemoryUSS()
 
             print("Total Memory in USS:", memUSS)
 
@@ -211,389 +493,327 @@
 
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
-
     Credits
     =======
 
-    The complete program was written by P. Likhitha  and revised by Tarun Sreepada under the supervision of Professor Rage Uday Kiran.
 
+            The complete program was written by  P.Likhitha   under the supervision of Professor Rage Uday Kiran.
     """
+
     _startTime = float()
     _endTime = float()
-    _minSup = str()
-    _maxPer = float()
+    _minSup = float()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
     _rank = {}
-    _rankedUp = {}
     _lno = 0
-
-    def _creatingItemSets(self) -> None:
+    def __init__(self, iFile, minSup, sep='\t'):
+        super().__init__(iFile, minSup, sep)
+    def _creatingItemSets(self):
         """
-        Storing the complete transactions of the database/input file in a database variable
-
-        :return: None
+        Scans the databases and stores the transactions into Database variable
         """
         self._Database = []
-        if isinstance(self._iFile, _ab._pd.DataFrame):
-            data, ts = [], []
+        if isinstance(self._iFile, _fp._pd.DataFrame):
+            uncertain, data = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
-            if 'TS' in i:
-                ts = self._iFile['TS'].tolist()
             if 'Transactions' in i:
-                data = self._iFile['Transactions'].tolist()
-            for i in range(len(data)):
-                tr = [ts[i][0]]
-                tr = tr + data[i]
+                self._Database = self._iFile['Transactions'].tolist()
+            if 'uncertain' in i:
+                uncertain = self._iFile['uncertain'].tolist()
+            for k in range(len(data)):
+                tr = []
+                for j in range(len(data[k])):
+                    product = _Item(data[k][j], uncertain[k][j])
+                    tr.append(product)
                 self._Database.append(tr)
+                self._lno += 1
 
+            # print(self.Database)
         if isinstance(self._iFile, str):
-            if _ab._validators.url(self._iFile):
-                data = _ab._urlopen(self._iFile)
+            if _fp._validators.url(self._iFile):
+                data = _fp._urlopen(self._iFile)
                 for line in data:
-                    line.strip()
+                    line = line.strip()
                     line = line.decode("utf-8")
-                    temp = [i.rstrip() for i in line.split(self._sep)]
-                    temp = [x for x in temp if x]
+                    temp1 = line.split(':')
+                    temp = [i.rstrip() for i in temp[0].split(self._sep)]
+                    uncertain = [float(i.rstrip()) for i in temp[1].split(self._sep)]
+                    tr = []
+                    for i in range(len(temp)):
+                        item = temp[i]
+                        probability = uncertain[i]
+                        product = _Item(item, probability)
+                        tr.append(product)
+                    self._lno += 1
                     self._Database.append(temp)
             else:
                 try:
-                    with open(self._iFile, 'r', encoding='utf-8') as f:
+                    with open(self._iFile, 'r') as f:
                         for line in f:
-                            line.strip()
-                            temp = [i.rstrip() for i in line.split(self._sep)]
-                            temp = [x for x in temp if x]
-                            self._Database.append(temp)
+                            temp1 = line.strip()
+                            temp1 = temp1.split(':')
+                            temp = [i.rstrip() for i in temp1[0].split(self._sep)]
+                            uncertain = [float(i.rstrip()) for i in temp1[1].split(self._sep)]
+                            tr = []
+                            for i in range(len(temp)):
+                                item = temp[i]
+                                probability = uncertain[i]
+                                product = _Item(item, probability)
+                                tr.append(product)
+                            self._lno += 1
+                            self._Database.append(tr)
                 except IOError:
                     print("File Not Found")
-                    quit()
 
-    def _convert(self, value) -> int:
+    def _frequentOneItem(self):
         """
-        To convert the given user specified value
+        Takes the transactions and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
+        """
+        global _minSup
+        mapSupport = {}
+        for i in self._Database:
+            for j in i:
+                if j.item not in mapSupport:
+                    mapSupport[j.item] = round(j.probability, 2)
+                else:
+                    mapSupport[j.item] += round(j.probability, 2)
+        mapSupport = {k: round(v, 2) for k, v in mapSupport.items() if v >= self._minSup}
+        plist = [k for k, v in sorted(mapSupport.items(), key=lambda x: x[1], reverse=True)]
+        self._rank = dict([(index, item) for (item, index) in enumerate(plist)])
+        return mapSupport, plist
+
+    def _buildTree(self, data, info):
+        """
+        It takes the transactions and support of each item and construct the main tree with setting root node as null
+
+        :param data : it represents the one transactions in database
+        :type data : list
+        :param info : it represents the support of each item
+        :type info : dictionary
+        """
+        rootNode = _Tree()
+        rootNode.info = info.copy()
+        for i in range(len(data)):
+            rootNode.addTransaction(data[i])
+        return rootNode
+
+    def updateTransactions(self, dict1):
+        """
+        Remove the items which are not frequent from transactions and updates the transactions with rank of items
+
+        :param dict1 : frequent items with support
+        :type dict1 : dictionary
+        """
+        list1 = []
+        for tr in self._Database:
+            list2 = []
+            for i in range(0, len(tr)):
+                if tr[i].item in dict1:
+                    list2.append(tr[i])
+            if (len(list2) >= 2):
+                basket = list2
+                basket.sort(key=lambda val: self._rank[val.item])
+                list2 = basket
+                list1.append(list2)
+        return list1
+
+    def _Check(self, i, x):
+        """
+        To check the presence of item or pattern in transaction
+
+        :param x: it represents the pattern
+        :type x : list
+        :param i : represents the uncertain transactions
+        :type i : list
+        """
+        for m in x:
+            k = 0
+            for n in i:
+                if m == n.item:
+                    k += 1
+            if k == 0:
+                return 0
+        return 1
 
-        :param value: user specified value
-        :return: converted value
+    def _convert(self, value):
+        """
+        To convert the type of user specified minSup value
+
+        :param value: user specified minSup value
+        :return: converted type minSup value
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self._Database) * value)
         if type(value) is str:
             if '.' in value:
-                value = float(value)
                 value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
-    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
-    def startMine(self) -> None:
-        self.mine()
-
-    def _getMaxPer(self, arr, maxTS):
-        """
-        This method appends `0` and `maxTS` to the input array, sorts the array,
-        and then computes the differences between consecutive elements.
-
-        :param arr: The input array of elements.
-        :type arr: numpy.ndarray
-        :param maxTS: The maximum timestamp to be appended to the array.
-        :type maxTS: int or float
-        :return: None
-        """
-        arr = np.append(arr, [0, maxTS])
-        arr = np.sort(arr)
-        arr = np.diff(arr)
-
-        return np.max(arr)
-
-    def _construct(self, items, data, minSup, maxPer, maxTS, patterns):
-
-        """
-        This method filters the items based on the minimum support (minSup) and
-        maximum period (maxPer). It then constructs a tree structure from the
-        filtered items and data.
-
-        :param items: A dictionary where keys are items and values are lists of timestamps.
-        :type items: dict
-        :param data: The dataset used to construct the tree, where each entry is a list with
-                     an index followed by items.
-        :type data: list of lists
-        :param minSup: The minimum support threshold.
-        :type minSup: int
-        :param maxPer: The maximum period threshold.
-        :type maxPer: int or float
-        :param maxTS: The maximum timestamp.
-        :type maxTS: int or float
-        :param patterns: A dictionary to store the patterns discovered during the construction.
-        :type patterns: dict
-        :return: A tuple containing the root node of the constructed tree and a dictionary
-                 of item nodes.
-        :rtype: tuple(_Node, dict)
-        """
-
-        # maxPerItems = {k: self.getMaxPer(v, maxTS) for k, v in items.items() if len(v) >= minSup}
-
-        items = {k: v for k, v in items.items() if len(v) >= minSup and self._getMaxPer(v, maxTS) <= maxPer}
-
-        #tested ok
-        for item, ts in items.items():
-            # pat = "\t".join(item)
-            # self.patCount += 1
-            # patterns[pat] = (len(ts), self.getMaxPer(ts, maxTS))
-            patterns[tuple([item])] = [len(ts), self._getMaxPer(ts, maxTS)]
-
-        root = _Node([], None, None)
-        itemNodes = {}
-        for line in data:
-            currNode = root
-            index = int(line[0])
-            line = line[1:]
-            line = sorted([item for item in line if item in items], key = lambda x: len(items[x]), reverse = True)
-            for item in line:
-                currNode = currNode.addChild(item, [index])   # heavy
-                if item in itemNodes:
-                    itemNodes[item].add(currNode)
-                else:
-                    itemNodes[item] = set([currNode])
-
-        return root, itemNodes
-
+    def _removeFalsePositives(self):
+        """
+        To remove the false positive patterns generated in frequent patterns.
 
-    def _recursive(self, root, itemNode, minSup, maxPer, patterns, maxTS):
+        :return: Patterns with accurate probability
         """
-        This method recursively constructs a pattern tree from the given root node,
-        filtering items based on the minimum support (minSup) and maximum period (maxPer).
-        It updates the patterns dictionary with the discovered patterns.
-
-        :param root: The current root node of the pattern tree.
-        :type root: _Node
-        :param itemNode: A dictionary where keys are items and values are sets of nodes
-                         associated with those items.
-        :type itemNode: dict
-        :param minSup: The minimum support threshold.
-        :type minSup: int
-        :param maxPer: The maximum period threshold.
-        :type maxPer: int or float
-        :param patterns: A dictionary to store the patterns discovered during the recursion.
-        :type patterns: dict
-        :param maxTS: The maximum timestamp.
-        :type maxTS: int or float
-        """
-
-        for item in itemNode:
-            newRoot = _Node(root.item + [item], None, None)
-
-            itemLocs = {}
-            transactions = {}
-            for node in itemNode[item]:
-                transaction, locs = node.traverse()
-                if len(transaction) < 1:
-                    continue
-                # transactions.append((transaction, locs))
-                if tuple(transaction) in transactions:
-                    transactions[tuple(transaction)].extend(locs)
+        global _finalPatterns
+        periods = {}
+        for i in self._Database:
+            for x, y in _finalPatterns.items():
+                if len(x) == 1:
+                    periods[x] = y
                 else:
-                    transactions[tuple(transaction)] = locs
-
-                for item in transaction:
-                    if item in itemLocs:
-                        itemLocs[item] += locs
-                    else:
-                        itemLocs[item] = list(locs)
-
-            # Precompute getMaxPer results for itemLocs
-            maxPerResults = {item: self._getMaxPer(itemLocs[item], maxTS) for item in itemLocs if len(itemLocs[item]) >= minSup}
-
-            # Filter itemLocs based on minSup and maxPer
-            itemLocs = {k: len(v) for k, v in itemLocs.items() if k in maxPerResults and maxPerResults[k] <= maxPer}
-
-            # Iterate over filtered itemLocs
-            for item in itemLocs:
-                # pat = "\t".join([str(x) for x in newRoot.item + [item]])
-                # self.patCount += 1
-                # patterns[pat] = [itemLocs[item], maxPerResults[item]]
-                patterns[tuple(newRoot.item + [item])] = [itemLocs[item], maxPerResults[item]]
-            
-            if not itemLocs:
-                continue
-
-            newItemNodes = {}
-
-            for transaction, locs in transactions.items():
-                transaction = sorted([item for item in transaction if item in itemLocs], key = lambda x: itemLocs[x], reverse = True)
-                if len(transaction) < 1:
-                    continue
-                currNode = newRoot
-                for item in transaction:
-                    currNode = currNode.addChild(item, locs)
-                    if item in newItemNodes:
-                        newItemNodes[item].add(currNode)
-                    else:
-                        newItemNodes[item] = set([currNode])
-
-            self._recursive(newRoot, newItemNodes, minSup, maxPer, patterns, _lno)
-
-    def mine(self) -> None:
-        """
-        Mining process will start from this function
-
-        :return: None
-        """
-
-        global _minSup, _maxPer, _lno
-        self._startTime = _ab._time.time()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        if self._minSup is None:
-            raise Exception("Please enter the Minimum Support")
-        if self._maxPer is None:
-            raise Exception("Please enter the Maximum Periodicity")
-        if self._sep is None:
-            raise Exception("Default separator is tab space, please enter the separator if you have different separator in the input file")
+                    s = 1
+                    check = self._Check(i, x)
+                    if check == 1:
+                        for j in i:
+                            if j.item in x:
+                                s *= j.probability
+                        if x in periods:
+                            periods[x] += s
+                        else:
+                            periods[x] = s
+        for x, y in periods.items():
+            if y >= self._minSup:
+                sample = str()
+                for i in x:
+                    sample = sample + i + "\t"
+                self._finalPatterns[sample] = y
 
+    def mine(self):
+        """
+        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
+        """
+        global _minSup
+        self._startTime = _fp._time.time()
         self._creatingItemSets()
         self._minSup = self._convert(self._minSup)
-        self._maxPer = self._convert(self._maxPer)
-        #tested ok
-        _minSup, _maxPer, _lno = self._minSup, self._maxPer, len(self._Database)
-        if self._minSup > len(self._Database):
-            raise Exception("Please enter the minSup in range between 0 to 1")
-        
-
-        items = {}
-
-        # tested ok
-        for line in self._Database:
-            index = int(line[0])
-            for item in line[1:]:
-                if item not in items:
-                    items[item] = []
-                items[item].append(index)
-
-        root, itemNodes = self._construct(items, self._Database, _minSup, _maxPer, _lno, self._finalPatterns)
-
-        self._recursive(root, itemNodes, _minSup, _maxPer, self._finalPatterns, _lno)
-
-        newPattern = {}
-        for k, v in self._finalPatterns.items():
-            newPattern["\t".join([str(x) for x in k])] = v
-
-        self._finalPatterns = newPattern
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
+        _minSup = self._minSup
+        self._finalPatterns = {}
+        mapSupport, plist = self._frequentOneItem()
+        transactions1 = self.updateTransactions(mapSupport)
+        info = {k: v for k, v in mapSupport.items()}
+        Tree1 = self._buildTree(transactions1, info)
+        Tree1.generatePatterns([])
+        self._removeFalsePositives()
+        print("Uncertain Frequent patterns were generated successfully using TubeS algorithm")
+        self._endTime = _fp._time.time()
+        process = _fp._psutil.Process(_fp._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
-        print("Periodic Frequent patterns were generated successfully using PFPGrowth algorithm ")
 
-    def getMemoryUSS(self) -> float:
+    def getMemoryUSS(self):
         """
+
         Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
-    def getMemoryRSS(self) -> float:
+    def getMemoryRSS(self):
         """
+
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
-    def getRuntime(self) -> float:
+    def getRuntime(self):
         """
+
         Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
-    def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
+    def getPatternsAsDataFrame(self):
         """
-        Storing final periodic-frequent patterns in a dataframe
 
-        :return: returning periodic-frequent patterns in a dataframe
+        Storing final frequent patterns in a dataframe
+
+        :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
-        dataFrame = {}
+        dataframe = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a, b[0], b[1]])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
-        return dataFrame
+            data.append([a.replace('\t', ' '), b])
+            dataframe = _fp._pd.DataFrame(data, columns=['Patterns', 'Support'])
+        return dataframe
 
-    def save(self, outFile: str) -> None:
+    def save(self, outFile):
         """
-        Complete set of periodic-frequent patterns will be loaded in to an output file
+
+        Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
-        :type outFile: csv file
-        :return: None
+        :type outFile: file
         """
-        self._oFile = outFile
-        writer = open(self._oFile, 'w+')
+        self.oFile = outFile
+        writer = open(self.oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            s1 = x + ":" + str(y[0]) + ":" + str(y[1])
-            #s1 = x.replace(' ', '\t').strip() + ":" + str(y[0]) + ":" + str(y[1])
+            s1 = x.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
-    def getPatterns(self) -> Dict[str, Tuple[int, int]]:
+    def getPatterns(self):
         """
-        Function to send the set of periodic-frequent patterns after completion of the mining process
 
-        :return: returning periodic-frequent patterns
+        Function to send the set of frequent patterns after completion of the mining process
+
+        :return: returning frequent patterns
         :rtype: dict
         """
-        return self._finalPatterns
+        return len(self._finalPatterns)
 
-    def printResults(self) -> None:
+    def printResults(self):
         """
         This function is used to print the results
-
-        :return: None
         """
-        print("Total number of Periodic Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of  Uncertain Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:",  self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
-        if len(_ab._sys.argv) == 6:
-            _ap = PFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
-        if len(_ab._sys.argv) == 5:
-            _ap = PFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
-        _ap.startMine()
+    if len(_fp._sys.argv) == 4 or len(_fp._sys.argv) == 5:
+        if len(_fp._sys.argv) == 5:
+            _ap = TubeS(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4])
+        if len(_fp._sys.argv) == 4:
+            _ap = TubeS(_fp._sys.argv[1], _fp._sys.argv[3])
         _ap.mine()
-        print("Total number of Periodic-Frequent Patterns:", len(_ap.getPatterns()))
-        _ap.save(_ab._sys.argv[2])
+        print("Total number of Uncertain Frequent Patterns:", len(_ap.getPatterns()))
+        _ap.save(_fp._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `pami-2024.5.28.1/PAMI/periodicFrequentPattern/basic/PFPGrowthPlus.py` & `pami-2024.5.7.1/PAMI/periodicFrequentPattern/basic/PFPGrowthPlus.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/periodicFrequentPattern/basic/PFPMC.py` & `pami-2024.5.7.1/PAMI/periodicFrequentPattern/basic/PFPMC.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/periodicFrequentPattern/basic/PSGrowth.py` & `pami-2024.5.7.1/PAMI/periodicFrequentPattern/basic/PSGrowth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/periodicFrequentPattern/basic/_PFECLAT.py` & `pami-2024.5.7.1/PAMI/periodicFrequentPattern/basic/_PFECLAT.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/periodicFrequentPattern/basic/_PFPGrowth.py` & `pami-2024.5.7.1/PAMI/periodicFrequentPattern/basic/_PFPGrowth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/periodicFrequentPattern/basic/__init__.py` & `pami-2024.5.7.1/PAMI/periodicFrequentPattern/basic/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/periodicFrequentPattern/basic/abstract.py` & `pami-2024.5.7.1/PAMI/periodicFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/periodicFrequentPattern/basic/parallelPFPGrowth.py` & `pami-2024.5.7.1/PAMI/periodicFrequentPattern/basic/parallelPFPGrowth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/periodicFrequentPattern/closed/CPFPMiner.py` & `pami-2024.5.7.1/PAMI/periodicFrequentPattern/closed/CPFPMiner.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/periodicFrequentPattern/closed/__init__.py` & `pami-2024.5.7.1/PAMI/periodicFrequentPattern/closed/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/periodicFrequentPattern/closed/abstract.py` & `pami-2024.5.7.1/PAMI/periodicFrequentPattern/closed/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/periodicFrequentPattern/cuda/abstract.py` & `pami-2024.5.7.1/PAMI/periodicFrequentPattern/cuda/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/periodicFrequentPattern/cuda/cuGPFMiner.py` & `pami-2024.5.7.1/PAMI/periodicFrequentPattern/cuda/cuGPFMiner.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/periodicFrequentPattern/cuda/gPFMinerBit.py` & `pami-2024.5.7.1/PAMI/periodicFrequentPattern/cuda/gPFMinerBit.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/periodicFrequentPattern/maximal/MaxPFGrowth.py` & `pami-2024.5.7.1/PAMI/periodicFrequentPattern/maximal/MaxPFGrowth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/periodicFrequentPattern/maximal/__init__.py` & `pami-2024.5.7.1/PAMI/periodicFrequentPattern/maximal/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/periodicFrequentPattern/maximal/abstract.py` & `pami-2024.5.7.1/PAMI/periodicFrequentPattern/maximal/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/periodicFrequentPattern/pyspark/abstract.py` & `pami-2024.5.7.1/PAMI/periodicFrequentPattern/pyspark/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/periodicFrequentPattern/pyspark/parallelPFPGrowth.py` & `pami-2024.5.7.1/PAMI/periodicFrequentPattern/pyspark/parallelPFPGrowth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/periodicFrequentPattern/topk/TopkPFP/TopkPFP.py` & `pami-2024.5.7.1/PAMI/periodicFrequentPattern/topk/TopkPFP/TopkPFP.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/periodicFrequentPattern/topk/TopkPFP/abstract.py` & `pami-2024.5.7.1/PAMI/periodicFrequentPattern/topk/TopkPFP/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/periodicFrequentPattern/topk/kPFPMiner/abstract.py` & `pami-2024.5.7.1/PAMI/periodicFrequentPattern/topk/kPFPMiner/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/periodicFrequentPattern/topk/kPFPMiner/kPFPMiner.py` & `pami-2024.5.7.1/PAMI/stablePeriodicFrequentPattern/basic/SPPEclat.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,24 +1,28 @@
+# Stable periodic pattern mining aims to discover all interesting patterns in a temporal database using three constraints minimum support,
+# maximum period and maximum liability, that have support no less than the user-specified minimum support  constraint and liability no
+# greater than maximum liability.
+#
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
-
-#             import PAMI.periodicFrequentPattern.kPFPMiner as alg
 #
-#             obj = alg.kPFPMiner(iFile, k)
+#             from PAMI.stablePeriodicFrequentPattern.basic import basic as alg
+#
+#             obj = alg.SPPEclat("../basic/sampleTDB.txt", 5, 3, 3)
 #
 #             obj.startMine()
 #
-#             periodicFrequentPatterns = obj.getPatterns()
+#             Patterns = obj.getPatterns()
 #
-#             print("Total number of top-k Periodic Frequent Patterns:", len(periodicFrequentPatterns))
+#             print("Total number of Stable Periodic Frequent Patterns:", len(Patterns))
 #
-#             obj.save(oFile)
+#             obj.save("patterns")
 #
-#             Df = obj.getPatternInDataFrame()
+#             Df = obj.getPatternsAsDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
 #             print("Total Memory in USS:", memUSS)
 #
 #             memRSS = obj.getMemoryRSS()
 #
@@ -45,164 +49,205 @@
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
-
-from PAMI.periodicFrequentPattern.basic import abstract as _ab
 import pandas as pd
 from deprecated import deprecated
 
-from PAMI.periodicFrequentPattern.topk.kPFPMiner import abstract as _ab
-
+from PAMI.stablePeriodicFrequentPattern.basic import abstract as _ab
 
-class kPFPMiner(_ab._periodicFrequentPatterns):
+class SPPEclat(_ab._stablePeriodicFrequentPatterns):
     """
-    :Description:   Top - K is and algorithm to discover top periodic-frequent patterns in a temporal database.
+    :Description:   Stable periodic pattern mining aims to dicover all interesting patterns in a temporal database using three contraints minimum support,
+                    maximum period and maximum lability, that have support no less than the user-specified minimum support  constraint and lability no
+                    greater than maximum lability.
 
-    :Reference:   Likhitha, P., Ravikumar, P., Kiran, R.U., Watanobe, Y. (2022).
-                  Discovering Top-k Periodic-Frequent Patterns in Very Large Temporal Databases. Big Data Analytics.
-                 BDA 2022. Lecture Notes in Computer Science, vol 13773. Springer, Cham. https://doi.org/10.1007/978-3-031-24094-2_14
+    :Reference:   Fournier-Viger, P., Yang, P., Lin, J. C.-W., Kiran, U. (2019). Discovering Stable Periodic-Frequent Patterns in Transactional Data. Proc.
+                  32nd Intern. Conf. on Industrial, Engineering and Other Applications of Applied Intelligent Systems (IEA AIE 2019), Springer LNAI, pp. 230-244
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of periodic frequent pattern's
+                   Name of the Input file to mine complete set of stable periodic Frequent Pattern.
     :param  oFile: str :
-                   Name of the output file to store complete set of periodic frequent pattern's
-
+                   Name of the output file to store complete set of stable periodic Frequent Pattern.
+    :param  minSup: float or int or str :
+                    The user can specify minSup either in count or proportion of database size.
+                    If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+                    Otherwise, it will be treated as float.
+                    Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+    :param  itemSup: int or float :
+                    Frequency of an item
+    :param maxLa: float :
+                  minimum loss of a pattern
     :param  sep: str :
-                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+                 This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+
 
     :Attributes:
 
-        iFile : str
-            Input file name or path of the input file
-        k: int
-            User specified counte of top-k periodic frequent patterns
+        iFile : file
+            Name of the Input file or path of the input file
+        oFile : file
+            Name of the output file or path of the output file
+        minSup : int or float or str
+            The user can specify minSup either in count or proportion of database size.
+            If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+        maxPer : int or float or str
+            The user can specify maxPer either in count or proportion of database size.
+            If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: maxPer=10 will be treated as integer, while maxPer=10.0 will be treated as float
+        maxLa : int or float or str
+            The user can specify maxLa either in count or proportion of database size.
+            If the program detects the data type of maxLa is integer, then it treats maxLa is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: maxLa=10 will be treated as integer, while maxLa=10.0 will be treated as float
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator.
-        oFile : str
-            Name of the output file or the path of the output file
-        startTime:float
-            To record the start time of the mining process
-        endTime:float
-            To record the completion time of the mining process
-        finalPatterns: dict
-            Storing the complete set of patterns in a dictionary variable
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
+        startTime:float
+            To record the start time of the mining process
+        endTime:float
+            To record the completion time of the mining process
+        Database : list
+            To store the transactions of a database in list
+        mapSupport : Dictionary
+            To maintain the information of item and their frequency
+        lno : int
+            it represents the total no of transactions
+        tree : class
+            it represents the Tree class
+        itemSetCount : int
+            it represents the total no of patterns
+        finalPatterns : dict
+            it represents to store the patterns
+        tidList : dict
+            stores the timestamps of an item
 
     :Methods:
 
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
-        savePatterns(oFile)
-            Complete set of frequent patterns will be loaded in to a output file
+        save(oFile)
+            Complete set of periodic-frequent patterns will be loaded in to an output file
         getPatternsAsDataFrame()
-            Complete set of frequent patterns will be loaded in to a dataframe
+            Complete set of periodic-frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
         creatingItemSets()
-            Scans the dataset or dataframes and stores in list format
-        frequentOneItem()
-            Generates one frequent patterns
-        eclatGeneration(candidateList)
-            It will generate the combinations of frequent items
-        generateFrequentPatterns(tidList)
-            It will generate the combinations of frequent items from a list of items
+            Scan the database and store the items with their timestamps which are periodic frequent
+        calculateLa()
+            Calculates the support and period for a list of timestamps.
+        Generation()
+            Used to implement prefix class equivalence method to generate the periodic patterns recursively
 
-    **Executing the code on terminal:**
-    ------------------------------------------
+
+
+    **Methods to execute code on terminal**
+    -----------------------------------------
     .. code-block:: console
 
 
        Format:
 
+       (.venv) $ python3 basic.py <inputFile> <outputFile> <minSup> <maxPer> <maxLa>
 
-       (.venv) $ python3 kPFPMiner.py <inputFile> <outputFile> <k>
+       Example usage:
 
-       Examples :
+       (.venv) $ python3 basic.py sampleDB.txt patterns.txt 10.0 4.0 2.0
 
-       (.venv) $  python3 kPFPMiner.py sampleDB.txt patterns.txt 10
 
+               .. note:: constraints will be considered in percentage of database transactions
 
-    **Sample run of the importing code:
-    --------------------------------------
-    .. code-block:: python
+    **Importing this algorithm into a python program**
+    ---------------------------------------------------
+    ... code-block:: python
 
-            import PAMI.periodicFrequentPattern.kPFPMiner as alg
+                    from PAMI.stablePeriodicFrequentPattern.basic import basic as alg
 
-            obj = alg.kPFPMiner(iFile, k)
+                    obj = alg.PFPECLAT("../basic/sampleTDB.txt", 5, 3, 3)
 
-            obj.startMine()
+                    obj.startMine()
 
-            periodicFrequentPatterns = obj.getPatterns()
+                    Patterns = obj.getPatterns()
 
-            print("Total number of top-k Periodic Frequent Patterns:", len(periodicFrequentPatterns))
+                    print("Total number of Stable Periodic Frequent Patterns:", len(Patterns))
 
-            obj.save(oFile)
+                    obj.save("patterns")
 
-            Df = obj.getPatternInDataFrame()
+                    Df = obj.getPatternsAsDataFrame()
 
-            memUSS = obj.getMemoryUSS()
+                    memUSS = obj.getMemoryUSS()
 
-            print("Total Memory in USS:", memUSS)
+                    print("Total Memory in USS:", memUSS)
 
-            memRSS = obj.getMemoryRSS()
+                    memRSS = obj.getMemoryRSS()
 
-            print("Total Memory in RSS", memRSS)
+                    print("Total Memory in RSS", memRSS)
 
-            run = obj.getRuntime()
+                    run = obj.getRuntime()
 
-            print("Total ExecutionTime in seconds:", run)
+                    print("Total ExecutionTime in seconds:", run)
 
     **Credits:**
     --------------
-            The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
+             The complete program was written by  P.Likhitha under the supervision of Professor Rage Uday Kiran.
 
-    """
-
-    _startTime = float()
-    _endTime = float()
-    _k = int()
-    _finalPatterns = {}
+       """
     _iFile = " "
     _oFile = " "
+    _minSup = str()
+    _maxPer = str()
+    _maxLa = float()
     _sep = " "
+    _SPPList = {}
+    _itemList = []
+    _last = int()
+    _finalPatterns = {}
+    _tsList = {}
+    _startTime = float()
+    _endTime = float()
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
-    _tidList = {}
-    lno = int()
-    _maximum = int()
 
-    def _creatingItemSets(self):
+    def __init__(self, inputFile, minSup, maxPer, maxLa, sep='\t'):
+        self._iFile = inputFile
+        self._minSup = minSup
+        self._maxPer = maxPer
+        self._maxLa = maxLa
+        self._sep = sep
+
+    def _creatingItemsets(self):
         """
         Storing the complete transactions of the database/input file in a database variable
         """
-
         self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
                 self._Database = self._iFile['Transactions'].tolist()
-
-            # print(self.Database)
+            if 'Patterns' in i:
+                self._Database = self._iFile['Patterns'].tolist()
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
@@ -215,132 +260,14 @@
                             line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
                             self._Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
-                    
-    def getPer_Sup(self, tids):
-        tids.sort()
-        cur=0
-        per=list()
-        sup=0
-        #print(tids)
-        for i in range(len(tids)-1):
-            j = i + 1
-            #if tids[j] - cur <= periodicity:
-                #return [0,0]
-            per.append(tids[j] - cur)
-            cur = tids[j]
-        per.append(self.lno - cur)
-        return max(per)
-
-    def _frequentOneItem(self):
-        """
-        Generating one frequent patterns
-        """
-        self._mapSupport = {}
-        self._tidList = {}
-        n = 0
-        for line in self._Database:
-            self.lno += 1
-            n = int(line[0])
-            for i in range(1, len(line)):
-                si = line[i]
-                if self._mapSupport.get(si) is None:
-                    self._mapSupport[si] = [1, abs(0 - n), n]
-                    self._tidList[si] = [n]
-                else:
-                    self._mapSupport[si][0] += 1
-                    self._mapSupport[si][1] = max(self._mapSupport[si][1], abs(n - self._mapSupport[si][2]))
-                    self._mapSupport[si][2] = n
-                    self._tidList[si].append(n)
-        for x, y in self._mapSupport.items():
-            self._mapSupport[x][1] = max(self._mapSupport[x][1], abs(n - self._mapSupport[x][2]))
-        plist = [key for key, value in sorted(self._mapSupport.items(), key=lambda x: x[1], reverse=True)]
-        for i in plist:
-            if len(self._finalPatterns) >= self._k:
-                break
-            else:
-                self._finalPatterns[i] = self._mapSupport[i][1]
-        self._maximum = max([self._finalPatterns[i] for i in self._finalPatterns.keys()])
-        plist = list(self._finalPatterns.keys())
-        return plist
-
-
-    def _save(self, prefix, suffix, tidSetI):
-        """Saves the patterns that satisfy the periodic frequent property.
-
-        :param prefix: the prefix of a pattern
-        :type prefix: list
-        :param suffix: the suffix of a patterns
-        :type suffix: list
-        :param tidSetI: the timestamp of a patterns
-        :type tidSetI: list
-        """
-
-        if prefix is None:
-            prefix = suffix
-        else:
-            prefix = prefix + suffix
-        val = self.getPer_Sup(tidSetI)
-        sample = str()
-        for i in prefix:
-            sample = sample + i + " "
-        if len(self._finalPatterns) < self._k:
-            if val < self._maximum:
-                self._finalPatterns[sample] = val
-                self._finalPatterns = {k: v for k, v in sorted(self._finalPatterns.items(), key=lambda item: item[1], reverse=True)}
-                self._maximum = max([i for i in self._finalPatterns.values()])
-        else:
-            for x, y in sorted(self._finalPatterns.items(), key=lambda x: x[1], reverse=True):
-                if val < y:
-                    del self._finalPatterns[x]
-                    self._finalPatterns[sample] = val
-                    self._finalPatterns = {k: v for k, v in
-                                              sorted(self._finalPatterns.items(), key=lambda item: item[1],
-                                                     reverse=True)}
-                    self._maximum = max([i for i in self._finalPatterns.values()])
-                    return
-
-    def _Generation(self, prefix, itemSets, tidSets):
-        """Equivalence class is followed  and checks for the patterns generated for periodic-frequent patterns.
-
-        :param prefix:  main equivalence prefix
-        :type prefix: periodic-frequent item or pattern
-        :param itemSets: patterns which are items combined with prefix and satisfying the periodicity and frequent with their timestamps
-        :type itemSets: list
-        :param tidSets: timestamps of the items in the argument itemSets
-        :type tidSets: list
-
-        """
-        if len(itemSets) == 1:
-            i = itemSets[0]
-            tidI = tidSets[0]
-            self._save(prefix, [i], tidI)
-            return
-        for i in range(len(itemSets)):
-            itemI = itemSets[i]
-            if itemI is None:
-                continue
-            tidSetI = tidSets[i]
-            classItemSets = []
-            classTidSets = []
-            itemSetX = [itemI]
-            for j in range(i + 1, len(itemSets)):
-                itemJ = itemSets[j]
-                tidSetJ = tidSets[j]
-                y = list(set(tidSetI).intersection(tidSetJ))
-                if self.getPer_Sup(y) <= self._maximum:
-                    classItemSets.append(itemJ)
-                    classTidSets.append(y)
-            newPrefix = list(set(itemSetX)) + prefix
-            self._Generation(newPrefix, classItemSets, classTidSets)
-            self._save(prefix, list(set(itemSetX)), tidSetI)
 
     def _convert(self, value):
         """
         to convert the type of user specified minSup value
 
         :param value: user specified minSup value
         :return: converted type
@@ -348,137 +275,193 @@
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self._Database) * value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
-                value = ((len(self._Database)) * value)
+                value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
+    def _createSPPList(self):
+        """
+        to convert the single length stable periodic patterns
+        """
+        tidLast = {}
+        la = {}
+        self._SPPList = {}
+        self._tsList = {}
+        for transaction in self._Database:
+            ts = int(transaction[0])
+            for item in transaction[1:]:
+                if item not in self._SPPList:
+                    la[item] = max(0, ts - self._maxPer)
+                    self._SPPList[item] = [1, la[item]]
+                    self._tsList[item] = [ts]
+                else:
+                    s = self._SPPList[item][0] + 1
+                    la[item] = max(0, la[item] + ts - tidLast.get(item) - self._maxPer)
+                    self._SPPList[item] = [s, max(la[item], self._SPPList[item][1])]
+                    self._tsList[item].append(ts)
+                tidLast[item] = ts
+            self._last = ts
+        for item in self._SPPList:
+            la[item] = max(0, la[item] + self._last - tidLast[item] - self._maxPer)
+            self._SPPList[item][1] = max(la[item], self._SPPList[item][1])
+        self._SPPList = {k: v for k, v in self._SPPList.items() if v[0] >= self._minSup and v[1] <= self._maxLa}
+        self._SPPList = {k: v for k, v in sorted(self._SPPList.items(), key=lambda x: x[1][0], reverse=True)}
+        self._Generation(list(self._SPPList), set())
+
+    def _Generation(self, GPPFList, CP):
+        """
+        To generate the patterns using depth-first search
+        """
+        for i in range(len(GPPFList)):
+            item = GPPFList[i]
+            CP1 = CP | {item}
+            if CP != set():
+                self._tsList['\t'.join(CP1)] = list(set(self._tsList['\t'.join(CP)]) & set(self._tsList[item]))
+            la = self._calculateLa(self._tsList['\t'.join(CP1)])
+            support = len(self._tsList['\t'.join(CP1)])
+            if la <= self._maxLa and len(self._tsList['\t'.join(CP1)]) >= self._minSup:
+                #CP = CP1
+                self._finalPatterns['\t'.join(CP1)] = [support, la]
+                if i+1 < len(GPPFList):
+                    self._Generation(GPPFList[i+1:], CP1)
+
+    def _calculateLa(self, tsList):
+        """
+        To calculate the liability of a patterns based on its timestamps
+        """
+        previous = 0
+        la = 0
+        tsList = sorted(tsList)
+        laList = []
+        for ts in tsList:
+            la = max(0, la + ts - previous - self._maxPer)
+            laList.append(la)
+            previous = ts
+            
+        la = max(0, la + self._last - previous - self._maxPer)
+        laList.append(la)
+        maxla = max(laList)
+        return maxla
+
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self):
         """
-        Main function of the program
+        Method to start the mining of patterns
+        """
+        self.mine()
 
+    def mine(self):
+        """
+        Method to start the mining of patterns
         """
         self._startTime = _ab._time.time()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        if self._k is None:
-            raise Exception("Please enter the Minimum Support")
-        self._creatingItemSets()
-        self._k = self._convert(self._k)
-        plist = self._frequentOneItem()
-        for i in range(len(plist)):
-            itemI = plist[i]
-            tidSetI = self._tidList[itemI]
-            itemSetX = [itemI]
-            itemSets = []
-            tidSets = []
-            for j in range(i + 1, len(plist)):
-                itemJ = plist[j]
-                tidSetJ = self._tidList[itemJ]
-                y1 = list(set(tidSetI).intersection(tidSetJ))
-                if self.getPer_Sup(y1) <= self._maximum:
-                    itemSets.append(itemJ)
-                    tidSets.append(y1)
-            self._Generation(itemSetX, itemSets, tidSets)
-        print("kPFPMiner has successfully generated top-k frequent patterns")
+        self._creatingItemsets()
+        self._minSup = self._convert(self._minSup)
+        self._maxPer = self._convert(self._maxPer)
+        self._maxLa = self._convert(self._maxLa)
+        self._finalPatterns = {}
+        #print(self._minSup, self._maxPer, self._maxLa)
+        self._createSPPList()
         self._endTime = _ab._time.time()
         self._memoryUSS = float()
         self._memoryRSS = float()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
+        print("Stable Periodic Frequent patterns were generated successfully using basic algorithm ")
 
-    def getMemoryUSS(self):
-        """Total amount of USS memory consumed by the mining process will be retrieved from this function
 
-        :return: returning USS memory consumed by the mining process
+    def getRuntime(self):
+        """
+        Calculating the total amount of runtime taken by the mining process
+
+        :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
+        return self._endTime - self._startTime
 
-        return self._memoryUSS
+    def getPatterns(self):
+        """
+        Function to return the set of stable periodic-frequent patterns after completion of the mining process
 
-    def getMemoryRSS(self):
-        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        :return: returning stable periodic-frequent patterns
+        :rtype: dict
+        """
+        return self._finalPatterns
 
-        :return: returning RSS memory consumed by the mining process
+    def getMemoryUSS(self):
+        """
+        Total amount of USS memory consumed by the mining process will be retrieved from this function
+        :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
-        return self._memoryRSS
-
-    def getRuntime(self):
-        """Calculating the total amount of runtime taken by the mining process
+        return self._memoryUSS
 
-        :return: returning total amount of runtime taken by the mining process
-        :rtype: float
+    def save(self, outFile):
         """
+        Complete set of periodic-frequent patterns will be loaded in to an output file
 
-        return self._endTime - self._startTime
+        :param outFile: name of the output file
+        :type outFile: csv file
+        """
+        self._oFile = outFile
+        writer = open(self._oFile, 'w+')
+        for x, y in self._finalPatterns.items():
+            s1 = x.strip() + ":" + str(y[0]) + ":" + str(y[1])
+            writer.write("%s \n" % s1)
 
     def getPatternsAsDataFrame(self):
-        """Storing final frequent patterns in a dataframe
+        """
+        Storing final periodic-frequent patterns in a dataframe
 
-        :return: returning frequent patterns in a dataframe
+        :return: returning periodic-frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a, b])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'periodicity'])
+            data.append([a.replace('\t', ' '), b[0], b[1]])
+            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
         return dataFrame
 
-    def save(self, outFile):
-        """Complete set of frequent patterns will be loaded in to a output file
-
-        :param outFile: name of the output file
-
-        :type outFile: file
+    def getMemoryRSS(self):
         """
-        self._oFile = outFile
-        writer = open(self._oFile, 'w+')
-        for x, y in self._finalPatterns.items():
-            patternsAndSupport = x + ":" + str(y)
-            writer.write("%s \n" % patternsAndSupport)
-
-    def getPatterns(self):
-        """ Function to send the set of frequent patterns after completion of the mining process
-
-        :return: returning frequent patterns
-        :rtype: dict
+        Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        :return: returning RSS memory consumed by the mining process
+        :rtype: float
         """
-        return self._finalPatterns
+
+        return self._memoryRSS
 
     def printResults(self):
-        print("Total number of  Top-k Periodic Frequent Patterns:", len(self.getPatterns()))
+        """
+        This function is used to print the results
+        """
+        print("Total number of Stable Periodic  Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:",  self.getRuntime())
-
+        print("Total ExecutionTime in ms:", self.getRuntime())
 
-if __name__ == "__main__":
+if __name__ == '__main__':
     _ap = str()
-    if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
-        if len(_ab._sys.argv) == 5:
-            _ap = kPFPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
-        if len(_ab._sys.argv) == 4:
-            _ap = kPFPMiner(_ab._sys.argv[1], _ab._sys.argv[3])
+    if len(_ab._sys.argv) == 6 or len(_ab._sys.argv) == 7:
+        if len(_ab._sys.argv) == 7:
+            _ap = SPPEclat(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5], _ab._sys.argv[6])
+        if len(_ab._sys.argv) == 6:
+            _ap = SPPEclat(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
         _ap.startMine()
-        _Patterns = _ap.getPatterns()
-        print("Total number of top-k periodic frequent patterns:", len(_Patterns))
+        _ap.mine()
+        print("Total number of Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
-        _memUSS = _ap.getMemoryUSS()
-        print("Total Memory in USS:", _memUSS)
-        _memRSS = _ap.getMemoryRSS()
-        print("Total Memory in RSS", _memRSS)
-        _run = _ap.getRuntime()
-        print("Total ExecutionTime in ms:", _run)
+        print("Total Memory in USS:", _ap.getMemoryUSS())
+        print("Total Memory in RSS", _ap.getMemoryRSS())
+        print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
-
-
```

### Comparing `pami-2024.5.28.1/PAMI/recurringPattern/basic/RPGrowth.py` & `pami-2024.5.7.1/PAMI/recurringPattern/basic/RPGrowth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/recurringPattern/basic/abstract.py` & `pami-2024.5.7.1/PAMI/recurringPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/relativeFrequentPattern/basic/RSFPGrowth.py` & `pami-2024.5.7.1/PAMI/relativeFrequentPattern/basic/RSFPGrowth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/relativeFrequentPattern/basic/abstract.py` & `pami-2024.5.7.1/PAMI/relativeFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/relativeHighUtilityPattern/basic/RHUIM.py` & `pami-2024.5.7.1/PAMI/relativeHighUtilityPattern/basic/RHUIM.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/relativeHighUtilityPattern/basic/abstract.py` & `pami-2024.5.7.1/PAMI/relativeHighUtilityPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/sequentialPatternMining/basic/SPADE.py` & `pami-2024.5.7.1/PAMI/sequentialPatternMining/basic/SPADE.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/sequentialPatternMining/basic/SPAM.py` & `pami-2024.5.7.1/PAMI/sequentialPatternMining/basic/SPAM.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/sequentialPatternMining/basic/abstract.py` & `pami-2024.5.7.1/PAMI/sequentialPatternMining/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/sequentialPatternMining/basic/prefixSpan.py` & `pami-2024.5.7.1/PAMI/sequentialPatternMining/basic/prefixSpan.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/sequentialPatternMining/closed/abstract.py` & `pami-2024.5.7.1/PAMI/sequentialPatternMining/closed/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/stablePeriodicFrequentPattern/basic/SPPEclat.py` & `pami-2024.5.7.1/PAMI/uncertainFrequentPattern/basic/UVECLAT.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,26 +1,26 @@
-# Stable periodic pattern mining aims to discover all interesting patterns in a temporal database using three constraints minimum support,
-# maximum period and maximum liability, that have support no less than the user-specified minimum support  constraint and liability no
-# greater than maximum liability.
+# UVEclat is one of the fundamental algorithm to discover frequent patterns in an uncertain transactional database using PUF-Tree.
 #
 # **Importing this algorithm into a python program**
-# --------------------------------------------------------
 #
+#             from PAMI.uncertainFrequentPattern.basic import UVECLAT as alg
 #
-#             from PAMI.stablePeriodicFrequentPattern.basic import basic as alg
+#             iFile = 'sampleDB.txt'
 #
-#             obj = alg.SPPEclat("../basic/sampleTDB.txt", 5, 3, 3)
+#             minSup = 10  # can also be specified between 0 and 1
 #
-#             obj.startMine()
+#             obj = alg.UVEclat(iFile, minSup)
 #
-#             Patterns = obj.getPatterns()
+#             obj.mine()
 #
-#             print("Total number of Stable Periodic Frequent Patterns:", len(Patterns))
+#             frequentPatterns = obj.getPatterns()
 #
-#             obj.save("patterns")
+#             print("Total number of Frequent Patterns:", len(frequentPatterns))
+#
+#             obj.save(oFile)
 #
 #             Df = obj.getPatternsAsDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
 #             print("Total Memory in USS:", memUSS)
 #
@@ -29,438 +29,562 @@
 #             print("Total Memory in RSS", memRSS)
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
-
-
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
-
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
-
+     
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
-
+     
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
-     Copyright (C)  2021 Rage Uday Kiran
-
 """
-import pandas as pd
-from deprecated import deprecated
 
-from PAMI.stablePeriodicFrequentPattern.basic import abstract as _ab
+import operator as _operator
+from PAMI.uncertainFrequentPattern.basic import abstract as _ab
+import deprecated
+
+_minSup = float()
+_finalPatterns = {}
 
-class SPPEclat(_ab._stablePeriodicFrequentPatterns):
+
+class _Item:
     """
-    :Description:   Stable periodic pattern mining aims to dicover all interesting patterns in a temporal database using three contraints minimum support,
-                    maximum period and maximum lability, that have support no less than the user-specified minimum support  constraint and lability no
-                    greater than maximum lability.
-
-    :Reference:   Fournier-Viger, P., Yang, P., Lin, J. C.-W., Kiran, U. (2019). Discovering Stable Periodic-Frequent Patterns in Transactional Data. Proc.
-                  32nd Intern. Conf. on Industrial, Engineering and Other Applications of Applied Intelligent Systems (IEA AIE 2019), Springer LNAI, pp. 230-244
-
-    :param  iFile: str :
-                   Name of the Input file to mine complete set of stable periodic Frequent Pattern.
-    :param  oFile: str :
-                   Name of the output file to store complete set of stable periodic Frequent Pattern.
-    :param  minSup: float or int or str :
-                    The user can specify minSup either in count or proportion of database size.
-                    If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
-                    Otherwise, it will be treated as float.
-                    Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
-    :param  itemSup: int or float :
-                    Frequency of an item
-    :param maxLa: float :
-                  minimum loss of a pattern
-    :param  sep: str :
-                 This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+    A class used to represent the item with probability in transaction of dataset
+
+    :Attributes:
+
+        item : int or word
+          Represents the name of the item
+
+        probability : float
+          Represent the existential probability(likelihood presence) of an item
+    """
+
+    def __init__(self, item, probability):
+        self.item = item
+        self.probability = probability
 
 
+class UVEclat(_ab._frequentPatterns):
+    """
+    About this algorithm
+    ====================
+
+    :Description: It is one of the fundamental algorithm to discover frequent patterns in an uncertain transactional database using PUF-Tree.
+
+    :Reference:  Carson Kai-Sang Leung, Lijing Sun: "Equivalence class transformation based mining of frequent itemsets from uncertain data",
+                 SAC '11: Proceedings of the 2011 ACM Symposium on Applied ComputingMarch, 2011, Pages 983984,
+                 https://doi.org/10.1145/1982185.1982399
+
     :Attributes:
 
         iFile : file
             Name of the Input file or path of the input file
+
         oFile : file
             Name of the output file or path of the output file
-        minSup : int or float or str
+
+        minSup : float or int or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
-        maxPer : int or float or str
-            The user can specify maxPer either in count or proportion of database size.
-            If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
-            Otherwise, it will be treated as float.
-            Example: maxPer=10 will be treated as integer, while maxPer=10.0 will be treated as float
-        maxLa : int or float or str
-            The user can specify maxLa either in count or proportion of database size.
-            If the program detects the data type of maxLa is integer, then it treats maxLa is expressed in count.
-            Otherwise, it will be treated as float.
-            Example: maxLa=10 will be treated as integer, while maxLa=10.0 will be treated as float
+
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator.
+
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
+
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
+
         startTime:float
             To record the start time of the mining process
+
         endTime:float
             To record the completion time of the mining process
+
         Database : list
             To store the transactions of a database in list
+
         mapSupport : Dictionary
             To maintain the information of item and their frequency
+
         lno : int
-            it represents the total no of transactions
+            To represent the total no of transaction
+
         tree : class
-            it represents the Tree class
+            To represent the Tree class
+
         itemSetCount : int
-            it represents the total no of patterns
+            To represents the total no of patterns
+
         finalPatterns : dict
-            it represents to store the patterns
-        tidList : dict
-            stores the timestamps of an item
+            To store the complete patterns
 
     :Methods:
-
-        startMine()
+        mine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
-        save(oFile)
-            Complete set of periodic-frequent patterns will be loaded in to an output file
-        getPatternsAsDataFrame()
-            Complete set of periodic-frequent patterns will be loaded in to a dataframe
+        storePatternsInFile(oFile)
+            Complete set of frequent patterns will be loaded in to a output file
+        getPatternsInDataFrame()
+            Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
-        creatingItemSets()
-            Scan the database and store the items with their timestamps which are periodic frequent
-        calculateLa()
-            Calculates the support and period for a list of timestamps.
-        Generation()
-            Used to implement prefix class equivalence method to generate the periodic patterns recursively
+        creatingItemSets(fileName)
+            Scans the dataset and stores in a list format
+        frequentOneItem()
+            Extracts the one-length frequent patterns from database
 
+    Execution methods
+    =================
 
 
-    **Methods to execute code on terminal**
-    -----------------------------------------
-    .. code-block:: console
+    **Terminal command**
 
 
-       Format:
+    .. code-block:: console
+
+      Format:
 
-       (.venv) $ python3 basic.py <inputFile> <outputFile> <minSup> <maxPer> <maxLa>
+      (.venv) $ python3 uveclat.py <inputFile> <outputFile> <minSup>
 
-       Example usage:
+      Example Usage:
 
-       (.venv) $ python3 basic.py sampleDB.txt patterns.txt 10.0 4.0 2.0
+      (.venv) $ python3 uveclat.py sampleDB.txt patterns.txt 3
 
+    .. note:: minSup can be specified  in support count or a value between 0 and 1.
 
-               .. note:: constraints will be considered in percentage of database transactions
 
-    **Importing this algorithm into a python program**
+    **Calling from a python program**
     ---------------------------------------------------
-    ... code-block:: python
+    .. code-block:: python
+
+            from PAMI.uncertainFrequentPattern.basic import UVECLAT as alg
+
+            iFile = 'sampleDB.txt'
 
-                    from PAMI.stablePeriodicFrequentPattern.basic import basic as alg
+            minSup = 10  # can also be specified between 0 and 1
 
-                    obj = alg.PFPECLAT("../basic/sampleTDB.txt", 5, 3, 3)
+            obj = alg.UVEclat(iFile, minSup)
 
-                    obj.startMine()
+            obj.mine()
 
-                    Patterns = obj.getPatterns()
+            frequentPatterns = obj.getPatterns()
 
-                    print("Total number of Stable Periodic Frequent Patterns:", len(Patterns))
+            print("Total number of Frequent Patterns:", len(frequentPatterns))
 
-                    obj.save("patterns")
+            obj.save(oFile)
 
-                    Df = obj.getPatternsAsDataFrame()
+            Df = obj.getPatternsAsDataFrame()
 
-                    memUSS = obj.getMemoryUSS()
+            memUSS = obj.getmemoryUSS()
 
-                    print("Total Memory in USS:", memUSS)
+            print("Total Memory in USS:", memUSS)
 
-                    memRSS = obj.getMemoryRSS()
+            memRSS = obj.getMemoryRSS()
 
-                    print("Total Memory in RSS", memRSS)
+            print("Total Memory in RSS", memRSS)
 
-                    run = obj.getRuntime()
+            run = obj.getRuntime()
 
-                    print("Total ExecutionTime in seconds:", run)
+            print("Total ExecutionTime in seconds:", run)
 
-    **Credits:**
-    --------------
-             The complete program was written by  P.Likhitha under the supervision of Professor Rage Uday Kiran.
+    Credits
+    =======
 
-       """
+             The complete program was written by   P.Likhitha   under the supervision of Professor Rage Uday Kiran.
+    """
+    _startTime = float()
+    _endTime = float()
+    _minSup = str()
+    _finalPatterns = {}
     _iFile = " "
     _oFile = " "
-    _minSup = str()
-    _maxPer = str()
-    _maxLa = float()
     _sep = " "
-    _SPPList = {}
-    _itemList = []
-    _last = int()
-    _finalPatterns = {}
-    _tsList = {}
-    _startTime = float()
-    _endTime = float()
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
+    _tidList = {}
+    _rank = {}
 
-    def __init__(self, inputFile, minSup, maxPer, maxLa, sep='\t'):
-        self._iFile = inputFile
-        self._minSup = minSup
-        self._maxPer = maxPer
-        self._maxLa = maxLa
-        self._sep = sep
-
-    def _creatingItemsets(self):
+    def _creatingItemSets(self):
         """
-        Storing the complete transactions of the database/input file in a database variable
+        Scans the dataset
         """
         self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
+            uncertain, data = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
                 self._Database = self._iFile['Transactions'].tolist()
-            if 'Patterns' in i:
-                self._Database = self._iFile['Patterns'].tolist()
+            if 'uncertain' in i:
+                uncertain = self._iFile['uncertain'].tolist()
+            for k in range(len(data)):
+                tr = []
+                for j in range(len(data[k])):
+                    product = _Item(data[k][j], uncertain[k][j])
+                    tr.append(product)
+                self._Database.append(tr)
+
+            # print(self.Database)
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
+                    tr = []
+                    for i in temp:
+                        i1 = i.index('(')
+                        i2 = i.index(')')
+                        item = i[0:i1]
+                        probability = float(i[i1 + 1:i2])
+                        product = _Item(item, probability)
+                        tr.append(product)
                     self._Database.append(temp)
             else:
                 try:
-                    with open(self._iFile, 'r', encoding='utf-8') as f:
+                    with open(self._iFile, 'r') as f:
                         for line in f:
-                            line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
-                            self._Database.append(temp)
+                            tr = []
+                            for i in temp:
+                                i1 = i.index('(')
+                                i2 = i.index(')')
+                                item = i[0:i1]
+                                probability = float(i[i1 + 1:i2])
+                                product = _Item(item, probability)
+                                tr.append(product)
+                            self._Database.append(tr)
                 except IOError:
                     print("File Not Found")
-                    quit()
 
-    def _convert(self, value):
+    def _frequentOneItem(self):
+        """
+        Takes the self.Database and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
         """
-        to convert the type of user specified minSup value
+        mapSupport = {}
+        k = 0
+        for i in self._Database:
+            k += 1
+            for j in i:
+                if j.item not in mapSupport:
+                    mapSupport[str(j.item)] = j.probability
+                    self._tidList[str(j.item)] = {k: j.probability}
+                else:
+                    mapSupport[str(j.item)] += j.probability
+                    self._tidList[str(j.item)].update({k: j.probability})
+        mapSupport = {k: v for k, v in mapSupport.items() if v >= self._minSup}
+        plist = dict( sorted(mapSupport.items(), key=_operator.itemgetter(1),reverse=True))
+        return list(plist.keys())
+
+    @staticmethod
+    def _check(i, x):
+        """
+        To check the presence of item or pattern in transaction
+
+        :param x: it represents the pattern
+        :type x : list
+        :param i : represents the uncertain self.Database
+        :type i : list
+        """
+
+        # This method taken a transaction as input and returns the tree
+        for m in x:
+            k = 0
+            for n in i:
+                if m == n.item:
+                    k += 1
+            if k == 0:
+                return 0
+        return 1
+
+    @staticmethod
+    def _convert(value):
+        """
+        To convert the type of user specified minSup value
 
         :param value: user specified minSup value
-        :return: converted type
+        :return: converted type minSup value
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
-            value = (len(self._Database) * value)
+            value = float(value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
-                value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
-    def _createSPPList(self):
-        """
-        to convert the single length stable periodic patterns
+    def _removeFalsePositives(self):
         """
-        tidLast = {}
-        la = {}
-        self._SPPList = {}
-        self._tsList = {}
-        for transaction in self._Database:
-            ts = int(transaction[0])
-            for item in transaction[1:]:
-                if item not in self._SPPList:
-                    la[item] = max(0, ts - self._maxPer)
-                    self._SPPList[item] = [1, la[item]]
-                    self._tsList[item] = [ts]
-                else:
-                    s = self._SPPList[item][0] + 1
-                    la[item] = max(0, la[item] + ts - tidLast.get(item) - self._maxPer)
-                    self._SPPList[item] = [s, max(la[item], self._SPPList[item][1])]
-                    self._tsList[item].append(ts)
-                tidLast[item] = ts
-            self._last = ts
-        for item in self._SPPList:
-            la[item] = max(0, la[item] + self._last - tidLast[item] - self._maxPer)
-            self._SPPList[item][1] = max(la[item], self._SPPList[item][1])
-        self._SPPList = {k: v for k, v in self._SPPList.items() if v[0] >= self._minSup and v[1] <= self._maxLa}
-        self._SPPList = {k: v for k, v in sorted(self._SPPList.items(), key=lambda x: x[1][0], reverse=True)}
-        self._Generation(list(self._SPPList), set())
-
-    def _Generation(self, GPPFList, CP):
-        """
-        To generate the patterns using depth-first search
-        """
-        for i in range(len(GPPFList)):
-            item = GPPFList[i]
-            CP1 = CP | {item}
-            if CP != set():
-                self._tsList['\t'.join(CP1)] = list(set(self._tsList['\t'.join(CP)]) & set(self._tsList[item]))
-            la = self._calculateLa(self._tsList['\t'.join(CP1)])
-            support = len(self._tsList['\t'.join(CP1)])
-            if la <= self._maxLa and len(self._tsList['\t'.join(CP1)]) >= self._minSup:
-                #CP = CP1
-                self._finalPatterns['\t'.join(CP1)] = [support, la]
-                if i+1 < len(GPPFList):
-                    self._Generation(GPPFList[i+1:], CP1)
-
-    def _calculateLa(self, tsList):
-        """
-        To calculate the liability of a patterns based on its timestamps
-        """
-        previous = 0
-        la = 0
-        tsList = sorted(tsList)
-        laList = []
-        for ts in tsList:
-            la = max(0, la + ts - previous - self._maxPer)
-            laList.append(la)
-            previous = ts
-            
-        la = max(0, la + self._last - previous - self._maxPer)
-        laList.append(la)
-        maxla = max(laList)
-        return maxla
+        To remove the false positive patterns generated in frequent patterns
 
-    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
-    def startMine(self):
+        :return: patterns with accurate probability
         """
-        Method to start the mining of patterns
-        """
-        self.mine()
+        global _finalPatterns
+        periods = {}
+        for i in self._Database:
+            for x, y in _finalPatterns.items():
+                if len(x) == 1:
+                    periods[x] = y
+                else:
+                    s = 1
+                    check = self._check(i, x)
+                    if check == 1:
+                        for j in i:
+                            if j.item in x:
+                                s *= j.probability
+                        if x in periods:
+                            periods[x] += s
+                        else:
+                            periods[x] = s
+        for x, y in periods.items():
+            if y >= self._minSup:
+                sample = str()
+                for i in x:
+                    sample = sample + i + "\t"
+                self._finalPatterns[sample] = y
+
+    @staticmethod
+    def _Intersection(tidSetx, tidSetY):
+        """
+        This function is used to find the intersection
+
+        :param tidSetx: the timestamp of a patterns
+        :type tidSetx: dict
+        :param tidSetY: the timestamp of a patterns
+        :type tidSetY: dict
+        """
+        tids = []
+        support = []
+        tidDict = {}
+        for x, y in tidSetx.items():
+            for x1, y1 in tidSetY.items():
+                if x == x1:
+                    tids.append(x)
+                    support.append(y * y1)
+                    tidDict.update({x: y * y1})
+        return tidDict
+
+    def _calculateExpSup(self, tidList):
+        """
+        This function is used to calculate support of tidList
+
+        :param tidList: timestamp of a list.
+        :type tidList: List
+        """
+        return sum(tidList.values())
+
+    def _save(self, prefix, suffix, tidSetI):
+        """
+        Saves the patterns that satisfy the periodic frequent property.
+
+        :param prefix: the prefix of a pattern
+        :type prefix: list
+        :param suffix: the suffix of a patterns
+        :type suffix: list
+        :param tidSetI: the timestamp of a patterns
+        :type tidSetI: dict
+        """
+
+        global _finalPatterns
+        if prefix is None:
+            prefix = suffix
+        else:
+            prefix = prefix + suffix
+        val = self._calculateExpSup(tidSetI)
+        _finalPatterns[tuple(prefix)] = val
+
+    def _Generation(self, prefix, itemSets, tidSets):
+        """
+        Equivalence class is followed  and checks for the patterns generated for periodic-frequent patterns.
+
+        :param prefix:  main equivalence prefix
+        :type prefix: periodic-frequent item or pattern
+        :param itemSets: patterns which are items combined with prefix and satisfying the periodicity and frequent with their timestamps
+        :type itemSets: list
+        :param tidSets: timestamps of the items in the argument itemSets
+        :type tidSets: list
+        """
+        if len(itemSets) == 1:
+            i = itemSets[0]
+            tidI = tidSets[0]
+            self._save(prefix, [i], tidI)
+            return
+        for i in range(len(itemSets)):
+            itemI = itemSets[i]
+            if itemI is None:
+                continue
+            tidSetI = tidSets[i]
+            classItemSets = []
+            classTidSets = []
+            itemSetX = [itemI]
+            for j in range(i + 1, len(itemSets)):
+                itemJ = itemSets[j]
+                tidSetJ = tidSets[j]
+                y = self._Intersection(tidSetI, tidSetJ)
+                if self._calculateExpSup(y) >= self._minSup:
+                    classItemSets.append(itemJ)
+                    classTidSets.append(y)
+            newPrefix = list(set(itemSetX)) + prefix
+            self._Generation(newPrefix, classItemSets, classTidSets)
+            self._save(prefix, list(set(itemSetX)), tidSetI)
 
     def mine(self):
         """
-        Method to start the mining of patterns
+        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
         """
+        global _minSup
         self._startTime = _ab._time.time()
-        self._creatingItemsets()
+        self._creatingItemSets()
         self._minSup = self._convert(self._minSup)
-        self._maxPer = self._convert(self._maxPer)
-        self._maxLa = self._convert(self._maxLa)
-        self._finalPatterns = {}
-        #print(self._minSup, self._maxPer, self._maxLa)
-        self._createSPPList()
+        _minSup = self._minSup
+        plist = self._frequentOneItem()
+        for i in range(len(plist)):
+            itemI = plist[i]
+            tidSetI = self._tidList[itemI]
+            itemSetX = [itemI]
+            itemSets = []
+            tidSets = []
+            for j in range(i+1, len(plist)):
+                itemJ = plist[j]
+                tidSetJ = self._tidList[itemJ]
+                y1 = self._Intersection(tidSetI, tidSetJ)
+                if self._calculateExpSup(y1) >= self._minSup:
+                    itemSets.append(itemJ)
+                    tidSets.append(y1)
+            self._Generation(itemSetX, itemSets, tidSets)
+            self._save(None, itemSetX, tidSetI)
+        self._removeFalsePositives()
+        print("Frequent patterns were generated from uncertain databases successfully using PUF algorithm")
         self._endTime = _ab._time.time()
-        self._memoryUSS = float()
-        self._memoryRSS = float()
         process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryRSS = float()
+        self._memoryUSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
-        print("Stable Periodic Frequent patterns were generated successfully using basic algorithm ")
 
-
-    def getRuntime(self):
+    def getMemoryUSS(self):
         """
-        Calculating the total amount of runtime taken by the mining process
 
-        :return: returning total amount of runtime taken by the mining process
+        Total amount of USS memory consumed by the mining process will be retrieved from this function
+
+        :return: returning USS memory consumed by the mining process
         :rtype: float
         """
-        return self._endTime - self._startTime
 
-    def getPatterns(self):
-        """
-        Function to return the set of stable periodic-frequent patterns after completion of the mining process
+        return self._memoryUSS
 
-        :return: returning stable periodic-frequent patterns
-        :rtype: dict
+    def getMemoryRSS(self):
         """
-        return self._finalPatterns
 
-    def getMemoryUSS(self):
-        """
-        Total amount of USS memory consumed by the mining process will be retrieved from this function
-        :return: returning USS memory consumed by the mining process
+        Total amount of RSS memory consumed by the mining process will be retrieved from this function
+
+        :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
-        return self._memoryUSS
+        return self._memoryRSS
 
-    def save(self, outFile):
+    def getRuntime(self):
         """
-        Complete set of periodic-frequent patterns will be loaded in to an output file
 
-        :param outFile: name of the output file
-        :type outFile: csv file
+        Calculating the total amount of runtime taken by the mining process
+
+        :return: returning total amount of runtime taken by the mining process
+        :rtype: float
         """
-        self._oFile = outFile
-        writer = open(self._oFile, 'w+')
-        for x, y in self._finalPatterns.items():
-            s1 = x.strip() + ":" + str(y[0]) + ":" + str(y[1])
-            writer.write("%s \n" % s1)
+
+        return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self):
         """
-        Storing final periodic-frequent patterns in a dataframe
 
-        :return: returning periodic-frequent patterns in a dataframe
+        Storing final frequent patterns in a dataframe
+
+        :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
-        dataFrame = {}
+        dataframe = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a.replace('\t', ' '), b[0], b[1]])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
-        return dataFrame
+            data.append([a.replace('\t', ' '), b])
+            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
+        return dataframe
 
-    def getMemoryRSS(self):
+    def save(self, oFile):
         """
-        Total amount of RSS memory consumed by the mining process will be retrieved from this function
-        :return: returning RSS memory consumed by the mining process
-        :rtype: float
+
+        Complete set of frequent patterns will be loaded in to an output file
+
+        :param oFile: name of the output file
+        :type oFile: csv file
         """
+        self.oFile = oFile
+        writer = open(self.oFile, 'w+')
+        for x, y in self._finalPatterns.items():
+            s1 = x.strip() + ":" + str(y)
+            writer.write("%s \n" % s1)
 
-        return self._memoryRSS
+    def getPatterns(self):
+        """
+
+        Function to send the set of frequent patterns after completion of the mining process
+
+        :return: returning frequent patterns
+        :rtype: dict
+        """
+        return self._finalPatterns
 
     def printResults(self):
         """
         This function is used to print the results
         """
-        print("Total number of Stable Periodic  Patterns:", len(self.getPatterns()))
+        print("Total number of  Uncertain Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:", self.getRuntime())
+        print("Total ExecutionTime in ms:",  self.getRuntime())
+
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 6 or len(_ab._sys.argv) == 7:
-        if len(_ab._sys.argv) == 7:
-            _ap = SPPEclat(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5], _ab._sys.argv[6])
-        if len(_ab._sys.argv) == 6:
-            _ap = SPPEclat(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
-        _ap.startMine()
+    if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
+        if len(_ab._sys.argv) == 5:
+            _ap = UVEclat(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+        if len(_ab._sys.argv) == 4:
+            _ap = UVEclat(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.mine()
         print("Total number of Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `pami-2024.5.28.1/PAMI/stablePeriodicFrequentPattern/basic/SPPGrowth.py` & `pami-2024.5.7.1/PAMI/stablePeriodicFrequentPattern/basic/SPPGrowth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/stablePeriodicFrequentPattern/basic/SPPGrowthDump.py` & `pami-2024.5.7.1/PAMI/stablePeriodicFrequentPattern/basic/SPPGrowthDump.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/stablePeriodicFrequentPattern/basic/abstract.py` & `pami-2024.5.7.1/PAMI/stablePeriodicFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/stablePeriodicFrequentPattern/topK/TSPIN.py` & `pami-2024.5.7.1/PAMI/stablePeriodicFrequentPattern/topK/TSPIN.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/stablePeriodicFrequentPattern/topK/abstract.py` & `pami-2024.5.7.1/PAMI/stablePeriodicFrequentPattern/topK/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/subgraphMining/basic/abstract.py` & `pami-2024.5.7.1/PAMI/subgraphMining/basic/abstract.py`

 * *Files 2% similar despite different names*

```diff
@@ -12,15 +12,15 @@
 import psutil as _psutil
 import os as _os
 
 
 class _gSpan(ABC):
 
     @abstractmethod
-    def mine(self):
+    def startMine(self):
         """
         Run the gSpan algorithm.
         """
         pass
 
     @abstractmethod
     def readGraphs(self, path):
```

### Comparing `pami-2024.5.28.1/PAMI/subgraphMining/basic/dfsCode.py` & `pami-2024.5.7.1/PAMI/subgraphMining/basic/dfsCode.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/subgraphMining/basic/edge.py` & `pami-2024.5.7.1/PAMI/subgraphMining/basic/edge.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/subgraphMining/basic/extendedEdge.py` & `pami-2024.5.7.1/PAMI/subgraphMining/basic/extendedEdge.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/subgraphMining/basic/frequentSubgraph.py` & `pami-2024.5.7.1/PAMI/subgraphMining/basic/frequentSubgraph.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/subgraphMining/basic/graph.py` & `pami-2024.5.7.1/PAMI/subgraphMining/basic/graph.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/subgraphMining/basic/gspan.py` & `pami-2024.5.7.1/PAMI/subgraphMining/basic/gspan.py`

 * *Files 3% similar despite different names*

```diff
@@ -2,15 +2,17 @@
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
 #             from PAMI.subgraphMining.basic import gspan as alg
 #
 #             obj = alg.GSpan(iFile, minSupport)
 #
-#             obj.mine()
+#             obj.startMine()
+#
+#             obj.run()
 #
 #             frequentGraphs = obj.getFrequentSubgraphs()
 #
 #             memUSS = obj.getMemoryUSS()
 #
 #             obj.save(oFile)
 #
@@ -59,15 +61,15 @@
         self.outputSingleVertices = outputSingleVertices
         self.maxNumberOfEdges = maxNumberOfEdges
         self.outputGraphIds = outputGraphIds
         self._memoryUSS = float()
         self._memoryRSS = float()
 
 
-    def mine(self):
+    def startMine(self):
 
         if self.maxNumberOfEdges <= 0:
             return
         
         self.frequentSubgraphs = []
 
         self.patternCount = 0
@@ -106,15 +108,15 @@
 
 
     def save(self, oFile):
         """
         The `save` function writes information about frequent subgraphs to a specified
         output file in a specific format.
         
-        :param oFile: The `save` method is used to write the results of frequent
+        :param outputPath: The `save` method is used to write the results of frequent
         subgraphs to a file specified by the `outputPath` parameter. The method iterates over each
         frequent subgraph in `self.frequentSubgraphs` and writes the subgraph information to the file
         """
         with open(oFile, 'w') as bw:
             i = 0
             for subgraph in self.frequentSubgraphs:
                 sb = []
@@ -227,15 +229,15 @@
                 mappedV1 = iso.get(v1)
                 # Forward edge
                 if v1 < v2:
                     mappedVertices = list(iso.values())
                     for mappedV2 in g.getAllNeighbors(mappedV1):
                         if (v2Label == mappedV2.getLabel() and
                             mappedV2.getId() not in mappedVertices and
-                                eLabel == g.getEdgeLabel(mappedV1, mappedV2.getId())):
+                            eLabel == g.getEdgeLabel(mappedV1, mappedV2.getId())):
 
                             tempM = iso.copy()
                             tempM[v2] = mappedV2.getId()
 
                             updateIsoms.append(tempM)
 
                 # Backward edge
@@ -441,40 +443,40 @@
         :param c: The parameter `c` is an instance of the `_ab.DFSCode` class
         :type c: _ab.DFSCode
         :return: a boolean value. It returns True if the input DFSCode `c` is canonical, and False if it is
         not canonical.
         """
         canC = _ab.DFSCode()
         for i in range(c.size):
-            extensions = self.rightMostPathExtensionsFromSingle(canC, _ab.Graph(-1, None, c))
+            extensions = self.rightMostPathExtensionsFromSingle(canC, _ab.Graph(c))
             minEe = None
             for ee in extensions.keys():
                 if minEe is None or ee.smallerThan(minEe):
                     minEe = ee
 
             if minEe is not None and minEe.smallerThan(c.getAt(i)):
                 return False
             
             if minEe is not None:
                 canC.add(minEe)
         return True
     
 
-    def gSpan(self, graphDb, outputSingleVertices):
+    def gSpan(self, graphDb, outputFrequentVertices):
         """
         The gSpan function in Python processes a graph database by precalculating vertex lists, removing
         infrequent vertex pairs, and performing a depth-first search algorithm.
         
         :param graphDb: The `graphDb` parameter  refers to a graph database that the algorithm is 
         operating on.
-        :param outputSingleVertices: The `outputFrequentVertices` parameter is a boolean flag that
-        determines whether single vertices should be output or not.
+        :param outputFrequentVertices: The `outputFrequentVertices` parameter is a boolean flag that
+        determines whether the frequent vertices should be output or not.
         """
-        if outputSingleVertices or GSpan.eliminate_infrequent_vertices:
-            self.findAllOnlyOneVertex(graphDb, outputSingleVertices)
+        if outputFrequentVertices or GSpan.eliminate_infrequent_vertices:
+            self.findAllOnlyOneVertex(graphDb, outputFrequentVertices)
 
         for g in graphDb:
             g.precalculateVertexList()
 
         if GSpan.eliminate_infrequent_vertex_pairs or GSpan.eliminate_infrequent_edge_labels:
             self.removeInfrequentVertexPairs(graphDb)
 
@@ -517,15 +519,15 @@
         The function `findAllOnlyOneVertex` iterates through a graph database to find frequent vertices
         based on a minimum support threshold, storing the results and optionally removing infrequent
         vertices.
         
         :param graphDb: The `graphDb` parameter  refers to a graph database that the algorithm is 
         operating on.
         :param outputFrequentVertices: The `outputFrequentVertices` parameter is a boolean flag that
-        determines whether single vertices should be included in the output or not.
+        determines whether the frequent vertices should be included in the output or not.
         """
         self.frequentVertexLabels = []
         labelM = {} 
         for g in graphDb:
             for v in g.getNonPrecalculatedAllVertices():
                 if v.getEdgeList():
                     vLabel = v.getLabel()
@@ -645,38 +647,8 @@
                     subgraphDescription.append(f"v {j} {vLabel}")
                 for ee in dfsCode.getEeList():
                     subgraphDescription.append(f"e {ee.v1} {ee.v2} {ee.edgeLabel}")
             
             sb.append('\n'.join(subgraphDescription))  
         return '\n'.join(sb)  
 
-    def getSubgraphGraphMapping(self):
-        """
-        Return a list of mappings from subgraphs to the graph IDs they belong to in the format <FID, Clabel, GIDs[]>.
-        """
-        mappings = []
-        for i, subgraph in enumerate(self.frequentSubgraphs):
-            mapping = {
-                "FID": i,
-                "Clabel": str(subgraph.dfsCode),
-                "GIDs": list(subgraph.setOfGraphsIds)
-            }
-            mappings.append(mapping)
-        return mappings
-
-    def saveSubgraphsByGraphId(self, oFile):
-        """
-        Save subgraphs by graph ID as a flat transaction, such that each row represents the graph ID and each row can contain multiple subgraph IDs.
-        """
-        graphToSubgraphs = {}
-        
-        for i, subgraph in enumerate(self.frequentSubgraphs):
-            for graphId in subgraph.setOfGraphsIds:
-                if graphId not in graphToSubgraphs:
-                    graphToSubgraphs[graphId] = []
-                graphToSubgraphs[graphId].append(i)
-
-        graphToSubgraphs = {k: graphToSubgraphs[k] for k in sorted(graphToSubgraphs)}
 
-        with open(oFile, 'w') as f:
-            for _, subgraphIds in graphToSubgraphs.items():
-                f.write(f"{' '.join(map(str, subgraphIds))}\n")
```

### Comparing `pami-2024.5.28.1/PAMI/subgraphMining/basic/sparseTriangularMatrix.py` & `pami-2024.5.7.1/PAMI/subgraphMining/basic/sparseTriangularMatrix.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/subgraphMining/basic/vertex.py` & `pami-2024.5.7.1/PAMI/subgraphMining/basic/vertex.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/subgraphMining/topK/DFSCode.py` & `pami-2024.5.7.1/PAMI/subgraphMining/topK/DFSCode.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/subgraphMining/topK/DFSThread.py` & `pami-2024.5.7.1/PAMI/subgraphMining/topK/DFSThread.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/subgraphMining/topK/abstract.py` & `pami-2024.5.7.1/PAMI/subgraphMining/topK/abstract.py`

 * *Files 2% similar despite different names*

```diff
@@ -15,17 +15,17 @@
 import os as _os
 import time
 
 
 class _TKG(ABC):
 
     @abstractmethod
-    def mine(self):
+    def startMine(self):
         """
-        Run the tkg algorithm.
+        Run the gSpan algorithm.
         """
         pass
 
     @abstractmethod
     def readGraphs(self, path):
         """
         Read graphs from a file.
```

### Comparing `pami-2024.5.28.1/PAMI/subgraphMining/topK/edge.py` & `pami-2024.5.7.1/PAMI/subgraphMining/topK/edge.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/subgraphMining/topK/extendedEdge.py` & `pami-2024.5.7.1/PAMI/subgraphMining/topK/extendedEdge.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/subgraphMining/topK/frequentSubgraph.py` & `pami-2024.5.7.1/PAMI/subgraphMining/topK/frequentSubgraph.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/subgraphMining/topK/graph.py` & `pami-2024.5.7.1/PAMI/subgraphMining/topK/graph.py`

 * *Files 2% similar despite different names*

```diff
@@ -40,15 +40,15 @@
     def removeInfrequentLabel(self, label):
         toRemove = [key for key, vertex in self.vMap.items() if vertex.getLabel() == label]
         for key in toRemove:
             del self.vMap[key]
 
         for vertex in self.vMap.values():
             edgesToRemove = [edge for edge in vertex.getEdgeList() 
-                               if edge.v1 not in self.vMap or edge.v2 not in self.vMap]
+                               if edge.getV1() not in self.vMap or edge.getV2() not in self.vMap]
 
             for edge in edgesToRemove:
                 vertex.getEdgeList().remove(edge)
 
     def precalculateVertexNeighbors(self):
         self.neighborCache = {}
         self.edgeCount = 0
```

### Comparing `pami-2024.5.28.1/PAMI/subgraphMining/topK/sparseTriangularMatrix.py` & `pami-2024.5.7.1/PAMI/subgraphMining/topK/sparseTriangularMatrix.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/subgraphMining/topK/tkg.py` & `pami-2024.5.7.1/PAMI/subgraphMining/topK/tkg.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 # from PAMI.subgraphMining.topK import tkg as alg
 
 # obj = alg.TKG(iFile, k)
 
-# obj.mine()
+# obj.startMine()
 
 # frequentGraphs = obj.getKSubgraphs()
 
 # memUSS = obj.getMemoryUSS()
 
 # obj.save(oFile)
 
@@ -38,28 +38,29 @@
 
     def __init__(self, iFile, k, maxNumberOfEdges=float('inf'), outputSingleVertices=True, outputGraphIds=False):
         self.iFile = iFile
         self.k = k
         self.outputGraphIds = outputGraphIds
         self.outputSingleVertices = outputSingleVertices
         self.maxNumberOfEdges = maxNumberOfEdges
+        self.frequentSubgraphs = []
         self.graphCount = 0
         self.patternCount = 0
         self.frequentVertexLabels = []
         self.infrequentVerticesRemovedCount = 0
         self.infrequentVertexPairsRemovedCount = 0
         self.skipStrategyCount = 0
         self.threadCount = 1
         self.edgeRemovedByLabel = 0
         self.eliminatedWithMaxSize = 0
         self.emptyGraphsRemoved = 0
         self.pruneByEdgeCount = 0
 
 
-    def mine(self):
+    def startMine(self):
         """
         This Python function starts a mining process on a graph database, calculates runtime, pattern count,
         and memory usage metrics.
         """
         if self.maxNumberOfEdges <= 0:
             return
 
@@ -354,15 +355,16 @@
         for extension, newGraphIds in extensions.items():
             sup = len(newGraphIds)
             if sup >= self.minSup:
                 newC = c.copy()
                 newC.add(extension)
     
                 if self.isCanonical(newC):
-                    self.savePattern(_ab.FrequentSubgraph(newC, newGraphIds, sup))
+                    subgraph = _ab.FrequentSubgraph(newC, newGraphIds, sup)
+                    self.frequentSubgraphs.append(subgraph)
                     self.gspanDfs(newC, graphDB, newGraphIds)
 
     
     def gspanDynamicDFS(self, c, graphDB, graphIds):
         if c.size == self.maxNumberOfEdges - 1:
             return
 
@@ -428,15 +430,15 @@
         for label, tempSupG in labelM.items():                
             sup = len(tempSupG)
             if sup >= self.minSup:
                 self.frequentVertexLabels.append(label)
                 if outputFrequentVertices:
                     tempD = _ab.DfsCode()
                     tempD.add(_ab.ExtendedEdge(0, 0, label, label, -1))
-                    self.savePattern(_ab.FrequentSubgraph(tempD, tempSupG, sup))
+                    self.frequentSubgraphs.append(_ab.FrequentSubgraph(tempD, tempSupG, sup))
             elif TKG.ELIMINATE_INFREQUENT_VERTICES:
                 for graphId in tempSupG:
                     g = graphDB[graphId]
                     g.removeInfrequentLabel(label)
                     self.infrequentVerticesRemovedCount += 1
 
     def removeInfrequentVertexPairs(self, graphDB):
@@ -488,15 +490,15 @@
                     for edge in iterEdges:
                         v2 = edge.another(v1.getId())
                         labelV2 = g.getVLabel(v2)
                         count = matrix.getSupportForItems(v1.getLabel(), labelV2)
 
                         if TKG.ELIMINATE_INFREQUENT_VERTEX_PAIRS and count < self.minSup:
                             v1.removeEdge(edge)
-                            self.infrequentVertexPairsRemovedCount += 1
+                            self.infrequentVertexPairsRemoved += 1
 
                         elif TKG.ELIMINATE_INFREQUENT_EDGE_LABELS and \
                                 mapEdgeLabelToSupport.get(edge.getEdgeLabel(), 0) < self.minSup:
                             v1.removeEdge(edge)
                             self.edgeRemovedByLabel += 1
 
     def getMemoryRSS(self):
@@ -508,42 +510,41 @@
     def getRuntime(self):
         return self.runtime
     
     def getMinSupport(self):
         return self.minSup
     
     def getKSubgraphs(self):
-        """ Return the formatted subgraphs as a single string with correct formatting and newlines. """
-        subgraphsList = self.getSubgraphsList()  
-        sb = [] 
+        subgraphsList = self.getSubgraphsList()
+
         for i, subgraph in enumerate(subgraphsList):
-            subgraphDescription = [f"t # {i} * {subgraph.support}"]  
+            sb = []
             dfsCode = subgraph.dfsCode
+
+            sb.append(f"t # {i} * {subgraph.support}\n")
             if len(dfsCode.eeList) == 1:
                 ee = dfsCode.eeList[0]
-                subgraphDescription.append(f"v 0 {ee.vLabel1}")
+                sb.append(f"v 0 {ee.vLabel1}\n")
                 if ee.edgeLabel != -1:
-                    subgraphDescription.append(f"v 1 {ee.vLabel2}")
-                    subgraphDescription.append(f"e 0 1 {ee.edgeLabel}")
+                    sb.append(f"v 1 {ee.vLabel2}\n")
+                    sb.append(f"e 0 1 {ee.edgeLabel}\n")
             else:
                 vLabels = dfsCode.getAllVLabels()
                 for j, vLabel in enumerate(vLabels):
-                    subgraphDescription.append(f"v {j} {vLabel}")
+                    sb.append(f"v {j} {vLabel}\n")
                 for ee in dfsCode.eeList:
-                    subgraphDescription.append(f"e {ee.v1} {ee.v2} {ee.edgeLabel}")
-
-            # Include graph IDs if the feature is enabled
-            if self.outputGraphIds and subgraph.setOfGraphsIds:
-                subgraphDescription.append("x " + " ".join(str(id) for id in subgraph.setOfGraphsIds))
-            sb.append('\n'.join(subgraphDescription)) 
-        return '\n\n'.join(sb)  
+                    sb.append(f"e {ee.v1} {ee.v2} {ee.edgeLabel}\n")
 
+            if self.outputGraphIds:
+                sb.append("x " + " ".join(str(id) for id in subgraph.setOfGraphsIds))
+            sb.append("\n\n")
+            print("".join(sb))
 
 
-    def getSubgraphsList(self):
+    def getSubgraphs(self):
         """Creates a copy of the queue's contents without emptying the original queue."""
         subgraphsList = list(self.kSubgraphs.queue)
         subgraphsList.sort(key=lambda sg: sg.support, reverse=True)
         return subgraphsList
```

### Comparing `pami-2024.5.28.1/PAMI/subgraphMining/topK/vertex.py` & `pami-2024.5.7.1/PAMI/subgraphMining/topK/vertex.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/uncertainFaultTolerantFrequentPattern/VBFTMine.py` & `pami-2024.5.7.1/PAMI/uncertainFrequentPattern/basic/TubeP.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,508 +1,574 @@
-# VBFTMine is one of the fundamental algorithm to discover fault-tolerant frequent patterns in an uncertain transactional database based on bitset representation.
+# TUFP is one of the fundamental algorithm to discover top-k frequent patterns in a uncertain transactional database using CUP-Lists.
 #
 # **Importing this algorithm into a python program**
 #
-#             import PAMI.uncertainFaultTolerantFrequentPattern.basic.VBFTMine as alg
+#             from PAMI.uncertainFrequentPattern.basic import TUFP as alg
 #
 #             iFile = 'sampleDB.txt'
 #
 #             minSup = 10  # can also be specified between 0 and 1
 #
-#             itemSup = 2  # can also be specified between 0 and 1
-#
-#             minLength = 3 # can also be specified between 0 and 1
-#
-#             faultTolerance = 2 # can also be specified between 0 and 1
-#
-#             obj = alg.VBFTMine(iFile, minSup, itemSup, minLength, faultTolerance)
+#             obj = alg.TUFP(iFile, minSup)
 #
 #             obj.mine()
 #
-#             faultTolerantFrequentPattern = obj.getPatterns()
+#             frequentPatterns = obj.getPatterns()
 #
-#             print("Total number of Fault Tolerant Frequent Patterns:", len(faultTolerantFrequentPattern))
+#             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
 #             obj.save(oFile)
 #
-#             Df = obj.getPatternInDataFrame()
+#             Df = obj.getPatternsAsDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
 #             print("Total Memory in USS:", memUSS)
 #
 #             memRSS = obj.getMemoryRSS()
 #
 #             print("Total Memory in RSS", memRSS)
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
+
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
-
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
-
+     
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
-
+     
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
+
+from PAMI.uncertainFrequentPattern.basic import abstract as _ab
+from typing import List, Dict, Union
 import pandas as pd
 from deprecated import deprecated
 
-import numpy as _np
-from PAMI.faultTolerantFrequentPattern.basic import abstract as _ab
+_minSup = float()
+_finalPatterns = {}
 
-class VBFTMine(_ab._faultTolerantFrequentPatterns):
+
+class _Item:
+    """
+    A class used to represent the item with probability in transaction of dataset
+
+    :Attributes:
+
+        item : int or word
+          Represents the name of the item
+
+        probability : float
+          Represent the existential probability(likelihood presence) of an item
+    """
+
+    def __init__(self, item, probability) -> None:
+        self.item = item
+        self.probability = probability
+
+
+class TUFP(_ab._frequentPatterns):
     """
     About this algorithm
     ====================
-    
-    :Description:  VBFTMine is one of the fundamental algorithm to discover fault tolerant frequent patterns in an uncertain transactional database based on
-                   bitset representation.
-                   This program employs apriori property (or downward closure property) to  reduce the search space effectively.
-
-    :Reference:   Koh, JL., Yo, PW. (2005). An Efficient Approach for Mining Fault-Tolerant Frequent Patterns Based on Bit Vector Representations.
-            In:   Zhou, L., Ooi, B.C., Meng, X. (eds) Database Systems for Advanced Applications. DASFAA 2005. Lecture Notes in Computer Science,
-                  vol 3453. Springer, Berlin, Heidelberg. https://doi.org/10.1007/11408079_51
-
-    :param  iFile: str :
-                   Name of the Input file to mine complete set of uncertain Fault Tolerant FrequentFrequent Patterns
-    :param  oFile: str :
-                   Name of the output file to store complete set of uncertain Fault Tolerant FrequentFrequent Patterns
-    :param  minSup: float or int or str :
-                   The user can specify minSup either in count or proportion of database size.
-                   If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
-                   Otherwise, it will be treated as float.
-                   Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
-    :param  itemSup: int or float :
-                    Frequency of an item
-    :param minLength: int
-                    minimum length of a pattern
-    :param faultTolerance: int :
-                    The ability of a pattern mining algorithm to handle errors or inconsistencies in the data without completely failing or producing incorrect results.
-    :param  sep: str :
-                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+
+    :Description: It is one of the fundamental algorithm to discover top-k frequent patterns in a uncertain transactional database using CUP-Lists.
+
+    :Reference:  Tuong Le, Bay Vo, Van-Nam Huynh, Ngoc Thanh Nguyen, Sung Wook Baik 5, "Mining top-k frequent patterns from uncertain databases",
+                 Springer Science+Business Media, LLC, part of Springer Nature 2020, https://doi.org/10.1007/s10489-019-01622-1
 
     :Attributes:
 
-        startTime : float
-          To record the start time of the mining process
+        iFile : file
+            Name of the Input file or path of the input file
 
-        endTime : float
-          To record the completion time of the mining process
+        oFile : file
+            Name of the output file or path of the output file
 
-        finalPatterns : dict
-          Storing the complete set of patterns in a dictionary variable
+        minSup : float or int or str
+            The user can specify minSup either in count or proportion of database size.
+            If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+
+        sep : str
+            This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
+            However, the users can override their default separator.
 
         memoryUSS : float
-          To store the total amount of USS memory consumed by the program
+            To store the total amount of USS memory consumed by the program
 
         memoryRSS : float
-          To store the total amount of RSS memory consumed by the program
+            To store the total amount of RSS memory consumed by the program
+
+        startTime : float
+            To record the start time of the mining process
+
+        endTime : float
+            To record the completion time of the mining process
 
         Database : list
-          To store the transactions of a database in list
+            To store the transactions of a database in list
+
+        mapSupport : Dictionary
+            To maintain the information of item and their frequency
 
+        lno : int
+            To represent the total no of transaction
+
+        tree : class
+            To represents the Tree class
+
+        itemSetCount : int
+            To represents the total no of patterns
+
+        finalPatterns : dict
+            To store the complete patterns
+
+    :Methods:
+        mine()
+            Mining process will start from here
+        getPatterns()
+            Complete set of patterns will be retrieved with this function
+        storePatternsInFile(oFile)
+            Complete set of frequent patterns will be loaded in to a output file
+        getPatternsInDataFrame()
+            Complete set of frequent patterns will be loaded in to a dataframe
+        getMemoryUSS()
+            Total amount of USS memory consumed by the mining process will be retrieved from this function
+        getMemoryRSS()
+            Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        getRuntime()
+            Total amount of runtime taken by the mining process will be retrieved from this function
+        creatingItemSets(fileName)
+            Scans the dataset and stores in a list format
+        frequentOneItem()
+            Extracts the one-length frequent patterns from database
+        updateTransactions()
+            Update the transactions by removing non-frequent items and sort the Database by item decreased support
+        buildTree()
+            After updating the Database, remaining items will be added into the tree by setting root node as null
+        convert()
+            to convert the user specified value
+        startMine()
+            Mining process will start from this function
 
     Execution methods
     =================
 
+
     **Terminal command**
 
 
     .. code-block:: console
 
-       Format:
+      Format:
 
-       (.venv) $ python3 VBFTMine.py <inputFile> <outputFile> <minSup> <itemSup> <minLength> <faultTolerance>
+      (.venv) $ python3 TUFP.py <inputFile> <outputFile> <minSup>
 
-       Examples usage:
+      Example Usage:
 
-       (.venv) $ python3 VBFTMine.py sampleDB.txt patterns.txt 10.0 3.0 3 1
+      (.venv) $ python3 TUFP.py sampleDB.txt patterns.txt 10.0
 
-    .. note:: minSup will be considered in times of minSup and count of database transactions
+    .. note:: minSup can be specified  in support count or a value between 0 and 1.
 
 
     **Calling from a python program**
 
     .. code-block:: python
-    
-            import PAMI.faultTolerantFrequentPattern.basic.VBFTMine as alg
-
-            iFile = 'sampleDB.txt'
 
-            minSup = 10  # can also be specified between 0 and 1
+            from PAMI.uncertainFrequentPattern.basic import TUFP as alg
 
-            itemSup = 2  # can also be specified between 0 and 1
 
-            minLength = 3 # can also be specified between 0 and 1
+            iFile = 'sampleDB.txt'
 
-            faultTolerance = 2 # can also be specified between 0 and 1
+            minSup = 10  # can also be specified between 0 and 1
 
-            obj = alg.VBFTMine(iFile, minSup, itemSup, minLength, faultTolerance)
+            obj = alg.TUFP(iFile, minSup)
 
             obj.mine()
 
-            faultTolerantFrequentPattern = obj.getPatterns()
+            frequentPatterns = obj.getPatterns()
 
-            print("Total number of Fault Tolerant Frequent Patterns:", len(faultTolerantFrequentPattern))
+            print("Total number of Frequent Patterns:", len(frequentPatterns))
 
             obj.save(oFile)
 
-            Df = obj.getPatternInDataFrame()
+            Df = obj.getPatternsAsDataFrame()
 
-            memUSS = obj.getMemoryUSS()
+            memUSS = obj.getmemoryUSS()
 
             print("Total Memory in USS:", memUSS)
 
             memRSS = obj.getMemoryRSS()
 
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
     Credits
     =======
 
-           The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
-
+            The complete program was written by   P.Likhitha   under the supervision of Professor Rage Uday Kiran.
     """
 
-    _minSup = float()
-    _itemSup = float()
-    _minLength = int()
-    _faultTolerance = int()
     _startTime = float()
     _endTime = float()
+    _minSup = str()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
-    _plist = []
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
-    _mapSupport = {}
+    _cupList = {}
+    _topk = {}
+    _minimum = 9999
 
-    def _creatingItemSets(self):
+    def _creatingItemSets(self) -> None:
         """
-        Storing the complete transactions of the database/input file in a database variable
+        Scans the dataset
         """
         self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
-            temp = []
+            uncertain, data = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
-                temp = self._iFile['Transactions'].tolist()
+                self._Database = self._iFile['Transactions'].tolist()
+            if 'uncertain' in i:
+                uncertain = self._iFile['uncertain'].tolist()
+            for k in range(len(data)):
+                tr = []
+                for j in range(len(data[k])):
+                    product = _Item(data[k][j], uncertain[k][j])
+                    tr.append(product)
+                self._Database.append(tr)
 
-            for k in temp:
-                self._Database.append(set(k))
+            # print(self.Database)
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    self._Database.append(set(temp))
+                    tr = []
+                    for i in temp:
+                        i1 = i.index('(')
+                        i2 = i.index(')')
+                        item = i[0:i1]
+                        probability = float(i[i1 + 1:i2])
+                        product = _Item(item, probability)
+                        tr.append(product)
+                    self._Database.append(temp)
             else:
                 try:
-                    with open(self._iFile, 'r', encoding='utf-8') as f:
+                    with open(self._iFile, 'r') as f:
                         for line in f:
-                            line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
+                            tr = []
                             for i in temp:
-                                if i not in self._plist:
-                                    self._plist.append(i)
-                            self._Database.append(set(temp))
+                                i1 = i.index('(')
+                                i2 = i.index(')')
+                                item = i[0:i1]
+                                probability = float(i[i1 + 1:i2])
+                                product = _Item(item, probability)
+                                tr.append(product)
+                            self._Database.append(tr)
                 except IOError:
                     print("File Not Found")
-                    quit()
 
-    def _convert(self, value):
+    def _frequentOneItem(self) -> List[str]:
+        """
+        Takes the self.Database and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
+
+        :param self.Database : it represents the one self.Database in database
+        :type self.Database : list
+        """
+
+        mapSupport = {}
+        k = 0
+        for i in self._Database:
+            k += 1
+            for j in i:
+                if j.item not in mapSupport:
+                    mapSupport[j.item] = j.probability
+                    self._cupList[j.item] = {k:j.probability}
+                else:
+                    mapSupport[j.item] += j.probability
+                    self._cupList[j.item].update({k: j.probability})
+        plist = [k for k,v in sorted(mapSupport.items(), key=lambda x: x[1], reverse=True)]
+        k = 0
+        for x, in plist:
+            k +=1
+            if k >= self._minSup:
+                break
+            self._finalPatterns[x] = mapSupport[x]
+        self._minimum = min(list(self._finalPatterns.values()))
+        return plist
+
+    @staticmethod
+    def _convert(value: Union[int, float, str]) -> Union[int, float]:
         """
-        To convert the user specified minSup value
+        To convert the type of user specified minSup value
 
         :param value: user specified minSup value
-        :return: converted type
+        :return: converted type minSup value
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
-            value = (len(self._Database) * value)
+            value = float(value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
-                value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
-    def _Count(self, tids):
-        """
-        Count the occurrences of 1s in the given list of transaction IDs.
-
-        :param tids: List of transaction IDs.
-        :type tids: List[int]
-        :return: Count of occurrences of 1s in the list.
-        :rtype: int
-        """
-        count = 0
-        for i in tids:
-            if i == 1:
-                count += 1
-        return count
-
-    def _save(self, prefix, suffix, tidsetx):
+    def _save(self, prefix: List[str], suffix: List[str], tidSetI: Dict[int, float]) -> None:
         """
-        Save the pattern with its support count if it meets the fault tolerance criteria.
+        Saves the patterns that satisfy the periodic frequent property.
 
-        :param prefix: Prefix part of the pattern.
+        :param prefix: the prefix of a pattern
         :type prefix: list
-        :param suffix: Suffix part of the pattern.
+        :param suffix: the suffix of a patterns
         :type suffix: list
-        :param tidsetx: Transaction IDs associated with the pattern.
-        :type tidsetx: list
-        :return: None
+        :param tidSetI: the timestamp of a patterns
+        :type tidSetI: dict
         """
-        if (prefix == None):
+
+        if prefix is None:
             prefix = suffix
         else:
             prefix = prefix + suffix
-        prefix = list(set(prefix))
-        prefix.sort()
-        val = self._Count(tidsetx)
-        if len(prefix) > self._faultTolerance:
-            self._finalPatterns[tuple(prefix)] = val
-
-    def _processEquivalenceClass(self, prefix, itemsets, tidsets):
-        """
-        Process the equivalence class to generate frequent patterns.
-
-        :param prefix: Prefix part of the pattern.
-        :type prefix: list.
-        :param itemsets: List of itemsets in the equivalence class.
-        :type itemsets: list.
-        :param tidsets: List of transaction IDs associated with each itemset.
-        :type tidsets: list
-        :return: None
-        """
-        if (len(itemsets) == 1):
-            i = itemsets[0]
-            tidi = tidsets[0]
-            self._save(prefix, [i], tidi)
+        val = sum(tidSetI.values())
+        #print(prefix, val)
+        if len(self._finalPatterns) <= self._minSup:
+            sample = str()
+            for i in prefix:
+                sample = sample + i + " "
+            self._finalPatterns[sample] = val
+        if len(self._finalPatterns) == self._minSup:
+            if val > self._minimum:
+                sample = str()
+                for i in prefix:
+                    sample = sample + i + " "
+                index = list(self._finalPatterns.keys())[list(self._finalPatterns.values()).index(self._minimum)]
+                del self._finalPatterns[index]
+                self._finalPatterns[sample] = val
+                self._minimum = min(list(self._finalPatterns.values()))
+        #print(self.finalPatterns, self.minimum, self.minSup)
+
+
+    def _Generation(self, prefix: List[str], itemSets: List[str], tidSets: List[Dict[int, float]]) -> None:
+        """
+        Equivalence class is followed  and checks for the patterns generated for periodic-frequent patterns.
+
+        :param prefix:  main equivalence prefix
+        :type prefix: periodic-frequent item or pattern
+        :param itemSets: patterns which are items combined with prefix and satisfying the periodicity and frequent with their timestamps
+        :type itemSets: list
+        :param tidSets: timestamps of the items in the argument itemSets
+        :type tidSets: list
+        """
+        if len(itemSets) == 1:
+            i = itemSets[0]
+            tidI = tidSets[0]
+            self._save(prefix, [i], tidI)
             return
-        for i in range(len(itemsets)):
-            itemx = itemsets[i]
-            if (itemx == None):
+        for i in range(0, len(itemSets)):
+            itemI = itemSets[i]
+            if itemI is None:
                 continue
-            tidsetx = tidsets[i]
-            classItemsets = []
-            classtidsets = []
-            itemsetx = [itemx]
-            for j in range(i + 1, len(itemsets)):
-                itemj = itemsets[j]
-                tidsetj = tidsets[j]
-                y = list(_np.array(tidsetx) & _np.array(tidsetj))
-                total = self._Count(y)
-                if total >= self._minSup:
-                    classItemsets.append(itemj)
-                    classtidsets.append(y)
-            if (len(classItemsets) > 0):
-                newprefix = list(set(itemsetx)) + prefix
-                self._processEquivalenceClass(newprefix, classItemsets, classtidsets)
-            self._save(prefix, list(set(itemsetx)), tidsetx)
+            tidSetI = tidSets[i]
+            classItemSets = []
+            classTidSets = []
+            itemSetX = [itemI]
+            for j in range(i+1, len(itemSets)):
+                itemJ = itemSets[j]
+                tidSetJ = tidSets[j]
+                y = {key: tidSetJ[key] * tidSetI.get(key, 0) for key in tidSetJ.keys()}
+                sum2 = sum(list(y.values()))
+                #print(prefix, itemJ, y, sum2)
+                #if sum2 >= self.minimum:
+                self._save(prefix, [itemJ], y)
+                classItemSets.append(itemJ)
+                classTidSets.append(y)
+            #print(itemI, tidSetI, classItemSets)
+            newPrefix = list(set(itemSetX)) + prefix
+            self._Generation(newPrefix, classItemSets, classTidSets)
+            #self.save(prefix, list(set(itemSetX)), tidSetI)
 
-    def _oneLengthFrequentItems(self):
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
+    def startMine(self) -> None:
         """
-        To calculate the one Length items
-        """
-        Vector = {}
-        items = []
-        for i in self._Database:
-            for j in self._plist:
-                count = 0
-                if j in i:
-                    count = 1
-                if j in Vector:
-                    Vector[j].append(count)
-                else:
-                    Vector[j] = [count]
-        for x, y in Vector.items():
-            v = self._Count(y)
-            if v >= self._itemSup:
-                items.append(x)
-        return Vector, items
-
-    @deprecated(
-        "It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
-    def startMine(self):
-        """
-        Frequent pattern mining process will start from here
+        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
         """
         self.mine()
 
-    def mine(self):
+
+    def mine(self) -> None:
         """
-        Frequent pattern mining process will start from here
+        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
         """
-        self._Database = []
+        global _minSup
         self._startTime = _ab._time.time()
         self._creatingItemSets()
         self._minSup = self._convert(self._minSup)
-        self._itemSup = self._convert(self._itemSup)
-        self._minLength = int(self._minLength)
-        self._faultTolerance = int(self._faultTolerance)
-        Vector, plist = self._oneLengthFrequentItems()
+        _minSup = self._minSup
+        plist = self._frequentOneItem()
         for i in range(len(plist)):
-            itemx = plist[i]
-            tidsetx = Vector[itemx]
-            itemsetx = [itemx]
-            itemsets = []
-            tidsets = []
-            for j in range(i + 1, len(plist)):
-                itemj = plist[j]
-                tidsetj = Vector[itemj]
-                y1 = list(_np.array(tidsetx) | _np.array(tidsetj))
-                total = self._Count(y1)
-                if total >= self._minSup:
-                    itemsets.append(itemj)
-                    tidsets.append(y1)
-            if (len(itemsets) > 0):
-                self._processEquivalenceClass(itemsetx, itemsets, tidsets)
-            self._save(None, itemsetx, tidsetx)
+            itemI = plist[i]
+            tidSetI = self._cupList[itemI]
+            itemSetX = [itemI]
+            itemSets = []
+            tidSets = []
+            for j in range(i+1, len(plist)):
+                itemJ = plist[j]
+                tidSetJ = self._cupList[itemJ]
+                y1 = {key: tidSetJ[key] * tidSetI.get(key, 0)  for key in tidSetJ.keys()}
+                self._save(itemSetX, [itemJ], y1)
+                itemSets.append(itemJ)
+                tidSets.append(y1)
+            self._Generation(itemSetX, itemSets, tidSets)
+        print("Top-K Frequent patterns were generated from uncertain databases successfully using TUFP algorithm")
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
-        print("Fault-Tolerant Frequent patterns were generated successfully using VBFTMine algorithm ")
 
-    def getMemoryUSS(self):
+    def getMemoryUSS(self) -> float:
         """
 
         Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
-    def getMemoryRSS(self):
+    def getMemoryRSS(self) -> float:
         """
 
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
-    def getRuntime(self):
+    def getRuntime(self) -> float:
         """
 
         Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
-    def getPatternsAsDataFrame(self):
+    def getPatternsAsDataFrame(self) -> pd.DataFrame:
         """
 
         Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
-        dataFrame = {}
+        dataframe = {}
         data = []
         for a, b in self._finalPatterns.items():
-            s = str()
-            for i in a:
-                s = s + i + ' '
-            data.append([s, b])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
-        # dataFrame = dataFrame.replace(r'\r+|\n+|\t+',' ', regex=True)
-        return dataFrame
+            data.append([a, b])
+            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
+        return dataframe
 
-    def save(self, outFile):
+    def save(self, outFile: str) -> None:
         """
 
         Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: file
         """
-        self._oFile = outFile
-        writer = open(self._oFile, 'w+')
+        self.oFile = outFile
+        writer = open(self.oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            s = str()
-            for i in x:
-                s = s + i + '\t'
-            s1 = s.strip() + ":" + str(y)
+            s1 = x + ":" + str(y)
             writer.write("%s \n" % s1)
 
-    def getPatterns(self):
+    def getPatterns(self) -> Dict[str, float]:
         """
 
         Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
-    def printResults(self):
+    def printResults(self) -> None:
         """
         This function is used to print the results
         """
-        print("Total number of Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of  Uncertain Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:", self.getRuntime())
-
+        print("Total ExecutionTime in ms:",  self.getRuntime())
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 7 or len(_ab._sys.argv) == 8:
-        if len(_ab._sys.argv) == 8:
-            _ap = VBFTMine(_ab._sys.argv[1], _ab._sys.argv[3],  _ab._sys.argv[4],
-                            _ab._sys.argv[5], _ab._sys.argv[6], _ab._sys.argv[7],)
-        if len(_ab._sys.argv) == 7:
-            _ap = VBFTMine(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5], _ab._sys.argv[6])
+    if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
+        if len(_ab._sys.argv) == 5:
+            _ap = TUFP(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+        if len(_ab._sys.argv) == 4:
+            _ap = TUFP(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
         _ap.mine()
-        print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
+        _Patterns = _ap.getPatterns()
+        print("Total number of Patterns:", len(_Patterns))
         _ap.save(_ab._sys.argv[2])
-        print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in ms:", _ap.getRuntime())
+        _memUSS = _ap.getMemoryUSS()
+        print("Total Memory in USS:", _memUSS)
+        _memRSS = _ap.getMemoryRSS()
+        print("Total Memory in RSS", _memRSS)
+        _run = _ap.getRuntime()
+        print("Total ExecutionTime in ms:", _run)
     else:
-        _ap = VBFTMine('/Users/Likhitha/Downloads/fault/sample4.txt', 5, 3, 2, 1, ' ')
-        _ap.startMine()
-        _ap.printResults()
-        print(_ap.getPatternsAsDataFrame())
+        '''ap = TUFP("/home/apiiit-rkv/Desktop/uncertain/tubeSample", 10, ' ')
+        ap.startMine()
+        Patterns = ap.getPatterns()
+        print("Total number of Patterns:", len(Patterns))
+        ap.save("patterns.txt")
+        memUSS = ap.getMemoryUSS()
+        print("Total Memory in USS:", memUSS)
+        memRSS = ap.getMemoryRSS()
+        print("Total Memory in RSS", memRSS)
+        run = ap.getRuntime()
+        print("Total ExecutionTime in ms:", run)'''
         print("Error! The number of input parameters do not match the total number of parameters provided")
+
```

### Comparing `pami-2024.5.28.1/PAMI/uncertainFaultTolerantFrequentPattern/abstract.py` & `pami-2024.5.7.1/PAMI/uncertainFaultTolerantFrequentPattern/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/uncertainFrequentPattern/__init__.py` & `pami-2024.5.7.1/PAMI/uncertainFrequentPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/uncertainFrequentPattern/basic/CUFPTree.py` & `pami-2024.5.7.1/PAMI/uncertainFrequentPattern/basic/PUFGrowth.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,19 +1,19 @@
-# CUFPTree is one of the fundamental algorithm to discover frequent patterns in a uncertain transactional database using CUFP-Tree
+# PUFGrowth is one of the fundamental algorithm to discover frequent patterns in a uncertain transactional database using PUF-Tree.
 #
 # **Importing this algorithm into a python program**
 #
-#             from PAMI.uncertainFrequentPattern.basic import CUFPTree as alg
-#
-#             obj = alg.CUFPTree(iFile, minSup,oFile,sep)
+#             from PAMI.uncertainFrequentPattern.basic import puf as alg
 #
 #             iFile = 'sampleDB.txt'
 #
 #             minSup = 10  # can also be specified between 0 and 1
 #
+#             obj = alg.PUFGrowth(iFile, minSup)
+#
 #             obj.mine()
 #
 #             frequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
 #             obj.save(oFile)
@@ -32,79 +32,74 @@
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
-
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
-
+     
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
-
+     
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
-import pandas as pd
-from deprecated import deprecated
-
 from PAMI.uncertainFrequentPattern.basic import abstract as _ab
 from typing import List, Tuple
-
+from deprecated import deprecated
 
 _minSup = str()
 _ab._sys.setrecursionlimit(20000)
 _finalPatterns = {}
 
 
 class _Item:
     """
     A class used to represent the item with probability in transaction of dataset
 
     :Attributes:
 
         item : int or word
-            Represents the name of the item
+          Represents the name of the item
 
         probability : float
-            Represent the existential probability(likelihood presence) of an item
+          Represent the existential probability(likelihood presence) of an item
     """
 
     def __init__(self, item, probability) -> None:
         self.item = item
         self.probability = probability
 
 
 class _Node(object):
     """
     A class used to represent the node of frequentPatternTree
 
     :Attributes:
 
         item : int
-            storing item of a node
+          storing item of a node
 
         probability : int
-            To maintain the expected support of node
+          To maintain the expected support of node
 
         parent : node
-            To maintain the parent of every node
+          To maintain the parent of every node
 
         children : list
-            To maintain the children of node
+          To maintain the children of node
 
     :Methods:
-
         addChild(itemName)
             storing the children to their respective parent nodes
     """
 
     def __init__(self, item, children) -> None:
         self.item = item
         self.probability = 1
@@ -124,27 +119,26 @@
         node.parent = self
 
 
 class _Tree(object):
     """
     A class used to represent the frequentPatternGrowth tree structure
 
-    :Attributes:
+    Attributes:
 
         root : Node
-            Represents the root node of the tree
+          Represents the root node of the tree
 
         summaries : dictionary
-            storing the nodes with same item name
+          storing the nodes with same item name
 
         info : dictionary
-            stores the support of items
+          stores the support of items
 
     :Methods:
-
         addTransaction(transaction)
             creating transaction as a branch in frequentPatternTree
         addConditionalPattern(prefixPaths, supportOfItems)
             construct the conditional tree for prefix paths
         conditionalPatterns(Node)
             generates the conditional patterns from tree for specific node
         conditionalTransactions(prefixPaths,Support)
@@ -159,19 +153,18 @@
     def __init__(self) -> None:
         self.root = _Node(None, {})
         self.summaries = {}
         self.info = {}
 
     def addTransaction(self, transaction) -> None:
         """
-        adding transaction into tree
+        Adding transaction into tree
 
         :param transaction : it represents the one self.Database in database
         :type transaction : list
-        :return: None
         """
 
         currentNode = self.root
         for i in range(len(transaction)):
             if transaction[i].item not in currentNode.children:
                 newNode = _Node(transaction[i].item, {})
                 l1 = i - 1
@@ -199,21 +192,20 @@
                 if len(lp) == 0:
                     currentNode.probability += transaction[i].probability
                 else:
                     currentNode.probability += max(lp) * transaction[i].probability
 
     def addConditionalPattern(self, transaction, sup) -> None:
         """
-        constructing conditional tree from prefixPaths
+        Constructing conditional tree from prefixPaths
 
         :param transaction : it represents the one self.Database in database
         :type transaction : list
         :param sup : support of prefixPath taken at last child of the path
         :type sup : int
-        :return: None
         """
 
         # This method takes transaction, support and constructs the conditional tree
         currentNode = self.root
         for i in range(len(transaction)):
             if transaction[i] not in currentNode.children:
                 newNode = _Node(transaction[i], {})
@@ -226,21 +218,19 @@
                 currentNode = newNode
             else:
                 currentNode = currentNode.children[transaction[i]]
                 currentNode.probability += sup
 
     def conditionalPatterns(self, alpha) -> Tuple[List, List, dict]:
         """
-        generates all the conditional patterns of respective node
+        Generates all the conditional patterns of respective node
 
         :param alpha : it represents the Node in tree
         :type alpha : _Node
-        :return: Tuple
         """
-
         # This method generates conditional patterns of node by traversing the tree
         finalPatterns = []
         sup = []
         for i in self.summaries[alpha]:
             s = i.probability
             set2 = []
             while i.parent.item is not None:
@@ -251,36 +241,32 @@
                 finalPatterns.append(set2)
                 sup.append(s)
         finalPatterns, support, info = self.conditionalTransactions(finalPatterns, sup)
         return finalPatterns, support, info
 
     def removeNode(self, nodeValue) -> None:
         """
-        removing the node from tree
+        Removing the node from tree
 
         :param nodeValue : it represents the node in tree
         :type nodeValue : node
-        :return: None
         """
 
         for i in self.summaries[nodeValue]:
             del i.parent.children[nodeValue]
 
     def conditionalTransactions(self, condPatterns, support) -> Tuple[List, List, dict]:
         """
         It generates the conditional patterns with frequent items
 
         :param condPatterns : conditionalPatterns generated from conditionalPattern method for respective node
         :type condPatterns : list
         :support : the support of conditional pattern in tree
         :support : int
-        :return: Tuple consist of patterns,support and updated Dictionary
-        :rtype: Tuple
         """
-
         global minSup
         pat = []
         sup = []
         count = {}
         for i in range(len(condPatterns)):
             for j in condPatterns[i]:
                 if j in count:
@@ -301,15 +287,14 @@
 
     def generatePatterns(self, prefix) -> None:
         """
         Generates the patterns
 
         :param prefix : forms the combination of items
         :type prefix : list
-        :return: None
         """
 
         global _finalPatterns, minSup
         for i in sorted(self.summaries, key=lambda x: (self.info.get(x))):
             pattern = prefix[:]
             pattern.append(i)
             s = 0
@@ -322,44 +307,34 @@
                 conditionalTree.info = info.copy()
                 for pat in range(len(patterns)):
                     conditionalTree.addConditionalPattern(patterns[pat], support[pat])
                 if len(patterns) > 0:
                     conditionalTree.generatePatterns(pattern)
             self.removeNode(i)
 
-class CUFPTree(_ab._frequentPatterns):
-  
+
+class PUFGrowth(_ab._frequentPatterns):
     """
     About this algorithm
     ====================
 
-    :Description: It is one of the fundamental algorithm to discover frequent patterns in a uncertain transactional database using CUFP-Tree.
-
-    :Reference: Chun-Wei Lin Tzung-PeiHong, 'new mining approach for uncertain databases using CUFP trees',
-                Expert Systems with Applications, Volume 39, Issue 4, March 2012, Pages 4084-4093, https://doi.org/10.1016/j.eswa.2011.09.087
-    
-    :param  iFile: str :
-                   Name of the Input file to mine complete set of Uncertain Frequent Patterns
-    :param  oFile: str :
-                   Name of the output file to store complete set of Uncertain frequent patterns
-    :param  minSup: int or float or str :
-                   minimum support thresholds were tuned to find the appropriate ranges in the limited memory
-    :param  sep: str :
-                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+    :Description: It is one of the fundamental algorithm to discover frequent patterns in a uncertain transactional database using PUF-Tree.
 
+    :Reference:  Carson Kai-Sang Leung, Syed Khairuzzaman Tanbeer, "PUF-Tree: A Compact Tree Structure for Frequent Pattern Mining of Uncertain Data",
+                 Pacific-Asia Conference on Knowledge Discovery and Data Mining(PAKDD 2013), https://link.springer.com/chapter/10.1007/978-3-642-37453-1_2
 
     :Attributes:
 
         iFile : file
             Name of the Input file or path of the input file
 
         oFile : file
             Name of the output file or path of the output file
 
-        minSup: float or int or str
+        minSup : float or int or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
 
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
@@ -367,18 +342,18 @@
 
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
 
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
 
-        startTime:float
+        startTime : float
             To record the start time of the mining process
 
-        endTime:float
+        endTime : float
             To record the completion time of the mining process
 
         Database : list
             To store the transactions of a database in list
 
         mapSupport : Dictionary
             To maintain the information of item and their frequency
@@ -392,15 +367,14 @@
         itemSetCount : int
             To represents the total no of patterns
 
         finalPatterns : dict
             To store the complete patterns
 
     :Methods:
-
         mine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
             Complete set of frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
@@ -426,50 +400,50 @@
 
     Execution methods
     =================
 
 
     **Terminal command**
 
-
     .. code-block:: console
 
-       Format:
+      Format:
 
-       (.venv) $ python3 CUFPTree.py <inputFile> <outputFile> <minSup>
+      (.venv) $ python3 PUFGrowth.py <inputFile> <outputFile> <minSup>
 
-       Example Usage:
+      Example Usage:
 
-       (.venv) $ python3 CUFPTree.py sampleTDB.txt patterns.txt 3
+      (.venv) $ python3 PUFGrowth.py sampleDB.txt patterns.txt 10.0
+
+    .. note:: minSup can be specified  in support count or a value between 0 and 1.
 
-    .. note:: minSup  will be considered in support count or frequency
 
     **Calling from a python program**
 
     .. code-block:: python
 
-            from PAMI.uncertainFrequentPattern.basic import CUFPTree as alg
+            from PAMI.uncertainFrequentPattern.basic import puf as alg
 
             iFile = 'sampleDB.txt'
 
             minSup = 10  # can also be specified between 0 and 1
 
-            obj = alg.CUFPTree(iFile, minSup)
+            obj = alg.PUFGrowth(iFile, minSup)
 
-            obj.mine()
+            obj.startmine()
 
             frequentPatterns = obj.getPatterns()
 
             print("Total number of Frequent Patterns:", len(frequentPatterns))
 
             obj.save(oFile)
 
             Df = obj.getPatternsAsDataFrame()
 
-            memUSS = obj.getMemoryUSS()
+            memUSS = obj.getmemoryUSS()
 
             print("Total Memory in USS:", memUSS)
 
             memRSS = obj.getMemoryRSS()
 
             print("Total Memory in RSS", memRSS)
 
@@ -477,15 +451,15 @@
 
             print("Total ExecutionTime in seconds:", run)
 
     Credits
     =======
 
 
-            The complete program was written by  P.Likhitha  under the supervision of Professor Rage Uday Kiran.
+             The complete program was written by  P.Likhitha  under the supervision of Professor Rage Uday Kiran.
     """
 
     _startTime = float()
     _endTime = float()
     _minSup = str()
     _finalPatterns = {}
     _iFile = " "
@@ -498,16 +472,14 @@
 
     def __init__(self, iFile, minSup, sep='\t') -> None:
         super().__init__(iFile, minSup, sep)
 
     def _creatingItemSets(self) -> None:
         """
         Scans the uncertain transactional dataset
-
-        :return: None
         """
         self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             uncertain, data = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
@@ -523,52 +495,52 @@
                 self._Database.append(tr)
 
             # print(self.Database)
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
-                    line.strip()
+                    line = line.strip()
                     line = line.decode("utf-8")
-                    temp = [i.rstrip() for i in line.split(self._sep)]
+                    temp1 = line.split(':')
+                    temp = [i.rstrip() for i in temp[0].split(self._sep)]
+                    uncertain = [float(i.rstrip()) for i in temp[1].split(self._sep)]
                     temp = [x for x in temp if x]
+                    uncertain = [x for x in uncertain if x]
                     tr = []
-                    for i in temp:
-                        i1 = i.index('(')
-                        i2 = i.index(')')
-                        item = i[0:i1]
-                        probability = float(i[i1 + 1:i2])
+                    for i in range(len(temp)):
+                        item = temp[i]
+                        probability = uncertain[i]
                         product = _Item(item, probability)
                         tr.append(product)
-                    self._Database.append(temp)
+                    self._Database.append(tr)
             else:
                 try:
                     with open(self._iFile, 'r') as f:
                         for line in f:
-                            temp = [i.rstrip() for i in line.split(self._sep)]
-                            temp = [x for x in temp if x]
+                            temp1 = line.strip()
+                            temp1 = temp1.split(':')
+                            temp = [i.rstrip() for i in temp1[0].split(self._sep)]
+                            uncertain = [float(i.rstrip()) for i in temp1[1].split(self._sep)]
                             tr = []
-                            for i in temp:
-                                i1 = i.index('(')
-                                i2 = i.index(')')
-                                item = i[0:i1]
-                                probability = float(i[i1 + 1:i2])
+                            for i in range(len(temp)):
+                                item = temp[i]
+                                probability = uncertain[i]
                                 product = _Item(item, probability)
                                 tr.append(product)
                             self._Database.append(tr)
                 except IOError:
                     print("File Not Found")
 
     def _frequentOneItem(self) -> Tuple[dict, List]:
         """
         Takes the self.Database and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
 
         :param self.Database : it represents the one self.Database in database
         :type self.Database : list
-        :return: tuple
         """
 
         mapSupport = {}
         for i in self._Database:
             for j in i:
                 if j.item not in mapSupport:
                     mapSupport[j.item] = j.probability
@@ -583,31 +555,29 @@
     def _buildTree(data, info) -> '_Tree':
         """
         It takes the self.Database and support of each item and construct the main tree with setting root node as null
 
         :param data : it represents the one self.Database in database
         :type data : list
         :param info : it represents the support of each item
-        :type info : dict
-        :return: Dictionary
+        :type info : dictionary
         """
 
         rootNode = _Tree()
         rootNode.info = info.copy()
         for i in range(len(data)):
             rootNode.addTransaction(data[i])
         return rootNode
 
     def _updateTransactions(self, dict1) -> List:
         """
         Remove the items which are not frequent from self.Database and updates the self.Database with rank of items
 
         :param dict1 : frequent items with support
         :type dict1 : dictionary
-        :return: list
         """
 
         list1 = []
         for tr in self._Database:
             list2 = []
             for i in range(0, len(tr)):
                 if tr[i].item in dict1:
@@ -624,15 +594,14 @@
         """
         To check the presence of item or pattern in transaction
 
         :param x: it represents the pattern
         :type x : list
         :param i : represents the uncertain self.Database
         :type i : list
-        :return: int
         """
 
         # This method taken a transaction as input and returns the tree
         for m in x:
             k = 0
             for n in i:
                 if m == n.item:
@@ -657,15 +626,15 @@
                 value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
     def _removeFalsePositives(self) -> None:
         """
-        To remove the false positive patterns generated in frequent patterns.
+        To remove the false positive patterns generated in frequent patterns
 
         :return: patterns with accurate probability
         """
         global _finalPatterns
         periods = {}
         for i in self._Database:
             for x, y in _finalPatterns.items():
@@ -685,51 +654,45 @@
         for x, y in periods.items():
             if y >= self._minSup:
                 sample = str()
                 for i in x:
                     sample = sample + i + "\t"
                 self._finalPatterns[sample] = y
 
-    @deprecated(
-        "It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
-        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns.
-
-        :return: None
+        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
         """
         self.mine()
 
     def mine(self) -> None:
         """
-        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns.
-
-        :return: None
+        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
         """
         global minSup
         self._startTime = _ab._time.time()
         self._creatingItemSets()
         self._minSup = self._convert(self._minSup)
         minSup = self._minSup
         self._finalPatterns = {}
         mapSupport, plist = self._frequentOneItem()
         self.Database1 = self._updateTransactions(mapSupport)
         info = {k: v for k, v in mapSupport.items()}
         Tree1 = self._buildTree(self.Database1, info)
         Tree1.generatePatterns([])
         self._removeFalsePositives()
-        print("Uncertain Frequent patterns were successfully generated using CUFPTree algorithm")
+        print("Uncertain Frequent patterns were generated successfully using PUFGrowth algorithm")
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self.memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self.memoryRSS = process.memory_info().rss
 
-
     def getMemoryUSS(self) -> float:
         """
 
         Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
@@ -778,15 +741,14 @@
     def save(self, outFile: str) -> None:
         """
 
         Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: csv file
-        :return: None
         """
         self.oFile = outFile
         writer = open(self.oFile, 'w+')
         for x, y in self._finalPatterns.items():
             s1 = x.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
@@ -810,19 +772,19 @@
         print("Total ExecutionTime in ms:",  self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
-            _ap = CUFPTree(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = PUFGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
-            _ap = CUFPTree(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap = PUFGrowth(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
         _ap.mine()
-        print("Total number of Uncertain Frequent Patterns:", len(_ap.getPatterns()))
+        print("Total number of Uncertain Frequent Patterns:", _ap.getPatterns())
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.5.28.1/PAMI/uncertainFrequentPattern/basic/PUFGrowth.py` & `pami-2024.5.7.1/PAMI/uncertainGeoreferencedFrequentPattern/basic/GFPGrowth.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,24 +1,24 @@
-# PUFGrowth is one of the fundamental algorithm to discover frequent patterns in a uncertain transactional database using PUF-Tree.
+# GFPGrowth algorithm is used to discover geo-referenced frequent patterns in a uncertain transactional database using GFP-Tree.
 #
 # **Importing this algorithm into a python program**
 #
-#             from PAMI.uncertainFrequentPattern.basic import puf as alg
+#             from PAMI.uncertainGeoreferencedFrequentPattern.basic import GFPGrowth as alg
 #
 #             iFile = 'sampleDB.txt'
 #
 #             minSup = 10  # can also be specified between 0 and 1
 #
-#             obj = alg.PUFGrowth(iFile, minSup)
+#             obj = alg.GFPGrowth(iFile, nFile, minSup,sep, oFile)
 #
 #             obj.mine()
 #
-#             frequentPatterns = obj.getPatterns()
+#             Patterns = obj.getPatterns()
 #
-#             print("Total number of Frequent Patterns:", len(frequentPatterns))
+#             print("Total number of  Patterns:", len(Patterns))
 #
 #             obj.save(oFile)
 #
 #             Df = obj.getPatternsAsDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
@@ -30,87 +30,91 @@
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
+
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
+
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
-     
+
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
-     
+
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
-from PAMI.uncertainFrequentPattern.basic import abstract as _ab
-from typing import List, Tuple
+from PAMI.uncertainGeoreferencedFrequentPattern.basic import abstract as _ab
+import pandas as pd
 from deprecated import deprecated
 
 _minSup = str()
+_neighbourList = {}
 _ab._sys.setrecursionlimit(20000)
 _finalPatterns = {}
 
 
 class _Item:
     """
     A class used to represent the item with probability in transaction of dataset
 
     :Attributes:
 
         item : int or word
-          Represents the name of the item
+            Represents the name of the item
 
         probability : float
-          Represent the existential probability(likelihood presence) of an item
+            Represent the existential probability(likelihood presence) of an item
     """
 
-    def __init__(self, item, probability) -> None:
+    def __init__(self, item, probability):
         self.item = item
         self.probability = probability
 
 
 class _Node(object):
     """
     A class used to represent the node of frequentPatternTree
 
     :Attributes:
 
         item : int
-          storing item of a node
+            storing item of a node
 
         probability : int
-          To maintain the expected support of node
+            To maintain the expected support of node
 
         parent : node
-          To maintain the parent of every node
+            To maintain the parent of every node
 
         children : list
-          To maintain the children of node
+            To maintain the children of node
 
     :Methods:
+
         addChild(itemName)
             storing the children to their respective parent nodes
     """
 
-    def __init__(self, item, children) -> None:
+    def __init__(self, item, children):
         self.item = item
         self.probability = 1
         self.children = children
         self.parent = None
 
-    def addChild(self, node) -> None:
+    def addChild(self, node):
         """
         This method adds a child node to the current node in the frequent pattern tree. It updates the children
         dictionary of the current node with the new child node and sets the parent of the child node to the current node.
 
         :param node: The child node to be added.
         :type node: _Node
         :return: None
@@ -119,26 +123,27 @@
         node.parent = self
 
 
 class _Tree(object):
     """
     A class used to represent the frequentPatternGrowth tree structure
 
-    Attributes:
+    :Attributes:
 
         root : Node
-          Represents the root node of the tree
+            Represents the root node of the tree
 
         summaries : dictionary
-          storing the nodes with same item name
+            storing the nodes with same item name
 
         info : dictionary
-          stores the support of items
+            stores the support of items
 
     :Methods:
+
         addTransaction(transaction)
             creating transaction as a branch in frequentPatternTree
         addConditionalPattern(prefixPaths, supportOfItems)
             construct the conditional tree for prefix paths
         conditionalPatterns(Node)
             generates the conditional patterns from tree for specific node
         conditionalTransactions(prefixPaths,Support)
@@ -146,35 +151,39 @@
             prefixPaths and generates prefixPaths with items which are frequent
         remove(Node)
             removes the node from tree once after generating all the patterns respective to the node
         generatePatterns(Node)
             starts from the root node of the tree and mines the frequent patterns
     """
 
-    def __init__(self) -> None:
+    def __init__(self):
         self.root = _Node(None, {})
         self.summaries = {}
         self.info = {}
 
-    def addTransaction(self, transaction) -> None:
+    def addTransaction(self, transaction):
         """
         Adding transaction into tree
 
         :param transaction : it represents the one self.Database in database
         :type transaction : list
         """
-
+        global _neighbourList
         currentNode = self.root
         for i in range(len(transaction)):
             if transaction[i].item not in currentNode.children:
                 newNode = _Node(transaction[i].item, {})
+                nei = _neighbourList.get(transaction[i].item)
                 l1 = i - 1
                 lp = []
                 while l1 >= 0:
-                    lp.append(transaction[l1].probability)
+                    if nei == None:
+                        break
+                    if transaction[l1].item in nei:
+                        lp.append(transaction[l1].probability)
                     l1 -= 1
                 if len(lp) == 0:
                     newNode.probability = transaction[i].probability
                 else:
                     newNode.probability = max(lp) * transaction[i].probability
                 currentNode.addChild(newNode)
                 if transaction[i].item in self.summaries:
@@ -190,17 +199,17 @@
                     lp.append(transaction[l1].probability)
                     l1 -= 1
                 if len(lp) == 0:
                     currentNode.probability += transaction[i].probability
                 else:
                     currentNode.probability += max(lp) * transaction[i].probability
 
-    def addConditionalPattern(self, transaction, sup) -> None:
+    def addConditionalPattern(self, transaction, sup):
         """
-        Constructing conditional tree from prefixPaths
+        constructing conditional tree from prefixPaths
 
         :param transaction : it represents the one self.Database in database
         :type transaction : list
         :param sup : support of prefixPath taken at last child of the path
         :type sup : int
         """
 
@@ -216,57 +225,64 @@
                 else:
                     self.summaries[transaction[i]] = [newNode]
                 currentNode = newNode
             else:
                 currentNode = currentNode.children[transaction[i]]
                 currentNode.probability += sup
 
-    def conditionalPatterns(self, alpha) -> Tuple[List, List, dict]:
+    def conditionalPatterns(self, alpha):
         """
         Generates all the conditional patterns of respective node
 
         :param alpha : it represents the Node in tree
         :type alpha : _Node
         """
+
         # This method generates conditional patterns of node by traversing the tree
+        global _neighbourList
         finalPatterns = []
         sup = []
         for i in self.summaries[alpha]:
+            j = i.item
             s = i.probability
             set2 = []
             while i.parent.item is not None:
-                set2.append(i.parent.item)
+                if _neighbourList.get(j) is not None:
+                    #print(_neighbourList.get(j))
+                    if i.parent.item in _neighbourList[j]:
+                        set2.append(i.parent.item)
                 i = i.parent
             if len(set2) > 0:
                 set2.reverse()
                 finalPatterns.append(set2)
                 sup.append(s)
         finalPatterns, support, info = self.conditionalTransactions(finalPatterns, sup)
         return finalPatterns, support, info
 
-    def removeNode(self, nodeValue) -> None:
+    def removeNode(self, nodeValue):
         """
         Removing the node from tree
 
         :param nodeValue : it represents the node in tree
         :type nodeValue : node
         """
 
         for i in self.summaries[nodeValue]:
             del i.parent.children[nodeValue]
 
-    def conditionalTransactions(self, condPatterns, support) -> Tuple[List, List, dict]:
+    def conditionalTransactions(self, condPatterns, support):
         """
         It generates the conditional patterns with frequent items
 
         :param condPatterns : conditionalPatterns generated from conditionalPattern method for respective node
         :type condPatterns : list
         :support : the support of conditional pattern in tree
         :support : int
         """
+
         global minSup
         pat = []
         sup = []
         count = {}
         for i in range(len(condPatterns)):
             for j in condPatterns[i]:
                 if j in count:
@@ -281,15 +297,15 @@
             trans = sorted(p1, key=lambda x: updatedDict[x], reverse=True)
             if len(trans) > 0:
                 pat.append(trans)
                 sup.append(support[count])
                 count += 1
         return pat, sup, updatedDict
 
-    def generatePatterns(self, prefix) -> None:
+    def generatePatterns(self, prefix):
         """
         Generates the patterns
 
         :param prefix : forms the combination of items
         :type prefix : list
         """
 
@@ -308,33 +324,45 @@
                 for pat in range(len(patterns)):
                     conditionalTree.addConditionalPattern(patterns[pat], support[pat])
                 if len(patterns) > 0:
                     conditionalTree.generatePatterns(pattern)
             self.removeNode(i)
 
 
-class PUFGrowth(_ab._frequentPatterns):
+class GFPGrowth(_ab._frequentPatterns):
     """
     About this algorithm
     ====================
 
-    :Description: It is one of the fundamental algorithm to discover frequent patterns in a uncertain transactional database using PUF-Tree.
+    :Description: GFPGrowth algorithm is used to discover geo-referenced frequent patterns in a uncertain transactional database using GFP-Tree.
 
-    :Reference:  Carson Kai-Sang Leung, Syed Khairuzzaman Tanbeer, "PUF-Tree: A Compact Tree Structure for Frequent Pattern Mining of Uncertain Data",
-                 Pacific-Asia Conference on Knowledge Discovery and Data Mining(PAKDD 2013), https://link.springer.com/chapter/10.1007/978-3-642-37453-1_2
+    :Reference:  Palla Likhitha,Pamalla Veena, Rage, Uday Kiran, Koji Zettsu (2023).
+                 "Discovering Geo-referenced Frequent Patterns in Uncertain Geo-referenced
+                 Transactional Databases".  PAKDD 2023.
+                 https://doi.org/10.1007/978-3-031-33380-4_3
+
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of uncertain Geo referenced Frequent Patterns
+    :param  oFile: str :
+                   Name of the output file to store complete set of Uncertain Geo referenced frequent patterns
+    :param  minSup: str:
+                   minimum support thresholds were tuned to find the appropriate ranges in the limited memory
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
     :Attributes:
 
         iFile : file
             Name of the Input file or path of the input file
 
         oFile : file
             Name of the output file or path of the output file
 
-        minSup : float or int or str
+        minSup: float or int or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
 
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
@@ -342,18 +370,18 @@
 
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
 
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
 
-        startTime : float
+        startTime:float
             To record the start time of the mining process
 
-        endTime : float
+        endTime:float
             To record the completion time of the mining process
 
         Database : list
             To store the transactions of a database in list
 
         mapSupport : Dictionary
             To maintain the information of item and their frequency
@@ -367,19 +395,20 @@
         itemSetCount : int
             To represents the total no of patterns
 
         finalPatterns : dict
             To store the complete patterns
 
     :Methods:
-        mine()
+
+        startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
-        save(oFile)
+        savePatterns(oFile)
             Complete set of frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
@@ -400,84 +429,86 @@
 
     Execution methods
     =================
 
 
     **Terminal command**
 
-    .. code-block:: console
 
-      Format:
-
-      (.venv) $ python3 PUFGrowth.py <inputFile> <outputFile> <minSup>
+    .. code-block:: console
 
-      Example Usage:
+       Format:
 
-      (.venv) $ python3 PUFGrowth.py sampleDB.txt patterns.txt 10.0
+       (.venv) $ python3 GFPGrowth.py <inputFile> <neighborFile> <outputFile> <minSup>
 
-    .. note:: minSup can be specified  in support count or a value between 0 and 1.
+       Examples usage:
 
+       (.venv) $ python3 GFPGrowth.py sampleTDB.txt sampleNeighbor.txt patterns.txt 3
 
-    **Calling from a python program**
+    .. note:: minSup  will be considered in support count or frequency
+    
+    **Calling from a python program**:
 
-    .. code-block:: python
+     .. code-block:: python
 
-            from PAMI.uncertainFrequentPattern.basic import puf as alg
+            from PAMI.uncertainGeoreferencedFrequentPattern.basic import GFPGrowth as alg
 
             iFile = 'sampleDB.txt'
 
             minSup = 10  # can also be specified between 0 and 1
 
-            obj = alg.PUFGrowth(iFile, minSup)
+            obj = alg.GFPGrowth(iFile, nFile, minSup)
 
-            obj.startmine()
+            obj.mine()
 
-            frequentPatterns = obj.getPatterns()
+            Patterns = obj.getPatterns()
 
-            print("Total number of Frequent Patterns:", len(frequentPatterns))
+            print("Total number of  Patterns:", len(Patterns))
 
             obj.save(oFile)
 
             Df = obj.getPatternsAsDataFrame()
 
-            memUSS = obj.getmemoryUSS()
+            memUSS = obj.getMemoryUSS()
 
             print("Total Memory in USS:", memUSS)
 
             memRSS = obj.getMemoryRSS()
 
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
-
+        
     Credits
     =======
 
 
-             The complete program was written by  P.Likhitha  under the supervision of Professor Rage Uday Kiran.
+            The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.\n
+
     """
 
+
     _startTime = float()
     _endTime = float()
     _minSup = str()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
     _rank = {}
 
-    def __init__(self, iFile, minSup, sep='\t') -> None:
-        super().__init__(iFile, minSup, sep)
+    def __init__(self, iFile, nFile, minSup, sep='\t'):
+        super().__init__(iFile, nFile, minSup, sep)
 
-    def _creatingItemSets(self) -> None:
+    def _creatingItemSets(self):
         """
         Scans the uncertain transactional dataset
         """
         self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             uncertain, data = [], []
             if self._iFile.empty:
@@ -495,47 +526,95 @@
                 self._Database.append(tr)
 
             # print(self.Database)
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
-                    line = line.strip()
-                    line = line.decode("utf-8")
-                    temp1 = line.split(':')
-                    temp = [i.rstrip() for i in temp[0].split(self._sep)]
-                    uncertain = [float(i.rstrip()) for i in temp[1].split(self._sep)]
-                    temp = [x for x in temp if x]
-                    uncertain = [x for x in uncertain if x]
+                    temp1 = line.strip()
+                    temp1 = temp1.split(':')
+                    temp = [i.rstrip() for i in temp1[0].split(self._sep)]
+                    uncertain = [float(i.rstrip()) for i in temp1[1].split(self._sep)]
                     tr = []
                     for i in range(len(temp)):
                         item = temp[i]
                         probability = uncertain[i]
                         product = _Item(item, probability)
                         tr.append(product)
                     self._Database.append(tr)
             else:
                 try:
                     with open(self._iFile, 'r') as f:
                         for line in f:
                             temp1 = line.strip()
                             temp1 = temp1.split(':')
-                            temp = [i.rstrip() for i in temp1[0].split(self._sep)]
-                            uncertain = [float(i.rstrip()) for i in temp1[1].split(self._sep)]
+                            #temp1[0], temp1[1] = [i for i in temp1[0] if i], [i for i in temp1[1] if i]
+                            temp = [i.rstrip() for i in temp1[0].split(self._sep) if i]
+                            uncertain = [float(i.rstrip()) for i in temp1[1].split(self._sep) if i]
                             tr = []
                             for i in range(len(temp)):
                                 item = temp[i]
                                 probability = uncertain[i]
                                 product = _Item(item, probability)
                                 tr.append(product)
                             self._Database.append(tr)
                 except IOError:
                     print("File Not Found")
+                    
+    def _creatingNeighbours(self):
+        """
+        Scans the uncertain transactional dataset
+        """
+        global _neighbourList
+        _neighbourList = {}
+        if isinstance(self._nFile, _ab._pd.DataFrame):
+            uncertain, data = [], []
+            if self._iFile.empty:
+                print("its empty..")
+            i = self._iFile.columns.values.tolist()
+            if 'Transactions' in i:
+                self._Database = self._iFile['Transactions'].tolist()
+            if 'uncertain' in i:
+                uncertain = self._iFile['uncertain'].tolist()
+            for k in range(len(data)):
+                tr = []
+                for j in range(len(data[k])):
+                    product = _Item(data[k][j], uncertain[k][j])
+                    tr.append(product)
+                self._Database.append(tr)
+
+            # print(self.Database)
+        if isinstance(self._nFile, str):
+            if _ab._validators.url(self._iFile):
+                data = _ab._urlopen(self._iFile)
+                for line in data:
+                    line.strip()
+                    line = line.decode("utf-8")
+                    temp = [i.rstrip() for i in line.split(self._sep)]
+                    temp = [x for x in temp if x]
+                    tr = []
+                    for i in temp:
+                        i1 = i.index('(')
+                        i2 = i.index(')')
+                        item = i[0:i1]
+                        probability = float(i[i1 + 1:i2])
+                        product = _Item(item, probability)
+                        tr.append(product)
+                    self._Database.append(temp)
+            else:
+                try:
+                    with open(self._nFile, 'r') as f:
+                        for line in f:
+                            temp = [i.rstrip() for i in line.split(self._sep)]
+                            temp = [x for x in temp if x]
+                            _neighbourList[temp[0]] = temp[1:]
+                except IOError:
+                    print("File Not Found")
 
-    def _frequentOneItem(self) -> Tuple[dict, List]:
+    def _frequentOneItem(self):
         """
         Takes the self.Database and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
 
         :param self.Database : it represents the one self.Database in database
         :type self.Database : list
         """
 
@@ -548,15 +627,15 @@
                     mapSupport[j.item] += j.probability
         mapSupport = {k: v for k, v in mapSupport.items() if v >= self._minSup}
         plist = [k for k, v in sorted(mapSupport.items(), key=lambda x: x[1], reverse=True)]
         self.rank = dict([(index, item) for (item, index) in enumerate(plist)])
         return mapSupport, plist
 
     @staticmethod
-    def _buildTree(data, info) -> '_Tree':
+    def _buildTree(data, info):
         """
         It takes the self.Database and support of each item and construct the main tree with setting root node as null
 
         :param data : it represents the one self.Database in database
         :type data : list
         :param info : it represents the support of each item
         :type info : dictionary
@@ -564,15 +643,15 @@
 
         rootNode = _Tree()
         rootNode.info = info.copy()
         for i in range(len(data)):
             rootNode.addTransaction(data[i])
         return rootNode
 
-    def _updateTransactions(self, dict1) -> List:
+    def _updateTransactions(self, dict1):
         """
         Remove the items which are not frequent from self.Database and updates the self.Database with rank of items
 
         :param dict1 : frequent items with support
         :type dict1 : dictionary
         """
 
@@ -586,15 +665,15 @@
                 basket = list2
                 basket.sort(key=lambda val: self.rank[val.item])
                 list2 = basket
                 list1.append(list2)
         return list1
 
     @staticmethod
-    def _check(i, x) -> int:
+    def _check(i, x):
         """
         To check the presence of item or pattern in transaction
 
         :param x: it represents the pattern
         :type x : list
         :param i : represents the uncertain self.Database
         :type i : list
@@ -606,15 +685,15 @@
             for n in i:
                 if m == n.item:
                     k += 1
             if k == 0:
                 return 0
         return 1
 
-    def _convert(self, value) -> float:
+    def _convert(self, value):
         """
         To convert the type of user specified minSup value
 
         :param value: user specified minSup value
         :return: converted type minSup value
         """
         if type(value) is int:
@@ -624,17 +703,17 @@
         if type(value) is str:
             if '.' in value:
                 value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
-    def _removeFalsePositives(self) -> None:
+    def _removeFalsePositives(self):
         """
-        To remove the false positive patterns generated in frequent patterns
+        To remove the false positive patterns generated in frequent patterns.
 
         :return: patterns with accurate probability
         """
         global _finalPatterns
         periods = {}
         for i in self._Database:
             for x, y in _finalPatterns.items():
@@ -654,137 +733,147 @@
         for x, y in periods.items():
             if y >= self._minSup:
                 sample = str()
                 for i in x:
                     sample = sample + i + "\t"
                 self._finalPatterns[sample] = y
 
-    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
-    def startMine(self) -> None:
+    @deprecated(
+        "It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
+    def startMine(self):
         """
         Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
         """
         self.mine()
 
-    def mine(self) -> None:
+    def mine(self):
         """
         Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
         """
         global minSup
         self._startTime = _ab._time.time()
         self._creatingItemSets()
-        self._minSup = self._convert(self._minSup)
+        self._creatingNeighbours()
+        # self._minSup = self._convert(self._minSup)
         minSup = self._minSup
         self._finalPatterns = {}
         mapSupport, plist = self._frequentOneItem()
         self.Database1 = self._updateTransactions(mapSupport)
         info = {k: v for k, v in mapSupport.items()}
         Tree1 = self._buildTree(self.Database1, info)
         Tree1.generatePatterns([])
         self._removeFalsePositives()
-        print("Uncertain Frequent patterns were generated successfully using PUFGrowth algorithm")
+        print("Geo-Referenced Frequent patterns were generated from uncertain databases successfully using GFP algorithm")
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self.memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self.memoryRSS = process.memory_info().rss
 
-    def getMemoryUSS(self) -> float:
+    def getMemoryUSS(self):
         """
 
         Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
-    def getMemoryRSS(self) -> float:
+    def getMemoryRSS(self):
         """
 
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self.memoryRSS
 
-    def getRuntime(self) -> float:
+    def getRuntime(self):
         """
 
         Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
-    def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
+    def getPatternsAsDataFrame(self):
         """
 
         Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataframe = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a.replace('\t', ' '), b])
+            data.append([a, b])
             dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataframe
 
-    def save(self, outFile: str) -> None:
+    def save(self, outFile):
         """
 
         Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: csv file
         """
         self.oFile = outFile
         writer = open(self.oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            s1 = x.strip() + ":" + str(y)
+            s1 = x + ":" + str(y)
             writer.write("%s \n" % s1)
 
-    def getPatterns(self) -> dict:
+    def getPatterns(self):
         """
 
         Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
-
-    def printResults(self) -> None:
+    
+    def printResults(self):
         """
-        This function is used to print the results
+        This function is used to print the result
         """
-        print("Total number of  Uncertain Frequent Patterns:", len(self.getPatterns()))
-        print("Total Memory in USS:", self.getMemoryUSS())
-        print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:",  self.getRuntime())
+        print("Total number of Patterns:", len(self.getPatterns()))
+        self.save("patterns.txt")
+        memUSS = self.getMemoryUSS()
+        print("Total Memory in USS:", memUSS)
+        memRSS = self.getMemoryRSS()
+        print("Total Memory in RSS", memRSS)
+        run = self.getRuntime()
+        print("Total ExecutionTime in ms:", run)
 
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
+    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
+        if len(_ab._sys.argv) == 6:
+            _ap = GFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
         if len(_ab._sys.argv) == 5:
-            _ap = PUFGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
-        if len(_ab._sys.argv) == 4:
-            _ap = PUFGrowth(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap = GFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         _ap.startMine()
         _ap.mine()
-        print("Total number of Uncertain Frequent Patterns:", _ap.getPatterns())
+        _Patterns = _ap.getPatterns()
+        print("Total number of Patterns:", len(_Patterns))
         _ap.save(_ab._sys.argv[2])
-        print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in ms:", _ap.getRuntime())
+        _memUSS = _ap.getMemoryUSS()
+        print("Total Memory in USS:", _memUSS)
+        _memRSS = _ap.getMemoryRSS()
+        print("Total Memory in RSS", _memRSS)
+        _run = _ap.getRuntime()
+        print("Total ExecutionTime in ms:", _run)
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.5.28.1/PAMI/uncertainFrequentPattern/basic/TUFP.py` & `pami-2024.5.7.1/PAMI/uncertainFrequentPattern/basic/TUFP.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/uncertainFrequentPattern/basic/TubeP.py` & `pami-2024.5.7.1/PAMI/periodicFrequentPattern/basic/PFPGrowth.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,24 +1,22 @@
-# TUFP is one of the fundamental algorithm to discover top-k frequent patterns in a uncertain transactional database using CUP-Lists.
+# PFPGrowth is one of the fundamental algorithm to discover periodic-frequent patterns in a transactional database.
 #
 # **Importing this algorithm into a python program**
+# --------------------------------------------------------
 #
-#             from PAMI.uncertainFrequentPattern.basic import TUFP as alg
 #
-#             iFile = 'sampleDB.txt'
+#             from PAMI.periodicFrequentPattern.basic import PFPGrowth as alg
 #
-#             minSup = 10  # can also be specified between 0 and 1
+#             obj = alg.PFPGrowth(iFile, minSup, maxPer)
 #
-#             obj = alg.TUFP(iFile, minSup)
+#             obj.startMine()
 #
-#             obj.mine()
+#             periodicFrequentPatterns = obj.getPatterns()
 #
-#             frequentPatterns = obj.getPatterns()
-#
-#             print("Total number of Frequent Patterns:", len(frequentPatterns))
+#             print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 #
 #             obj.save(oFile)
 #
 #             Df = obj.getPatternsAsDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
@@ -30,545 +28,560 @@
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
+
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
+
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
-     
+
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
-     
+
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
+     Copyright (C)  2021 Rage Uday Kiran
+
 """
 
+from PAMI.periodicFrequentPattern.basic import abstract as _ab
+from typing import List, Dict, Tuple, Set, Union, Any, Generator
 
-from PAMI.uncertainFrequentPattern.basic import abstract as _ab
-from typing import List, Dict, Union
 import pandas as pd
 from deprecated import deprecated
+import numpy as np
 
+_maxPer = float()
 _minSup = float()
-_finalPatterns = {}
+_lno = int()
 
 
-class _Item:
+class _Node(object):
     """
-    A class used to represent the item with probability in transaction of dataset
+    A class used to represent the node of frequentPatternTree
 
     :Attributes:
 
-        item : int or word
-          Represents the name of the item
+        item : int or None
+            Storing item of a node
+        timeStamps : list
+            To maintain the timestamps of a database at the end of the branch
+        parent : node
+            To maintain the parent of every node
+        children : list
+            To maintain the children of a node
 
-        probability : float
-          Represent the existential probability(likelihood presence) of an item
-    """
+    :Methods:
 
-    def __init__(self, item, probability) -> None:
-        self.item = item
-        self.probability = probability
+        addChild(itemName)
+            Storing the children to their respective parent nodes
+        """
 
+    def __init__(self, item, locations, parent=None):
+        self.item = item
+        self.locations = locations
+        self.parent = parent
+        self.children = {}
+
+    def addChild(self, item, locations):
+        if item not in self.children:
+            self.children[item] = _Node(item, locations, self)
+        else:
+            self.children[item].locations = locations + self.children[item].locations
+            
+        return self.children[item]
+
+    def traverse(self):
+        transaction = []
+        locs = self.locations
+        node = self.parent
+        while node.parent is not None:
+            transaction.append(node.item)
+            node = node.parent
+        return transaction[::-1], locs
+
+    def traverse(self):
+        transaction = []
+        locs = self.locations
+        node = self.parent
+        while node.parent is not None:
+            transaction.append(node.item)
+            node = node.parent
+        return transaction[::-1], locs
 
-class TUFP(_ab._frequentPatterns):
+class PFPGrowth(_ab._periodicFrequentPatterns):
     """
-    About this algorithm
-    ====================
+    :Description:   PFPGrowth is one of the fundamental algorithm to discover periodic-frequent patterns in a transactional database.
 
-    :Description: It is one of the fundamental algorithm to discover top-k frequent patterns in a uncertain transactional database using CUP-Lists.
+    :Reference:   Syed Khairuzzaman Tanbeer, Chowdhury Farhan, Byeong-Soo Jeong, and Young-Koo Lee, "Discovering Periodic-Frequent
+                   Patterns in Transactional Databases", PAKDD 2009, https://doi.org/10.1007/978-3-642-01307-2_24
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of periodic frequent pattern's
+    :param  oFile: str :
+                   Name of the output file to store complete set of periodic frequent pattern's
+    :param  minSup: str:
+                   Controls the minimum number of transactions in which every item must appear in a database.
+    :param  maxPer: float:
+                   Controls the maximum number of transactions in which any two items within a pattern can reappear.
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
-    :Reference:  Tuong Le, Bay Vo, Van-Nam Huynh, Ngoc Thanh Nguyen, Sung Wook Baik 5, "Mining top-k frequent patterns from uncertain databases",
-                 Springer Science+Business Media, LLC, part of Springer Nature 2020, https://doi.org/10.1007/s10489-019-01622-1
 
     :Attributes:
 
         iFile : file
             Name of the Input file or path of the input file
-
         oFile : file
             Name of the output file or path of the output file
-
-        minSup : float or int or str
+        minSup : int or float or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
-
+        maxPer : int or float or str
+            The user can specify maxPer either in count or proportion of database size.
+            If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: maxPer=10 will be treated as integer, while maxPer=10.0 will be treated as float
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator.
-
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
-
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
-
-        startTime : float
+        startTime:float
             To record the start time of the mining process
-
-        endTime : float
+        endTime:float
             To record the completion time of the mining process
-
         Database : list
             To store the transactions of a database in list
-
         mapSupport : Dictionary
             To maintain the information of item and their frequency
-
         lno : int
             To represent the total no of transaction
-
         tree : class
             To represents the Tree class
-
         itemSetCount : int
             To represents the total no of patterns
-
         finalPatterns : dict
             To store the complete patterns
 
     :Methods:
-        mine()
+
+        startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
-        storePatternsInFile(oFile)
-            Complete set of frequent patterns will be loaded in to a output file
-        getPatternsInDataFrame()
-            Complete set of frequent patterns will be loaded in to a dataframe
+        save(oFile)
+            Complete set of periodic-frequent patterns will be loaded in to a output file
+        getPatternsAsDataFrame()
+            Complete set of periodic-frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
         creatingItemSets(fileName)
             Scans the dataset and stores in a list format
-        frequentOneItem()
-            Extracts the one-length frequent patterns from database
-        updateTransactions()
-            Update the transactions by removing non-frequent items and sort the Database by item decreased support
+        PeriodicFrequentOneItem()
+            Extracts the one-periodic-frequent patterns from database
+        updateDatabases()
+            Update the database by removing aperiodic items and sort the Database by item decreased support
         buildTree()
             After updating the Database, remaining items will be added into the tree by setting root node as null
         convert()
             to convert the user specified value
-        startMine()
-            Mining process will start from this function
-
-    Execution methods
-    =================
 
 
-    **Terminal command**
 
-
-    .. code-block:: console
-
-      Format:
-
-      (.venv) $ python3 TUFP.py <inputFile> <outputFile> <minSup>
-
-      Example Usage:
-
-      (.venv) $ python3 TUFP.py sampleDB.txt patterns.txt 10.0
-
-    .. note:: minSup can be specified  in support count or a value between 0 and 1.
-
-
-    **Calling from a python program**
-
-    .. code-block:: python
-
-            from PAMI.uncertainFrequentPattern.basic import TUFP as alg
-
-
-            iFile = 'sampleDB.txt'
-
-            minSup = 10  # can also be specified between 0 and 1
-
-            obj = alg.TUFP(iFile, minSup)
-
-            obj.mine()
-
-            frequentPatterns = obj.getPatterns()
-
-            print("Total number of Frequent Patterns:", len(frequentPatterns))
-
-            obj.save(oFile)
-
-            Df = obj.getPatternsAsDataFrame()
-
-            memUSS = obj.getmemoryUSS()
-
-            print("Total Memory in USS:", memUSS)
-
-            memRSS = obj.getMemoryRSS()
-
-            print("Total Memory in RSS", memRSS)
-
-            run = obj.getRuntime()
-
-            print("Total ExecutionTime in seconds:", run)
-
-    Credits
-    =======
-
-            The complete program was written by   P.Likhitha   under the supervision of Professor Rage Uday Kiran.
+    **Credits:**
+    --------------
+             The complete program was written by  P.Likhitha  under the supervision of Professor Rage Uday Kiran.
     """
-
     _startTime = float()
     _endTime = float()
     _minSup = str()
+    _maxPer = float()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
-    _cupList = {}
-    _topk = {}
-    _minimum = 9999
+    _rank = {}
+    _rankedUp = {}
+    _lno = 0
 
     def _creatingItemSets(self) -> None:
         """
-        Scans the dataset
+            Storing the complete transactions of the database/input file in a database variable
+        :return: None
         """
         self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
-            uncertain, data = [], []
+            data, ts = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
+            if 'TS' in i:
+                ts = self._iFile['TS'].tolist()
             if 'Transactions' in i:
-                self._Database = self._iFile['Transactions'].tolist()
-            if 'uncertain' in i:
-                uncertain = self._iFile['uncertain'].tolist()
-            for k in range(len(data)):
-                tr = []
-                for j in range(len(data[k])):
-                    product = _Item(data[k][j], uncertain[k][j])
-                    tr.append(product)
+                data = self._iFile['Transactions'].tolist()
+            for i in range(len(data)):
+                tr = [ts[i][0]]
+                tr = tr + data[i]
                 self._Database.append(tr)
 
-            # print(self.Database)
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    tr = []
-                    for i in temp:
-                        i1 = i.index('(')
-                        i2 = i.index(')')
-                        item = i[0:i1]
-                        probability = float(i[i1 + 1:i2])
-                        product = _Item(item, probability)
-                        tr.append(product)
                     self._Database.append(temp)
             else:
                 try:
-                    with open(self._iFile, 'r') as f:
+                    with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
+                            line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
-                            tr = []
-                            for i in temp:
-                                i1 = i.index('(')
-                                i2 = i.index(')')
-                                item = i[0:i1]
-                                probability = float(i[i1 + 1:i2])
-                                product = _Item(item, probability)
-                                tr.append(product)
-                            self._Database.append(tr)
+                            self._Database.append(temp)
                 except IOError:
                     print("File Not Found")
+                    quit()
 
-    def _frequentOneItem(self) -> List[str]:
+    def _convert(self, value) -> int:
         """
-        Takes the self.Database and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
+        To convert the given user specified value
 
-        :param self.Database : it represents the one self.Database in database
-        :type self.Database : list
-        """
-
-        mapSupport = {}
-        k = 0
-        for i in self._Database:
-            k += 1
-            for j in i:
-                if j.item not in mapSupport:
-                    mapSupport[j.item] = j.probability
-                    self._cupList[j.item] = {k:j.probability}
-                else:
-                    mapSupport[j.item] += j.probability
-                    self._cupList[j.item].update({k: j.probability})
-        plist = [k for k,v in sorted(mapSupport.items(), key=lambda x: x[1], reverse=True)]
-        k = 0
-        for x, in plist:
-            k +=1
-            if k >= self._minSup:
-                break
-            self._finalPatterns[x] = mapSupport[x]
-        self._minimum = min(list(self._finalPatterns.values()))
-        return plist
-
-    @staticmethod
-    def _convert(value: Union[int, float, str]) -> Union[int, float]:
-        """
-        To convert the type of user specified minSup value
-
-        :param value: user specified minSup value
-        :return: converted type minSup value
+        :param value: user specified value
+        :return: converted value
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
-            value = float(value)
+            value = (len(self._Database) * value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
+                value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
-    def _save(self, prefix: List[str], suffix: List[str], tidSetI: Dict[int, float]) -> None:
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
+    def startMine(self) -> None:
         """
-        Saves the patterns that satisfy the periodic frequent property.
-
-        :param prefix: the prefix of a pattern
-        :type prefix: list
-        :param suffix: the suffix of a patterns
-        :type suffix: list
-        :param tidSetI: the timestamp of a patterns
-        :type tidSetI: dict
+        Mining process will start from this function
+        :return: None
         """
 
-        if prefix is None:
-            prefix = suffix
-        else:
-            prefix = prefix + suffix
-        val = sum(tidSetI.values())
-        #print(prefix, val)
-        if len(self._finalPatterns) <= self._minSup:
-            sample = str()
-            for i in prefix:
-                sample = sample + i + " "
-            self._finalPatterns[sample] = val
-        if len(self._finalPatterns) == self._minSup:
-            if val > self._minimum:
-                sample = str()
-                for i in prefix:
-                    sample = sample + i + " "
-                index = list(self._finalPatterns.keys())[list(self._finalPatterns.values()).index(self._minimum)]
-                del self._finalPatterns[index]
-                self._finalPatterns[sample] = val
-                self._minimum = min(list(self._finalPatterns.values()))
-        #print(self.finalPatterns, self.minimum, self.minSup)
-
-
-    def _Generation(self, prefix: List[str], itemSets: List[str], tidSets: List[Dict[int, float]]) -> None:
-        """
-        Equivalence class is followed  and checks for the patterns generated for periodic-frequent patterns.
-
-        :param prefix:  main equivalence prefix
-        :type prefix: periodic-frequent item or pattern
-        :param itemSets: patterns which are items combined with prefix and satisfying the periodicity and frequent with their timestamps
-        :type itemSets: list
-        :param tidSets: timestamps of the items in the argument itemSets
-        :type tidSets: list
-        """
-        if len(itemSets) == 1:
-            i = itemSets[0]
-            tidI = tidSets[0]
-            self._save(prefix, [i], tidI)
-            return
-        for i in range(0, len(itemSets)):
-            itemI = itemSets[i]
-            if itemI is None:
+        self.Mine()
+
+    def _getMaxPer(self, arr, maxTS):
+        arr = np.append(arr, [0, maxTS])
+        arr = np.sort(arr)
+        arr = np.diff(arr)
+
+        return np.max(arr)
+
+    def _construct(self, items, data, minSup, maxPer, maxTS, patterns):
+
+        # maxPerItems = {k: self.getMaxPer(v, maxTS) for k, v in items.items() if len(v) >= minSup}
+
+        items = {k: v for k, v in items.items() if len(v) >= minSup and self._getMaxPer(v, maxTS) <= maxPer}
+
+        #tested ok
+        for item, ts in items.items():
+            # pat = "\t".join(item)
+            # self.patCount += 1
+            # patterns[pat] = (len(ts), self.getMaxPer(ts, maxTS))
+            patterns[tuple([item])] = [len(ts), self._getMaxPer(ts, maxTS)]
+
+        root = _Node([], None, None)
+        itemNodes = {}
+        for line in data:
+            currNode = root
+            index = int(line[0])
+            line = line[1:]
+            line = sorted([item for item in line if item in items], key = lambda x: len(items[x]), reverse = True)
+            for item in line:
+                currNode = currNode.addChild(item, [index])   # heavy
+                if item in itemNodes:
+                    itemNodes[item].add(currNode)
+                else:
+                    itemNodes[item] = set([currNode])
+
+        return root, itemNodes
+
+
+    def _recursive(self, root, itemNode, minSup, maxPer, patterns, maxTS):
+
+        for item in itemNode:
+            newRoot = _Node(root.item + [item], None, None)
+
+            itemLocs = {}
+            transactions = {}
+            for node in itemNode[item]:
+                transaction, locs = node.traverse()
+                if len(transaction) < 1:
+                    continue
+                # transactions.append((transaction, locs))
+                if tuple(transaction) in transactions:
+                    transactions[tuple(transaction)].extend(locs)
+                else:
+                    transactions[tuple(transaction)] = locs
+
+                for item in transaction:
+                    if item in itemLocs:
+                        itemLocs[item] += locs
+                    else:
+                        itemLocs[item] = list(locs)
+
+            # Precompute getMaxPer results for itemLocs
+            maxPerResults = {item: self._getMaxPer(itemLocs[item], maxTS) for item in itemLocs if len(itemLocs[item]) >= minSup}
+
+            # Filter itemLocs based on minSup and maxPer
+            itemLocs = {k: len(v) for k, v in itemLocs.items() if k in maxPerResults and maxPerResults[k] <= maxPer}
+
+            # Iterate over filtered itemLocs
+            for item in itemLocs:
+                # pat = "\t".join([str(x) for x in newRoot.item + [item]])
+                # self.patCount += 1
+                # patterns[pat] = [itemLocs[item], maxPerResults[item]]
+                patterns[tuple(newRoot.item + [item])] = [itemLocs[item], maxPerResults[item]]
+            
+            if not itemLocs:
                 continue
-            tidSetI = tidSets[i]
-            classItemSets = []
-            classTidSets = []
-            itemSetX = [itemI]
-            for j in range(i+1, len(itemSets)):
-                itemJ = itemSets[j]
-                tidSetJ = tidSets[j]
-                y = {key: tidSetJ[key] * tidSetI.get(key, 0) for key in tidSetJ.keys()}
-                sum2 = sum(list(y.values()))
-                #print(prefix, itemJ, y, sum2)
-                #if sum2 >= self.minimum:
-                self._save(prefix, [itemJ], y)
-                classItemSets.append(itemJ)
-                classTidSets.append(y)
-            #print(itemI, tidSetI, classItemSets)
-            newPrefix = list(set(itemSetX)) + prefix
-            self._Generation(newPrefix, classItemSets, classTidSets)
-            #self.save(prefix, list(set(itemSetX)), tidSetI)
 
-    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
-    def startMine(self) -> None:
-        """
-        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
-        """
-        self.mine()
+            newItemNodes = {}
+
+            for transaction, locs in transactions.items():
+                transaction = sorted([item for item in transaction if item in itemLocs], key = lambda x: itemLocs[x], reverse = True)
+                if len(transaction) < 1:
+                    continue
+                currNode = newRoot
+                for item in transaction:
+                    currNode = currNode.addChild(item, locs)
+                    if item in newItemNodes:
+                        newItemNodes[item].add(currNode)
+                    else:
+                        newItemNodes[item] = set([currNode])
 
+            self._recursive(newRoot, newItemNodes, minSup, maxPer, patterns, _lno)
 
-    def mine(self) -> None:
+    def Mine(self) -> None:
         """
-        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
+        Mining process will start from this function
+        :return: None
         """
-        global _minSup
+
+        global _minSup, _maxPer, _lno
         self._startTime = _ab._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._minSup is None:
+            raise Exception("Please enter the Minimum Support")
+        if self._maxPer is None:
+            raise Exception("Please enter the Maximum Periodicity")
+        if self._sep is None:
+            raise Exception("Default separator is tab space, please enter the separator if you have different separator in the input file")
+
         self._creatingItemSets()
         self._minSup = self._convert(self._minSup)
-        _minSup = self._minSup
-        plist = self._frequentOneItem()
-        for i in range(len(plist)):
-            itemI = plist[i]
-            tidSetI = self._cupList[itemI]
-            itemSetX = [itemI]
-            itemSets = []
-            tidSets = []
-            for j in range(i+1, len(plist)):
-                itemJ = plist[j]
-                tidSetJ = self._cupList[itemJ]
-                y1 = {key: tidSetJ[key] * tidSetI.get(key, 0)  for key in tidSetJ.keys()}
-                self._save(itemSetX, [itemJ], y1)
-                itemSets.append(itemJ)
-                tidSets.append(y1)
-            self._Generation(itemSetX, itemSets, tidSets)
-        print("Top-K Frequent patterns were generated from uncertain databases successfully using TUFP algorithm")
+        self._maxPer = self._convert(self._maxPer)
+        #tested ok
+        _minSup, _maxPer, _lno = self._minSup, self._maxPer, len(self._Database)
+        if self._minSup > len(self._Database):
+            raise Exception("Please enter the minSup in range between 0 to 1")
+        
+
+        items = {}
+
+        # tested ok
+        for line in self._Database:
+            index = int(line[0])
+            for item in line[1:]:
+                if item not in items:
+                    items[item] = []
+                items[item].append(index)
+
+        root, itemNodes = self._construct(items, self._Database, _minSup, _maxPer, _lno, self._finalPatterns)
+
+        self._recursive(root, itemNodes, _minSup, _maxPer, self._finalPatterns, _lno)
+
+    
+
+        newPattern = {}
+        for k, v in self._finalPatterns.items():
+            newPattern["\t".join([str(x) for x in k])] = v
+
+        self._finalPatterns = newPattern
+
+
+
+
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
+        print("Periodic Frequent patterns were generated successfully using PFPGrowth algorithm ")
 
     def getMemoryUSS(self) -> float:
-        """
-
-        Total amount of USS memory consumed by the mining process will be retrieved from this function
+        """Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self) -> float:
-        """
-
-        Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
     def getRuntime(self) -> float:
-        """
+        """Calculating the total amount of runtime taken by the mining process
 
-        Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
-    def getPatternsAsDataFrame(self) -> pd.DataFrame:
+    def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
         """
+        Storing final periodic-frequent patterns in a dataframe
 
-        Storing final frequent patterns in a dataframe
-
-        :return: returning frequent patterns in a dataframe
+        :return: returning periodic-frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
-        dataframe = {}
+        dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a, b])
-            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
-        return dataframe
+            data.append([a, b[0], b[1]])
+            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
+        return dataFrame
 
     def save(self, outFile: str) -> None:
         """
-
-        Complete set of frequent patterns will be loaded in to an output file
+        Complete set of periodic-frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
-        :type outFile: file
+        :type outFile: csv file
+        :return: None
         """
-        self.oFile = outFile
-        writer = open(self.oFile, 'w+')
+        self._oFile = outFile
+        writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            s1 = x + ":" + str(y)
+            s1 = x + ":" + str(y[0]) + ":" + str(y[1])
+            #s1 = x.replace(' ', '\t').strip() + ":" + str(y[0]) + ":" + str(y[1])
             writer.write("%s \n" % s1)
 
-    def getPatterns(self) -> Dict[str, float]:
+    def getPatterns(self) -> Dict[str, Tuple[int, int]]:
         """
+        Function to send the set of periodic-frequent patterns after completion of the mining process
 
-        Function to send the set of frequent patterns after completion of the mining process
-
-        :return: returning frequent patterns
+        :return: returning periodic-frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self) -> None:
         """
         This function is used to print the results
+        :return: None
         """
-        print("Total number of  Uncertain Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of Periodic Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:",  self.getRuntime())
 
+
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
+    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
+        if len(_ab._sys.argv) == 6:
+            _ap = PFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
         if len(_ab._sys.argv) == 5:
-            _ap = TUFP(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
-        if len(_ab._sys.argv) == 4:
-            _ap = TUFP(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap = PFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         _ap.startMine()
-        _ap.mine()
-        _Patterns = _ap.getPatterns()
-        print("Total number of Patterns:", len(_Patterns))
+        print("Total number of Periodic-Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
-        _memUSS = _ap.getMemoryUSS()
-        print("Total Memory in USS:", _memUSS)
-        _memRSS = _ap.getMemoryRSS()
-        print("Total Memory in RSS", _memRSS)
-        _run = _ap.getRuntime()
-        print("Total ExecutionTime in ms:", _run)
+        print("Total Memory in USS:", _ap.getMemoryUSS())
+        print("Total Memory in RSS", _ap.getMemoryRSS())
+        print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
-        '''ap = TUFP("/home/apiiit-rkv/Desktop/uncertain/tubeSample", 10, ' ')
-        ap.startMine()
-        Patterns = ap.getPatterns()
-        print("Total number of Patterns:", len(Patterns))
-        ap.save("patterns.txt")
-        memUSS = ap.getMemoryUSS()
-        print("Total Memory in USS:", memUSS)
-        memRSS = ap.getMemoryRSS()
-        print("Total Memory in RSS", memRSS)
-        run = ap.getRuntime()
-        print("Total ExecutionTime in ms:", run)'''
         print("Error! The number of input parameters do not match the total number of parameters provided")
 
+
+
+    """
+    **Methods to execute code on terminal**
+    --------------------------------------------
+    .. code-block:: console
+
+      Format:
+            
+      (.venv) $ python3 PFPGrowth.py <inputFile> <outputFile> <minSup> <maxPer>
+
+      Example:
+      
+      (.venv) $ python3 PFPGrowth.py sampleTDB.txt patterns.txt 0.3 0.4
+
+    .. note:: minSup will be considered in percentage of database transactions
+
+    **Importing this algorithm into a python program**
+    ---------------------------------------------------
+    .. code-block:: python
+
+                from PAMI.periodicFrequentPattern.basic import PFPGrowth as alg
+
+                obj = alg.PFPGrowth(iFile, minSup, maxPer)
+
+                obj.startMine()
+
+                periodicFrequentPatterns = obj.getPatterns()
+
+                print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
+
+                obj.save(oFile)
+
+                Df = obj.getPatternsAsDataFrame()
+
+                memUSS = obj.getMemoryUSS()
+
+                print("Total Memory in USS:", memUSS)
+
+                memRSS = obj.getMemoryRSS()
+
+                print("Total Memory in RSS", memRSS)
+
+                run = obj.getRuntime()
+
+                print("Total ExecutionTime in seconds:", run)
+    """
```

### Comparing `pami-2024.5.28.1/PAMI/uncertainFrequentPattern/basic/TubeS.py` & `pami-2024.5.7.1/PAMI/uncertainPeriodicFrequentPattern/basic/UPFPGrowthPlus.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,85 +1,106 @@
-# TubeS is one of the fastest algorithm to discover frequent patterns in a uncertain transactional database.
+# UPFPGrowthPlus is used to discover periodic-frequent patterns in an uncertain temporal database.
 #
 # **Importing this algorithm into a python program**
 #
-#             from PAMI.uncertainFrequentPattern.basic import TubeS as alg
+#             from PAMI.uncertainPeriodicFrequentPattern.basic import UPFPGrowthPlus as alg
 #
 #             iFile = 'sampleDB.txt'
 #
 #             minSup = 10  # can also be specified between 0 and 1
 #
-#             obj = alg.TubeS(iFile, minSup)
+#             maxPer = 3   # can also be specified between 0 and 1
+#
+#             obj = alg.UPFPGrowthPlus(iFile, minSup, maxPer)
 #
 #             obj.mine()
 #
-#             frequentPatterns = obj.getPatterns()
+#             periodicFrequentPatterns = obj.getPatterns()
 #
-#             print("Total number of Frequent Patterns:", len(frequentPatterns))
+#             print("Total number of uncertain Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 #
 #             obj.save(oFile)
 #
 #             Df = obj.getPatternsAsDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
-#              print("Total Memory in USS:", memUSS)
+#             print("Total Memory in USS:", memUSS)
 #
-#              memRSS = obj.getMemoryRSS()
+#             memRSS = obj.getMemoryRSS()
 #
-#              print("Total Memory in RSS", memRSS)
+#             print("Total Memory in RSS", memRSS)
 #
-#              run = obj.getRuntime()
+#             run = obj.getRuntime()
 #
-#              print("Total ExecutionTime in seconds:", run)
+#             print("Total ExecutionTime in seconds:", run)
 #
 
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
+
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
-     
+
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
-     
+
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
-from PAMI.uncertainFrequentPattern.basic import abstract as _fp
-import deprecated
+
+from PAMI.uncertainPeriodicFrequentPattern.basic import abstract as _ab
+import pandas as pd
+from deprecated import deprecated
+
+from PAMI.uncertainPeriodicFrequentPattern.basic import abstract as _ab
 
 _minSup = float()
-_fp._sys.setrecursionlimit(20000)
-_finalPatterns = {}
+_maxPer = float()
+_lno = int()
+_first = int()
+_last = int()
 
 
 class _Item:
     """
     A class used to represent the item with probability in transaction of dataset
 
     :Attributes:
 
-        item : int or word
+        item : int or string
           Represents the name of the item
 
         probability : float
           Represent the existential probability(likelihood presence) of an item
     """
 
     def __init__(self, item, probability):
         self.item = item
         self.probability = probability
 
 
+def printTree(root):
+    """
+    To print the tree with nodes with item name, probability, timestamps, and second probability respectively.
+
+    :param root: Node
+    :return: print all Tree with nodes with items, probability, parent item, timestamps, second probability respectively.
+    """
+    for x, y in root.children.items():
+        print(x, y.item, y.probability, y.parent.item, y.tids, y.secondProbability)
+        printTree(y)
+
+
 class _Node(object):
     """
     A class used to represent the node of frequentPatternTree
 
     :Attributes:
 
         item : int
@@ -93,113 +114,93 @@
 
         children : list
             To maintain the children of node
 
     :Methods:
 
         addChild(itemName)
-             storing the children to their respective parent nodes
+            storing the children to their respective parent nodes
     """
 
     def __init__(self, item, children):
         self.item = item
         self.probability = 1
         self.secondProbability = 1
+        self.p = 1
         self.children = children
         self.parent = None
+        self.TimeStamps = []
 
     def addChild(self, node):
         """
-        This function is used to add child
+        To add children details to parent node
+
+        :param node: children node
+        :return: update parent node children
         """
         self.children[node.item] = node
         node.parent = self
 
 
-def Second(transaction, i):
-    """
-    To calculate the second probability of a node in transaction
-
-    :param transaction: transaction in a database
-    :param i: index of item in transaction
-    :return: second probability of a node
-    """
-    temp = []
-    for j in range(0, i):
-        temp.append(transaction[j].probability)
-    l1 = max(temp)
-    temp.remove(l1)
-    l2 = max(temp)
-    return l2 * l2
-
-
-def printTree(root):
-    """
-    To print the tree with root node through recursion
-
-    :param root: root node of  tree
-    :return: details of tree
-    """
-    for x, y in root.children.items():
-        print(x, y.item, y.probability, y.parent.item, y.tids, y.secondProbability)
-        printTree(y)
-
-
 class _Tree(object):
     """
     A class used to represent the frequentPatternGrowth tree structure
 
-    :Attributes:
+    Attributes:
+
+        root: Node
+            Represents the root node of the tree
 
-        root : Node
-          Represents the root node of the tree
+        summaries: dictionary
+            storing the nodes with same item name
 
-        summaries : dictionary
-          storing the nodes with same item name
+        info: dictionary
+            stores the support of items
 
-        info : dictionary
-          stores the support of items
 
     :Methods:
+
         addTransaction(transaction)
-            creating transaction as a branch in frequentPatternTree
+            creating transaction as a branch in Tree
         addConditionalTransaction(prefixPaths, supportOfItems)
             construct the conditional tree for prefix paths
-        conditionalPatterns(Node)
+        getConditionalPatterns(Node)
             generates the conditional patterns from tree for specific node
         conditionalTransactions(prefixPaths,Support)
-            takes the prefixPath of a node and support at child of the path and extract the frequent items from
-            prefixPaths and generates prefixPaths with items which are frequent
-        removeNode(Node)
+            takes the prefixPath of a node and support at child of the path and extract the frequent items from prefixPaths and generates prefixPaths with items which are frequent
+        remove(Node)
             removes the node from tree once after generating all the patterns respective to the node
-        generate_patterns(Node)
+        generatePatterns(Node)
             starts from the root node of the tree and mines the frequent patterns
-            """
+
+    """
 
     def __init__(self):
         self.root = _Node(None, {})
         self.summaries = {}
         self.info = {}
 
-    def addTransaction(self, transaction):
+
+    def addTransaction(self, transaction, tid):
         """
         Adding transaction into tree
 
-        :param transaction : it represents the one transactions in database
+        :param transaction : it represents the one transaction in database
         :type transaction : list
+        :param tid : the timestamp of transaction
+        :type tid : list
         """
         currentNode = self.root
         k = 0
         for i in range(len(transaction)):
             k += 1
             if transaction[i].item not in currentNode.children:
                 newNode = _Node(transaction[i].item, {})
                 newNode.k = k
-                if k >= 3:
-                    newNode.secondProbability = Second(transaction, i)
+                newNode.secondProbability = transaction[i].probability
                 l1 = i - 1
                 temp = []
                 while l1 >= 0:
                     temp.append(transaction[l1].probability)
                     l1 -= 1
                 if len(temp) == 0:
                     newNode.probability = round(transaction[i].probability, 2)
@@ -209,281 +210,348 @@
                 if transaction[i].item in self.summaries:
                     self.summaries[transaction[i].item].append(newNode)
                 else:
                     self.summaries[transaction[i].item] = [newNode]
                 currentNode = newNode
             else:
                 currentNode = currentNode.children[transaction[i].item]
-                if k >= 3:
-                    currentNode.secondProbability = max(transaction[i].probability, currentNode.secondProbability)
+                currentNode.secondProbability = max(transaction[i].probability, currentNode.secondProbability)
                 currentNode.k = k
                 l1 = i - 1
                 temp = []
                 while l1 >= 0:
                     temp.append(transaction[l1].probability)
                     l1 -= 1
                 if len(temp) == 0:
                     currentNode.probability += round(transaction[i].probability, 2)
                 else:
                     nn = max(temp) * transaction[i].probability
                     currentNode.probability += round(nn, 2)
+        currentNode.TimeStamps = currentNode.TimeStamps + tid
 
-    def addConditionalTransaction(self, transaction, sup, second):
+    def addConditionalPatterns(self, transaction, tid, sup, probability):
         """
         Constructing conditional tree from prefixPaths
 
-        :param transaction : it represents the one transactions in database
+        :param transaction : it represents the one transaction in database
         :type transaction : list
+        :param tid : timestamps of a pattern or transaction in tree
+        :param tid : list
         :param sup : support of prefixPath taken at last child of the path
         :type sup : int
-        :param second: second probability of the leaf node
-        :type second: float
+        :para probability : highest existential probability value among all periodic-frequent items
+        :type probability : list
         """
         currentNode = self.root
         k = 0
         for i in range(len(transaction)):
             k += 1
             if transaction[i] not in currentNode.children:
                 newNode = _Node(transaction[i], {})
                 newNode.k = k
-                newNode.secondProbability = second
                 newNode.probability = sup
+                newNode.secondProbability = probability
                 currentNode.addChild(newNode)
                 if transaction[i] in self.summaries:
                     self.summaries[transaction[i]].append(newNode)
                 else:
                     self.summaries[transaction[i]] = [newNode]
                 currentNode = newNode
             else:
                 currentNode = currentNode.children[transaction[i]]
                 currentNode.k = k
-                currentNode.secondProbability = max(currentNode.secondProbability, second)
                 currentNode.probability += sup
+                currentNode.secondProbability = max(probability, currentNode.secondProbability)
+        currentNode.TimeStamps = currentNode.TimeStamps + tid
 
     def conditionalPatterns(self, alpha):
         """
         Generates all the conditional patterns of respective node
 
         :param alpha : it represents the Node in tree
-        :type alpha : _Node
+        :type alpha : Node
         """
         finalPatterns = []
+        finalSets = []
         sup = []
-        second = []
+        prob = []
         for i in self.summaries[alpha]:
+            set1 = i.TimeStamps
             s = i.probability
-            s1 = i.secondProbability
+            p = i.secondProbability
             set2 = []
             while i.parent.item is not None:
                 set2.append(i.parent.item)
                 i = i.parent
             if len(set2) > 0:
                 set2.reverse()
                 finalPatterns.append(set2)
-                second.append(s1)
+                finalSets.append(set1)
                 sup.append(s)
-        finalPatterns, support, info = self.conditionalTransactions(finalPatterns, sup)
-        return finalPatterns, support, info, second
+                prob.append(p)
+        finalPatterns, finalSets, support, prob, info = self.conditionalTransactions(finalPatterns, finalSets, sup, prob)
+        return finalPatterns, finalSets, support, prob, info
+
+    def removeNode(self, nodeValue):
+        """
+        Removing the node from tree
 
-    def conditionalTransactions(self, condPatterns, support):
+        :param nodeValue : it represents the node in tree
+        :type nodeValue : node
+        """
+        for i in self.summaries[nodeValue]:
+            i.parent.TimeStamps = i.parent.TimeStamps + i.TimeStamps
+            del i.parent.children[nodeValue]
+
+    def getPeriodAndSupport(self, support, TimeStamps):
+        """
+        To calculate the periodicity of given timestamps
+
+        :param support: support of pattern
+        :param TimeStamps: timmeStamps of a pattern
+        :return: support and period
+        """
+        global _maxPer
+        global _lno
+        TimeStamps.sort()
+        cur = 0
+        per = 0
+        sup = support
+        for j in range(len(TimeStamps)):
+            per = max(per, TimeStamps[j] - cur)
+            if per > _maxPer:
+                return [0, 0]
+            cur = TimeStamps[j]
+        per = max(per, _lno - cur)
+        return [sup, per]
+
+    def conditionalTransactions(self, conditionalPatterns, conditionalTimeStamps, support, probability):
         """
         It generates the conditional patterns with frequent items
 
-        :param condPatterns : conditional patterns generated from conditionalPatterns() method for respective node
-        :type condPatterns : list
+        :param conditionalPatterns : conditional patterns generated from conditionalPatterns() method for respective node
+        :type conditionalPatterns : list
+        :param conditionalTimeStamps : timestamps of respective conditional timestamps
+        :type conditionalTimeStamps : list
         :param support : the support of conditional pattern in tree
         :type support : list
+        :para probability : highest existential probability value among all periodic-frequent items
+        :type probability : list
         """
-        global _minSup
+        global _minSup, _maxPer, _lno
         pat = []
+        TimeStamps = []
         sup = []
+        prob = []
         data1 = {}
-        for i in range(len(condPatterns)):
-            for j in condPatterns[i]:
+        count = {}
+        for i in range(len(conditionalPatterns)):
+            for j in conditionalPatterns[i]:
                 if j in data1:
-                    data1[j] += support[i]
+                    data1[j] = data1[j] + conditionalTimeStamps[i]
+                    count[j] += support[i]
                 else:
-                    data1[j] = support[i]
+                    data1[j] = conditionalTimeStamps[i]
+                    count[j] = support[i]
         updatedDict = {}
-        updatedDict = {k: v for k, v in data1.items() if v >= _minSup}
+        for m in data1:
+            updatedDict[m] = self.getPeriodAndSupport(count[m], data1[m])
+        updatedDict = {k: v for k, v in updatedDict.items() if v[0] >= _minSup and v[1] <= _maxPer}
         count = 0
-        for p in condPatterns:
+        for p in conditionalPatterns:
             p1 = [v for v in p if v in updatedDict]
-            trans = sorted(p1, key=lambda x: (updatedDict.get(x)), reverse=True)
+            trans = sorted(p1, key=lambda x: (updatedDict.get(x)[0]), reverse=True)
             if len(trans) > 0:
                 pat.append(trans)
+                TimeStamps.append(conditionalTimeStamps[count])
                 sup.append(support[count])
+                prob.append(probability[count])
             count += 1
-        return pat, sup, updatedDict
+        return pat, TimeStamps, sup, prob, updatedDict
 
-    def removeNode(self, nodeValue):
-        """
-        Removing the node from tree
-
-        :param nodeValue : it represents the node in tree
-        :type nodeValue : node
-        """
-        for i in self.summaries[nodeValue]:
-            del i.parent.children[nodeValue]
-
-    def generatePatterns(self, prefix):
+    def generatePatterns(self, prefix, periodic):
         """
         Generates the patterns
 
         :param prefix : forms the combination of items
         :type prefix : list
+        :para periodic : occurring at intervals
+        :type periodic : list
         """
-        global _finalPatterns, _minSup
-        for i in sorted(self.summaries, key=lambda x: (self.info.get(x))):
+        global _minSup
+        for i in sorted(self.summaries, key=lambda x: (self.info.get(x)[0])):
             pattern = prefix[:]
             pattern.append(i)
             s = 0
+            secProb = []
+            kk = int()
             for x in self.summaries[i]:
-                #if x.k <= 2:
-                    #s += x.probability
-                #elif x.k >= 3:
-                    #n = x.probability * pow(x.secondProbability, (x.k - 2))
-                    #s += n
-                if len(pattern) <= 2:
+                if x.k <= 2:
                     s += x.probability
-                elif len(pattern) >= 3:
+                elif x.k >= 3:
                     n = x.probability * pow(x.secondProbability, (x.k - 2))
                     s += n
-            _finalPatterns[tuple(pattern)] = self.info[i]
+            periodic[tuple(pattern)] = self.info[i]
+            periodic[tuple(pattern)] = self.info[i]
             if s >= _minSup:
-                patterns, support, info, second = self.conditionalPatterns(i)
+                periodic[tuple(pattern)] = self.info[i]
+                patterns, TimeStamps, support, probability, info = self.conditionalPatterns(i)
                 conditionalTree = _Tree()
                 conditionalTree.info = info.copy()
                 for pat in range(len(patterns)):
-                    conditionalTree.addConditionalTransaction(patterns[pat], support[pat], second[pat])
+                    conditionalTree.addConditionalPatterns(patterns[pat], TimeStamps[pat], support[pat], probability[pat])
                 if len(patterns) > 0:
-                    conditionalTree.generatePatterns(pattern)
+                    conditionalTree.generatePatterns(pattern, periodic)
             self.removeNode(i)
 
-
-class TubeS(_fp._frequentPatterns):
+class UPFPGrowthPlus(_ab._periodicFrequentPatterns):
     """
     About this algorithm
     ====================
 
-    :Description: TubeS is one of the fastest algorithm to discover frequent patterns in a uncertain transactional database.
+    :Description: Basic Plus is  to discover periodic-frequent patterns in a uncertain temporal database.
+
+    :Reference:  Palla Likhitha, Rage Veena,Rage Uday Kiran, Koji Zettsu, Masashi Toyoda, Philippe Fournier-Viger, (2023).
+                 UPFP-growth++: An Efficient Algorithm to Find Periodic-Frequent Patterns in Uncertain Temporal Databases.
+                 ICONIP 2022. Communications in Computer and Information Science, vol 1792. Springer, Singapore.
+                 https://doi.org/10.1007/978-981-99-1642-9_16
+
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of Uncertain Periodic Frequent Patterns
+    :param  oFile: str :
+                   Name of the output file to store complete set of Uncertain Periodic Frequent patterns
+    :param  minSup: str:
+                   minimum support thresholds were tuned to find the appropriate ranges in the limited memory
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+    :param  maxper: floot :
+                   where maxPer represents the maximum periodicity threshold value specified by the user.
 
-    :Reference:  Carson Kai-Sang Leung and Richard Kyle MacKinnon. 2014. Fast Algorithms for Frequent Itemset Mining from Uncertain Data.
-                 In Proceedings of the 2014 IEEE International Conference on Data Mining (ICDM '14). IEEE Computer Society, USA, 893898. https://doi.org/10.1109/ICDM.2014.146
 
     :Attributes:
 
-        iFile : file
-            Name of the Input file or path of the input file
+        iFile: file
+            Name of the Input file or path of input file
 
-        oFile : file
-            Name of the output file or path of the output file
+        oFile: file
+            Name of the output file or path of output file
 
-        minSup : float or int or str
+        minSup: int or float or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
 
-        sep : str
+        maxPer: int or float or str
+            The user can specify maxPer either in count or proportion of database size.
+            If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: maxPer=10 will be treated as integer, while maxPer=10.0 will be treated as float
+
+        sep: str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator.
 
-        memoryUSS : float
+        memoryUSS: float
             To store the total amount of USS memory consumed by the program
 
-        memoryRSS : float
+        memoryRSS: float
             To store the total amount of RSS memory consumed by the program
 
-        startTime : float
+        startTime: float
             To record the start time of the mining process
 
-        endTime : float
+        endTime: float
             To record the completion time of the mining process
 
-        Database : list
+        Database: list
             To store the transactions of a database in list
 
-        mapSupport : Dictionary
+        mapSupport: Dictionary
             To maintain the information of item and their frequency
 
-        lno : int
+        lno: int
             To represent the total no of transaction
 
-        tree : class
+        tree: class
             To represents the Tree class
 
-        itemSetCount : int
+        itemSetCount: int
             To represents the total no of patterns
 
-        finalPatterns : dict
+        finalPatterns: dict
             To store the complete patterns
 
     :Methods:
-        mine()
+
+        startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
-        save(oFile)
-            Complete set of frequent patterns will be loaded in to a output file
+        savePatterns(oFile)
+            Complete set of periodic-frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
-            Complete set of frequent patterns will be loaded in to a dataframe
+            Complete set of periodic-frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
         creatingItemSets(fileName)
             Scans the dataset and stores in a list format
-        frequentOneItem()
-            Extracts the one-length frequent patterns from database
-        updateTransactions()
-            Update the transactions by removing non-frequent items and sort the Database by item decreased support
+        updateDatabases()
+            Update the database by removing aperiodic items and sort the Database by item decreased support
         buildTree()
             After updating the Database, remaining items will be added into the tree by setting root node as null
         convert()
             to convert the user specified value
+        PeriodicFrequentOneItems()
+            To extract the one-length periodic-frequent items
 
     Execution methods
     =================
 
 
     **Terminal command**
 
 
     .. code-block:: console
 
-      Format:
+       Format:
 
-      (.venv) $ python3 TubeS.py <inputFile> <outputFile> <minSup>
+       (.venv) $ python3 UPFPGrowthPlus.py <inputFile> <outputFile> <minSup> <maxPer>
 
-      Example Usage:
+       Examples Usage:
 
-      (.venv) $ python3 TubeS.py sampleDB.txt patterns.txt 10.0
+       (.venv) $ python3 UPFPGrowthPlus.py sampleTDB.txt patterns.txt 0.3 4
+
+    .. note:: minSup and maxPer will be considered in support count or frequency
 
-    .. note:: minSup can be specified  in support count or a value between 0 and 1.
 
     **Calling from a python program**
 
     .. code-block:: python
 
-            from PAMI.uncertainFrequentPattern.basic import TubeS as alg
+            from PAMI.uncertainPeriodicFrequentPattern import UPFPGrowthPlus as alg
 
             iFile = 'sampleDB.txt'
 
             minSup = 10  # can also be specified between 0 and 1
 
-            obj = alg.TubeS(iFile, minSup)
+            maxPer = 2   # can also be specified between 0 and 1
+
+            obj = alg.UPFPGrowthPlus(iFile, minSup, maxPer)
 
             obj.mine()
 
-            frequentPatterns = obj.getPatterns()
+            periodicFrequentPatterns = obj.getPatterns()
 
-            print("Total number of Frequent Patterns:", len(frequentPatterns))
+            print("Total number of uncertain Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 
             obj.save(oFile)
 
             Df = obj.getPatternsAsDataFrame()
 
             memUSS = obj.getMemoryUSS()
 
@@ -493,143 +561,160 @@
 
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
-    Credits
-    =======
 
+    **Credits**:
+    --------------
+        The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.\n
 
-            The complete program was written by  P.Likhitha   under the supervision of Professor Rage Uday Kiran.
-    """
-
+        """
     _startTime = float()
     _endTime = float()
     _minSup = float()
+    _maxPer = float()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
     _rank = {}
     _lno = 0
-    def __init__(self, iFile, minSup, sep='\t'):
-        super().__init__(iFile, minSup, sep)
+    _periodic = {}
+
     def _creatingItemSets(self):
         """
-        Scans the databases and stores the transactions into Database variable
+        Storing the complete transactions of the database/input file in a database variable
         """
+
         self._Database = []
-        if isinstance(self._iFile, _fp._pd.DataFrame):
-            uncertain, data = [], []
+        if isinstance(self._iFile, _ab._pd.DataFrame):
+            uncertain, data, ts = [], [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
+            if 'TS' in i:
+                ts = self._iFile['TS'].tolist()
             if 'Transactions' in i:
-                self._Database = self._iFile['Transactions'].tolist()
+                data = self._iFile['Transactions'].tolist()
             if 'uncertain' in i:
                 uncertain = self._iFile['uncertain'].tolist()
             for k in range(len(data)):
-                tr = []
-                for j in range(len(data[k])):
+                tr = [ts[k]]
+                for j in range(len(k)):
                     product = _Item(data[k][j], uncertain[k][j])
                     tr.append(product)
                 self._Database.append(tr)
                 self._lno += 1
 
             # print(self.Database)
         if isinstance(self._iFile, str):
-            if _fp._validators.url(self._iFile):
-                data = _fp._urlopen(self._iFile)
+            if _ab._validators.url(self._iFile):
+                data = _ab._urlopen(self._iFile)
                 for line in data:
-                    line = line.strip()
                     line = line.decode("utf-8")
-                    temp1 = line.split(':')
-                    temp = [i.rstrip() for i in temp[0].split(self._sep)]
-                    uncertain = [float(i.rstrip()) for i in temp[1].split(self._sep)]
-                    tr = []
-                    for i in range(len(temp)):
-                        item = temp[i]
-                        probability = uncertain[i]
+                    line = line.strip()
+                    line = [i for i in line.split(':')]
+                    temp1 = [i.rstrip() for i in line[0].split(self._sep)]
+                    temp2 = [i.rstrip() for i in line[1].split(self._sep)]
+                    temp1 = [x for x in temp1 if x]
+                    temp2 = [x for x in temp2 if x]
+                    tr = [int(temp1[0])]
+                    for i in range(len(temp1[1:])):
+                        item = temp1[i]
+                        probability = float(temp2[i])
                         product = _Item(item, probability)
                         tr.append(product)
                     self._lno += 1
-                    self._Database.append(temp)
+                    self._Database.append(tr)
             else:
                 try:
+                    count = 0
                     with open(self._iFile, 'r') as f:
                         for line in f:
-                            temp1 = line.strip()
-                            temp1 = temp1.split(':')
-                            temp = [i.rstrip() for i in temp1[0].split(self._sep)]
-                            uncertain = [float(i.rstrip()) for i in temp1[1].split(self._sep)]
-                            tr = []
-                            for i in range(len(temp)):
-                                item = temp[i]
-                                probability = uncertain[i]
+                            line = line.strip()
+                            line = [i for i in line.split(':')]
+                            temp1 = [i.rstrip() for i in line[0].split(self._sep)]
+                            temp2 = [i.rstrip() for i in line[1].split(self._sep)]
+                            temp1 = [x for x in temp1 if x]
+                            temp2 = [x for x in temp2 if x]
+                            tr = [int(temp1[0])]
+                            for i in range(len(temp1[1:])):
+                                item = temp1[i]
+                                probability = float(temp2[i])
                                 product = _Item(item, probability)
                                 tr.append(product)
                             self._lno += 1
                             self._Database.append(tr)
                 except IOError:
                     print("File Not Found")
 
-    def _frequentOneItem(self):
+    def _PeriodicFrequentOneItems(self):
         """
         Takes the transactions and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
         """
-        global _minSup
+        global first, last
         mapSupport = {}
         for i in self._Database:
-            for j in i:
+            n = int(i[0])
+            for j in i[1:]:
                 if j.item not in mapSupport:
-                    mapSupport[j.item] = round(j.probability, 2)
+                    mapSupport[j.item] = [round(j.probability, 3), abs(0 - n), n]
                 else:
-                    mapSupport[j.item] += round(j.probability, 2)
-        mapSupport = {k: round(v, 2) for k, v in mapSupport.items() if v >= self._minSup}
-        plist = [k for k, v in sorted(mapSupport.items(), key=lambda x: x[1], reverse=True)]
+                    mapSupport[j.item][0] += round(j.probability, 2)
+                    mapSupport[j.item][1] = max(mapSupport[j.item][1], abs(n - mapSupport[j.item][2]))
+                    mapSupport[j.item][2] = n
+        for key in mapSupport:
+            mapSupport[key][1] = max(mapSupport[key][1], self._lno - mapSupport[key][2])
+        mapSupport = {k: [round(v[0], 2), v[1]] for k, v in mapSupport.items() if
+                      v[1] <= self._maxPer and v[0] >= self._minSup}
+        plist = [k for k, v in sorted(mapSupport.items(), key=lambda x: (x[1][0], x[0]), reverse=True)]
         self._rank = dict([(index, item) for (item, index) in enumerate(plist)])
         return mapSupport, plist
 
     def _buildTree(self, data, info):
         """
         It takes the transactions and support of each item and construct the main tree with setting root node as null
 
-        :param data : it represents the one transactions in database
+        :param data : it represents the one transaction in database
         :type data : list
         :param info : it represents the support of each item
         :type info : dictionary
         """
         rootNode = _Tree()
         rootNode.info = info.copy()
         for i in range(len(data)):
-            rootNode.addTransaction(data[i])
+            set1 = [data[i][0]]
+            rootNode.addTransaction(data[i][1:], set1)
+            #printTree(rootNode)
+            #print("....")
         return rootNode
 
-    def updateTransactions(self, dict1):
+    def _updateTransactions(self, dict1):
         """
         Remove the items which are not frequent from transactions and updates the transactions with rank of items
 
         :param dict1 : frequent items with support
         :type dict1 : dictionary
         """
         list1 = []
         for tr in self._Database:
-            list2 = []
-            for i in range(0, len(tr)):
+            list2 = [int(tr[0])]
+            for i in range(1, len(tr)):
                 if tr[i].item in dict1:
                     list2.append(tr[i])
-            if (len(list2) >= 2):
-                basket = list2
+            if len(list2) >= 2:
+                basket = list2[1:]
                 basket.sort(key=lambda val: self._rank[val.item])
-                list2 = basket
+                list2[1:] = basket[0:]
                 list1.append(list2)
         return list1
 
     def _Check(self, i, x):
         """
         To check the presence of item or pattern in transaction
 
@@ -645,88 +730,100 @@
                     k += 1
             if k == 0:
                 return 0
         return 1
 
     def _convert(self, value):
         """
-        To convert the type of user specified minSup value
+        To convert the given user specified value
 
-        :param value: user specified minSup value
-        :return: converted type minSup value
+        :param value: user specified value
+        :return: converted value
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
-            value = (len(self._Database) * value)
+            value = float(value)
         if type(value) is str:
             if '.' in value:
-                value = (len(self._Database) * value)
+                value = float(value)
             else:
                 value = int(value)
         return value
 
     def _removeFalsePositives(self):
         """
-        To remove the false positive patterns generated in frequent patterns.
+        To remove false positives in generated patterns
 
-        :return: Patterns with accurate probability
+        :return: original patterns
         """
-        global _finalPatterns
         periods = {}
         for i in self._Database:
-            for x, y in _finalPatterns.items():
+            for x, y in self._periodic.items():
                 if len(x) == 1:
                     periods[x] = y
                 else:
                     s = 1
-                    check = self._Check(i, x)
+                    check = self._Check(i[1:], x)
                     if check == 1:
-                        for j in i:
+                        for j in i[1:]:
                             if j.item in x:
                                 s *= j.probability
                         if x in periods:
-                            periods[x] += s
+                            periods[x][0] += s
                         else:
-                            periods[x] = s
+                            periods[x] = [s, y[1]]
+        count = 0
         for x, y in periods.items():
-            if y >= self._minSup:
+            if y[0] >= _minSup:
+                count += 1
                 sample = str()
                 for i in x:
-                    sample = sample + i + "\t"
+                    sample = sample + i + " "
                 self._finalPatterns[sample] = y
+        #print("Total false patterns generated:", len(self._periodic) - count)
+
+    @deprecated(
+         "It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
+    def startMine(self):
+        """
+        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
+        """
+        self.mine()
 
     def mine(self):
         """
         Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
         """
-        global _minSup
-        self._startTime = _fp._time.time()
+        global _minSup, _maxPer, _first, _last, _lno
+        self._startTime = _ab._time.time()
         self._creatingItemSets()
         self._minSup = self._convert(self._minSup)
-        _minSup = self._minSup
+        self._maxPer = self._convert(self._maxPer)
         self._finalPatterns = {}
-        mapSupport, plist = self._frequentOneItem()
-        transactions1 = self.updateTransactions(mapSupport)
+        _minSup, _maxPer, _lno = self._minSup, self._maxPer, len(self._Database)
+        mapSupport, plist = self._PeriodicFrequentOneItems()
+        updatedTrans = self._updateTransactions(mapSupport)
         info = {k: v for k, v in mapSupport.items()}
-        Tree1 = self._buildTree(transactions1, info)
-        Tree1.generatePatterns([])
+        root = self._buildTree(updatedTrans, info)
+        self._periodic = {}
+        root.generatePatterns([], self._periodic)
         self._removeFalsePositives()
-        print("Uncertain Frequent patterns were generated successfully using TubeS algorithm")
-        self._endTime = _fp._time.time()
-        process = _fp._psutil.Process(_fp._os.getpid())
+        print("Periodic Frequent patterns were generated successfully using UPFP-Growth++ algorithm")
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
 
     def getMemoryUSS(self):
         """
 
-        Total amount of USS memory consumed by the mining process will be retrieved from this function
+        Total amount of USS memory consumed by the mining process will be retrieved from this function.
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
@@ -745,75 +842,80 @@
         """
 
         Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
-
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self):
         """
 
         Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataframe = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a.replace('\t', ' '), b])
-            dataframe = _fp._pd.DataFrame(data, columns=['Patterns', 'Support'])
+            data.append([a, b[0], b[1]])
+            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
         return dataframe
 
     def save(self, outFile):
         """
 
         Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
-        :type outFile: file
+        :type outFile: csv file
         """
-        self.oFile = outFile
-        writer = open(self.oFile, 'w+')
+        self._oFile = outFile
+        writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            s1 = x.strip() + ":" + str(y)
+            s1 = x + ":" + str(y)
             writer.write("%s \n" % s1)
 
     def getPatterns(self):
         """
 
         Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
-        return len(self._finalPatterns)
+        return self._finalPatterns
 
     def printResults(self):
         """
         This function is used to print the results
         """
-        print("Total number of  Uncertain Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of  Uncertain Periodic-Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:",  self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_fp._sys.argv) == 4 or len(_fp._sys.argv) == 5:
-        if len(_fp._sys.argv) == 5:
-            _ap = TubeS(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4])
-        if len(_fp._sys.argv) == 4:
-            _ap = TubeS(_fp._sys.argv[1], _fp._sys.argv[3])
+    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
+        if len(_ab._sys.argv) == 6:
+            _ap = UPFPGrowthPlus(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
+        if len(_ab._sys.argv) == 5:
+            _ap = UPFPGrowthPlus(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+        _ap.startMine()
         _ap.mine()
-        print("Total number of Uncertain Frequent Patterns:", len(_ap.getPatterns()))
-        _ap.save(_fp._sys.argv[2])
-        print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in ms:", _ap.getRuntime())
+        _Patterns = _ap.getPatterns()
+        print("Total number of Patterns:", len(_Patterns))
+        _ap.savePatterns(_ab._sys.argv[2])
+        # print(ap.getPatternsAsDataFrame())
+        _memUSS = _ap.getMemoryUSS()
+        print("Total Memory in USS:", _memUSS)
+        _memRSS = _ap.getMemoryRSS()
+        print("Total Memory in RSS", _memRSS)
+        _run = _ap.getRuntime()
+        print("Total ExecutionTime in ms:", _run)
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `pami-2024.5.28.1/PAMI/uncertainFrequentPattern/basic/UFGrowth.py` & `pami-2024.5.7.1/PAMI/uncertainFrequentPattern/basic/UFGrowth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/uncertainFrequentPattern/basic/UVECLAT.py` & `pami-2024.5.7.1/PAMI/weightedFrequentPattern/basic/WFIM.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,26 +1,28 @@
-# UVEclat is one of the fundamental algorithm to discover frequent patterns in an uncertain transactional database using PUF-Tree.
+# WFMiner is one of the fundamental algorithm to discover weighted frequent patterns in a transactional database.
+# It stores the database in compressed fp-tree decreasing the memory usage and extracts the
+# patterns from tree.It employs downward closure property to  reduce the search space effectively.
 #
 # **Importing this algorithm into a python program**
 #
-#             from PAMI.uncertainFrequentPattern.basic import UVECLAT as alg
+#             from PAMI.weightFrequentPattern.basic import basic as alg
 #
 #             iFile = 'sampleDB.txt'
 #
 #             minSup = 10  # can also be specified between 0 and 1
 #
-#             obj = alg.UVEclat(iFile, minSup)
+#             obj = alg.basic(iFile, wFile, minSup, minWeight)
 #
 #             obj.mine()
 #
 #             frequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
-#             obj.save(oFile)
+#             obj.savePatterns(oFile)
 #
 #             Df = obj.getPatternsAsDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
 #             print("Total Memory in USS:", memUSS)
 #
@@ -31,172 +33,350 @@
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
+
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
-     
+
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
-     
+
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
-import operator as _operator
-from PAMI.uncertainFrequentPattern.basic import abstract as _ab
-import deprecated
+from PAMI.weightedFrequentPattern.basic import abstract as _fp
+from typing import List, Dict, Tuple, Union, Generator
+import pandas as pd
+from deprecated import deprecated
+
 
-_minSup = float()
-_finalPatterns = {}
+_minSup = str()
+_minWeight = int()
+_miniWeight = int()
+_maxWeight = int()
+_weights = {}
+_fp._sys.setrecursionlimit(20000)
 
 
-class _Item:
+class _Node:
     """
-    A class used to represent the item with probability in transaction of dataset
+    A class used to represent the node of frequentPatternTree
 
     :Attributes:
 
-        item : int or word
-          Represents the name of the item
+        itemId: int
+            storing item of a node
+
+        counter: int
+            To maintain the support of node
+
+        parent: node
+            To maintain the parent of node
+
+        children: list
+            To maintain the children of node
 
-        probability : float
-          Represent the existential probability(likelihood presence) of an item
+    :Methods:
+
+        addChild(node)
+            Updates the nodes children list and parent for the given node
     """
 
-    def __init__(self, item, probability):
-        self.item = item
-        self.probability = probability
+    def __init__(self, item: str, children: list) -> None:
+        self.itemId = item
+        self.counter = 1
+        self.parent = None
+        self.children = children
+
+    def addChild(self, node: '_Node') -> None:
+        """
+        Retrieving the child from the tree
+
+        :param node: Children node
+        :type node: _Node
+        :return: Updates the children nodes and parent nodes
+        """
+        self.children[node.itemId] = node
+        node.parent = self
 
 
-class UVEclat(_ab._frequentPatterns):
+class _Tree:
+    """
+    A class used to represent the frequentPatternGrowth tree structure
+
+    :Attributes:
+
+        root : Node
+            The first node of the tree set to Null.
+
+        summaries : dictionary
+            Stores the nodes itemId which shares same itemId
+
+        info : dictionary
+            frequency of items in the transactions
+
+    :Methods:
+
+        addTransaction(transaction, freq)
+            adding items of  transactions into the tree as nodes and freq is the count of nodes
+        getFinalConditionalPatterns(node)
+            getting the conditional patterns from fp-tree for a node
+        getConditionalPatterns(patterns, frequencies)
+            sort the patterns by removing the items with lower minSup
+        generatePatterns(prefix)
+            generating the patterns from fp-tree
+    """
+
+    def __init__(self) -> None:
+        self.root = _Node(None, {})
+        self.summaries = {}
+        self.info = {}
+
+    def addTransaction(self, transaction: List[str], count: int) -> None:
+        """
+        Adding transaction into tree
+
+        :param transaction: it represents the one transaction in database
+        :type transaction: list
+        :param count: frequency of item
+        :type count: int
+        :return: None
+        """
+        # This method takes transaction as input and returns the tree
+        currentNode = self.root
+        for i in range(len(transaction)):
+            if transaction[i] not in currentNode.children:
+                newNode = _Node(transaction[i], {})
+                newNode.freq = count
+                currentNode.addChild(newNode)
+                if transaction[i] in self.summaries:
+                    self.summaries[transaction[i]].append(newNode)
+                else:
+                    self.summaries[transaction[i]] = [newNode]
+                currentNode = newNode
+            else:
+                currentNode = currentNode.children[transaction[i]]
+                currentNode.freq += count
+
+    def getFinalConditionalPatterns(self, alpha: str) -> Tuple[List[List[str]], List[int], Dict[str, int]]:
+        """
+        Generates the conditional patterns for a node
+
+        :param alpha: node to generate conditional patterns
+        :return: returns conditional patterns, frequency of each item in conditional patterns
+        """
+        finalPatterns = []
+        finalFreq = []
+        for i in self.summaries[alpha]:
+            set1 = i.freq
+            set2 = []
+            while i.parent.itemId is not None:
+                set2.append(i.parent.itemId)
+                i = i.parent
+            if len(set2) > 0:
+                set2.reverse()
+                finalPatterns.append(set2)
+                finalFreq.append(set1)
+        finalPatterns, finalFreq, info = self.getConditionalTransactions(finalPatterns, finalFreq)
+        return finalPatterns, finalFreq, info
+
+    @staticmethod
+    def getConditionalTransactions(ConditionalPatterns: List[List[str]], conditionalFreq: List[int]) -> Tuple[List[List[str]], List[int], Dict[str, int]]:
+        """
+        To calculate the frequency of items in conditional patterns and sorting the patterns
+
+        :param ConditionalPatterns: paths of a node
+        :param conditionalFreq: frequency of each item in the path
+        :return: conditional patterns and frequency of each item in transactions
+        """
+        global _minSup, _miniWeight
+        pat = []
+        freq = []
+        data1 = {}
+        for i in range(len(ConditionalPatterns)):
+            for j in ConditionalPatterns[i]:
+                if j in data1:
+                    data1[j] += conditionalFreq[i]
+                else:
+                    data1[j] = conditionalFreq[i]
+        up_dict = {k: v for k, v in data1.items() if v >= _minSup and v * _miniWeight > _minSup}
+        count = 0
+        for p in ConditionalPatterns:
+            p1 = [v for v in p if v in up_dict]
+            trans = sorted(p1, key=lambda x: (up_dict.get(x), -x), reverse=True)
+            if len(trans) > 0:
+                pat.append(trans)
+                freq.append(conditionalFreq[count])
+            count += 1
+        return pat, freq, up_dict
+
+    def generatePatterns(self, prefix: List[str]) -> Generator[Tuple[List[str], int], None, None]:
+        """
+        To generate the frequent patterns
+
+        :param prefix: an empty list
+        :return: Frequent patterns that are extracted from fp-tree
+        """
+        global _miniWeight, _maxWeight, _minWeight, _minSup
+        for i in sorted(self.summaries, key=lambda x: (self.info.get(x), -x)):
+            pattern = prefix[:]
+            pattern.append(i)
+            yield pattern, self.info[i]
+            patterns, freq, info = self.getFinalConditionalPatterns(i)
+            conditionalTree = _Tree()
+            conditionalTree.info = info.copy()
+            for pat in range(len(patterns)):
+                conditionalTree.addTransaction(patterns[pat], freq[pat])
+            if len(patterns) > 0:
+                for q in conditionalTree.generatePatterns(pattern):
+                    yield q
+
+
+class WFIM(_fp._weightedFrequentPatterns):
     """
     About this algorithm
     ====================
 
-    :Description: It is one of the fundamental algorithm to discover frequent patterns in an uncertain transactional database using PUF-Tree.
+    :Description: * WFMiner is one of the fundamental algorithm to discover weighted frequent patterns in a transactional database.
+                  * It stores the database in compressed fp-tree decreasing the memory usage and extracts the patterns from tree.It employs employs downward closure property to  reduce the search space effectively.
 
-    :Reference:  Carson Kai-Sang Leung, Lijing Sun: "Equivalence class transformation based mining of frequent itemsets from uncertain data",
-                 SAC '11: Proceedings of the 2011 ACM Symposium on Applied ComputingMarch, 2011, Pages 983984,
-                 https://doi.org/10.1145/1982185.1982399
+    :Reference:  U. Yun and J. J. Leggett, Wfim: weighted frequent itemset mining with a weight range and a minimum weight,
+           In:   Proceedings of the 2005 SIAM International Conference on Data Mining. SIAM, 2005, pp. 636640.
+                 https://epubs.siam.org/doi/pdf/10.1137/1.9781611972757.76
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of weighted Frequent Patterns.
+    :param  oFile: str :
+                   Name of the output file to store complete set of weighted Frequent Patterns.
+    :param  minSup: str or int or float:
+                   minimum support thresholds were tuned to find the appropriate ranges in the limited memory
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
-    :Attributes:
 
-        iFile : file
-            Name of the Input file or path of the input file
+    :Attributes :
 
-        oFile : file
-            Name of the output file or path of the output file
+        iFile : file
+            Input file name or path of the input file
 
-        minSup : float or int or str
+        minSup: float or int or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
 
+        minWeight: float or int or str
+            The user can specify minWeight either in count or proportion of database size.
+            If the program detects the data type of minWeight is integer, then it treats minWeight is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: minWeight=10 will be treated as integer, while minWeight=10.0 will be treated as float
+
         sep : str
-            This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
+            This variable is used to distinguish items from one another in a transaction. The default separator is tab space or \t.
             However, the users can override their default separator.
 
-        memoryUSS : float
-            To store the total amount of USS memory consumed by the program
-
-        memoryRSS : float
-            To store the total amount of RSS memory consumed by the program
+        oFile : file
+            Name of the output file or the path of the output file
 
         startTime:float
             To record the start time of the mining process
 
         endTime:float
             To record the completion time of the mining process
 
+        memoryUSS : float
+            To store the total amount of USS memory consumed by the program
+
+        memoryRSS : float
+            To store the total amount of RSS memory consumed by the program
+
         Database : list
             To store the transactions of a database in list
 
         mapSupport : Dictionary
             To maintain the information of item and their frequency
 
         lno : int
-            To represent the total no of transaction
+            it represents the total no of transactions
 
         tree : class
-            To represent the Tree class
-
-        itemSetCount : int
-            To represents the total no of patterns
+            it represents the Tree class
 
         finalPatterns : dict
-            To store the complete patterns
+            it represents to store the patterns
+
+    :Methods :
 
-    :Methods:
         mine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
-        storePatternsInFile(oFile)
+        save(oFile)
             Complete set of frequent patterns will be loaded in to a output file
-        getPatternsInDataFrame()
+        getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
-        creatingItemSets(fileName)
-            Scans the dataset and stores in a list format
+        creatingItemSets()
+            Scans the dataset or dataframes and stores in list format
         frequentOneItem()
-            Extracts the one-length frequent patterns from database
+            Extracts the one-frequent patterns from transactions
 
     Execution methods
     =================
 
 
     **Terminal command**
 
 
     .. code-block:: console
 
-      Format:
+       Format:
 
-      (.venv) $ python3 uveclat.py <inputFile> <outputFile> <minSup>
+       (.venv) $ python3 basic.py <inputFile> <weightFile> <outputFile> <minSup> <minWeight>
 
-      Example Usage:
+       Example Usage:
 
-      (.venv) $ python3 uveclat.py sampleDB.txt patterns.txt 3
+       (.venv) $ python3 basic.py sampleDB.txt weightSample.txt patterns.txt 10.0 3.4
 
-    .. note:: minSup can be specified  in support count or a value between 0 and 1.
+    .. note:: minSup and maxPer will be considered in support count or frequency
 
 
     **Calling from a python program**
-    ---------------------------------------------------
+
     .. code-block:: python
 
-            from PAMI.uncertainFrequentPattern.basic import UVECLAT as alg
+            from PAMI.weightFrequentPattern.basic import basic as alg
 
             iFile = 'sampleDB.txt'
 
             minSup = 10  # can also be specified between 0 and 1
 
-            obj = alg.UVEclat(iFile, minSup)
+            obj = alg.basic(iFile, wFile, minSup, minWeight)
 
             obj.mine()
 
             frequentPatterns = obj.getPatterns()
 
             print("Total number of Frequent Patterns:", len(frequentPatterns))
 
-            obj.save(oFile)
+            obj.savePatterns(oFile)
 
             Df = obj.getPatternsAsDataFrame()
 
             memUSS = obj.getmemoryUSS()
 
             print("Total Memory in USS:", memUSS)
 
@@ -207,385 +387,347 @@
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
     Credits
     =======
 
-             The complete program was written by   P.Likhitha   under the supervision of Professor Rage Uday Kiran.
+
+             The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
     """
-    _startTime = float()
-    _endTime = float()
+
+    __startTime = float()
+    __endTime = float()
     _minSup = str()
-    _finalPatterns = {}
+    __finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
-    _memoryUSS = float()
-    _memoryRSS = float()
-    _Database = []
-    _tidList = {}
-    _rank = {}
-
-    def _creatingItemSets(self):
-        """
-        Scans the dataset
-        """
-        self._Database = []
-        if isinstance(self._iFile, _ab._pd.DataFrame):
-            uncertain, data = [], []
+    __memoryUSS = float()
+    __memoryRSS = float()
+    __Database = []
+    __mapSupport = {}
+    __lno = 0
+    __tree = _Tree()
+    __rank = {}
+    __rankDup = {}
+
+    def __init__(self, iFile: str, wFile: str, minSup: str, minWeight: int, sep: str='\t') -> None:
+        super().__init__(iFile, wFile, minSup, minWeight, sep)
+
+    def __creatingItemSets(self) -> None:
+        """
+        Storing the complete transactions of the database/input file in a database variable
+
+        :return: None
+        """
+        self.__Database = []
+        if isinstance(self._iFile, _fp._pd.DataFrame):
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
-                self._Database = self._iFile['Transactions'].tolist()
-            if 'uncertain' in i:
-                uncertain = self._iFile['uncertain'].tolist()
-            for k in range(len(data)):
-                tr = []
-                for j in range(len(data[k])):
-                    product = _Item(data[k][j], uncertain[k][j])
-                    tr.append(product)
-                self._Database.append(tr)
+                self.__Database = self._iFile['Transactions'].tolist()
 
             # print(self.Database)
         if isinstance(self._iFile, str):
-            if _ab._validators.url(self._iFile):
-                data = _ab._urlopen(self._iFile)
+            if _fp._validators.url(self._iFile):
+                data = _fp._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    tr = []
-                    for i in temp:
-                        i1 = i.index('(')
-                        i2 = i.index(')')
-                        item = i[0:i1]
-                        probability = float(i[i1 + 1:i2])
-                        product = _Item(item, probability)
-                        tr.append(product)
-                    self._Database.append(temp)
+                    self.__Database.append(temp)
             else:
                 try:
-                    with open(self._iFile, 'r') as f:
+                    with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
+                            line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
-                            tr = []
-                            for i in temp:
-                                i1 = i.index('(')
-                                i2 = i.index(')')
-                                item = i[0:i1]
-                                probability = float(i[i1 + 1:i2])
-                                product = _Item(item, probability)
-                                tr.append(product)
-                            self._Database.append(tr)
+                            # print(len(temp))
+                            self.__Database.append(temp)
                 except IOError:
                     print("File Not Found")
+                    quit()
 
-    def _frequentOneItem(self):
+    def _scanningWeights(self) -> None:
         """
-        Takes the self.Database and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
-        """
-        mapSupport = {}
-        k = 0
-        for i in self._Database:
-            k += 1
-            for j in i:
-                if j.item not in mapSupport:
-                    mapSupport[str(j.item)] = j.probability
-                    self._tidList[str(j.item)] = {k: j.probability}
-                else:
-                    mapSupport[str(j.item)] += j.probability
-                    self._tidList[str(j.item)].update({k: j.probability})
-        mapSupport = {k: v for k, v in mapSupport.items() if v >= self._minSup}
-        plist = dict( sorted(mapSupport.items(), key=_operator.itemgetter(1),reverse=True))
-        return list(plist.keys())
+        Storing the weights of the variables in input file in a weights variable
 
-    @staticmethod
-    def _check(i, x):
+        :return: None
         """
-        To check the presence of item or pattern in transaction
+        global _weights
+        _weights = {}
+        if isinstance(self._wFile, _fp._pd.DataFrame):
+            items, weights = [], []
+            if self._wFile.empty:
+                print("its empty..")
+            i = self._wFile.columns.values.tolist()
+            if 'items' in i:
+                items = self._wFile['items'].tolist()
+            if 'weights' in i:
+                weights = self._wFile['weights'].tolist()
+            for i in range(len(weights)):
+                _weights[items[i]] = weights[i]
 
-        :param x: it represents the pattern
-        :type x : list
-        :param i : represents the uncertain self.Database
-        :type i : list
-        """
-
-        # This method taken a transaction as input and returns the tree
-        for m in x:
-            k = 0
-            for n in i:
-                if m == n.item:
-                    k += 1
-            if k == 0:
-                return 0
-        return 1
+            # print(self.Database)
+        if isinstance(self._wFile, str):
+            if _fp._validators.url(self._wFile):
+                data = _fp._urlopen(self._wFile)
+                for line in data:
+                    line.strip()
+                    line = line.decode("utf-8")
+                    temp = [i.rstrip() for i in line.split(self._sep)]
+                    temp = [x for x in temp if x]
+                    _weights[temp[0]] = temp[1]
+            else:
+                try:
+                    with open(self._wFile, 'r', encoding='utf-8') as f:
+                        for line in f:
+                            line.strip()
+                            temp = [i.rstrip() for i in line.split(self._sep)]
+                            temp = [x for x in temp if x]
+                            s = int(float(temp[1]))
+                            _weights[temp[0]] = s
+                except IOError:
+                    print("File Not Found")
+                    quit()
 
-    @staticmethod
-    def _convert(value):
+    def __convert(self, value: Union[int, float, str]) -> Union[int, float]:
         """
-        To convert the type of user specified minSup value
+        To convert the type of user specified minSup value.
 
         :param value: user specified minSup value
-        :return: converted type minSup value
+        :return: converted type
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
-            value = float(value)
+            value = (len(self.__Database) * value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
+                value = (len(self.__Database) * value)
             else:
                 value = int(value)
         return value
 
-    def _removeFalsePositives(self):
+    def __frequentOneItem(self) -> List[str]:
         """
-        To remove the false positive patterns generated in frequent patterns
+        Generating One frequent items sets
 
-        :return: patterns with accurate probability
+        :return: list
         """
-        global _finalPatterns
-        periods = {}
-        for i in self._Database:
-            for x, y in _finalPatterns.items():
-                if len(x) == 1:
-                    periods[x] = y
+        global _maxWeight
+        self.__mapSupport = {}
+        for tr in self.__Database:
+            for i in range(0, len(tr)):
+                if tr[i] not in self.__mapSupport:
+                    self.__mapSupport[tr[i]] = 1
                 else:
-                    s = 1
-                    check = self._check(i, x)
-                    if check == 1:
-                        for j in i:
-                            if j.item in x:
-                                s *= j.probability
-                        if x in periods:
-                            periods[x] += s
-                        else:
-                            periods[x] = s
-        for x, y in periods.items():
-            if y >= self._minSup:
-                sample = str()
-                for i in x:
-                    sample = sample + i + "\t"
-                self._finalPatterns[sample] = y
+                    self.__mapSupport[tr[i]] += 1
+        self.__mapSupport = {k: v for k, v in self.__mapSupport.items() if v >= self._minSup and v * _maxWeight > self._minSup}
+        genList = [k for k, v in sorted(self.__mapSupport.items(), key=lambda x: x[1], reverse=True)]
+        self.__rank = dict([(index, item) for (item, index) in enumerate(genList)])
+        return genList
+
+    def __updateTransactions(self, itemSet: List[str]) -> List[List[int]]:
+        """
+        Updates the items in transactions with rank of items according to their support
+
+        :Example: oneLength = {'a':7, 'b': 5, 'c':'4', 'd':3}
+                    rank = {'a':0, 'b':1, 'c':2, 'd':3}
+
+        :param itemSet: list of one-frequent items
+        :return: list
+        """
+        list1 = []
+        for tr in self.__Database:
+            list2 = []
+            for i in range(len(tr)):
+                if tr[i] in itemSet:
+                    list2.append(self.__rank[tr[i]])
+            if len(list2) >= 1:
+                list2.sort()
+                list1.append(list2)
+        return list1
 
     @staticmethod
-    def _Intersection(tidSetx, tidSetY):
+    def __buildTree(transactions: List[List[int]], info: Dict[int, int]) -> '_Tree':
+        """
+        Builds the tree with updated transactions
+
+        :param transactions: updated transactions
+        :param info: support details of each item in transactions.
+        :return: Transactions compressed in fp-tree
+        """
+        rootNode = _Tree()
+        rootNode.info = info.copy()
+        for i in range(len(transactions)):
+            rootNode.addTransaction(transactions[i], 1)
+        return rootNode
+
+    def __savePeriodic(self, itemSet: List[int]) -> str:
+        """
+        The duplication items and their ranks
+
+        :param itemSet: frequent itemSet that generated
+        :return: patterns with original item names.
+        """
+        temp = str()
+        for i in itemSet:
+            temp = temp + self.__rankDup[i] + "\t"
+        return temp
+
+    @deprecated(
+        "It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
+    def startMine(self) -> None:
+        """
+        main program to start the operation
+
+        :return: None
+        """
+        self.mine()
+
+    def mine(self) -> None:
         """
-        This function is used to find the intersection
+        main program to start the operation
 
-        :param tidSetx: the timestamp of a patterns
-        :type tidSetx: dict
-        :param tidSetY: the timestamp of a patterns
-        :type tidSetY: dict
-        """
-        tids = []
-        support = []
-        tidDict = {}
-        for x, y in tidSetx.items():
-            for x1, y1 in tidSetY.items():
-                if x == x1:
-                    tids.append(x)
-                    support.append(y * y1)
-                    tidDict.update({x: y * y1})
-        return tidDict
-
-    def _calculateExpSup(self, tidList):
-        """
-        This function is used to calculate support of tidList
-
-        :param tidList: timestamp of a list.
-        :type tidList: List
-        """
-        return sum(tidList.values())
-
-    def _save(self, prefix, suffix, tidSetI):
-        """
-        Saves the patterns that satisfy the periodic frequent property.
-
-        :param prefix: the prefix of a pattern
-        :type prefix: list
-        :param suffix: the suffix of a patterns
-        :type suffix: list
-        :param tidSetI: the timestamp of a patterns
-        :type tidSetI: dict
-        """
-
-        global _finalPatterns
-        if prefix is None:
-            prefix = suffix
-        else:
-            prefix = prefix + suffix
-        val = self._calculateExpSup(tidSetI)
-        _finalPatterns[tuple(prefix)] = val
-
-    def _Generation(self, prefix, itemSets, tidSets):
-        """
-        Equivalence class is followed  and checks for the patterns generated for periodic-frequent patterns.
-
-        :param prefix:  main equivalence prefix
-        :type prefix: periodic-frequent item or pattern
-        :param itemSets: patterns which are items combined with prefix and satisfying the periodicity and frequent with their timestamps
-        :type itemSets: list
-        :param tidSets: timestamps of the items in the argument itemSets
-        :type tidSets: list
-        """
-        if len(itemSets) == 1:
-            i = itemSets[0]
-            tidI = tidSets[0]
-            self._save(prefix, [i], tidI)
-            return
-        for i in range(len(itemSets)):
-            itemI = itemSets[i]
-            if itemI is None:
-                continue
-            tidSetI = tidSets[i]
-            classItemSets = []
-            classTidSets = []
-            itemSetX = [itemI]
-            for j in range(i + 1, len(itemSets)):
-                itemJ = itemSets[j]
-                tidSetJ = tidSets[j]
-                y = self._Intersection(tidSetI, tidSetJ)
-                if self._calculateExpSup(y) >= self._minSup:
-                    classItemSets.append(itemJ)
-                    classTidSets.append(y)
-            newPrefix = list(set(itemSetX)) + prefix
-            self._Generation(newPrefix, classItemSets, classTidSets)
-            self._save(prefix, list(set(itemSetX)), tidSetI)
-
-    def mine(self):
-        """
-        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
-        """
-        global _minSup
-        self._startTime = _ab._time.time()
-        self._creatingItemSets()
-        self._minSup = self._convert(self._minSup)
+        :return: None
+        """
+        global _minSup, _minWeight, _miniWeight, _maxWeight, _weights
+        self.__startTime = _fp._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._minSup is None:
+            raise Exception("Please enter the Minimum Support")
+        self.__creatingItemSets()
+        self._scanningWeights()
+        _weights = {k: v for k, v in _weights.items() if v >= _minWeight}
+        _maxWeight = max([s for s in _weights.values()])
+        _miniWeight = min([s for s in _weights.values()])
+        self._minSup = self.__convert(self._minSup)
         _minSup = self._minSup
-        plist = self._frequentOneItem()
-        for i in range(len(plist)):
-            itemI = plist[i]
-            tidSetI = self._tidList[itemI]
-            itemSetX = [itemI]
-            itemSets = []
-            tidSets = []
-            for j in range(i+1, len(plist)):
-                itemJ = plist[j]
-                tidSetJ = self._tidList[itemJ]
-                y1 = self._Intersection(tidSetI, tidSetJ)
-                if self._calculateExpSup(y1) >= self._minSup:
-                    itemSets.append(itemJ)
-                    tidSets.append(y1)
-            self._Generation(itemSetX, itemSets, tidSets)
-            self._save(None, itemSetX, tidSetI)
-        self._removeFalsePositives()
-        print("Frequent patterns were generated from uncertain databases successfully using PUF algorithm")
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryRSS = float()
-        self._memoryUSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
+        itemSet = self.__frequentOneItem()
+        updatedTransactions = self.__updateTransactions(itemSet)
+        for x, y in self.__rank.items():
+            self.__rankDup[y] = x
+        info = {self.__rank[k]: v for k, v in self.__mapSupport.items()}
+        __Tree = self.__buildTree(updatedTransactions, info)
+        patterns = __Tree.generatePatterns([])
+        self.__finalPatterns = {}
+        for k in patterns:
+            s = self.__savePeriodic(k[0])
+            self.__finalPatterns[str(s)] = k[1]
+        print("Weighted Frequent patterns were generated successfully using basic algorithm")
+        self.__endTime = _fp._time.time()
+        self.__memoryUSS = float()
+        self.__memoryRSS = float()
+        process = _fp._psutil.Process(_fp._os.getpid())
+        self.__memoryUSS = process.memory_full_info().uss
+        self.__memoryRSS = process.memory_info().rss
+
 
-    def getMemoryUSS(self):
+    def getMemoryUSS(self) -> float:
         """
 
         Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
-        return self._memoryUSS
+        return self.__memoryUSS
 
-    def getMemoryRSS(self):
+    def getMemoryRSS(self) -> float:
         """
 
-        Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        Total amount of RSS memory consumed by the mining process will be retrieved from this function.
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
-        return self._memoryRSS
+        return self.__memoryRSS
 
-    def getRuntime(self):
+    def getRuntime(self) -> float:
         """
 
-        Calculating the total amount of runtime taken by the mining process
+        Calculating the total amount of runtime taken by the mining process.
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
-        return self._endTime - self._startTime
+        return self.__endTime - self.__startTime
 
-    def getPatternsAsDataFrame(self):
+    def getPatternsAsDataFrame(self) -> pd.DataFrame:
         """
 
-        Storing final frequent patterns in a dataframe
+        Storing final frequent patterns in a dataframe.
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataframe = {}
         data = []
-        for a, b in self._finalPatterns.items():
+        for a, b in self.__finalPatterns.items():
             data.append([a.replace('\t', ' '), b])
-            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
+            dataframe = _fp._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataframe
 
-    def save(self, oFile):
+    def save(self, outFile: str) -> None:
         """
 
-        Complete set of frequent patterns will be loaded in to an output file
+        Complete set of frequent patterns will be loaded in to an output file.
 
-        :param oFile: name of the output file
-        :type oFile: csv file
+        :param outFile: name of the output file
+        :type outFile: csv file
+        :return: None
         """
-        self.oFile = oFile
-        writer = open(self.oFile, 'w+')
-        for x, y in self._finalPatterns.items():
+        self._oFile = outFile
+        writer = open(self._oFile, 'w+')
+        for x, y in self.__finalPatterns.items():
             s1 = x.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
-    def getPatterns(self):
+    def getPatterns(self) -> Dict[str, int]:
         """
 
-        Function to send the set of frequent patterns after completion of the mining process
+        Function to send the set of frequent patterns after completion of the mining process.
 
         :return: returning frequent patterns
         :rtype: dict
         """
-        return self._finalPatterns
+        return self.__finalPatterns
 
-    def printResults(self):
+    def printResults(self) -> None:
         """
         This function is used to print the results
         """
-        print("Total number of  Uncertain Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of  Weighted Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:",  self.getRuntime())
 
+        
+
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
-        if len(_ab._sys.argv) == 5:
-            _ap = UVEclat(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
-        if len(_ab._sys.argv) == 4:
-            _ap = UVEclat(_ab._sys.argv[1], _ab._sys.argv[3])
+    if len(_fp._sys.argv) == 6 or len(_fp._sys.argv) == 7:
+        if len(_fp._sys.argv) == 7:
+            _ap = WFIM(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4], _fp._sys.argv[5], _fp._sys.argv[6])
+        if len(_fp._sys.argv) == 6:
+            _ap = WFIM(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4], _fp._sys.argv[5])
+        _ap.startMine()
         _ap.mine()
-        print("Total number of Patterns:", len(_ap.getPatterns()))
-        _ap.save(_ab._sys.argv[2])
-        print("Total Memory in USS:", _ap.getMemoryUSS())
+        print("Total number of Weighted Frequent Patterns:", len(_ap.getPatterns()))
+        _ap.save(_fp._sys.argv[2])
+        print("Total Memory in USS:",  _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.5.28.1/PAMI/uncertainFrequentPattern/basic/abstract.py` & `pami-2024.5.7.1/PAMI/uncertainFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/uncertainGeoreferencedFrequentPattern/basic/GFPGrowth.py` & `pami-2024.5.7.1/PAMI/uncertainPeriodicFrequentPattern/basic/UPFPGrowth.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,24 +1,28 @@
-# GFPGrowth algorithm is used to discover geo-referenced frequent patterns in a uncertain transactional database using GFP-Tree.
+# UPFPGrowth is used to discover periodic-frequent patterns in an uncertain temporal database.
 #
 # **Importing this algorithm into a python program**
+# --------------------------------------------------------
 #
-#             from PAMI.uncertainGeoreferencedFrequentPattern.basic import GFPGrowth as alg
+#
+#             from PAMI.uncertainPeriodicFrequentPattern.basic import UPFPGrowth as alg
 #
 #             iFile = 'sampleDB.txt'
 #
 #             minSup = 10  # can also be specified between 0 and 1
 #
-#             obj = alg.GFPGrowth(iFile, nFile, minSup,sep, oFile)
+#             maxPer = 2   # can also be specified between 0 and 1
+#
+#             obj = alg.UPFPGrowth(iFile, minSup, maxPer)
 #
 #             obj.mine()
 #
-#             Patterns = obj.getPatterns()
+#             periodicFrequentPatterns = obj.getPatterns()
 #
-#             print("Total number of  Patterns:", len(Patterns))
+#             print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 #
 #             obj.save(oFile)
 #
 #             Df = obj.getPatternsAsDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
@@ -30,15 +34,14 @@
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
-
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
@@ -48,425 +51,481 @@
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
-from PAMI.uncertainGeoreferencedFrequentPattern.basic import abstract as _ab
+
 import pandas as pd
 from deprecated import deprecated
+from PAMI.uncertainPeriodicFrequentPattern.basic import abstract as _ab
+from typing import List, Dict, Tuple, Union
 
-_minSup = str()
-_neighbourList = {}
-_ab._sys.setrecursionlimit(20000)
-_finalPatterns = {}
-
+_minSup = float()
+__maxPer = float()
+__first = int()
+_last = int()
+__lno = int()
+#rank = {}
+#periodic = {}
 
 class _Item:
     """
     A class used to represent the item with probability in transaction of dataset
 
     :Attributes:
 
-        item : int or word
+        item: int or word
             Represents the name of the item
 
-        probability : float
+        probability: float
             Represent the existential probability(likelihood presence) of an item
     """
 
-    def __init__(self, item, probability):
+    def __init__(self, item: str, probability: float) -> None:
         self.item = item
         self.probability = probability
 
 
 class _Node(object):
     """
     A class used to represent the node of frequentPatternTree
 
     :Attributes:
 
-        item : int
+        item: int
             storing item of a node
 
-        probability : int
+        probability: int
             To maintain the expected support of node
 
-        parent : node
+        parent: node
             To maintain the parent of every node
 
-        children : list
+        children: list
             To maintain the children of node
 
+        timeStamps: list
+            To maintain the timeStamps of node
+
     :Methods:
 
         addChild(itemName)
             storing the children to their respective parent nodes
     """
 
-    def __init__(self, item, children):
+    def __init__(self, item: str, children: Dict) -> None:
         self.item = item
         self.probability = 1
         self.children = children
         self.parent = None
+        self.timeStamps = []
 
-    def addChild(self, node):
+    def addChild(self, node: '_Node') -> None:
         """
-        This method adds a child node to the current node in the frequent pattern tree. It updates the children
-        dictionary of the current node with the new child node and sets the parent of the child node to the current node.
+        To add the children details to parent node
 
-        :param node: The child node to be added.
-        :type node: _Node
-        :return: None
+        :param node: children node
+        :return: updated parent node children
         """
         self.children[node.item] = node
         node.parent = self
 
 
+def _printTree(root) -> None:
+    """
+    To print the details of tree
+
+    :param root: root node of the tree
+    :return: details of tree
+    """
+    for x, y in root.children.items():
+        print(x, y.item, y.probability, y.parent.item, y.timeStamps)
+        _printTree(y)
+
+
 class _Tree(object):
     """
     A class used to represent the frequentPatternGrowth tree structure
 
     :Attributes:
-
         root : Node
             Represents the root node of the tree
 
         summaries : dictionary
             storing the nodes with same item name
 
         info : dictionary
             stores the support of items
 
     :Methods:
-
-        addTransaction(transaction)
+        addTransactions(transaction)
             creating transaction as a branch in frequentPatternTree
-        addConditionalPattern(prefixPaths, supportOfItems)
+        addConditionalTransaction(prefixPaths, supportOfItems)
             construct the conditional tree for prefix paths
         conditionalPatterns(Node)
             generates the conditional patterns from tree for specific node
         conditionalTransactions(prefixPaths,Support)
             takes the prefixPath of a node and support at child of the path and extract the frequent items from
             prefixPaths and generates prefixPaths with items which are frequent
         remove(Node)
-            removes the node from tree once after generating all the patterns respective to the node
-        generatePatterns(Node)
-            starts from the root node of the tree and mines the frequent patterns
+            removes the node from tree once after generating all the patterns respective to the node generatePatterns(Node) starts from the root node of the tree and mines the frequent patterns
     """
 
-    def __init__(self):
+    def __init__(self) -> None:
         self.root = _Node(None, {})
         self.summaries = {}
         self.info = {}
 
-    def addTransaction(self, transaction):
+    def addTransactions(self, transaction: List['_Item'], tid: int) -> None:
         """
         Adding transaction into tree
 
-        :param transaction : it represents the one self.Database in database
-        :type transaction : list
+        :param transaction: it represents the one transaction in database
+        :type transaction: list
+        :param tid: the timestamp of transaction
+        :type tid: list
+        :return: None
         """
-        global _neighbourList
         currentNode = self.root
         for i in range(len(transaction)):
             if transaction[i].item not in currentNode.children:
                 newNode = _Node(transaction[i].item, {})
-                nei = _neighbourList.get(transaction[i].item)
                 l1 = i - 1
-                lp = []
+                temp = []
                 while l1 >= 0:
-                    if nei == None:
-                        break
-                    if transaction[l1].item in nei:
-                        lp.append(transaction[l1].probability)
+                    temp.append(transaction[l1].probability)
                     l1 -= 1
-                if len(lp) == 0:
+                if len(temp) == 0:
                     newNode.probability = transaction[i].probability
                 else:
-                    newNode.probability = max(lp) * transaction[i].probability
+                    newNode.probability = max(temp) * transaction[i].probability
                 currentNode.addChild(newNode)
                 if transaction[i].item in self.summaries:
                     self.summaries[transaction[i].item].append(newNode)
                 else:
                     self.summaries[transaction[i].item] = [newNode]
                 currentNode = newNode
             else:
                 currentNode = currentNode.children[transaction[i].item]
                 l1 = i - 1
-                lp = []
+                temp = []
                 while l1 >= 0:
-                    lp.append(transaction[l1].probability)
+                    temp.append(transaction[l1].probability)
                     l1 -= 1
-                if len(lp) == 0:
+                if len(temp) == 0:
                     currentNode.probability += transaction[i].probability
                 else:
-                    currentNode.probability += max(lp) * transaction[i].probability
+                    currentNode.probability += max(temp) * transaction[i].probability
+        currentNode.timeStamps = currentNode.timeStamps + tid
 
-    def addConditionalPattern(self, transaction, sup):
+    def addConditionalTransaction(self, transaction: List[str], ts: List[int], sup: float) -> None:
         """
-        constructing conditional tree from prefixPaths
+        Constructing conditional tree from prefixPaths
 
-        :param transaction : it represents the one self.Database in database
+        :param transaction : it represents the one transaction in database
         :type transaction : list
+        :param ts: timeStamp of a transaction
+        :type ts: list
         :param sup : support of prefixPath taken at last child of the path
         :type sup : int
+        :return: None
         """
-
-        # This method takes transaction, support and constructs the conditional tree
         currentNode = self.root
         for i in range(len(transaction)):
             if transaction[i] not in currentNode.children:
                 newNode = _Node(transaction[i], {})
                 newNode.probability = sup
                 currentNode.addChild(newNode)
                 if transaction[i] in self.summaries:
                     self.summaries[transaction[i]].append(newNode)
                 else:
                     self.summaries[transaction[i]] = [newNode]
                 currentNode = newNode
             else:
                 currentNode = currentNode.children[transaction[i]]
                 currentNode.probability += sup
+        currentNode.timeStamps = currentNode.timeStamps + ts
 
-    def conditionalPatterns(self, alpha):
+    def getConditionalPatterns(self, alpha: str) -> Tuple[List[List[str]], List[List[int]], List[float], Dict[str, List[float]]]:
         """
-        Generates all the conditional patterns of respective node
+        Generates all the conditional patterns of respective node.
 
         :param alpha : it represents the Node in tree
-        :type alpha : _Node
+        :type alpha : Node
+        :return: tuple
         """
 
-        # This method generates conditional patterns of node by traversing the tree
-        global _neighbourList
         finalPatterns = []
+        finalTimeStamps = []
         sup = []
         for i in self.summaries[alpha]:
-            j = i.item
+            set1 = i.timeStamps
             s = i.probability
             set2 = []
             while i.parent.item is not None:
-                if _neighbourList.get(j) is not None:
-                    #print(_neighbourList.get(j))
-                    if i.parent.item in _neighbourList[j]:
-                        set2.append(i.parent.item)
+                set2.append(i.parent.item)
                 i = i.parent
             if len(set2) > 0:
                 set2.reverse()
                 finalPatterns.append(set2)
+                finalTimeStamps.append(set1)
                 sup.append(s)
-        finalPatterns, support, info = self.conditionalTransactions(finalPatterns, sup)
-        return finalPatterns, support, info
+        finalPatterns, finalTimeStamps, support, info = self.conditionalTransactions(finalPatterns, finalTimeStamps,
+                                                                                     sup)
+        return finalPatterns, finalTimeStamps, support, info
 
-    def removeNode(self, nodeValue):
+    def removeNode(self, nodeValue: str) -> None:
         """
         Removing the node from tree
 
         :param nodeValue : it represents the node in tree
         :type nodeValue : node
+        :return: None
         """
-
         for i in self.summaries[nodeValue]:
+            i.parent.timeStamps = i.parent.timeStamps + i.timeStamps
             del i.parent.children[nodeValue]
 
-    def conditionalTransactions(self, condPatterns, support):
+    def getPeriodAndSupport(self, s: float, timeStamps: List[int]) -> List[float]:
+        """
+        Calculates the period and support of an item based on the given support value and list of timestamps.
+
+        :param s: The support value.
+        :type s: float
+        :param timeStamps: A list of timestamps.
+        :type timeStamps: List[int]
+        :return: A list containing the support and period of the item.
+        :rtype: List[float]
+        """
+        global _lno, _maxPer
+        timeStamps.sort()
+        cur = 0
+        per = 0
+        sup = s
+        for j in range(len(timeStamps)):
+            per = max(per, timeStamps[j] - cur)
+            if per > _maxPer:
+                return [0, 0]
+            cur = timeStamps[j]
+        per = max(per, _lno - cur)
+        return [sup, per]
+
+    def conditionalTransactions(self, condPatterns: List[List[str]], condTimeStamps: List[List[int]], support: List[float]) -> Tuple[List[List[str]], List[List[int]], List[float], Dict[str, List[float]]]:
         """
         It generates the conditional patterns with frequent items
 
-        :param condPatterns : conditionalPatterns generated from conditionalPattern method for respective node
+        :param condPatterns : conditional patterns generated from getConditionalPatterns method for respective node
         :type condPatterns : list
-        :support : the support of conditional pattern in tree
-        :support : int
+        :param condTimeStamps: timeStamps of conditional transactions
+        :type condTimeStamps: list
+        :param support : the support of conditional pattern in tree
+        :type support : list
         """
-
-        global minSup
+        global _minSup, _maxPer
         pat = []
+        timeStamps = []
         sup = []
+        data1 = {}
         count = {}
         for i in range(len(condPatterns)):
             for j in condPatterns[i]:
-                if j in count:
+                if j in data1:
+                    data1[j] = data1[j] + condTimeStamps[i]
                     count[j] += support[i]
                 else:
+                    data1[j] = condTimeStamps[i]
                     count[j] = support[i]
         updatedDict = {}
-        updatedDict = {k: v for k, v in count.items() if v >= minSup}
+        for m in data1:
+            updatedDict[m] = self.getPeriodAndSupport(count[m], data1[m])
+        updatedDict = {k: v for k, v in updatedDict.items() if v[0] >= _minSup and v[1] <= _maxPer}
         count = 0
         for p in condPatterns:
             p1 = [v for v in p if v in updatedDict]
-            trans = sorted(p1, key=lambda x: updatedDict[x], reverse=True)
+            trans = sorted(p1, key=lambda x: (updatedDict.get(x)[0]), reverse=True)
             if len(trans) > 0:
                 pat.append(trans)
+                timeStamps.append(condTimeStamps[count])
                 sup.append(support[count])
-                count += 1
-        return pat, sup, updatedDict
+            count += 1
+        return pat, timeStamps, sup, updatedDict
 
-    def generatePatterns(self, prefix):
+    def generatePatterns(self, prefix: List[str], periodic: Dict) -> None:
         """
         Generates the patterns
 
         :param prefix : forms the combination of items
         :type prefix : list
+        :return: None
         """
 
-        global _finalPatterns, minSup
-        for i in sorted(self.summaries, key=lambda x: (self.info.get(x))):
+        global _minSup
+        for i in sorted(self.summaries, key=lambda x: (self.info.get(x)[0])):
             pattern = prefix[:]
             pattern.append(i)
             s = 0
             for x in self.summaries[i]:
                 s += x.probability
-            _finalPatterns[tuple(pattern)] = self.info[i]
-            if s >= minSup:
-                patterns, support, info = self.conditionalPatterns(i)
+            periodic[tuple(pattern)] = self.info[i]
+            if s >= _minSup:
+                patterns, timeStamps, support, info = self.getConditionalPatterns(i)
                 conditionalTree = _Tree()
                 conditionalTree.info = info.copy()
                 for pat in range(len(patterns)):
-                    conditionalTree.addConditionalPattern(patterns[pat], support[pat])
+                    conditionalTree.addConditionalTransaction(patterns[pat], timeStamps[pat], support[pat])
                 if len(patterns) > 0:
-                    conditionalTree.generatePatterns(pattern)
+                    conditionalTree.generatePatterns(pattern, periodic)
             self.removeNode(i)
 
 
-class GFPGrowth(_ab._frequentPatterns):
+class UPFPGrowth(_ab._periodicFrequentPatterns):
     """
     About this algorithm
     ====================
 
-    :Description: GFPGrowth algorithm is used to discover geo-referenced frequent patterns in a uncertain transactional database using GFP-Tree.
-
-    :Reference:  Palla Likhitha,Pamalla Veena, Rage, Uday Kiran, Koji Zettsu (2023).
-                 "Discovering Geo-referenced Frequent Patterns in Uncertain Geo-referenced
-                 Transactional Databases".  PAKDD 2023.
-                 https://doi.org/10.1007/978-3-031-33380-4_3
+    :Description: Basic is  to discover periodic-frequent patterns in a uncertain temporal database.
 
+    :Reference:  Uday Kiran, R., Likhitha, P., Dao, MS., Zettsu, K., Zhang, J. (2021).Discovering Periodic-Frequent Patterns in Uncertain Temporal Databases.
+            In:  Mantoro, T., Lee, M., Ayu, M.A., Wong, K.W., Hidayanto, A.N. (eds) Neural Information Processing.
+                 ICONIP 2021. Communications in Computer and Information Science, vol 1516. Springer, Cham.
+                 https://doi.org/10.1007/978-3-030-92307-5_83
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of uncertain Geo referenced Frequent Patterns
+                   Name of the Input file to mine complete set of Uncertain Periodic Frequent Patterns
     :param  oFile: str :
-                   Name of the output file to store complete set of Uncertain Geo referenced frequent patterns
-    :param  minSup: str:
+                   Name of the output file to store complete set of Uncertain Periodic Frequent patterns
+    :param  minSup: float:
                    minimum support thresholds were tuned to find the appropriate ranges in the limited memory
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+    :param  maxper: float :
+                   where maxPer represents the maximum periodicity threshold value specified by the user.
+
 
     :Attributes:
 
         iFile : file
             Name of the Input file or path of the input file
 
         oFile : file
-            Name of the output file or path of the output file
+            Name of the output file or path of output file
 
-        minSup: float or int or str
+        minSup: int or float or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
 
-        sep : str
+        maxPer: int or float or str
+            The user can specify maxPer either in count or proportion of database size.
+            If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: maxPer=10 will be treated as integer, while maxPer=10.0 will be treated as float
+
+        sep: str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator.
 
-        memoryUSS : float
+        memoryUSS: float
             To store the total amount of USS memory consumed by the program
 
-        memoryRSS : float
+        memoryRSS: float
             To store the total amount of RSS memory consumed by the program
 
-        startTime:float
+        startTime: float
             To record the start time of the mining process
 
-        endTime:float
+        endTime: float
             To record the completion time of the mining process
 
         Database : list
             To store the transactions of a database in list
 
         mapSupport : Dictionary
             To maintain the information of item and their frequency
 
-        lno : int
+        _lno : int
             To represent the total no of transaction
 
         tree : class
             To represents the Tree class
 
-        itemSetCount : int
-            To represents the total no of patterns
-
         finalPatterns : dict
             To store the complete patterns
 
     :Methods:
 
-        startMine()
+        mine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
-        savePatterns(oFile)
-            Complete set of frequent patterns will be loaded in to a output file
+        save(oFile)
+            Complete set of periodic-frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
-            Complete set of frequent patterns will be loaded in to a dataframe
+            Complete set of periodic-frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
-        creatingItemSets(fileName)
+        creatingItemSets()
             Scans the dataset and stores in a list format
-        frequentOneItem()
-            Extracts the one-length frequent patterns from database
-        updateTransactions()
-            Update the transactions by removing non-frequent items and sort the Database by item decreased support
+        PeriodicFrequentOneItem()
+            Extracts the one-periodic-frequent patterns from database
+        updateTransaction()
+            Update the database by removing aperiodic items and sort the Database by item decreased support
         buildTree()
             After updating the Database, remaining items will be added into the tree by setting root node as null
         convert()
-            to convert the user specified value
-        startMine()
-            Mining process will start from this function
+            To convert the user specified value
+        removeFalsePositives()
+            To remove the false positives in generated patterns
 
     Execution methods
     =================
 
 
     **Terminal command**
 
 
     .. code-block:: console
 
        Format:
 
-       (.venv) $ python3 GFPGrowth.py <inputFile> <neighborFile> <outputFile> <minSup>
+       (.venv) $ python3 basic.py <inputFile> <outputFile> <minSup> <maxPer>
+
+       Example Usage:
 
-       Examples usage:
+       (.venv) $ python3 basic.py sampleTDB.txt patterns.txt 0.3 4
 
-       (.venv) $ python3 GFPGrowth.py sampleTDB.txt sampleNeighbor.txt patterns.txt 3
+    .. note:: minSup and maxPer will be considered in support count or frequency
 
-    .. note:: minSup  will be considered in support count or frequency
-    
-    **Calling from a python program**:
 
-     .. code-block:: python
+    **Calling from a python program**
 
-            from PAMI.uncertainGeoreferencedFrequentPattern.basic import GFPGrowth as alg
+    .. code-block:: python
+
+            from PAMI.uncertainPeriodicFrequentPattern.basic import UPFPGrowth as alg
 
             iFile = 'sampleDB.txt'
 
             minSup = 10  # can also be specified between 0 and 1
 
-            obj = alg.GFPGrowth(iFile, nFile, minSup)
+            maxPer = 2   # can also be specified between 0 and 1
+
+            obj = alg.UPFPGrowth(iFile, minSup, maxPer)
 
             obj.mine()
 
-            Patterns = obj.getPatterns()
+            periodicFrequentPatterns = obj.getPatterns()
 
-            print("Total number of  Patterns:", len(Patterns))
+            print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 
             obj.save(oFile)
 
             Df = obj.getPatternsAsDataFrame()
 
             memUSS = obj.getMemoryUSS()
 
@@ -475,405 +534,392 @@
             memRSS = obj.getMemoryRSS()
 
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
-        
+
+
     Credits
     =======
 
 
-            The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.\n
-
+            The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
     """
-
-
+    _rank = {}
     _startTime = float()
     _endTime = float()
-    _minSup = str()
+    _minSup = float()
+    _maxPer = float()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
-    _rank = {}
+    _lno = 0
+    _periodic = {}
 
-    def __init__(self, iFile, nFile, minSup, sep='\t'):
-        super().__init__(iFile, nFile, minSup, sep)
-
-    def _creatingItemSets(self):
+    def _creatingItemSets(self) -> None:
         """
-        Scans the uncertain transactional dataset
+        Storing the complete transactions of the database/input file in a database variable
+
+        :return: None
         """
         self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
-            uncertain, data = [], []
+            uncertain, data, ts = [], [], []
             if self._iFile.empty:
                 print("its empty..")
-            i = self._iFile.columns.values.tolist()
+            i = self._iFile._columns.values.tolist()
+            if 'TS' in i:
+                ts = self._iFile['TS'].tolist()
             if 'Transactions' in i:
-                self._Database = self._iFile['Transactions'].tolist()
+                data = self._iFile['Transactions'].tolist()
             if 'uncertain' in i:
                 uncertain = self._iFile['uncertain'].tolist()
             for k in range(len(data)):
-                tr = []
-                for j in range(len(data[k])):
+                tr = [ts[k]]
+                for j in range(len(k)):
                     product = _Item(data[k][j], uncertain[k][j])
                     tr.append(product)
                 self._Database.append(tr)
+                self._lno += 1
 
             # print(self.Database)
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
-                    temp1 = line.strip()
-                    temp1 = temp1.split(':')
-                    temp = [i.rstrip() for i in temp1[0].split(self._sep)]
-                    uncertain = [float(i.rstrip()) for i in temp1[1].split(self._sep)]
-                    tr = []
-                    for i in range(len(temp)):
-                        item = temp[i]
-                        probability = uncertain[i]
+                    line = line.decode("utf-8")
+                    line = line.strip()
+                    line = [i for i in line.split(':')]
+                    temp1 = [i.rstrip() for i in line[0].split(self._sep)]
+                    temp2 = [i.rstrip() for i in line[1].split(self._sep)]
+                    temp1 = [x for x in temp1 if x]
+                    temp2 = [x for x in temp2 if x]
+                    tr = [int(temp1[0])]
+                    for i in range(len(temp1[1:])):
+                        item = temp1[i]
+                        probability = float(temp2[i])
                         product = _Item(item, probability)
                         tr.append(product)
+                    self._lno += 1
                     self._Database.append(tr)
             else:
                 try:
+                    count = 0
                     with open(self._iFile, 'r') as f:
                         for line in f:
-                            temp1 = line.strip()
-                            temp1 = temp1.split(':')
-                            #temp1[0], temp1[1] = [i for i in temp1[0] if i], [i for i in temp1[1] if i]
-                            temp = [i.rstrip() for i in temp1[0].split(self._sep) if i]
-                            uncertain = [float(i.rstrip()) for i in temp1[1].split(self._sep) if i]
-                            tr = []
-                            for i in range(len(temp)):
-                                item = temp[i]
-                                probability = uncertain[i]
+                            #count += 1
+                            line = line.strip()
+                            line = [i for i in line.split(':')]
+                            temp1 = [i.rstrip() for i in line[0].split(self._sep)]
+                            temp2 = [i.rstrip() for i in line[1].split(self._sep)]
+                            temp1 = [x for x in temp1 if x]
+                            temp2 = [x for x in temp2 if x]
+                            tr = [int(temp1[0])]
+                            for i in range(len(temp1[1:])):
+                                item = temp1[i]
+                                probability = float(temp2[i])
                                 product = _Item(item, probability)
                                 tr.append(product)
+                            self._lno += 1
                             self._Database.append(tr)
                 except IOError:
                     print("File Not Found")
-                    
-    def _creatingNeighbours(self):
-        """
-        Scans the uncertain transactional dataset
-        """
-        global _neighbourList
-        _neighbourList = {}
-        if isinstance(self._nFile, _ab._pd.DataFrame):
-            uncertain, data = [], []
-            if self._iFile.empty:
-                print("its empty..")
-            i = self._iFile.columns.values.tolist()
-            if 'Transactions' in i:
-                self._Database = self._iFile['Transactions'].tolist()
-            if 'uncertain' in i:
-                uncertain = self._iFile['uncertain'].tolist()
-            for k in range(len(data)):
-                tr = []
-                for j in range(len(data[k])):
-                    product = _Item(data[k][j], uncertain[k][j])
-                    tr.append(product)
-                self._Database.append(tr)
-
-            # print(self.Database)
-        if isinstance(self._nFile, str):
-            if _ab._validators.url(self._iFile):
-                data = _ab._urlopen(self._iFile)
-                for line in data:
-                    line.strip()
-                    line = line.decode("utf-8")
-                    temp = [i.rstrip() for i in line.split(self._sep)]
-                    temp = [x for x in temp if x]
-                    tr = []
-                    for i in temp:
-                        i1 = i.index('(')
-                        i2 = i.index(')')
-                        item = i[0:i1]
-                        probability = float(i[i1 + 1:i2])
-                        product = _Item(item, probability)
-                        tr.append(product)
-                    self._Database.append(temp)
-            else:
-                try:
-                    with open(self._nFile, 'r') as f:
-                        for line in f:
-                            temp = [i.rstrip() for i in line.split(self._sep)]
-                            temp = [x for x in temp if x]
-                            _neighbourList[temp[0]] = temp[1:]
-                except IOError:
-                    print("File Not Found")
 
-    def _frequentOneItem(self):
+    def _periodicFrequentOneItem(self) -> Tuple[Dict, List]:
         """
-        Takes the self.Database and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
+        Takes the transactions and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
 
-        :param self.Database : it represents the one self.Database in database
-        :type self.Database : list
+        :return: Tuple
         """
-
         mapSupport = {}
         for i in self._Database:
-            for j in i:
+            n = i[0]
+            for j in i[1:]:
                 if j.item not in mapSupport:
-                    mapSupport[j.item] = j.probability
+                    mapSupport[j.item] = [round(j.probability, 3), abs(0 - n), n]
                 else:
-                    mapSupport[j.item] += j.probability
-        mapSupport = {k: v for k, v in mapSupport.items() if v >= self._minSup}
-        plist = [k for k, v in sorted(mapSupport.items(), key=lambda x: x[1], reverse=True)]
-        self.rank = dict([(index, item) for (item, index) in enumerate(plist)])
+                    mapSupport[j.item][0] += round(j.probability, 3)
+                    mapSupport[j.item][1] = max(mapSupport[j.item][1], abs(n - mapSupport[j.item][2]))
+                    mapSupport[j.item][2] = n
+        for key in mapSupport:
+            mapSupport[key][1] = max(mapSupport[key][1], self._lno - mapSupport[key][2])
+        mapSupport = {k: [v[0], v[1]] for k, v in mapSupport.items() if v[1] <= self._maxPer and v[0] >= self._minSup}
+        plist = [k for k, v in sorted(mapSupport.items(), key=lambda x: (x[1][0], x[0]), reverse=True)]
+        self._rank = dict([(index, item) for (item, index) in enumerate(plist)])
         return mapSupport, plist
 
-    @staticmethod
-    def _buildTree(data, info):
+    def _check(self, i: List, x: List) -> int:
         """
-        It takes the self.Database and support of each item and construct the main tree with setting root node as null
+        To check the presence of item or pattern in transaction
 
-        :param data : it represents the one self.Database in database
-        :type data : list
-        :param info : it represents the support of each item
-        :type info : dictionary
+        :param x: it represents the pattern
+        :type x : list
+        :param i : represents the uncertain transactions
+        :type i : list
+        :return: value
+        :rtype: int
+        """
+
+        for m in x:
+            k = 0
+            for n in i:
+                if m == n.item:
+                    k += 1
+            if k == 0:
+                return 0
+        return 1
+
+    def _getPeriodAndSupport(self, s: float, timeStamps: List[int]) -> List[float]:
         """
+        To calculate periodicity of timeStamps
 
+        :param s: support of a pattern
+        :param timeStamps: timeStamps of a pattern
+        :return: periodicity and Support
+        """
+        global __lno, _maxPer
+        timeStamps.sort()
+        cur = 0
+        per = 0
+        sup = s
+        for j in range(len(timeStamps)):
+            per = max(per, timeStamps[j] - cur)
+            if per > _maxPer:
+                return [0, 0]
+            cur = timeStamps[j]
+        per = max(per, _lno - cur)
+        return [sup, per]
+
+    def _buildTree(self, data: List[List], info: Dict) -> '_Tree':
+        """
+        It takes the transactions and support of each item and construct the main tree with setting root node as null
+
+        :param data: it represents the one transaction in database
+        :type data: list
+        :param info: it represents the support of each item
+        :type info : dictionary
+        """
         rootNode = _Tree()
         rootNode.info = info.copy()
         for i in range(len(data)):
-            rootNode.addTransaction(data[i])
+            set1 = [data[i][0]]
+            rootNode.addTransactions(data[i][1:], set1)
         return rootNode
 
-    def _updateTransactions(self, dict1):
+    def _updateTransactions(self, dict1: Dict) -> List[List]:
         """
-        Remove the items which are not frequent from self.Database and updates the self.Database with rank of items
+        Remove the items which are not frequent from transactions and updates the transactions with rank of items
 
         :param dict1 : frequent items with support
         :type dict1 : dictionary
+        :return: list
         """
-
         list1 = []
         for tr in self._Database:
-            list2 = []
-            for i in range(0, len(tr)):
+            list2 = [int(tr[0])]
+            for i in range(1, len(tr)):
                 if tr[i].item in dict1:
                     list2.append(tr[i])
             if len(list2) >= 2:
-                basket = list2
-                basket.sort(key=lambda val: self.rank[val.item])
-                list2 = basket
+                basket = list2[1:]
+                basket.sort(key=lambda val: self._rank[val.item])
+                list2[1:] = basket[0:]
                 list1.append(list2)
         return list1
 
-    @staticmethod
-    def _check(i, x):
-        """
-        To check the presence of item or pattern in transaction
-
-        :param x: it represents the pattern
-        :type x : list
-        :param i : represents the uncertain self.Database
-        :type i : list
-        """
-
-        # This method taken a transaction as input and returns the tree
-        for m in x:
-            k = 0
-            for n in i:
-                if m == n.item:
-                    k += 1
-            if k == 0:
-                return 0
-        return 1
-
-    def _convert(self, value):
+    def _convert(self, value: Union[int, float, str]) -> Union[int, float]:
         """
-        To convert the type of user specified minSup value
+        To convert the given user specified value
 
-        :param value: user specified minSup value
-        :return: converted type minSup value
+        :param value: user specified value
+        :return: converted value
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
-            value = (len(self._Database) * value)
+            value = float(value)
         if type(value) is str:
             if '.' in value:
-                value = (len(self._Database) * value)
+                value = float(value)
             else:
                 value = int(value)
+
         return value
 
-    def _removeFalsePositives(self):
+    def _removeFalsePositives(self) -> None:
         """
-        To remove the false positive patterns generated in frequent patterns.
+        Removes the false positive patterns from the generated patterns.
+
+        This method iterates through the database to identify false positive patterns and removes them from the
+        generated patterns.
 
-        :return: patterns with accurate probability
+        :return: None
         """
-        global _finalPatterns
         periods = {}
         for i in self._Database:
-            for x, y in _finalPatterns.items():
+            for x, y in self._periodic.items():
                 if len(x) == 1:
                     periods[x] = y
                 else:
                     s = 1
-                    check = self._check(i, x)
+                    check = self._check(i[1:], x)
                     if check == 1:
-                        for j in i:
+                        for j in i[1:]:
                             if j.item in x:
                                 s *= j.probability
                         if x in periods:
-                            periods[x] += s
+                            periods[x][0] += s
                         else:
-                            periods[x] = s
+                            periods[x] = [s, y[1]]
         for x, y in periods.items():
-            if y >= self._minSup:
+            if y[0] >= _minSup:
                 sample = str()
                 for i in x:
                     sample = sample + i + "\t"
                 self._finalPatterns[sample] = y
 
     @deprecated(
-        "It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
-    def startMine(self):
+          "It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
+    def startMine(self) -> None:
         """
-        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
+        Main method where the patterns are mined by constructing tree and remove the false patterns
+        by counting the original support of a patterns.
+
+        :return: None
         """
         self.mine()
 
-    def mine(self):
+    def mine(self) -> None:
         """
-        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
+        Main method where the patterns are mined by constructing tree and remove the false patterns
+        by counting the original support of a patterns.
+
+        :return: None
         """
-        global minSup
+        global _lno, _maxPer, _minSup, _first, _last, periodic
         self._startTime = _ab._time.time()
         self._creatingItemSets()
-        self._creatingNeighbours()
-        # self._minSup = self._convert(self._minSup)
-        minSup = self._minSup
         self._finalPatterns = {}
-        mapSupport, plist = self._frequentOneItem()
-        self.Database1 = self._updateTransactions(mapSupport)
+        self._minSup = self._convert(self._minSup)
+        self._maxPer = self._convert(self._maxPer)
+        _minSup, _maxPer, _lno = self._minSup, self._maxPer, self._lno
+        mapSupport, plist = self._periodicFrequentOneItem()
+        updatedTrans = self._updateTransactions(mapSupport)
         info = {k: v for k, v in mapSupport.items()}
-        Tree1 = self._buildTree(self.Database1, info)
-        Tree1.generatePatterns([])
+        Tree1 = self._buildTree(updatedTrans, info)
+        self._periodic = {}
+        Tree1.generatePatterns([], self._periodic)
         self._removeFalsePositives()
-        print("Geo-Referenced Frequent patterns were generated from uncertain databases successfully using GFP algorithm")
+        print("Periodic frequent patterns were generated successfully using UPFP algorithm")
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
-        self.memoryRSS = float()
+        self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
-        self.memoryRSS = process.memory_info().rss
+        self._memoryRSS = process.memory_info().rss
 
-    def getMemoryUSS(self):
+    def getMemoryUSS(self) -> float:
         """
 
         Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
-    def getMemoryRSS(self):
+    def getMemoryRSS(self) -> float:
         """
 
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
-        return self.memoryRSS
+        return self._memoryRSS
 
-    def getRuntime(self):
+    def getRuntime(self) -> float:
         """
 
         Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
-    def getPatternsAsDataFrame(self):
+    def getPatternsAsDataFrame(self) -> '_ab._pd.DataFrame':
         """
 
         Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataframe = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a, b])
-            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
+            data.append([a.replace('\t', ' '), b[0], b[1]])
+            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
         return dataframe
 
-    def save(self, outFile):
+    def save(self, outFile: str) -> None:
         """
 
         Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: csv file
+        :return: None
         """
         self.oFile = outFile
         writer = open(self.oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            s1 = x + ":" + str(y)
+            s1 = x.strip() + ":" + str(y[0]) + ":" + str(y[1])
             writer.write("%s \n" % s1)
 
-    def getPatterns(self):
+    def getPatterns(self) -> Dict[str, List[float]]:
         """
 
         Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
-    
-    def printResults(self):
+
+    def printResults(self) -> None:
         """
-        This function is used to print the result
+        This function is used to print the results
         """
-        print("Total number of Patterns:", len(self.getPatterns()))
-        self.save("patterns.txt")
-        memUSS = self.getMemoryUSS()
-        print("Total Memory in USS:", memUSS)
-        memRSS = self.getMemoryRSS()
-        print("Total Memory in RSS", memRSS)
-        run = self.getRuntime()
-        print("Total ExecutionTime in ms:", run)
+        print("Total number of  Uncertain Periodic-Frequent Patterns:", len(self.getPatterns()))
+        print("Total Memory in USS:", self.getMemoryUSS())
+        print("Total Memory in RSS", self.getMemoryRSS())
+        print("Total ExecutionTime in ms:",  self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
     if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
         if len(_ab._sys.argv) == 6:
-            _ap = GFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
+            _ap = UPFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
         if len(_ab._sys.argv) == 5:
-            _ap = GFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = UPFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         _ap.startMine()
         _ap.mine()
-        _Patterns = _ap.getPatterns()
-        print("Total number of Patterns:", len(_Patterns))
+        print("Total number of Uncertain Periodic-Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
-        _memUSS = _ap.getMemoryUSS()
-        print("Total Memory in USS:", _memUSS)
-        _memRSS = _ap.getMemoryRSS()
-        print("Total Memory in RSS", _memRSS)
-        _run = _ap.getRuntime()
-        print("Total ExecutionTime in ms:", _run)
+        print("Total Memory in USS:", _ap.getMemoryUSS())
+        print("Total Memory in RSS", _ap.getMemoryRSS())
+        print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.5.28.1/PAMI/uncertainGeoreferencedFrequentPattern/basic/abstract.py` & `pami-2024.5.7.1/PAMI/uncertainGeoreferencedFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/uncertainPeriodicFrequentPattern/__init__.py` & `pami-2024.5.7.1/PAMI/uncertainPeriodicFrequentPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/uncertainPeriodicFrequentPattern/basic/UPFPGrowth.py` & `pami-2024.5.7.1/PAMI/weightedFrequentRegularPattern/basic/WFRIMiner.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,32 +1,28 @@
-# UPFPGrowth is used to discover periodic-frequent patterns in an uncertain temporal database.
+# WFRIMiner is one of the fundamental algorithm to discover weighted frequent regular patterns in a transactional database. It stores the database in compressed WFRI-tree decreasing the memory usage and extracts the patterns from tree.It employs downward closure property to  reduce the search space effectively.
 #
 # **Importing this algorithm into a python program**
-# --------------------------------------------------------
 #
-#
-#             from PAMI.uncertainPeriodicFrequentPattern.basic import UPFPGrowth as alg
+#             from PAMI.weightedFrequentRegularPattern.basic import WFRIMiner as alg
 #
 #             iFile = 'sampleDB.txt'
 #
 #             minSup = 10  # can also be specified between 0 and 1
 #
-#             maxPer = 2   # can also be specified between 0 and 1
-#
-#             obj = alg.UPFPGrowth(iFile, minSup, maxPer)
+#             obj = alg.WFRIMiner(iFile, WS, regularity)
 #
 #             obj.mine()
 #
-#             periodicFrequentPatterns = obj.getPatterns()
+#             weightedFrequentRegularPatterns = obj.getPatterns()
 #
-#             print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
+#             print("Total number of Frequent Patterns:", len(weightedFrequentRegularPatterns))
 #
 #             obj.save(oFile)
 #
-#             Df = obj.getPatternsAsDataFrame()
+#             Df = obj.getPatternInDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
 #             print("Total Memory in USS:", memUSS)
 #
 #             memRSS = obj.getMemoryRSS()
 #
@@ -51,779 +47,686 @@
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
-
+from PAMI.weightedFrequentRegularPattern.basic import abstract as _fp
 import pandas as pd
 from deprecated import deprecated
-from PAMI.uncertainPeriodicFrequentPattern.basic import abstract as _ab
-from typing import List, Dict, Tuple, Union
+from typing import List, Dict
 
-_minSup = float()
-__maxPer = float()
-__first = int()
-_last = int()
-__lno = int()
-#rank = {}
-#periodic = {}
 
-class _Item:
-    """
-    A class used to represent the item with probability in transaction of dataset
+_WS = str()
+_regularity = str()
+_lno = int()
+_weights = {}
+_wf = {}
+_fp._sys.setrecursionlimit(20000)
 
-    :Attributes:
 
-        item: int or word
-            Represents the name of the item
-
-        probability: float
-            Represent the existential probability(likelihood presence) of an item
-    """
-
-    def __init__(self, item: str, probability: float) -> None:
-        self.item = item
-        self.probability = probability
-
-
-class _Node(object):
+class _Node:
     """
     A class used to represent the node of frequentPatternTree
 
     :Attributes:
 
-        item: int
+        itemId: int
             storing item of a node
 
-        probability: int
-            To maintain the expected support of node
+        counter: int
+            To maintain the support of node
 
         parent: node
-            To maintain the parent of every node
+            To maintain the parent of node
 
         children: list
             To maintain the children of node
 
-        timeStamps: list
-            To maintain the timeStamps of node
-
     :Methods:
 
-        addChild(itemName)
-            storing the children to their respective parent nodes
+        addChild(node)
+            Updates the nodes children list and parent for the given node
+
     """
 
-    def __init__(self, item: str, children: Dict) -> None:
+    def __init__(self, item: int, children: dict) -> None:
+        """
+        Initializing the Node class
+
+        :param item: Storing the item of a node
+        :type item: int or None
+        :param children: To maintain the children of a node
+        :type children: dict
+        :return: None
+        """
+
         self.item = item
-        self.probability = 1
         self.children = children
         self.parent = None
         self.timeStamps = []
 
-    def addChild(self, node: '_Node') -> None:
+    def addChild(self, node) -> None:
         """
-        To add the children details to parent node
+        To add the children to a node
 
-        :param node: children node
-        :return: updated parent node children
+        :param node: parent node in the tree
+        :return: None
         """
+
         self.children[node.item] = node
         node.parent = self
 
 
-def _printTree(root) -> None:
-    """
-    To print the details of tree
-
-    :param root: root node of the tree
-    :return: details of tree
-    """
-    for x, y in root.children.items():
-        print(x, y.item, y.probability, y.parent.item, y.timeStamps)
-        _printTree(y)
-
-
-class _Tree(object):
+class _Tree:
     """
     A class used to represent the frequentPatternGrowth tree structure
 
     :Attributes:
+
         root : Node
-            Represents the root node of the tree
+            The first node of the tree set to Null.
 
         summaries : dictionary
-            storing the nodes with same item name
+            Stores the nodes itemId which shares same itemId
 
         info : dictionary
-            stores the support of items
+            frequency of items in the transactions
 
     :Methods:
-        addTransactions(transaction)
-            creating transaction as a branch in frequentPatternTree
-        addConditionalTransaction(prefixPaths, supportOfItems)
-            construct the conditional tree for prefix paths
-        conditionalPatterns(Node)
-            generates the conditional patterns from tree for specific node
-        conditionalTransactions(prefixPaths,Support)
-            takes the prefixPath of a node and support at child of the path and extract the frequent items from
-            prefixPaths and generates prefixPaths with items which are frequent
-        remove(Node)
-            removes the node from tree once after generating all the patterns respective to the node generatePatterns(Node) starts from the root node of the tree and mines the frequent patterns
+        addTransaction(transaction, freq)
+            adding items of  transactions into the tree as nodes and freq is the count of nodes
+        getFinalConditionalPatterns(node)
+            getting the conditional patterns from fp-tree for a node
+        getConditionalPatterns(patterns, frequencies)
+            sort the patterns by removing the items with lower minSup
+        generatePatterns(prefix)
+            generating the patterns from fp-tree
     """
 
     def __init__(self) -> None:
         self.root = _Node(None, {})
         self.summaries = {}
         self.info = {}
 
-    def addTransactions(self, transaction: List['_Item'], tid: int) -> None:
+    def addTransaction(self, transaction: list, tid: list) -> None:
         """
-        Adding transaction into tree
+        Adding a transaction into tree
 
-        :param transaction: it represents the one transaction in database
+        :param transaction: To represent the complete database
         :type transaction: list
-        :param tid: the timestamp of transaction
+        :param tid: To represent the timestamp of a database
         :type tid: list
-        :return: None
-        """
-        currentNode = self.root
-        for i in range(len(transaction)):
-            if transaction[i].item not in currentNode.children:
-                newNode = _Node(transaction[i].item, {})
-                l1 = i - 1
-                temp = []
-                while l1 >= 0:
-                    temp.append(transaction[l1].probability)
-                    l1 -= 1
-                if len(temp) == 0:
-                    newNode.probability = transaction[i].probability
-                else:
-                    newNode.probability = max(temp) * transaction[i].probability
-                currentNode.addChild(newNode)
-                if transaction[i].item in self.summaries:
-                    self.summaries[transaction[i].item].append(newNode)
-                else:
-                    self.summaries[transaction[i].item] = [newNode]
-                currentNode = newNode
-            else:
-                currentNode = currentNode.children[transaction[i].item]
-                l1 = i - 1
-                temp = []
-                while l1 >= 0:
-                    temp.append(transaction[l1].probability)
-                    l1 -= 1
-                if len(temp) == 0:
-                    currentNode.probability += transaction[i].probability
-                else:
-                    currentNode.probability += max(temp) * transaction[i].probability
-        currentNode.timeStamps = currentNode.timeStamps + tid
-
-    def addConditionalTransaction(self, transaction: List[str], ts: List[int], sup: float) -> None:
-        """
-        Constructing conditional tree from prefixPaths
-
-        :param transaction : it represents the one transaction in database
-        :type transaction : list
-        :param ts: timeStamp of a transaction
-        :type ts: list
-        :param sup : support of prefixPath taken at last child of the path
-        :type sup : int
-        :return: None
+        :return: pfp-growth tree
         """
         currentNode = self.root
         for i in range(len(transaction)):
             if transaction[i] not in currentNode.children:
                 newNode = _Node(transaction[i], {})
-                newNode.probability = sup
                 currentNode.addChild(newNode)
                 if transaction[i] in self.summaries:
                     self.summaries[transaction[i]].append(newNode)
                 else:
                     self.summaries[transaction[i]] = [newNode]
                 currentNode = newNode
             else:
                 currentNode = currentNode.children[transaction[i]]
-                currentNode.probability += sup
-        currentNode.timeStamps = currentNode.timeStamps + ts
+        currentNode.timeStamps = currentNode.timeStamps + tid
 
-    def getConditionalPatterns(self, alpha: str) -> Tuple[List[List[str]], List[List[int]], List[float], Dict[str, List[float]]]:
+    def getConditionalPatterns(self, alpha, pattern) -> tuple:
         """
-        Generates all the conditional patterns of respective node.
+        Generates all the conditional patterns of a respective node
 
-        :param alpha : it represents the Node in tree
-        :type alpha : Node
-        :return: tuple
+        :param alpha: To represent a Node in the tree
+        :type alpha: Node
+        :param pattern: prefix of the pattern
+        :type alpha: list
+        :return: A tuple consisting of finalPatterns, conditional pattern base and information
         """
-
         finalPatterns = []
-        finalTimeStamps = []
-        sup = []
+        finalSets = []
         for i in self.summaries[alpha]:
             set1 = i.timeStamps
-            s = i.probability
             set2 = []
             while i.parent.item is not None:
                 set2.append(i.parent.item)
                 i = i.parent
             if len(set2) > 0:
                 set2.reverse()
                 finalPatterns.append(set2)
-                finalTimeStamps.append(set1)
-                sup.append(s)
-        finalPatterns, finalTimeStamps, support, info = self.conditionalTransactions(finalPatterns, finalTimeStamps,
-                                                                                     sup)
-        return finalPatterns, finalTimeStamps, support, info
+                finalSets.append(set1)
+        finalPatterns, finalSets, info = self.conditionalDatabases(finalPatterns, finalSets, pattern)
+        return finalPatterns, finalSets, info
 
-    def removeNode(self, nodeValue: str) -> None:
+    @staticmethod
+    def generateTimeStamps(node) -> list:
+        """
+        To get the timestamps of a node
+
+        :param node: A node in the tree
+        :return: Timestamps of a node
+        """
+
+        finalTimeStamps = node.timeStamps
+        return finalTimeStamps
+
+    def removeNode(self, nodeValue) -> None:
         """
         Removing the node from tree
 
-        :param nodeValue : it represents the node in tree
-        :type nodeValue : node
-        :return: None
+        :param nodeValue: To represent a node in the tree
+        :type nodeValue: node
+        :return: Tree with their nodes updated with timestamps
         """
+
         for i in self.summaries[nodeValue]:
             i.parent.timeStamps = i.parent.timeStamps + i.timeStamps
             del i.parent.children[nodeValue]
 
-    def getPeriodAndSupport(self, s: float, timeStamps: List[int]) -> List[float]:
+    def getTimeStamps(self, alpha) -> list:
+        """
+        To get all the timestamps of the nodes which share same item name
+
+        :param alpha: Node in a tree
+        :return: Timestamps of a  node
         """
-        Calculates the period and support of an item based on the given support value and list of timestamps.
+        temporary = []
+        for i in self.summaries[alpha]:
+            temporary += i.timeStamps
+        return temporary
 
-        :param s: The support value.
-        :type s: float
-        :param timeStamps: A list of timestamps.
-        :type timeStamps: List[int]
-        :return: A list containing the support and period of the item.
-        :rtype: List[float]
+    @staticmethod
+    def getSupportAndPeriod(timeStamps: list, pattern: list) -> list:
         """
-        global _lno, _maxPer
+        To calculate the periodicity and support
+
+        :param timeStamps: Timestamps of an item set
+        :type timeStamps: list
+        :param pattern: pattern to evaluate the weighted frequent regular or not
+        :type pattern: list
+        :return: support, periodicity
+        """
+        global _WS, _regularity, _lno, _weights
         timeStamps.sort()
         cur = 0
-        per = 0
-        sup = s
+        per = list()
+        sup = 0
         for j in range(len(timeStamps)):
-            per = max(per, timeStamps[j] - cur)
-            if per > _maxPer:
-                return [0, 0]
+            per.append(timeStamps[j] - cur)
             cur = timeStamps[j]
-        per = max(per, _lno - cur)
-        return [sup, per]
-
-    def conditionalTransactions(self, condPatterns: List[List[str]], condTimeStamps: List[List[int]], support: List[float]) -> Tuple[List[List[str]], List[List[int]], List[float], Dict[str, List[float]]]:
+            sup += 1
+        per.append(_lno - cur)
+        l = int()
+        for i in pattern:
+            l = l + _weights[i]
+        wf = (l / (len(pattern))) * sup
+        if len(per) == 0:
+            return [0, 0]
+        return [sup, max(per), wf]
+
+    def conditionalDatabases(self, conditionalPatterns: list, conditionalTimeStamps: list, pattern: list) -> tuple:
+        """
+        It generates the conditional patterns with periodic-frequent items
+
+        :param conditionalPatterns: conditionalPatterns generated from conditionPattern method of a respective node
+        :type conditionalPatterns: list
+        :param conditionalTimeStamps: Represents the timestamps of a conditional patterns of a node
+        :type conditionalTimeStamps: list
+        :param pattern: prefix of the pattern
+        :type pattern: list
+        :returns: Returns conditional transactions by removing non-periodic and non-frequent items
         """
-        It generates the conditional patterns with frequent items
-
-        :param condPatterns : conditional patterns generated from getConditionalPatterns method for respective node
-        :type condPatterns : list
-        :param condTimeStamps: timeStamps of conditional transactions
-        :type condTimeStamps: list
-        :param support : the support of conditional pattern in tree
-        :type support : list
-        """
-        global _minSup, _maxPer
+        global _WS, _regularity
         pat = []
         timeStamps = []
-        sup = []
         data1 = {}
-        count = {}
-        for i in range(len(condPatterns)):
-            for j in condPatterns[i]:
+        for i in range(len(conditionalPatterns)):
+            for j in conditionalPatterns[i]:
                 if j in data1:
-                    data1[j] = data1[j] + condTimeStamps[i]
-                    count[j] += support[i]
+                    data1[j] = data1[j] + conditionalTimeStamps[i]
                 else:
-                    data1[j] = condTimeStamps[i]
-                    count[j] = support[i]
-        updatedDict = {}
+                    data1[j] = conditionalTimeStamps[i]
+        updatedDictionary = {}
         for m in data1:
-            updatedDict[m] = self.getPeriodAndSupport(count[m], data1[m])
-        updatedDict = {k: v for k, v in updatedDict.items() if v[0] >= _minSup and v[1] <= _maxPer}
+            updatedDictionary[m] = self.getSupportAndPeriod(data1[m], pattern + [m])
+        updatedDictionary = {k: v for k, v in updatedDictionary.items() if v[0] >= _WS and v[1] <= _regularity}
         count = 0
-        for p in condPatterns:
-            p1 = [v for v in p if v in updatedDict]
-            trans = sorted(p1, key=lambda x: (updatedDict.get(x)[0]), reverse=True)
+        for p in conditionalPatterns:
+            p1 = [v for v in p if v in updatedDictionary]
+            trans = sorted(p1, key=lambda x: (updatedDictionary.get(x)[0], -x), reverse=True)
             if len(trans) > 0:
                 pat.append(trans)
-                timeStamps.append(condTimeStamps[count])
-                sup.append(support[count])
+                timeStamps.append(conditionalTimeStamps[count])
             count += 1
-        return pat, timeStamps, sup, updatedDict
+        return pat, timeStamps, updatedDictionary
 
-    def generatePatterns(self, prefix: List[str], periodic: Dict) -> None:
+    def generatePatterns(self, prefix: list) -> None:
         """
         Generates the patterns
 
-        :param prefix : forms the combination of items
-        :type prefix : list
-        :return: None
+        :param prefix: Forms the combination of items
+        :type prefix: list
+        :returns: yields patterns with their support and periodicity
         """
-
-        global _minSup
-        for i in sorted(self.summaries, key=lambda x: (self.info.get(x)[0])):
+        global _WS
+        for i in sorted(self.summaries, key=lambda x: (self.info.get(x)[0], -x)):
             pattern = prefix[:]
             pattern.append(i)
-            s = 0
-            for x in self.summaries[i]:
-                s += x.probability
-            periodic[tuple(pattern)] = self.info[i]
-            if s >= _minSup:
-                patterns, timeStamps, support, info = self.getConditionalPatterns(i)
+            if self.info[i][2] >= _WS:
+                yield pattern, self.info[i]
+                patterns, timeStamps, info = self.getConditionalPatterns(i, pattern)
                 conditionalTree = _Tree()
                 conditionalTree.info = info.copy()
                 for pat in range(len(patterns)):
-                    conditionalTree.addConditionalTransaction(patterns[pat], timeStamps[pat], support[pat])
+                    conditionalTree.addTransaction(patterns[pat], timeStamps[pat])
                 if len(patterns) > 0:
-                    conditionalTree.generatePatterns(pattern, periodic)
+                    for q in conditionalTree.generatePatterns(pattern):
+                        yield q
             self.removeNode(i)
 
 
-class UPFPGrowth(_ab._periodicFrequentPatterns):
+class WFRIMiner(_fp._weightedFrequentRegularPatterns):
     """
     About this algorithm
     ====================
 
-    :Description: Basic is  to discover periodic-frequent patterns in a uncertain temporal database.
+    :Description: WFRIMiner is one of the fundamental algorithm to discover weighted frequent regular patterns in a transactional database.
+       * It stores the database in compressed WFRI-tree decreasing the memory usage and extracts the patterns from tree.It employs downward closure property to  reduce the search space effectively.
 
-    :Reference:  Uday Kiran, R., Likhitha, P., Dao, MS., Zettsu, K., Zhang, J. (2021).Discovering Periodic-Frequent Patterns in Uncertain Temporal Databases.
-            In:  Mantoro, T., Lee, M., Ayu, M.A., Wong, K.W., Hidayanto, A.N. (eds) Neural Information Processing.
-                 ICONIP 2021. Communications in Computer and Information Science, vol 1516. Springer, Cham.
-                 https://doi.org/10.1007/978-3-030-92307-5_83
+    :Reference:  K. Klangwisan and K. Amphawan, "Mining weighted-frequent-regular itemsets from transactional database,"
+                 2017 9th International Conference on Knowledge and Smart Technology (KST), 2017, pp. 66-71,
+                 doi: 10.1109/KST.2017.7886090.
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of Uncertain Periodic Frequent Patterns
+                   Name of the Input file to mine complete set of Weighted Frequent Regular Patterns.
     :param  oFile: str :
-                   Name of the output file to store complete set of Uncertain Periodic Frequent patterns
-    :param  minSup: float:
-                   minimum support thresholds were tuned to find the appropriate ranges in the limited memory
+                   Name of the output file to store complete set of Weighted Frequent Regular Patterns.
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
-    :param  maxper: float :
-                   where maxPer represents the maximum periodicity threshold value specified by the user.
+    :param  wFile: str :
+                   This is a weighted file.
+
 
 
     :Attributes:
 
         iFile : file
-            Name of the Input file or path of the input file
-
-        oFile : file
-            Name of the output file or path of output file
+            Input file name or path of the input file
 
-        minSup: int or float or str
-            The user can specify minSup either in count or proportion of database size.
-            If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+        WS: float or int or str
+            The user can specify WS either in count or proportion of database size.
+            If the program detects the data type of WS is integer, then it treats WS is expressed in count.
             Otherwise, it will be treated as float.
-            Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+            Example: WS=10 will be treated as integer, while WS=10.0 will be treated as float
 
-        maxPer: int or float or str
-            The user can specify maxPer either in count or proportion of database size.
-            If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
+        regularity: float or int or str
+            The user can specify regularity either in count or proportion of database size.
+            If the program detects the data type of regularity is integer, then it treats regularity is expressed in count.
             Otherwise, it will be treated as float.
-            Example: maxPer=10 will be treated as integer, while maxPer=10.0 will be treated as float
+            Example: regularity=10 will be treated as integer, while regularity=10.0 will be treated as float
 
-        sep: str
-            This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
+        sep : str
+            This variable is used to distinguish items from one another in a transaction. The default separator is tab space or \t.
             However, the users can override their default separator.
 
-        memoryUSS: float
-            To store the total amount of USS memory consumed by the program
-
-        memoryRSS: float
-            To store the total amount of RSS memory consumed by the program
+        oFile : file
+            Name of the output file or the path of the output file
 
-        startTime: float
+        startTime:float
             To record the start time of the mining process
 
-        endTime: float
+        endTime:float
             To record the completion time of the mining process
 
+        memoryUSS : float
+            To store the total amount of USS memory consumed by the program
+
+        memoryRSS : float
+            To store the total amount of RSS memory consumed by the program
+
         Database : list
             To store the transactions of a database in list
 
         mapSupport : Dictionary
             To maintain the information of item and their frequency
 
-        _lno : int
-            To represent the total no of transaction
+        lno : int
+            it represents the total no of transactions
 
         tree : class
-            To represents the Tree class
+            it represents the Tree class
 
         finalPatterns : dict
-            To store the complete patterns
+            it represents to store the patterns
 
     :Methods:
 
-        mine()
+        startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
-            Complete set of periodic-frequent patterns will be loaded in to a output file
+            Complete set of frequent patterns will be loaded in to an output file
         getPatternsAsDataFrame()
-            Complete set of periodic-frequent patterns will be loaded in to a dataframe
+            Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
         creatingItemSets()
-            Scans the dataset and stores in a list format
-        PeriodicFrequentOneItem()
-            Extracts the one-periodic-frequent patterns from database
-        updateTransaction()
-            Update the database by removing aperiodic items and sort the Database by item decreased support
-        buildTree()
-            After updating the Database, remaining items will be added into the tree by setting root node as null
-        convert()
-            To convert the user specified value
-        removeFalsePositives()
-            To remove the false positives in generated patterns
+            Scans the dataset or dataframes and stores in list format
+        frequentOneItem()
+            Extracts the one-frequent patterns from transactions
+
 
     Execution methods
     =================
 
 
     **Terminal command**
 
-
     .. code-block:: console
 
-       Format:
+      Format:
 
-       (.venv) $ python3 basic.py <inputFile> <outputFile> <minSup> <maxPer>
+      (.venv) $ python3 WFRIMiner.py <inputFile> <outputFile> <weightSupport> <regularity>
 
-       Example Usage:
+      Example Usage:
 
-       (.venv) $ python3 basic.py sampleTDB.txt patterns.txt 0.3 4
+      (.venv) $ python3 WFRIMiner.py sampleDB.txt patterns.txt 10 5
 
-    .. note:: minSup and maxPer will be considered in support count or frequency
+    .. note:: WS & regularity will be considered in support count or frequency
 
 
     **Calling from a python program**
 
     .. code-block:: python
 
-            from PAMI.uncertainPeriodicFrequentPattern.basic import UPFPGrowth as alg
+            from PAMI.weightedFrequentRegularpattern.basic import WFRIMiner as alg
 
             iFile = 'sampleDB.txt'
 
             minSup = 10  # can also be specified between 0 and 1
 
-            maxPer = 2   # can also be specified between 0 and 1
-
-            obj = alg.UPFPGrowth(iFile, minSup, maxPer)
+            obj = alg.WFRIMiner(iFile, WS, regularity)
 
             obj.mine()
 
-            periodicFrequentPatterns = obj.getPatterns()
+            weightedFrequentRegularPatterns = obj.getPatterns()
 
-            print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
+            print("Total number of Frequent Patterns:", len(weightedFrequentRegularPatterns))
 
             obj.save(oFile)
 
-            Df = obj.getPatternsAsDataFrame()
+            Df = obj.getPatternInDataFrame()
 
             memUSS = obj.getMemoryUSS()
 
             print("Total Memory in USS:", memUSS)
 
             memRSS = obj.getMemoryRSS()
 
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
-
     Credits
     =======
 
+             The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
 
-            The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
     """
-    _rank = {}
+
     _startTime = float()
     _endTime = float()
-    _minSup = float()
-    _maxPer = float()
+    _WS = str()
+    _regularity = str()
+    _weight = {}
     _finalPatterns = {}
+    _wFile = " "
     _iFile = " "
     _oFile = " "
     _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
+    _mapSupport = {}
     _lno = 0
-    _periodic = {}
+    _tree = _Tree()
+    _rank = {}
+    _rankDup = {}
+
+    def __init__(self, iFile, _wFile, WS, regularity, sep='\t') -> None:
+        super().__init__(iFile, _wFile, WS, regularity, sep)
 
     def _creatingItemSets(self) -> None:
         """
         Storing the complete transactions of the database/input file in a database variable
 
         :return: None
         """
         self._Database = []
-        if isinstance(self._iFile, _ab._pd.DataFrame):
-            uncertain, data, ts = [], [], []
+        self._weight = {}
+        if isinstance(self._iFile, _fp._pd.DataFrame):
             if self._iFile.empty:
                 print("its empty..")
-            i = self._iFile._columns.values.tolist()
-            if 'TS' in i:
-                ts = self._iFile['TS'].tolist()
+            i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
-                data = self._iFile['Transactions'].tolist()
-            if 'uncertain' in i:
-                uncertain = self._iFile['uncertain'].tolist()
-            for k in range(len(data)):
-                tr = [ts[k]]
-                for j in range(len(k)):
-                    product = _Item(data[k][j], uncertain[k][j])
-                    tr.append(product)
-                self._Database.append(tr)
-                self._lno += 1
+                self._Database = self._iFile['Transactions'].tolist()
+
+        if isinstance(self._wFile, _fp._pd.DataFrame):
+            _items, _weights = [], []
+            if self._wFile.empty:
+                print("its empty..")
+            i = self._wFile.columns.values.tolist()
+            if 'items' in i:
+                _items = self._wFile['items'].tolist()
+            if 'weight' in i:
+                _weights = self._wFile['weight'].tolist()
+            for i in range(len(_items)):
+                self._weight[_items[i]] = _weights[i]
 
             # print(self.Database)
         if isinstance(self._iFile, str):
-            if _ab._validators.url(self._iFile):
-                data = _ab._urlopen(self._iFile)
+            if _fp._validators.url(self._iFile):
+                data = _fp._urlopen(self._iFile)
                 for line in data:
+                    line.strip()
                     line = line.decode("utf-8")
-                    line = line.strip()
-                    line = [i for i in line.split(':')]
-                    temp1 = [i.rstrip() for i in line[0].split(self._sep)]
-                    temp2 = [i.rstrip() for i in line[1].split(self._sep)]
-                    temp1 = [x for x in temp1 if x]
-                    temp2 = [x for x in temp2 if x]
-                    tr = [int(temp1[0])]
-                    for i in range(len(temp1[1:])):
-                        item = temp1[i]
-                        probability = float(temp2[i])
-                        product = _Item(item, probability)
-                        tr.append(product)
-                    self._lno += 1
-                    self._Database.append(tr)
+                    temp = [i.rstrip() for i in line.split(self._sep)]
+                    temp = [x for x in temp if x]
+                    self._Database.append(temp)
             else:
                 try:
-                    count = 0
-                    with open(self._iFile, 'r') as f:
+                    with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
-                            #count += 1
-                            line = line.strip()
-                            line = [i for i in line.split(':')]
-                            temp1 = [i.rstrip() for i in line[0].split(self._sep)]
-                            temp2 = [i.rstrip() for i in line[1].split(self._sep)]
-                            temp1 = [x for x in temp1 if x]
-                            temp2 = [x for x in temp2 if x]
-                            tr = [int(temp1[0])]
-                            for i in range(len(temp1[1:])):
-                                item = temp1[i]
-                                probability = float(temp2[i])
-                                product = _Item(item, probability)
-                                tr.append(product)
-                            self._lno += 1
-                            self._Database.append(tr)
+                            line.strip()
+                            temp = [i.rstrip() for i in line.split(self._sep)]
+                            temp = [x for x in temp if x]
+                            self._Database.append(temp)
                 except IOError:
                     print("File Not Found")
+                    quit()
 
-    def _periodicFrequentOneItem(self) -> Tuple[Dict, List]:
-        """
-        Takes the transactions and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
+        if isinstance(self._wFile, str):
+            if _fp._validators.url(self._wFile):
+                data = _fp._urlopen(self._wFile)
+                for line in data:
+                    line.strip()
+                    line = line.decode("utf-8")
+                    temp = [i.rstrip() for i in line.split(self._sep)]
+                    temp = [x for x in temp if x]
+                    self._weight[temp[0]] = float(temp[1])
+            else:
+                try:
+                    with open(self._wFile, 'r', encoding='utf-8') as f:
+                        for line in f:
+                            line.strip()
+                            temp = [i.rstrip() for i in line.split(self._sep)]
+                            temp = [x for x in temp if x]
+                            self._weight[temp[0]] = float(temp[1])
+                except IOError:
+                    print("File Not Found")
+                    quit()
 
-        :return: Tuple
+    def _convert(self, value) -> float:
         """
-        mapSupport = {}
-        for i in self._Database:
-            n = i[0]
-            for j in i[1:]:
-                if j.item not in mapSupport:
-                    mapSupport[j.item] = [round(j.probability, 3), abs(0 - n), n]
-                else:
-                    mapSupport[j.item][0] += round(j.probability, 3)
-                    mapSupport[j.item][1] = max(mapSupport[j.item][1], abs(n - mapSupport[j.item][2]))
-                    mapSupport[j.item][2] = n
-        for key in mapSupport:
-            mapSupport[key][1] = max(mapSupport[key][1], self._lno - mapSupport[key][2])
-        mapSupport = {k: [v[0], v[1]] for k, v in mapSupport.items() if v[1] <= self._maxPer and v[0] >= self._minSup}
-        plist = [k for k, v in sorted(mapSupport.items(), key=lambda x: (x[1][0], x[0]), reverse=True)]
-        self._rank = dict([(index, item) for (item, index) in enumerate(plist)])
-        return mapSupport, plist
-
-    def _check(self, i: List, x: List) -> int:
-        """
-        To check the presence of item or pattern in transaction
-
-        :param x: it represents the pattern
-        :type x : list
-        :param i : represents the uncertain transactions
-        :type i : list
-        :return: value
-        :rtype: int
-        """
-
-        for m in x:
-            k = 0
-            for n in i:
-                if m == n.item:
-                    k += 1
-            if k == 0:
-                return 0
-        return 1
-
-    def _getPeriodAndSupport(self, s: float, timeStamps: List[int]) -> List[float]:
-        """
-        To calculate periodicity of timeStamps
-
-        :param s: support of a pattern
-        :param timeStamps: timeStamps of a pattern
-        :return: periodicity and Support
+        To convert the type of user specified minSup value
+
+        :param value: user specified minSup value
+        :return: converted type
         """
-        global __lno, _maxPer
-        timeStamps.sort()
-        cur = 0
-        per = 0
-        sup = s
-        for j in range(len(timeStamps)):
-            per = max(per, timeStamps[j] - cur)
-            if per > _maxPer:
-                return [0, 0]
-            cur = timeStamps[j]
-        per = max(per, _lno - cur)
-        return [sup, per]
+        if type(value) is int:
+            value = int(value)
+        if type(value) is float:
+            value = (len(self._Database) * value)
+        if type(value) is str:
+            if '.' in value:
+                value = float(value)
+                value = (len(self._Database) * value)
+            else:
+                value = int(value)
+        return value
 
-    def _buildTree(self, data: List[List], info: Dict) -> '_Tree':
+    def _frequentOneItem(self) -> List[str]:
         """
-        It takes the transactions and support of each item and construct the main tree with setting root node as null
+        Generating One frequent items sets
 
-        :param data: it represents the one transaction in database
-        :type data: list
-        :param info: it represents the support of each item
-        :type info : dictionary
+        :return: list
         """
-        rootNode = _Tree()
-        rootNode.info = info.copy()
-        for i in range(len(data)):
-            set1 = [data[i][0]]
-            rootNode.addTransactions(data[i][1:], set1)
-        return rootNode
+        global _lno, _wf, _weights
+        self._mapSupport = {}
+        _owf = {}
+        for tr in self._Database:
+            for i in range(1, len(tr)):
+                if tr[i] not in self._mapSupport:
+                    self._mapSupport[tr[i]] = [int(tr[0]), int(tr[0]), 1]
+                else:
+                    self._mapSupport[tr[i]][0] = max(self._mapSupport[tr[i]][0], (int(tr[0]) - self._mapSupport[tr[i]][1]))
+                    self._mapSupport[tr[i]][1] = int(tr[0])
+                    self._mapSupport[tr[i]][2] += 1
+        for key in self._mapSupport:
+            self._mapSupport[key][0] = max(self._mapSupport[key][0], abs(len(self._Database) - self._mapSupport[key][1]))
+        _lno = len(self._Database)
+        self._mapSupport = {k: [v[2], v[0]] for k, v in self._mapSupport.items() if v[0] <= self._regularity}
+        for x, y in self._mapSupport.items():
+            if self._weight.get(x) is None:
+                self._weight[x] = 0
+        gmax = max([self._weight[values] for values in self._mapSupport.keys()])
+        for x, y in self._mapSupport.items():
+            _owf[x] = y[0] * gmax
+        self._mapSupport = {k: v for k, v in self._mapSupport.items() if v[0] * _owf[k] >= self._WS}
+        for x, y in self._mapSupport.items():
+            temp = self._weight[x] * y[0]
+            _wf[x] = temp
+            self._mapSupport[x].append(temp)
+        genList = [k for k, v in sorted(self._mapSupport.items(), key=lambda x: x[1], reverse= True)]
+        self._rank = dict([(index, item) for (item, index) in enumerate(genList)])
+        for x, y in self._rank.items():
+            _weights[y] = self._weight[x]
+        return genList
 
-    def _updateTransactions(self, dict1: Dict) -> List[List]:
+    def _updateTransactions(self, itemSet) -> List[List[int]]:
         """
-        Remove the items which are not frequent from transactions and updates the transactions with rank of items
+        Updates the items in transactions with rank of items according to their support
 
-        :param dict1 : frequent items with support
-        :type dict1 : dictionary
-        :return: list
+        :Example: oneLength = {'a':7, 'b': 5, 'c':'4', 'd':3}
+                  rank = {'a':0, 'b':1, 'c':2, 'd':3}
+
+        :param itemSet: list of one-frequent items
+        :return: None
         """
         list1 = []
         for tr in self._Database:
             list2 = [int(tr[0])]
             for i in range(1, len(tr)):
-                if tr[i].item in dict1:
-                    list2.append(tr[i])
+                if tr[i] in itemSet:
+                    list2.append(self._rank[tr[i]])
             if len(list2) >= 2:
                 basket = list2[1:]
-                basket.sort(key=lambda val: self._rank[val.item])
+                basket.sort()
                 list2[1:] = basket[0:]
                 list1.append(list2)
         return list1
 
-    def _convert(self, value: Union[int, float, str]) -> Union[int, float]:
+    @staticmethod
+    def _buildTree(transactions, info) -> _Tree:
         """
-        To convert the given user specified value
+        Builds the tree with updated transactions
 
-        :param value: user specified value
-        :return: converted value
-        """
-        if type(value) is int:
-            value = int(value)
-        if type(value) is float:
-            value = float(value)
-        if type(value) is str:
-            if '.' in value:
-                value = float(value)
-            else:
-                value = int(value)
+        :param transactions: updated transactions
+        :param info: support details of each item in transactions
+        :return: transactions compressed in fp-tree
 
-        return value
+        """
+        rootNode = _Tree()
+        rootNode.info = info.copy()
+        for i in range(len(transactions)):
+            set1 = [transactions[i][0]]
+            rootNode.addTransaction(transactions[i][1:], set1)
+        return rootNode
 
-    def _removeFalsePositives(self) -> None:
+    def _savePeriodic(self, itemSet) -> str:
         """
-        Removes the false positive patterns from the generated patterns.
+        The duplication items and their ranks
 
-        This method iterates through the database to identify false positive patterns and removes them from the
-        generated patterns.
+        :param itemSet: frequent itemSet that generated
+        :return: patterns with original item names.
 
-        :return: None
         """
-        periods = {}
-        for i in self._Database:
-            for x, y in self._periodic.items():
-                if len(x) == 1:
-                    periods[x] = y
-                else:
-                    s = 1
-                    check = self._check(i[1:], x)
-                    if check == 1:
-                        for j in i[1:]:
-                            if j.item in x:
-                                s *= j.probability
-                        if x in periods:
-                            periods[x][0] += s
-                        else:
-                            periods[x] = [s, y[1]]
-        for x, y in periods.items():
-            if y[0] >= _minSup:
-                sample = str()
-                for i in x:
-                    sample = sample + i + "\t"
-                self._finalPatterns[sample] = y
+        temp = str()
+        for i in itemSet:
+            temp = temp + self._rankDup[i] + "\t"
+        return temp
 
     @deprecated(
-          "It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
+        "It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
-        Main method where the patterns are mined by constructing tree and remove the false patterns
-        by counting the original support of a patterns.
-
-        :return: None
+        Frequent pattern mining process will start from here
         """
         self.mine()
 
     def mine(self) -> None:
         """
-        Main method where the patterns are mined by constructing tree and remove the false patterns
-        by counting the original support of a patterns.
-
-        :return: None
+        Frequent pattern mining process will start from here
         """
-        global _lno, _maxPer, _minSup, _first, _last, periodic
-        self._startTime = _ab._time.time()
+        global _WS, _regularity, _weights
+        self._startTime = _fp._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._WS is None:
+            raise Exception("Please enter the Minimum Support")
         self._creatingItemSets()
+        self._WS = self._convert(self._WS)
+        self._regularity = self._convert(self._regularity)
+        _WS, _regularity, _weights = self._WS, self._regularity, self._weight
+        itemSet = self._frequentOneItem()
+        updatedTransactions = self._updateTransactions(itemSet)
+        for x, y in self._rank.items():
+            self._rankDup[y] = x
+        info = {self._rank[k]: v for k, v in self._mapSupport.items()}
+        _Tree = self._buildTree(updatedTransactions, info)
+        patterns = _Tree.generatePatterns([])
         self._finalPatterns = {}
-        self._minSup = self._convert(self._minSup)
-        self._maxPer = self._convert(self._maxPer)
-        _minSup, _maxPer, _lno = self._minSup, self._maxPer, self._lno
-        mapSupport, plist = self._periodicFrequentOneItem()
-        updatedTrans = self._updateTransactions(mapSupport)
-        info = {k: v for k, v in mapSupport.items()}
-        Tree1 = self._buildTree(updatedTrans, info)
-        self._periodic = {}
-        Tree1.generatePatterns([], self._periodic)
-        self._removeFalsePositives()
-        print("Periodic frequent patterns were generated successfully using UPFP algorithm")
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
+        for k in patterns:
+            s = self._savePeriodic(k[0])
+            self._finalPatterns[str(s)] = k[1]
+        print("Weighted Frequent Regular patterns were generated successfully using WFRIM algorithm")
+        self._endTime = _fp._time.time()
         self._memoryUSS = float()
         self._memoryRSS = float()
+        process = _fp._psutil.Process(_fp._os.getpid())
+        self._memoryRSS = float()
+        self._memoryUSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
 
     def getMemoryUSS(self) -> float:
         """
 
         Total amount of USS memory consumed by the mining process will be retrieved from this function
@@ -852,74 +755,74 @@
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
-    def getPatternsAsDataFrame(self) -> '_ab._pd.DataFrame':
+    def getPatternsAsDataFrame(self) -> _fp._pd.DataFrame:
         """
 
         Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataframe = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a.replace('\t', ' '), b[0], b[1]])
-            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
+            data.append([a.replace('\t', ' '), b])
+            dataframe = _fp._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataframe
 
     def save(self, outFile: str) -> None:
         """
 
         Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: csv file
         :return: None
         """
-        self.oFile = outFile
-        writer = open(self.oFile, 'w+')
+        self._oFile = outFile
+        writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            s1 = x.strip() + ":" + str(y[0]) + ":" + str(y[1])
+            s1 = x.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
-    def getPatterns(self) -> Dict[str, List[float]]:
+    def getPatterns(self) -> Dict[str, float]:
         """
 
         Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self) -> None:
         """
         This function is used to print the results
         """
-        print("Total number of  Uncertain Periodic-Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of  Weighted Frequent Regular Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:",  self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
-        if len(_ab._sys.argv) == 6:
-            _ap = UPFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
-        if len(_ab._sys.argv) == 5:
-            _ap = UPFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+    if len(_fp._sys.argv) == 6 or len(_fp._sys.argv) == 7:
+        if len(_fp._sys.argv) == 7:
+            _ap = WFRIMiner(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4], _fp._sys.argv[5], _fp._sys.argv[6])
+        if len(_fp._sys.argv) == 5:
+            _ap = WFRIMiner(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4], _fp._sys.argv[5])
         _ap.startMine()
         _ap.mine()
-        print("Total number of Uncertain Periodic-Frequent Patterns:", len(_ap.getPatterns()))
-        _ap.save(_ab._sys.argv[2])
-        print("Total Memory in USS:", _ap.getMemoryUSS())
+        print("Total number of Weighted Frequent Regular Patterns:", len(_ap.getPatterns()))
+        _ap.save(_fp._sys.argv[2])
+        print("Total Memory in USS:",  _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.5.28.1/PAMI/uncertainPeriodicFrequentPattern/basic/UPFPGrowthPlus.py` & `pami-2024.5.7.1/PAMI/weightedFrequentNeighbourhoodPattern/basic/SWFPGrowth.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,26 +1,24 @@
-# UPFPGrowthPlus is used to discover periodic-frequent patterns in an uncertain temporal database.
+# SWFPGrowth is an algorithm to mine the weighted spatial frequent patterns in spatiotemporal databases.
 #
 # **Importing this algorithm into a python program**
 #
-#             from PAMI.uncertainPeriodicFrequentPattern.basic import UPFPGrowthPlus as alg
+#             from PAMI.weightFrequentNeighbourhoodPattern.basic import SWFPGrowth as alg
 #
 #             iFile = 'sampleDB.txt'
 #
 #             minSup = 10  # can also be specified between 0 and 1
 #
-#             maxPer = 3   # can also be specified between 0 and 1
-#
-#             obj = alg.UPFPGrowthPlus(iFile, minSup, maxPer)
+#             obj = alg.SWFPGrowth(iFile, wFile, nFile, minSup, minWeight, sep)
 #
 #             obj.mine()
 #
-#             periodicFrequentPatterns = obj.getPatterns()
+#             frequentPatterns = obj.getPatterns()
 #
-#             print("Total number of uncertain Periodic Frequent Patterns:", len(periodicFrequentPatterns))
+#             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
 #             obj.save(oFile)
 #
 #             Df = obj.getPatternsAsDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
@@ -49,873 +47,782 @@
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
-
-from PAMI.uncertainPeriodicFrequentPattern.basic import abstract as _ab
+from PAMI.weightedFrequentNeighbourhoodPattern.basic import abstract as _fp
 import pandas as pd
 from deprecated import deprecated
+from typing import List, Dict, Tuple, Union, Iterable
 
-from PAMI.uncertainPeriodicFrequentPattern.basic import abstract as _ab
+_minWS = str()
+_weights = {}
+_rank = {}
+_neighbourList = {}
 
-_minSup = float()
-_maxPer = float()
-_lno = int()
-_first = int()
-_last = int()
+_fp._sys.setrecursionlimit(20000)
 
 
-class _Item:
+class _WeightedItem:
     """
-    A class used to represent the item with probability in transaction of dataset
+    A class used to represent the weight of the item
 
     :Attributes:
 
-        item : int or string
-          Represents the name of the item
+        item: str
+            storing item of the frequent pattern
 
-        probability : float
-          Represent the existential probability(likelihood presence) of an item
-    """
+        weight: float
+            stores the weight of the item
 
-    def __init__(self, item, probability):
-        self.item = item
-        self.probability = probability
-
-
-def printTree(root):
     """
-    To print the tree with nodes with item name, probability, timestamps, and second probability respectively.
-
-    :param root: Node
-    :return: print all Tree with nodes with items, probability, parent item, timestamps, second probability respectively.
-    """
-    for x, y in root.children.items():
-        print(x, y.item, y.probability, y.parent.item, y.tids, y.secondProbability)
-        printTree(y)
+    def __init__(self, item: str, weight: float) -> None:
+        self.item = item
+        self.weight = weight
 
 
-class _Node(object):
+class _Node:
     """
     A class used to represent the node of frequentPatternTree
 
     :Attributes:
 
-        item : int
+        itemId: int
             storing item of a node
 
-        probability : int
-            To maintain the expected support of node
+        counter: int
+            To maintain the support of node
 
-        parent : node
-            To maintain the parent of every node
+        parent: node
+            To maintain the parent of node
 
-        children : list
+        children: list
             To maintain the children of node
 
     :Methods:
 
-        addChild(itemName)
-            storing the children to their respective parent nodes
+        addChild(node)
+            Updates the nodes children list and parent for the given node
+
     """
 
-    def __init__(self, item, children):
-        self.item = item
-        self.probability = 1
-        self.secondProbability = 1
-        self.p = 1
-        self.children = children
+    def __init__(self, item: str, children: Dict[str, '_Node']) -> None:
+        self.itemId = item
+        self.counter = 1
+        self.weight = 0
         self.parent = None
-        self.TimeStamps = []
+        self.children = children
 
-    def addChild(self, node):
+    def addChild(self, node: '_Node') -> None:
         """
-        To add children details to parent node
+        Retrieving the child from the tree
+
+        :param node: Children node.
+        :type node: Node
+        :return: Updates the children nodes and parent nodes
+        :return: None
 
-        :param node: children node
-        :return: update parent node children
         """
-        self.children[node.item] = node
+        self.children[node.itemId] = node
         node.parent = self
 
 
-class _Tree(object):
+class _Tree:
     """
     A class used to represent the frequentPatternGrowth tree structure
 
-    Attributes:
-
-        root: Node
-            Represents the root node of the tree
+    :Attributes:
 
-        summaries: dictionary
-            storing the nodes with same item name
+        root : Node
+            The first node of the tree set to Null.
 
-        info: dictionary
-            stores the support of items
+        summaries : dictionary
+            Stores the nodes itemId which shares same itemId
 
+        info : dictionary
+            frequency of items in the transactions
 
     :Methods:
 
-        addTransaction(transaction)
-            creating transaction as a branch in Tree
-        addConditionalTransaction(prefixPaths, supportOfItems)
-            construct the conditional tree for prefix paths
-        getConditionalPatterns(Node)
-            generates the conditional patterns from tree for specific node
-        conditionalTransactions(prefixPaths,Support)
-            takes the prefixPath of a node and support at child of the path and extract the frequent items from prefixPaths and generates prefixPaths with items which are frequent
-        remove(Node)
-            removes the node from tree once after generating all the patterns respective to the node
-        generatePatterns(Node)
-            starts from the root node of the tree and mines the frequent patterns
-
+        addTransaction(transaction, freq)
+            adding items of  transactions into the tree as nodes and freq is the count of nodes
+        getFinalConditionalPatterns(node)
+            getting the conditional patterns from fp-tree for a node
+        getConditionalPatterns(patterns, frequencies)
+            sort the patterns by removing the items with lower minWS
+        generatePatterns(prefix)
+            generating the patterns from fp-tree
     """
 
-    def __init__(self):
+    def __init__(self) -> None:
         self.root = _Node(None, {})
         self.summaries = {}
         self.info = {}
 
-
-    def addTransaction(self, transaction, tid):
+    def addTransaction(self, transaction: List[_WeightedItem], count: int) -> None:
         """
         Adding transaction into tree
 
-        :param transaction : it represents the one transaction in database
-        :type transaction : list
-        :param tid : the timestamp of transaction
-        :type tid : list
+        :param transaction: it represents the one transaction in database
+        :type transaction: list
+        :param count: frequency of item
+        :type count: int
+        :return: None
         """
+
+        # This method takes transaction as input and returns the tree
+        global _neighbourList, _rank
         currentNode = self.root
-        k = 0
         for i in range(len(transaction)):
-            k += 1
+            wei = 0
+            l1 = i
+            while l1 >= 0:
+                wei += transaction[l1].weight
+                l1 -= 1
             if transaction[i].item not in currentNode.children:
                 newNode = _Node(transaction[i].item, {})
-                newNode.k = k
-                newNode.secondProbability = transaction[i].probability
-                l1 = i - 1
-                temp = []
-                while l1 >= 0:
-                    temp.append(transaction[l1].probability)
-                    l1 -= 1
-                if len(temp) == 0:
-                    newNode.probability = round(transaction[i].probability, 2)
-                else:
-                    newNode.probability = round(max(temp) * transaction[i].probability, 2)
+                newNode.freq = count
+                newNode.weight = wei
                 currentNode.addChild(newNode)
-                if transaction[i].item in self.summaries:
-                    self.summaries[transaction[i].item].append(newNode)
+                if _rank[transaction[i].item] in self.summaries:
+                    self.summaries[_rank[transaction[i].item]].append(newNode)
                 else:
-                    self.summaries[transaction[i].item] = [newNode]
+                    self.summaries[_rank[transaction[i].item]] = [newNode]
                 currentNode = newNode
             else:
                 currentNode = currentNode.children[transaction[i].item]
-                currentNode.secondProbability = max(transaction[i].probability, currentNode.secondProbability)
-                currentNode.k = k
-                l1 = i - 1
-                temp = []
-                while l1 >= 0:
-                    temp.append(transaction[l1].probability)
-                    l1 -= 1
-                if len(temp) == 0:
-                    currentNode.probability += round(transaction[i].probability, 2)
-                else:
-                    nn = max(temp) * transaction[i].probability
-                    currentNode.probability += round(nn, 2)
-        currentNode.TimeStamps = currentNode.TimeStamps + tid
-
-    def addConditionalPatterns(self, transaction, tid, sup, probability):
-        """
-        Constructing conditional tree from prefixPaths
-
-        :param transaction : it represents the one transaction in database
-        :type transaction : list
-        :param tid : timestamps of a pattern or transaction in tree
-        :param tid : list
-        :param sup : support of prefixPath taken at last child of the path
-        :type sup : int
-        :para probability : highest existential probability value among all periodic-frequent items
-        :type probability : list
+                currentNode.freq += count
+                currentNode.weight += wei
+
+    def addConditionalPattern(self, transaction: List[_WeightedItem], count: int) -> None:
+        """
+        Adding transaction into tree
+
+        :param transaction: it represents the one transaction in database
+        :type transaction: list
+        :param count: frequency of item
+        :type count: int
+        :return : None
         """
+        # This method takes transaction as input and returns the tree
+        global _neighbourList, _rank
         currentNode = self.root
-        k = 0
         for i in range(len(transaction)):
-            k += 1
-            if transaction[i] not in currentNode.children:
-                newNode = _Node(transaction[i], {})
-                newNode.k = k
-                newNode.probability = sup
-                newNode.secondProbability = probability
+            wei = 0
+            l1 = i
+            while l1 >= 0:
+                wei += transaction[l1].weight
+                l1 -= 1
+            if transaction[i].itemId not in currentNode.children:
+                newNode = _Node(transaction[i].itemId, {})
+                newNode.freq = count
+                newNode.weight = wei
                 currentNode.addChild(newNode)
-                if transaction[i] in self.summaries:
-                    self.summaries[transaction[i]].append(newNode)
+                if _rank[transaction[i].itemId] in self.summaries:
+                    self.summaries[_rank[transaction[i].itemId]].append(newNode)
                 else:
-                    self.summaries[transaction[i]] = [newNode]
+                    self.summaries[_rank[transaction[i].itemId]] = [newNode]
                 currentNode = newNode
             else:
-                currentNode = currentNode.children[transaction[i]]
-                currentNode.k = k
-                currentNode.probability += sup
-                currentNode.secondProbability = max(probability, currentNode.secondProbability)
-        currentNode.TimeStamps = currentNode.TimeStamps + tid
+                currentNode = currentNode.children[transaction[i].itemId]
+                currentNode.freq += count
+                currentNode.weight += wei
 
-    def conditionalPatterns(self, alpha):
+    def printTree(self, root: _Node) -> None:
         """
-        Generates all the conditional patterns of respective node
+        To print the details of tree
 
-        :param alpha : it represents the Node in tree
-        :type alpha : Node
+        :param root: root node of the tree
+        :return: details of tree
+        """
+        if len(root.children) == 0:
+            return
+        else:
+            for x, y in root.children.items():
+                #print(y.itemId, y.parent.itemId, y.freq, y.weight)
+                self.printTree(y)
+
+
+    def getFinalConditionalPatterns(self, alpha: int) -> Tuple[List[List[_Node]], List[float], Dict[int, float]]:
+        """
+        Generates the conditional patterns for a node
+
+        :param alpha: node to generate conditional patterns
+        :return: returns conditional patterns, frequency of each item in conditional patterns
         """
         finalPatterns = []
-        finalSets = []
-        sup = []
-        prob = []
+        finalFreq = []
+        global _neighbourList
         for i in self.summaries[alpha]:
-            set1 = i.TimeStamps
-            s = i.probability
-            p = i.secondProbability
+            set1 = i.weight
             set2 = []
-            while i.parent.item is not None:
-                set2.append(i.parent.item)
+            while i.parent.itemId is not None:
+                if i.parent.itemId in _neighbourList[i.itemId]:
+                    set2.append(i.parent)
                 i = i.parent
             if len(set2) > 0:
                 set2.reverse()
                 finalPatterns.append(set2)
-                finalSets.append(set1)
-                sup.append(s)
-                prob.append(p)
-        finalPatterns, finalSets, support, prob, info = self.conditionalTransactions(finalPatterns, finalSets, sup, prob)
-        return finalPatterns, finalSets, support, prob, info
-
-    def removeNode(self, nodeValue):
-        """
-        Removing the node from tree
-
-        :param nodeValue : it represents the node in tree
-        :type nodeValue : node
-        """
-        for i in self.summaries[nodeValue]:
-            i.parent.TimeStamps = i.parent.TimeStamps + i.TimeStamps
-            del i.parent.children[nodeValue]
-
-    def getPeriodAndSupport(self, support, TimeStamps):
-        """
-        To calculate the periodicity of given timestamps
-
-        :param support: support of pattern
-        :param TimeStamps: timmeStamps of a pattern
-        :return: support and period
-        """
-        global _maxPer
-        global _lno
-        TimeStamps.sort()
-        cur = 0
-        per = 0
-        sup = support
-        for j in range(len(TimeStamps)):
-            per = max(per, TimeStamps[j] - cur)
-            if per > _maxPer:
-                return [0, 0]
-            cur = TimeStamps[j]
-        per = max(per, _lno - cur)
-        return [sup, per]
-
-    def conditionalTransactions(self, conditionalPatterns, conditionalTimeStamps, support, probability):
-        """
-        It generates the conditional patterns with frequent items
-
-        :param conditionalPatterns : conditional patterns generated from conditionalPatterns() method for respective node
-        :type conditionalPatterns : list
-        :param conditionalTimeStamps : timestamps of respective conditional timestamps
-        :type conditionalTimeStamps : list
-        :param support : the support of conditional pattern in tree
-        :type support : list
-        :para probability : highest existential probability value among all periodic-frequent items
-        :type probability : list
+                finalFreq.append(set1)
+        finalPatterns, finalFreq, info = self.getConditionalTransactions(finalPatterns, finalFreq)
+        return finalPatterns, finalFreq, info
+
+    @staticmethod
+    def getConditionalTransactions(ConditionalPatterns: List[List[_Node]], conditionalFreq: List[float]) -> Tuple[List[List[_Node]], List[float], Dict[int, float]]:
+        """
+        To calculate the frequency of items in conditional patterns and sorting the patterns
+
+        :param ConditionalPatterns: paths of a node
+        :param conditionalFreq: frequency of each item in the path
+        :return: conditional patterns and frequency of each item in transactions
         """
-        global _minSup, _maxPer, _lno
+        global _rank
         pat = []
-        TimeStamps = []
-        sup = []
-        prob = []
+        freq = []
         data1 = {}
-        count = {}
-        for i in range(len(conditionalPatterns)):
-            for j in conditionalPatterns[i]:
-                if j in data1:
-                    data1[j] = data1[j] + conditionalTimeStamps[i]
-                    count[j] += support[i]
+        for i in range(len(ConditionalPatterns)):
+            for j in ConditionalPatterns[i]:
+                if j.itemId in data1:
+                    data1[j.itemId] += conditionalFreq[i]
                 else:
-                    data1[j] = conditionalTimeStamps[i]
-                    count[j] = support[i]
-        updatedDict = {}
-        for m in data1:
-            updatedDict[m] = self.getPeriodAndSupport(count[m], data1[m])
-        updatedDict = {k: v for k, v in updatedDict.items() if v[0] >= _minSup and v[1] <= _maxPer}
+                    data1[j.itemId] = conditionalFreq[i]
+        up_dict = {k: v for k, v in data1.items() if v >= _minWS}
         count = 0
-        for p in conditionalPatterns:
-            p1 = [v for v in p if v in updatedDict]
-            trans = sorted(p1, key=lambda x: (updatedDict.get(x)[0]), reverse=True)
+        for p in ConditionalPatterns:
+            p1 = [v for v in p if v.itemId in up_dict]
+            trans = sorted(p1, key=lambda x: (up_dict.get(x)), reverse=True)
             if len(trans) > 0:
                 pat.append(trans)
-                TimeStamps.append(conditionalTimeStamps[count])
-                sup.append(support[count])
-                prob.append(probability[count])
+                freq.append(conditionalFreq[count])
             count += 1
-        return pat, TimeStamps, sup, prob, updatedDict
+        up_dict = {_rank[k]: v for k, v in up_dict.items()}
+        return pat, freq, up_dict
 
-    def generatePatterns(self, prefix, periodic):
+    def generatePatterns(self, prefix: List[int]) -> Iterable[Tuple[List[int], float]]:
         """
-        Generates the patterns
+        To generate the frequent patterns
+
+        :param prefix: an empty list
+        :return: Frequent patterns that are extracted from fp-tree
 
-        :param prefix : forms the combination of items
-        :type prefix : list
-        :para periodic : occurring at intervals
-        :type periodic : list
         """
-        global _minSup
-        for i in sorted(self.summaries, key=lambda x: (self.info.get(x)[0])):
+        global _minWS
+        for i in sorted(self.summaries, key=lambda x: (self.info.get(x))):
             pattern = prefix[:]
             pattern.append(i)
-            s = 0
-            secProb = []
-            kk = int()
-            for x in self.summaries[i]:
-                if x.k <= 2:
-                    s += x.probability
-                elif x.k >= 3:
-                    n = x.probability * pow(x.secondProbability, (x.k - 2))
-                    s += n
-            periodic[tuple(pattern)] = self.info[i]
-            periodic[tuple(pattern)] = self.info[i]
-            if s >= _minSup:
-                periodic[tuple(pattern)] = self.info[i]
-                patterns, TimeStamps, support, probability, info = self.conditionalPatterns(i)
-                conditionalTree = _Tree()
-                conditionalTree.info = info.copy()
-                for pat in range(len(patterns)):
-                    conditionalTree.addConditionalPatterns(patterns[pat], TimeStamps[pat], support[pat], probability[pat])
-                if len(patterns) > 0:
-                    conditionalTree.generatePatterns(pattern, periodic)
-            self.removeNode(i)
+            yield pattern, self.info[i]
+            patterns, freq, info = self.getFinalConditionalPatterns(i)
+            conditionalTree = _Tree()
+            conditionalTree.info = info.copy()
+            for pat in range(len(patterns)):
+                conditionalTree.addConditionalPattern(patterns[pat], freq[pat])
+            if len(patterns) > 0:
+                for q in conditionalTree.generatePatterns(pattern):
+                    yield q
+
 
-class UPFPGrowthPlus(_ab._periodicFrequentPatterns):
+class SWFPGrowth(_fp._weightedFrequentSpatialPatterns):
     """
     About this algorithm
     ====================
 
-    :Description: Basic Plus is  to discover periodic-frequent patterns in a uncertain temporal database.
-
-    :Reference:  Palla Likhitha, Rage Veena,Rage Uday Kiran, Koji Zettsu, Masashi Toyoda, Philippe Fournier-Viger, (2023).
-                 UPFP-growth++: An Efficient Algorithm to Find Periodic-Frequent Patterns in Uncertain Temporal Databases.
-                 ICONIP 2022. Communications in Computer and Information Science, vol 1792. Springer, Singapore.
-                 https://doi.org/10.1007/978-981-99-1642-9_16
+    :Description: SWFPGrowth is an algorithm to mine the weighted spatial frequent patterns in spatiotemporal databases.
 
+    :Reference: R. Uday Kiran, P. P. C. Reddy, K. Zettsu, M. Toyoda, M. Kitsuregawa and P. Krishna Reddy,
+                "Discovering Spatial Weighted Frequent Itemsets in Spatiotemporal Databases," 2019 International
+                Conference on Data Mining Workshops (ICDMW), 2019, pp. 987-996, doi: 10.1109/ICDMW.2019.00143.
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of Uncertain Periodic Frequent Patterns
+                   Name of the Input file to mine complete set of weighted Frequent Neighbourhood Patterns.
     :param  oFile: str :
-                   Name of the output file to store complete set of Uncertain Periodic Frequent patterns
-    :param  minSup: str:
+                   Name of the output file to store complete set of weighted Frequent Neighbourhood Patterns.
+    :param  minSup: int or str or float:
                    minimum support thresholds were tuned to find the appropriate ranges in the limited memory
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
     :param  maxper: floot :
                    where maxPer represents the maximum periodicity threshold value specified by the user.
 
 
     :Attributes:
 
-        iFile: file
-            Name of the Input file or path of input file
+        iFile : file
+            Input file name or path of the input file
 
-        oFile: file
-            Name of the output file or path of output file
-
-        minSup: int or float or str
-            The user can specify minSup either in count or proportion of database size.
-            If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+        minWS: float or int or str
+            The user can specify minWS either in count or proportion of database size.
+            If the program detects the data type of minWS is integer, then it treats minWS is expressed in count.
             Otherwise, it will be treated as float.
-            Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+            Example: minWS=10 will be treated as integer, while minWS=10.0 will be treated as float
 
-        maxPer: int or float or str
-            The user can specify maxPer either in count or proportion of database size.
-            If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
+        minWeight: float or int or str
+            The user can specify minWeight either in count or proportion of database size.
+            If the program detects the data type of minWeight is integer, then it treats minWeight is expressed in count.
             Otherwise, it will be treated as float.
-            Example: maxPer=10 will be treated as integer, while maxPer=10.0 will be treated as float
+            Example: minWeight=10 will be treated as integer, while minWeight=10.0 will be treated as float
 
-        sep: str
-            This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
+        sep : str
+            This variable is used to distinguish items from one another in a transaction. The default separator is tab space or \t.
             However, the users can override their default separator.
 
-        memoryUSS: float
-            To store the total amount of USS memory consumed by the program
+        oFile : file
+            Name of the output file or the path of the output file
 
-        memoryRSS: float
-            To store the total amount of RSS memory consumed by the program
-
-        startTime: float
+        startTime:float
             To record the start time of the mining process
 
-        endTime: float
+        endTime:float
             To record the completion time of the mining process
 
-        Database: list
+        memoryUSS : float
+            To store the total amount of USS memory consumed by the program
+
+        memoryRSS : float
+            To store the total amount of RSS memory consumed by the program
+
+        Database : list
             To store the transactions of a database in list
 
-        mapSupport: Dictionary
+        mapSupport : Dictionary
             To maintain the information of item and their frequency
 
-        lno: int
-            To represent the total no of transaction
-
-        tree: class
-            To represents the Tree class
+        lno : int
+            it represents the total no of transactions
 
-        itemSetCount: int
-            To represents the total no of patterns
+        tree : class
+            it represents the Tree class
 
-        finalPatterns: dict
-            To store the complete patterns
+        finalPatterns : dict
+            it represents to store the patterns
 
-    :Methods:
+    :Methods :
 
-        startMine()
+        mine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
-        savePatterns(oFile)
-            Complete set of periodic-frequent patterns will be loaded in to a output file
+        save(oFile)
+            Complete set of frequent patterns will be loaded in to an output file
         getPatternsAsDataFrame()
-            Complete set of periodic-frequent patterns will be loaded in to a dataframe
+            Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
-        creatingItemSets(fileName)
-            Scans the dataset and stores in a list format
-        updateDatabases()
-            Update the database by removing aperiodic items and sort the Database by item decreased support
-        buildTree()
-            After updating the Database, remaining items will be added into the tree by setting root node as null
-        convert()
-            to convert the user specified value
-        PeriodicFrequentOneItems()
-            To extract the one-length periodic-frequent items
+        creatingItemSets()
+            Scans the dataset or dataframes and stores in list format
+        frequentOneItem()
+            Extracts the one-frequent patterns from transactions
 
     Execution methods
     =================
 
 
     **Terminal command**
 
 
     .. code-block:: console
 
        Format:
 
-       (.venv) $ python3 UPFPGrowthPlus.py <inputFile> <outputFile> <minSup> <maxPer>
+       (.venv) $ python3 SWFPGrowth.py <inputFile> <weightFile> <outputFile> <minSup> <minWeight>
 
-       Examples Usage:
+       Example usage :
 
-       (.venv) $ python3 UPFPGrowthPlus.py sampleTDB.txt patterns.txt 0.3 4
+       (.venv) $ python3 SWFPGrowth.py sampleDB.txt weightFile.txt patterns.txt 10  2
 
-    .. note:: minSup and maxPer will be considered in support count or frequency
+    .. note:: minSup will be considered in support count or frequency
 
 
     **Calling from a python program**
 
     .. code-block:: python
 
-            from PAMI.uncertainPeriodicFrequentPattern import UPFPGrowthPlus as alg
+            from PAMI.weightFrequentNeighbourhoodPattern.basic import SWFPGrowth as alg
+
+            obj = alg.SWFPGrowth(iFile, wFile, nFile, minSup, minWeight, seperator)
 
             iFile = 'sampleDB.txt'
 
             minSup = 10  # can also be specified between 0 and 1
 
-            maxPer = 2   # can also be specified between 0 and 1
-
-            obj = alg.UPFPGrowthPlus(iFile, minSup, maxPer)
-
             obj.mine()
 
-            periodicFrequentPatterns = obj.getPatterns()
+            frequentPatterns = obj.getPatterns()
 
-            print("Total number of uncertain Periodic Frequent Patterns:", len(periodicFrequentPatterns))
+            print("Total number of Frequent Patterns:", len(frequentPatterns))
 
             obj.save(oFile)
 
             Df = obj.getPatternsAsDataFrame()
 
-            memUSS = obj.getMemoryUSS()
+            memUSS = obj.getmemoryUSS()
 
             print("Total Memory in USS:", memUSS)
 
             memRSS = obj.getMemoryRSS()
 
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
+    Credits
+    =======
 
-    **Credits**:
-    --------------
-        The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.\n
 
-        """
-    _startTime = float()
-    _endTime = float()
-    _minSup = float()
-    _maxPer = float()
-    _finalPatterns = {}
+            The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
+
+    """
+
+    __startTime = float()
+    __endTime = float()
+    _Weights = {}
+    _minWS = str()
+    __finalPatterns = {}
+    _neighbourList = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
-    _memoryUSS = float()
-    _memoryRSS = float()
-    _Database = []
-    _rank = {}
-    _lno = 0
-    _periodic = {}
+    __memoryUSS = float()
+    __memoryRSS = float()
+    __Database = []
+    __mapSupport = {}
+    __lno = 0
+    __tree = _Tree()
+    __rank = {}
+    __rankDup = {}
 
-    def _creatingItemSets(self):
+    def __init__(self, iFile: Union[str, _fp._pd.DataFrame], nFile: Union[str, _fp._pd.DataFrame], minWS: Union[int, float, str], sep='\t') -> None:
+        super().__init__(iFile, nFile, minWS, sep)
+
+    def __creatingItemSets(self) -> None:
         """
         Storing the complete transactions of the database/input file in a database variable
-        """
 
+        :return: None
+        """
         self._Database = []
-        if isinstance(self._iFile, _ab._pd.DataFrame):
-            uncertain, data, ts = [], [], []
+        if isinstance(self._iFile, _fp._pd.DataFrame):
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
-            if 'TS' in i:
-                ts = self._iFile['TS'].tolist()
             if 'Transactions' in i:
-                data = self._iFile['Transactions'].tolist()
-            if 'uncertain' in i:
-                uncertain = self._iFile['uncertain'].tolist()
-            for k in range(len(data)):
-                tr = [ts[k]]
-                for j in range(len(k)):
-                    product = _Item(data[k][j], uncertain[k][j])
-                    tr.append(product)
-                self._Database.append(tr)
-                self._lno += 1
-
+                self._Database = self._iFile['Transactions'].tolist()
             # print(self.Database)
         if isinstance(self._iFile, str):
-            if _ab._validators.url(self._iFile):
-                data = _ab._urlopen(self._iFile)
+            if _fp._validators.url(self._iFile):
+                data = _fp._urlopen(self._iFile)
                 for line in data:
+                    line.strip()
                     line = line.decode("utf-8")
-                    line = line.strip()
-                    line = [i for i in line.split(':')]
-                    temp1 = [i.rstrip() for i in line[0].split(self._sep)]
-                    temp2 = [i.rstrip() for i in line[1].split(self._sep)]
-                    temp1 = [x for x in temp1 if x]
-                    temp2 = [x for x in temp2 if x]
-                    tr = [int(temp1[0])]
-                    for i in range(len(temp1[1:])):
-                        item = temp1[i]
-                        probability = float(temp2[i])
-                        product = _Item(item, probability)
-                        tr.append(product)
-                    self._lno += 1
-                    self._Database.append(tr)
+                    temp = [i.rstrip() for i in line.split(self._sep)]
+                    temp = [x for x in temp if x]
+                    self._Database.append(temp)
             else:
                 try:
-                    count = 0
-                    with open(self._iFile, 'r') as f:
+                    with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
                             line = line.strip()
-                            line = [i for i in line.split(':')]
+                            line = line.split(':')
                             temp1 = [i.rstrip() for i in line[0].split(self._sep)]
-                            temp2 = [i.rstrip() for i in line[1].split(self._sep)]
-                            temp1 = [x for x in temp1 if x]
-                            temp2 = [x for x in temp2 if x]
-                            tr = [int(temp1[0])]
-                            for i in range(len(temp1[1:])):
-                                item = temp1[i]
-                                probability = float(temp2[i])
-                                product = _Item(item, probability)
-                                tr.append(product)
-                            self._lno += 1
+                            temp2 = [int(i.strip()) for i in line[1].split(self._sep)]
+                            tr = []
+                            for i in range(len(temp1)):
+                                we = _WeightedItem(temp1[i], temp2[i])
+                                tr.append(we)
                             self._Database.append(tr)
                 except IOError:
                     print("File Not Found")
+                    quit()
 
-    def _PeriodicFrequentOneItems(self):
-        """
-        Takes the transactions and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
+    def _scanNeighbours(self) -> None:
         """
-        global first, last
-        mapSupport = {}
-        for i in self._Database:
-            n = int(i[0])
-            for j in i[1:]:
-                if j.item not in mapSupport:
-                    mapSupport[j.item] = [round(j.probability, 3), abs(0 - n), n]
-                else:
-                    mapSupport[j.item][0] += round(j.probability, 2)
-                    mapSupport[j.item][1] = max(mapSupport[j.item][1], abs(n - mapSupport[j.item][2]))
-                    mapSupport[j.item][2] = n
-        for key in mapSupport:
-            mapSupport[key][1] = max(mapSupport[key][1], self._lno - mapSupport[key][2])
-        mapSupport = {k: [round(v[0], 2), v[1]] for k, v in mapSupport.items() if
-                      v[1] <= self._maxPer and v[0] >= self._minSup}
-        plist = [k for k, v in sorted(mapSupport.items(), key=lambda x: (x[1][0], x[0]), reverse=True)]
-        self._rank = dict([(index, item) for (item, index) in enumerate(plist)])
-        return mapSupport, plist
-
-    def _buildTree(self, data, info):
-        """
-        It takes the transactions and support of each item and construct the main tree with setting root node as null
-
-        :param data : it represents the one transaction in database
-        :type data : list
-        :param info : it represents the support of each item
-        :type info : dictionary
-        """
-        rootNode = _Tree()
-        rootNode.info = info.copy()
-        for i in range(len(data)):
-            set1 = [data[i][0]]
-            rootNode.addTransaction(data[i][1:], set1)
-            #printTree(rootNode)
-            #print("....")
-        return rootNode
+        Scans the neighbors file and creates a dictionary of items and their corresponding neighbor lists.
 
-    def _updateTransactions(self, dict1):
+        :return: None
         """
-        Remove the items which are not frequent from transactions and updates the transactions with rank of items
-
-        :param dict1 : frequent items with support
-        :type dict1 : dictionary
-        """
-        list1 = []
-        for tr in self._Database:
-            list2 = [int(tr[0])]
-            for i in range(1, len(tr)):
-                if tr[i].item in dict1:
-                    list2.append(tr[i])
-            if len(list2) >= 2:
-                basket = list2[1:]
-                basket.sort(key=lambda val: self._rank[val.item])
-                list2[1:] = basket[0:]
-                list1.append(list2)
-        return list1
-
-    def _Check(self, i, x):
-        """
-        To check the presence of item or pattern in transaction
-
-        :param x: it represents the pattern
-        :type x : list
-        :param i : represents the uncertain transactions
-        :type i : list
-        """
-        for m in x:
-            k = 0
-            for n in i:
-                if m == n.item:
-                    k += 1
-            if k == 0:
-                return 0
-        return 1
+        self._neighbourList = {}
+        if isinstance(self._nFile, _fp._pd.DataFrame):
+            data, items = [], []
+            if self._nFile.empty:
+                print("its empty..")
+            i = self._nFile.columns.values.tolist()
+            if 'item' in i:
+                items = self._nFile['items'].tolist()
+            if 'Neighbours' in i:
+                data = self._nFile['Neighbours'].tolist()
+            for k in range(len(items)):
+                self._neighbourList[items[k][0]] = data[k]
+            # print(self.Database)
+        if isinstance(self._nFile, str):
+            if _fp._validators.url(self._nFile):
+                data = _fp._urlopen(self._nFile)
+                for line in data:
+                    line.strip()
+                    line = line.decode("utf-8")
+                    temp = [i.rstrip() for i in line.split(self._sep)]
+                    temp = [x for x in temp if x]
+                    self._neighbourList[temp[0]] = temp[1:]
+            else:
+                try:
+                    with open(self._nFile, 'r', encoding='utf-8') as f:
+                        for line in f:
+                            line.strip()
+                            temp = [i.rstrip() for i in line.split(self._sep)]
+                            temp = [x for x in temp if x]
+                            self._neighbourList[temp[0]] = temp[1:]
+                except IOError:
+                    print("File Not Found2")
+                    quit()
 
-    def _convert(self, value):
+    def __convert(self, value: Union[int, float, str]) -> Union[int, float]:
         """
-        To convert the given user specified value
+        To convert the type of user specified minWS value
 
-        :param value: user specified value
-        :return: converted value
+        :param value: user specified minWS value
+        :return: converted type
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
-            value = float(value)
+            value = (len(self._Database) * value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
+                value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
-    def _removeFalsePositives(self):
+    def __frequentOneItem(self) -> List[str]:
         """
-        To remove false positives in generated patterns
+        Generating One frequent items sets
 
-        :return: original patterns
+        :return: None
         """
-        periods = {}
-        for i in self._Database:
-            for x, y in self._periodic.items():
-                if len(x) == 1:
-                    periods[x] = y
+        global _maxWeight
+        self._mapSupport = {}
+        for tr in self._Database:
+            for i in tr:
+                nn = [j for j in tr if j.item in self._neighbourList[i.item]]
+                if i.item not in self._mapSupport:
+                    self._mapSupport[i.item] = i.weight
                 else:
-                    s = 1
-                    check = self._Check(i[1:], x)
-                    if check == 1:
-                        for j in i[1:]:
-                            if j.item in x:
-                                s *= j.probability
-                        if x in periods:
-                            periods[x][0] += s
-                        else:
-                            periods[x] = [s, y[1]]
-        count = 0
-        for x, y in periods.items():
-            if y[0] >= _minSup:
-                count += 1
-                sample = str()
-                for i in x:
-                    sample = sample + i + " "
-                self._finalPatterns[sample] = y
-        #print("Total false patterns generated:", len(self._periodic) - count)
+                    self._mapSupport[i.item] += i.weight
+                for k in nn:
+                    self._mapSupport[i.item] += k.weight
+        self._mapSupport = {k: v for k, v in self._mapSupport.items() if v >= self._minWS}
+        genList = [k for k, v in sorted(self._mapSupport.items(), key=lambda x: x[1], reverse=True)]
+        self.__rank = dict([(index, item) for (item, index) in enumerate(genList)])
+        return genList
+
+    def __updateTransactions(self, itemSet: List[str]) -> List[List[_WeightedItem]]:
+        """
+        Updates the items in transactions with rank of items according to their support
+
+        :Example: oneLength = {'a':7, 'b': 5, 'c':'4', 'd':3}
+                  rank = {'a':0, 'b':1, 'c':2, 'd':3}
+        :param itemSet: list of one-frequent items
+        :return: list
+        """
+        list1 = []
+        for tr in self._Database:
+            list2 = []
+            for i in range(len(tr)):
+                if tr[i].item in itemSet:
+                    list2.append(tr[i])
+            if len(list2) >= 1:
+                basket = list2
+                basket.sort(key=lambda val: self.__rank[val.item])
+                list1.append(basket)
+        return list1
+
+    @staticmethod
+    def __buildTree(transactions: List[List[_WeightedItem]], info: Dict[int, float]) -> _Tree:
+        """
+        Builds the tree with updated transactions
+
+        :param transactions: updated transactions
+        :param info: support details of each item in transactions.
+        :return: transactions compressed in fp-tree.
+        """
+        rootNode = _Tree()
+        rootNode.info = info.copy()
+        for i in range(len(transactions)):
+            rootNode.addTransaction(transactions[i], 1)
+        return rootNode
+
+    def __savePeriodic(self, itemSet: List[str]) -> str:
+        """
+        The duplication items and their ranks
+
+        :param itemSet: frequent itemSet that generated
+        :return: patterns with original item names.
+
+        """
+        temp = str()
+        for i in itemSet:
+            temp = temp + self.__rankDup[i] + "\t"
+        return temp
 
     @deprecated(
          "It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
-    def startMine(self):
+    def startMine(self) -> None:
         """
-        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
+        Frequent pattern mining process will start from here
         """
         self.mine()
 
-    def mine(self):
+    def mine(self) -> None:
         """
-        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
+        Frequent pattern mining process will start from here
         """
-        global _minSup, _maxPer, _first, _last, _lno
-        self._startTime = _ab._time.time()
-        self._creatingItemSets()
-        self._minSup = self._convert(self._minSup)
-        self._maxPer = self._convert(self._maxPer)
-        self._finalPatterns = {}
-        _minSup, _maxPer, _lno = self._minSup, self._maxPer, len(self._Database)
-        mapSupport, plist = self._PeriodicFrequentOneItems()
-        updatedTrans = self._updateTransactions(mapSupport)
-        info = {k: v for k, v in mapSupport.items()}
-        root = self._buildTree(updatedTrans, info)
-        self._periodic = {}
-        root.generatePatterns([], self._periodic)
-        self._removeFalsePositives()
-        print("Periodic Frequent patterns were generated successfully using UPFP-Growth++ algorithm")
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
+        global _minWS, _neighbourList, _rank
+        self.__startTime = _fp._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._minWS is None:
+            raise Exception("Please enter the Minimum Support")
+        self.__creatingItemSets()
+        self._scanNeighbours()
+        self._minWS = self.__convert(self._minWS)
+        _minWS = self._minWS
+        itemSet = self.__frequentOneItem()
+        updatedTransactions = self.__updateTransactions(itemSet)
+        info = {self.__rank[k]: v for k, v in self._mapSupport.items()}
+        _rank = self.__rank
+        for x, y in self.__rank.items():
+            self.__rankDup[y] = x
+        _neighbourList = self._neighbourList
+        #self._neighbourList = {k:v for k, v in self._neighbourList.items() if k in self._mapSupport.keys()}
+        # for x, y in self._neighbourList.items():
+        #     xx = [self.__rank[i] for i in y if i in self._mapSupport.keys()]
+        #     _neighbourList[self.__rank[x]] = xx
+        # print(_neighbourList)
+        __Tree = self.__buildTree(updatedTransactions, info)
+        patterns = __Tree.generatePatterns([])
+        self.__finalPatterns = {}
+        for k in patterns:
+            s = self.__savePeriodic(k[0])
+            self.__finalPatterns[str(s)] = k[1]
+        print("Weighted Frequent patterns were generated successfully using SWFPGrowth algorithm")
+        self.__endTime = _fp._time.time()
+        self.__memoryUSS = float()
+        self.__memoryRSS = float()
+        process = _fp._psutil.Process(_fp._os.getpid())
+        self.__memoryUSS = process.memory_full_info().uss
+        self.__memoryRSS = process.memory_info().rss
 
-    def getMemoryUSS(self):
+    def getMemoryUSS(self) -> float:
         """
 
-        Total amount of USS memory consumed by the mining process will be retrieved from this function.
+        Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
-        return self._memoryUSS
+        return self.__memoryUSS
 
-    def getMemoryRSS(self):
+    def getMemoryRSS(self) -> float:
         """
 
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
-        return self._memoryRSS
+        return self.__memoryRSS
 
-    def getRuntime(self):
+    def getRuntime(self) -> float:
         """
 
         Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
-        return self._endTime - self._startTime
 
-    def getPatternsAsDataFrame(self):
+        return self.__endTime - self.__startTime
+
+    def getPatternsAsDataFrame(self) -> _fp._pd.DataFrame:
         """
 
         Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataframe = {}
         data = []
-        for a, b in self._finalPatterns.items():
-            data.append([a, b[0], b[1]])
-            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
+        for a, b in self.__finalPatterns.items():
+            data.append([a.replace('\t', ' '), b])
+            dataframe = _fp._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataframe
 
-    def save(self, outFile):
+    def save(self, outFile: str) -> None:
         """
 
         Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: csv file
+        :return: None
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
-        for x, y in self._finalPatterns.items():
-            s1 = x + ":" + str(y)
+        for x, y in self.__finalPatterns.items():
+            s1 = x.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
-    def getPatterns(self):
+    def getPatterns(self) -> Dict[str, float]:
         """
 
         Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
-        return self._finalPatterns
+        return self.__finalPatterns
 
-    def printResults(self):
+    def printResults(self) -> None:
         """
         This function is used to print the results
         """
-        print("Total number of  Uncertain Periodic-Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of  Weighted Spatial Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:",  self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
-        if len(_ab._sys.argv) == 6:
-            _ap = UPFPGrowthPlus(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
-        if len(_ab._sys.argv) == 5:
-            _ap = UPFPGrowthPlus(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+    if len(_fp._sys.argv) == 7 or len(_fp._sys.argv) == 8:
+        if len(_fp._sys.argv) == 8:
+            _ap = SWFPGrowth(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4], _fp._sys.argv[5], _fp._sys.argv[6],
+                             _fp._sys.argv[7])
+        if len(_fp._sys.argv) == 7:
+            _ap = SWFPGrowth(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4], _fp._sys.argv[5], _fp._sys.argv[6])
         _ap.startMine()
         _ap.mine()
-        _Patterns = _ap.getPatterns()
-        print("Total number of Patterns:", len(_Patterns))
-        _ap.savePatterns(_ab._sys.argv[2])
-        # print(ap.getPatternsAsDataFrame())
-        _memUSS = _ap.getMemoryUSS()
-        print("Total Memory in USS:", _memUSS)
-        _memRSS = _ap.getMemoryRSS()
-        print("Total Memory in RSS", _memRSS)
-        _run = _ap.getRuntime()
-        print("Total ExecutionTime in ms:", _run)
+        print("Total number of Weighted Spatial Frequent Patterns:", len(_ap.getPatterns()))
+        _ap.save(_fp._sys.argv[2])
+        print("Total Memory in USS:", _ap.getMemoryUSS())
+        print("Total Memory in RSS",  _ap.getMemoryRSS())
+        print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
+        _ap = SWFPGrowth('sample.txt', 'neighbourSample.txt', 150, ' ')
+        _ap.startMine()
+        print("Total number of Weighted Spatial Frequent Patterns:", len(_ap.getPatterns()))
+        _ap.save('output.txt')
+        print("Total Memory in USS:", _ap.getMemoryUSS())
+        print("Total Memory in RSS", _ap.getMemoryRSS())
+        print("Total ExecutionTime in ms:", _ap.getRuntime())
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.5.28.1/PAMI/uncertainPeriodicFrequentPattern/basic/abstract.py` & `pami-2024.5.7.1/PAMI/uncertainPeriodicFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/weightedFrequentNeighbourhoodPattern/basic/abstract.py` & `pami-2024.5.7.1/PAMI/weightedFrequentNeighbourhoodPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/weightedFrequentPattern/basic/WFIM.py` & `pami-2024.5.7.1/PAMI/weightedUncertainFrequentPattern/basic/WUFIM.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,28 +1,26 @@
-# WFMiner is one of the fundamental algorithm to discover weighted frequent patterns in a transactional database.
-# It stores the database in compressed fp-tree decreasing the memory usage and extracts the
-# patterns from tree.It employs downward closure property to  reduce the search space effectively.
+# WUFIM is one of the algorithm to discover weighted frequent patterns in an uncertain transactional database using PUF-Tree.
 #
 # **Importing this algorithm into a python program**
 #
-#             from PAMI.weightFrequentPattern.basic import basic as alg
+#             from PAMI.weightedUncertainFrequentPattern.basic import basic as alg
 #
 #             iFile = 'sampleDB.txt'
 #
-#             minSup = 10  # can also be specified between 0 and 1
+#             minSup = 10
 #
-#             obj = alg.basic(iFile, wFile, minSup, minWeight)
+#             obj = alg.basic(iFile, wFile, minSup, sep)
 #
 #             obj.mine()
 #
-#             frequentPatterns = obj.getPatterns()
+#             Patterns = obj.getPatterns()
 #
-#             print("Total number of Frequent Patterns:", len(frequentPatterns))
+#             print("Total number of  Patterns:", len(Patterns))
 #
-#             obj.savePatterns(oFile)
+#             obj.save(oFile)
 #
 #             Df = obj.getPatternsAsDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
 #             print("Total Memory in USS:", memUSS)
 #
@@ -31,703 +29,851 @@
 #             print("Total Memory in RSS", memRSS)
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
+
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
-     along with this program.  If not, see <https://www.gnu.org/licenses/>.
+     along with this program.  If not, see `<https://www.gnu.org/licenses/>`_.    
 """
 
-from PAMI.weightedFrequentPattern.basic import abstract as _fp
-from typing import List, Dict, Tuple, Union, Generator
+from PAMI.weightedUncertainFrequentPattern.basic import abstract as _ab
 import pandas as pd
 from deprecated import deprecated
 
-
-_minSup = str()
-_minWeight = int()
-_miniWeight = int()
-_maxWeight = int()
+_expSup = str()
+_expWSup = str()
 _weights = {}
-_fp._sys.setrecursionlimit(20000)
+_finalPatterns = {}
+_ab._sys.setrecursionlimit(20000)
+class _Item:
+    """
+    A class used to represent the item with probability in transaction of dataset
 
+    :Attributes:
 
-class _Node:
+        item : int or word
+          Represents the name of the item
+
+        probability : float
+          Represent the existential probability(likelihood presence) of an item
+    """
+
+    def __init__(self, item: int, probability: float) -> None:
+        self.item = item
+        self.probability = probability
+
+
+class _Node(object):
     """
     A class used to represent the node of frequentPatternTree
 
     :Attributes:
 
-        itemId: int
-            storing item of a node
+        item : int
+          storing item of a node
 
-        counter: int
-            To maintain the support of node
+        probability : int
+          To maintain the expected support of node
 
-        parent: node
-            To maintain the parent of node
+        parent : node
+          To maintain the parent of every node
 
-        children: list
-            To maintain the children of node
+        children : list
+          To maintain the children of node
 
     :Methods:
 
-        addChild(node)
-            Updates the nodes children list and parent for the given node
+        addChild(itemName)
+            storing the children to their respective parent nodes
     """
 
-    def __init__(self, item: str, children: list) -> None:
-        self.itemId = item
-        self.counter = 1
-        self.parent = None
+    def __init__(self, item, children: list) -> None:
+        self.item = item
+        self.probability = 1
         self.children = children
+        self.parent = None
 
-    def addChild(self, node: '_Node') -> None:
+    def addChild(self, node) -> None:
         """
-        Retrieving the child from the tree
+        This method is used to add a child node to the current node in the frequent pattern tree.
 
-        :param node: Children node
-        :type node: _Node
-        :return: Updates the children nodes and parent nodes
+        :param node:The node to be added as a child
+        :type node:_Node
+        :return: None
         """
-        self.children[node.itemId] = node
+        self.children[node.item] = node
         node.parent = self
 
 
-class _Tree:
+class _Tree(object):
     """
     A class used to represent the frequentPatternGrowth tree structure
 
     :Attributes:
 
         root : Node
-            The first node of the tree set to Null.
+            Represents the root node of the tree
 
         summaries : dictionary
-            Stores the nodes itemId which shares same itemId
+            storing the nodes with same item name
 
         info : dictionary
-            frequency of items in the transactions
+            stores the support of items
 
     :Methods:
 
-        addTransaction(transaction, freq)
-            adding items of  transactions into the tree as nodes and freq is the count of nodes
-        getFinalConditionalPatterns(node)
-            getting the conditional patterns from fp-tree for a node
-        getConditionalPatterns(patterns, frequencies)
-            sort the patterns by removing the items with lower minSup
-        generatePatterns(prefix)
-            generating the patterns from fp-tree
+        addTransaction(transaction)
+            creating transaction as a branch in frequentPatternTree
+        addConditionalPattern(prefixPaths, supportOfItems)
+            construct the conditional tree for prefix paths
+        conditionalPatterns(Node)
+            generates the conditional patterns from tree for specific node
+        conditionalTransactions(prefixPaths,Support)
+            takes the prefixPath of a node and support at child of the path and extract the frequent items from prefixPaths and generates prefixPaths with items which are frequent
+        remove(Node)
+            removes the node from tree once after generating all the patterns respective to the node
+        generatePatterns(Node)
+            starts from the root node of the tree and mines the frequent patterns
+
     """
 
     def __init__(self) -> None:
         self.root = _Node(None, {})
         self.summaries = {}
         self.info = {}
 
-    def addTransaction(self, transaction: List[str], count: int) -> None:
+    def addTransaction(self, transaction) -> None:
         """
         Adding transaction into tree
 
-        :param transaction: it represents the one transaction in database
+        :param transaction: it represents the one self.Database in database
         :type transaction: list
-        :param count: frequency of item
-        :type count: int
         :return: None
         """
-        # This method takes transaction as input and returns the tree
+        currentNode = self.root
+        for i in range(len(transaction)):
+            if transaction[i].item not in currentNode.children:
+                newNode = _Node(transaction[i].item, {})
+                l1 = i - 1
+                lp = []
+                while l1 >= 0:
+                    lp.append(transaction[l1].probability)
+                    l1 -= 1
+                if len(lp) == 0:
+                    newNode.probability = transaction[i].probability
+                else:
+                    newNode.probability = max(lp) * transaction[i].probability
+                currentNode.addChild(newNode)
+                if transaction[i].item in self.summaries:
+                    self.summaries[transaction[i].item].append(newNode)
+                else:
+                    self.summaries[transaction[i].item] = [newNode]
+                currentNode = newNode
+            else:
+                currentNode = currentNode.children[transaction[i].item]
+                l1 = i - 1
+                lp = []
+                while l1 >= 0:
+                    lp.append(transaction[l1].probability)
+                    l1 -= 1
+                if len(lp) == 0:
+                    currentNode.probability += transaction[i].probability
+                else:
+                    currentNode.probability += max(lp) * transaction[i].probability
+
+    def addConditionalPattern(self, transaction, sup) -> None:
+        """
+        constructing conditional tree from prefixPaths
+
+        :param transaction : it represents the one self.Database in database
+        :type transaction : list
+        :param sup : support of prefixPath taken at last child of the path
+        :type sup : int
+        :return: None
+        """
+        # This method takes transaction, support and constructs the conditional tree
         currentNode = self.root
         for i in range(len(transaction)):
             if transaction[i] not in currentNode.children:
                 newNode = _Node(transaction[i], {})
-                newNode.freq = count
+                newNode.probability = sup
                 currentNode.addChild(newNode)
                 if transaction[i] in self.summaries:
                     self.summaries[transaction[i]].append(newNode)
                 else:
                     self.summaries[transaction[i]] = [newNode]
                 currentNode = newNode
             else:
                 currentNode = currentNode.children[transaction[i]]
-                currentNode.freq += count
+                currentNode.probability += sup
 
-    def getFinalConditionalPatterns(self, alpha: str) -> Tuple[List[List[str]], List[int], Dict[str, int]]:
+    def conditionalPatterns(self, alpha) -> tuple:
         """
-        Generates the conditional patterns for a node
+        generates all the conditional patterns of respective node
 
-        :param alpha: node to generate conditional patterns
-        :return: returns conditional patterns, frequency of each item in conditional patterns
+        :param alpha : it represents the Node in tree
+        :type alpha : _Node
+        :return: tuple
         """
+        # This method generates conditional patterns of node by traversing the tree
         finalPatterns = []
-        finalFreq = []
+        sup = []
         for i in self.summaries[alpha]:
-            set1 = i.freq
+            s = i.probability
             set2 = []
-            while i.parent.itemId is not None:
-                set2.append(i.parent.itemId)
+            while i.parent.item is not None:
+                set2.append(i.parent.item)
                 i = i.parent
             if len(set2) > 0:
                 set2.reverse()
                 finalPatterns.append(set2)
-                finalFreq.append(set1)
-        finalPatterns, finalFreq, info = self.getConditionalTransactions(finalPatterns, finalFreq)
-        return finalPatterns, finalFreq, info
+                sup.append(s)
+        finalPatterns, support, info = self.conditionalTransactions(finalPatterns, sup)
+        return finalPatterns, support, info
 
-    @staticmethod
-    def getConditionalTransactions(ConditionalPatterns: List[List[str]], conditionalFreq: List[int]) -> Tuple[List[List[str]], List[int], Dict[str, int]]:
+    def removeNode(self, nodeValue) -> None:
+        """
+        Removing the node from tree
+
+        :param nodeValue : it represents the node in tree
+        :type nodeValue : node
+        :return: None
+        """
+
+        for i in self.summaries[nodeValue]:
+            del i.parent.children[nodeValue]
+
+    def conditionalTransactions(self, condPatterns, support) -> tuple:
         """
-        To calculate the frequency of items in conditional patterns and sorting the patterns
+        It generates the conditional patterns with frequent items
 
-        :param ConditionalPatterns: paths of a node
-        :param conditionalFreq: frequency of each item in the path
-        :return: conditional patterns and frequency of each item in transactions
+        :param condPatterns : conditionalPatterns generated from conditionalPattern method for respective node
+        :type condPatterns : list
+        :support : the support of conditional pattern in tree
+        :support : int
+        :return: tuple
         """
-        global _minSup, _miniWeight
+        global _expSup, _expWSup
         pat = []
-        freq = []
-        data1 = {}
-        for i in range(len(ConditionalPatterns)):
-            for j in ConditionalPatterns[i]:
-                if j in data1:
-                    data1[j] += conditionalFreq[i]
+        sup = []
+        count = {}
+        for i in range(len(condPatterns)):
+            for j in condPatterns[i]:
+                if j in count:
+                    count[j] += support[i]
                 else:
-                    data1[j] = conditionalFreq[i]
-        up_dict = {k: v for k, v in data1.items() if v >= _minSup and v * _miniWeight > _minSup}
+                    count[j] = support[i]
+        updatedDict = {}
+        updatedDict = {k: v for k, v in count.items() if v >= _expSup}
         count = 0
-        for p in ConditionalPatterns:
-            p1 = [v for v in p if v in up_dict]
-            trans = sorted(p1, key=lambda x: (up_dict.get(x), -x), reverse=True)
+        for p in condPatterns:
+            p1 = [v for v in p if v in updatedDict]
+            trans = sorted(p1, key=lambda x: updatedDict[x], reverse=True)
             if len(trans) > 0:
                 pat.append(trans)
-                freq.append(conditionalFreq[count])
-            count += 1
-        return pat, freq, up_dict
+                sup.append(support[count])
+                count += 1
+        return pat, sup, updatedDict
 
-    def generatePatterns(self, prefix: List[str]) -> Generator[Tuple[List[str], int], None, None]:
+    def generatePatterns(self, prefix) -> None:
         """
-        To generate the frequent patterns
+        Generates the patterns
 
-        :param prefix: an empty list
-        :return: Frequent patterns that are extracted from fp-tree
+        :param prefix : forms the combination of items
+        :type prefix : list
+        :return: None
         """
-        global _miniWeight, _maxWeight, _minWeight, _minSup
-        for i in sorted(self.summaries, key=lambda x: (self.info.get(x), -x)):
+
+        global _finalPatterns, _expSup, _expWSup, _weights
+        for i in sorted(self.summaries, key=lambda x: (self.info.get(x))):
             pattern = prefix[:]
             pattern.append(i)
-            yield pattern, self.info[i]
-            patterns, freq, info = self.getFinalConditionalPatterns(i)
-            conditionalTree = _Tree()
-            conditionalTree.info = info.copy()
-            for pat in range(len(patterns)):
-                conditionalTree.addTransaction(patterns[pat], freq[pat])
-            if len(patterns) > 0:
-                for q in conditionalTree.generatePatterns(pattern):
-                    yield q
-
+            weight = 0
+            for k in pattern:
+                weight = weight + _weights[k]
+            weight = weight/len(pattern)
+            if self.info.get(i) >= _expSup and self.info.get(i) * weight >= _expWSup:
+                _finalPatterns[tuple(pattern)] = self.info.get(i)
+                patterns, support, info = self.conditionalPatterns(i)
+                conditionalTree = _Tree()
+                conditionalTree.info = info.copy()
+                for pat in range(len(patterns)):
+                    conditionalTree.addConditionalPattern(patterns[pat], support[pat])
+                if len(patterns) > 0:
+                    conditionalTree.generatePatterns(pattern)
+            self.removeNode(i)
 
-class WFIM(_fp._weightedFrequentPatterns):
+class WUFIM(_ab._weightedFrequentPatterns):
     """
     About this algorithm
     ====================
 
-    :Description: * WFMiner is one of the fundamental algorithm to discover weighted frequent patterns in a transactional database.
-                  * It stores the database in compressed fp-tree decreasing the memory usage and extracts the patterns from tree.It employs employs downward closure property to  reduce the search space effectively.
+    :Description: It is one of the algorithm to discover weighted frequent patterns in a uncertain transactional database using PUF-Tree.
 
-    :Reference:  U. Yun and J. J. Leggett, Wfim: weighted frequent itemset mining with a weight range and a minimum weight,
-           In:   Proceedings of the 2005 SIAM International Conference on Data Mining. SIAM, 2005, pp. 636640.
-                 https://epubs.siam.org/doi/pdf/10.1137/1.9781611972757.76
+    :Reference: Efficient Mining of Weighted Frequent Itemsets in Uncertain Databases.
+           In : Machine Learning and Data Mining in Pattern Recognition book Chun-Wei Jerry Lin, Wensheng Gan, Philippe Fournier Viger, Tzung-Pei Hong
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of weighted Frequent Patterns.
+                   Name of the Input file to mine complete set of Weighted Uncertain Periodic Frequent Patterns
     :param  oFile: str :
-                   Name of the output file to store complete set of weighted Frequent Patterns.
-    :param  minSup: str or int or float:
+                   Name of the output file to store complete set of Weighted  Uncertain Periodic Frequent Patterns
+    :param  minSup: str:
                    minimum support thresholds were tuned to find the appropriate ranges in the limited memory
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+    :param  wFile: str :
+                    This is a weighted file.
 
 
-    :Attributes :
+    :Attributes:
 
         iFile : file
-            Input file name or path of the input file
-
-        minSup: float or int or str
-            The user can specify minSup either in count or proportion of database size.
-            If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
-            Otherwise, it will be treated as float.
-            Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
-
-        minWeight: float or int or str
-            The user can specify minWeight either in count or proportion of database size.
-            If the program detects the data type of minWeight is integer, then it treats minWeight is expressed in count.
-            Otherwise, it will be treated as float.
-            Example: minWeight=10 will be treated as integer, while minWeight=10.0 will be treated as float
+          Name of the Input file or path of the input file
 
-        sep : str
-            This variable is used to distinguish items from one another in a transaction. The default separator is tab space or \t.
-            However, the users can override their default separator.
+        wFile : file
+          Name of the Input file or path of the input file
 
         oFile : file
-            Name of the output file or the path of the output file
+          Name of the output file or path of the output file
 
-        startTime:float
-            To record the start time of the mining process
+        minSup : float or int or str
+          The user can specify minSup either in count or proportion of database size.
+          If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+          Otherwise, it will be treated as float.
+          Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
 
-        endTime:float
-            To record the completion time of the mining process
+        sep : str
+          This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
+          However, the users can override their default separator.
 
         memoryUSS : float
-            To store the total amount of USS memory consumed by the program
+          To store the total amount of USS memory consumed by the program
 
         memoryRSS : float
-            To store the total amount of RSS memory consumed by the program
+          To store the total amount of RSS memory consumed by the program
+
+        startTime:float
+          To record the start time of the mining process
+
+        endTime:float
+          To record the completion time of the mining process
 
         Database : list
-            To store the transactions of a database in list
+          To store the transactions of a database in list
 
         mapSupport : Dictionary
-            To maintain the information of item and their frequency
+          To maintain the information of item and their frequency
 
         lno : int
-            it represents the total no of transactions
+          To represent the total no of transaction
 
         tree : class
-            it represents the Tree class
+          To represents the Tree class
+
+        itemSetCount : int
+          To represents the total no of patterns
 
         finalPatterns : dict
-            it represents to store the patterns
+          To store the complete patterns
 
-    :Methods :
+    :Methods:
 
-        mine()
+        startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
             Complete set of frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
-        creatingItemSets()
-            Scans the dataset or dataframes and stores in list format
+        creatingItemSets(fileName)
+            Scans the dataset and stores in a list format
         frequentOneItem()
-            Extracts the one-frequent patterns from transactions
+            Extracts the one-length frequent patterns from database
+        updateTransactions()
+            Update the transactions by removing non-frequent items and sort the Database by item decreased support
+        buildTree()
+            After updating the Database, remaining items will be added into the tree by setting root node as null
+        convert()
+            to convert the user specified value
+        startMine()
+            Mining process will start from this function
 
     Execution methods
     =================
 
 
     **Terminal command**
 
 
     .. code-block:: console
 
-       Format:
-
-       (.venv) $ python3 basic.py <inputFile> <weightFile> <outputFile> <minSup> <minWeight>
+      Format:
 
-       Example Usage:
+      (.venv) $ python3 basic.py <inputFile> <outputFile> <minSup>
 
-       (.venv) $ python3 basic.py sampleDB.txt weightSample.txt patterns.txt 10.0 3.4
+      Example Usage:
 
-    .. note:: minSup and maxPer will be considered in support count or frequency
+      (.venv) $ python3 basic.py sampleTDB.txt patterns.txt 3
 
+    .. note:: minSup  will be considered in support count or frequency
 
     **Calling from a python program**
 
     .. code-block:: python
 
-            from PAMI.weightFrequentPattern.basic import basic as alg
+            from PAMI.weightedUncertainFrequentPattern.basic import basic as alg
 
             iFile = 'sampleDB.txt'
 
             minSup = 10  # can also be specified between 0 and 1
 
-            obj = alg.basic(iFile, wFile, minSup, minWeight)
+            obj = alg.basic(iFile, wFile, expSup, expWSup)
 
             obj.mine()
 
-            frequentPatterns = obj.getPatterns()
+            Patterns = obj.getPatterns()
 
-            print("Total number of Frequent Patterns:", len(frequentPatterns))
+            print("Total number of  Patterns:", len(Patterns))
 
-            obj.savePatterns(oFile)
+            obj.save(oFile)
 
             Df = obj.getPatternsAsDataFrame()
 
-            memUSS = obj.getmemoryUSS()
+            memUSS = obj.getMemoryUSS()
 
             print("Total Memory in USS:", memUSS)
 
             memRSS = obj.getMemoryRSS()
 
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
-    Credits
-    =======
-
-
-             The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
-    """
-
-    __startTime = float()
-    __endTime = float()
+   """
+    _startTime = float()
+    _endTime = float()
     _minSup = str()
-    __finalPatterns = {}
+    _finalPatterns = {}
     _iFile = " "
+    _wFile = " "
     _oFile = " "
     _sep = " "
-    __memoryUSS = float()
-    __memoryRSS = float()
-    __Database = []
-    __mapSupport = {}
-    __lno = 0
-    __tree = _Tree()
-    __rank = {}
-    __rankDup = {}
+    _memoryUSS = float()
+    _memoryRSS = float()
+    _Database = []
+    _rank = {}
+    _expSup = float()
+    _expWSup = float()
 
-    def __init__(self, iFile: str, wFile: str, minSup: str, minWeight: int, sep: str='\t') -> None:
-        super().__init__(iFile, wFile, minSup, minWeight, sep)
+    def __init__(self, iFile, wFile, expSup, expWSup, sep='\t') -> None:
+        super().__init__(iFile, wFile, expSup, expWSup, sep)
 
-    def __creatingItemSets(self) -> None:
+    def _creatingItemSets(self) -> None:
         """
-        Storing the complete transactions of the database/input file in a database variable
+        Scans the uncertain transactional dataset
 
         :return: None
         """
-        self.__Database = []
-        if isinstance(self._iFile, _fp._pd.DataFrame):
+        self._Database = []
+        if isinstance(self._iFile, _ab._pd.DataFrame):
+            uncertain, data = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
-                self.__Database = self._iFile['Transactions'].tolist()
+                self._Database = self._iFile['Transactions'].tolist()
+            if 'uncertain' in i:
+                uncertain = self._iFile['uncertain'].tolist()
+            for k in range(len(data)):
+                tr = []
+                for j in range(len(data[k])):
+                    product = _Item(data[k][j], uncertain[k][j])
+                    tr.append(product)
+                self._Database.append(tr)
 
             # print(self.Database)
         if isinstance(self._iFile, str):
-            if _fp._validators.url(self._iFile):
-                data = _fp._urlopen(self._iFile)
+            if _ab._validators.url(self._iFile):
+                data = _ab._urlopen(self._iFile)
                 for line in data:
-                    line.strip()
                     line = line.decode("utf-8")
-                    temp = [i.rstrip() for i in line.split(self._sep)]
-                    temp = [x for x in temp if x]
-                    self.__Database.append(temp)
+                    line = line.strip()
+                    line = [i for i in line.split(':')]
+                    temp1 = [i.rstrip() for i in line[0].split(self._sep)]
+                    temp2 = [i.rstrip() for i in line[1].split(self._sep)]
+                    temp1 = [x for x in temp1 if x]
+                    temp2 = [x for x in temp2 if x]
+                    tr = []
+                    for i in range(len(temp1)):
+                        item = temp1[i]
+                        probability = float(temp2[i])
+                        product = _Item(item, probability)
+                        tr.append(product)
+                    self._Database.append(tr)
             else:
                 try:
-                    with open(self._iFile, 'r', encoding='utf-8') as f:
+                    with open(self._iFile, 'r') as f:
                         for line in f:
-                            line.strip()
-                            temp = [i.rstrip() for i in line.split(self._sep)]
-                            temp = [x for x in temp if x]
-                            # print(len(temp))
-                            self.__Database.append(temp)
+                            line = line.strip()
+                            line = [i for i in line.split(':')]
+                            temp1 = [i.rstrip() for i in line[0].split(self._sep)]
+                            temp2 = [i.rstrip() for i in line[1].split(self._sep)]
+                            temp1 = [x for x in temp1 if x]
+                            temp2 = [x for x in temp2 if x]
+                            tr = []
+                            for i in range(len(temp1)):
+                                item = temp1[i]
+                                probability = float(temp2[i])
+                                product = _Item(item, probability)
+                                tr.append(product)
+                            self._Database.append(tr)
                 except IOError:
                     print("File Not Found")
-                    quit()
 
     def _scanningWeights(self) -> None:
         """
-        Storing the weights of the variables in input file in a weights variable
+        Scans the uncertain transactional dataset
 
         :return: None
         """
-        global _weights
-        _weights = {}
-        if isinstance(self._wFile, _fp._pd.DataFrame):
-            items, weights = [], []
+        self._weights = {}
+        if isinstance(self._wFile, _ab._pd.DataFrame):
+            weights, data = [], []
             if self._wFile.empty:
                 print("its empty..")
             i = self._wFile.columns.values.tolist()
             if 'items' in i:
-                items = self._wFile['items'].tolist()
+                data = self._wFile['items'].tolist()
             if 'weights' in i:
                 weights = self._wFile['weights'].tolist()
-            for i in range(len(weights)):
-                _weights[items[i]] = weights[i]
+            for k in range(len(data)):
+                self._weights[data[k]] = int(float(weights[k]))
 
             # print(self.Database)
         if isinstance(self._wFile, str):
-            if _fp._validators.url(self._wFile):
-                data = _fp._urlopen(self._wFile)
+            if _ab._validators.url(self._wFile):
+                data = _ab._urlopen(self._wFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    _weights[temp[0]] = temp[1]
+                    self._weights[temp[0]] = int(float(temp[1]))
             else:
                 try:
-                    with open(self._wFile, 'r', encoding='utf-8') as f:
+                    with open(self._wFile, 'r') as f:
                         for line in f:
-                            line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
-                            s = int(float(temp[1]))
-                            _weights[temp[0]] = s
+                            self._weights[temp[0]] = float(temp[1])
                 except IOError:
                     print("File Not Found")
-                    quit()
 
-    def __convert(self, value: Union[int, float, str]) -> Union[int, float]:
+    def _frequentOneItem(self) -> tuple:
         """
-        To convert the type of user specified minSup value.
+        Takes the self.Database and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
 
-        :param value: user specified minSup value
-        :return: converted type
+        :param self.Database : it represents the one self.Database in database
+        :type self.Database : list
+        :return: tuple
         """
-        if type(value) is int:
-            value = int(value)
-        if type(value) is float:
-            value = (len(self.__Database) * value)
-        if type(value) is str:
-            if '.' in value:
-                value = float(value)
-                value = (len(self.__Database) * value)
-            else:
-                value = int(value)
-        return value
 
-    def __frequentOneItem(self) -> List[str]:
-        """
-        Generating One frequent items sets
+        mapSupport = {}
+        for i in self._Database:
+            for j in i:
+                if j.item not in mapSupport:
+                    if self._weights.get(j.item) is not None:
+                        mapSupport[j.item] = [j.probability, self._weights[j.item]]
+                else:
+                    mapSupport[j.item][0] += j.probability
+        mapSupport = {k: v[0] for k, v in mapSupport.items() if v[0] >= self._expSup and v[0] * v[1] >= self._expWSup}
+        plist = [k for k, v in sorted(mapSupport.items(), key=lambda x: x[1], reverse=True)]
+        self.rank = dict([(index, item) for (item, index) in enumerate(plist)])
+        return mapSupport, plist
 
-        :return: list
+    @staticmethod
+    def _buildTree(data, info) -> _Tree:
         """
-        global _maxWeight
-        self.__mapSupport = {}
-        for tr in self.__Database:
-            for i in range(0, len(tr)):
-                if tr[i] not in self.__mapSupport:
-                    self.__mapSupport[tr[i]] = 1
-                else:
-                    self.__mapSupport[tr[i]] += 1
-        self.__mapSupport = {k: v for k, v in self.__mapSupport.items() if v >= self._minSup and v * _maxWeight > self._minSup}
-        genList = [k for k, v in sorted(self.__mapSupport.items(), key=lambda x: x[1], reverse=True)]
-        self.__rank = dict([(index, item) for (item, index) in enumerate(genList)])
-        return genList
+        It takes the self.Database and support of each item and construct the main tree with setting root node as null
 
-    def __updateTransactions(self, itemSet: List[str]) -> List[List[int]]:
+        :param data : it represents the one self.Database in database
+        :type data : list
+        :param info : it represents the support of each item
+        :type info : dictionary
+        :return: tree
         """
-        Updates the items in transactions with rank of items according to their support
 
-        :Example: oneLength = {'a':7, 'b': 5, 'c':'4', 'd':3}
-                    rank = {'a':0, 'b':1, 'c':2, 'd':3}
+        rootNode = _Tree()
+        rootNode.info = info.copy()
+        for i in range(len(data)):
+            rootNode.addTransaction(data[i])
+        return rootNode
+
+    def _updateTransactions(self, dict1) -> list:
+        """
+        Remove the items which are not frequent from self.Database and updates the self.Database with rank of items
 
-        :param itemSet: list of one-frequent items
+        :param dict1 : frequent items with support
+        :type dict1 : dictionary
         :return: list
         """
         list1 = []
-        for tr in self.__Database:
+        for tr in self._Database:
             list2 = []
-            for i in range(len(tr)):
-                if tr[i] in itemSet:
-                    list2.append(self.__rank[tr[i]])
-            if len(list2) >= 1:
-                list2.sort()
+            for i in range(0, len(tr)):
+                if tr[i].item in dict1:
+                    list2.append(tr[i])
+            if len(list2) >= 2:
+                basket = list2
+                basket.sort(key=lambda val: self.rank[val.item])
+                list2 = basket
                 list1.append(list2)
         return list1
 
     @staticmethod
-    def __buildTree(transactions: List[List[int]], info: Dict[int, int]) -> '_Tree':
+    def _check(i, x) -> int:
         """
-        Builds the tree with updated transactions
+        To check the presence of item or pattern in transaction
+
+        :param x: it represents the pattern
+        :type x : list
+        :param i : represents the uncertain self.Database
+        :type i : list
+        :return: integer number
+        """
+
+        # This method taken a transaction as input and returns the tree
+        for m in x:
+            k = 0
+            for n in i:
+                if m == n.item:
+                    k += 1
+            if k == 0:
+                return 0
+        return 1
 
-        :param transactions: updated transactions
-        :param info: support details of each item in transactions.
-        :return: Transactions compressed in fp-tree
+    def _convert(self, value) -> float:
         """
-        rootNode = _Tree()
-        rootNode.info = info.copy()
-        for i in range(len(transactions)):
-            rootNode.addTransaction(transactions[i], 1)
-        return rootNode
+        To convert the type of user specified minSup value
+
+        :param value: user specified minSup value
+        :return: converted type minSup value
+        """
+        if type(value) is int:
+            value = int(value)
+        if type(value) is float:
+            value = (len(self._Database) * value)
+        if type(value) is str:
+            if '.' in value:
+                value = (len(self._Database) * value)
+            else:
+                value = int(value)
+        return value
 
-    def __savePeriodic(self, itemSet: List[int]) -> str:
+    def _removeFalsePositives(self) -> None:
         """
-        The duplication items and their ranks
+        To remove the false positive patterns generated in frequent patterns.
 
-        :param itemSet: frequent itemSet that generated
-        :return: patterns with original item names.
+        :return: patterns with accurate probability
         """
-        temp = str()
-        for i in itemSet:
-            temp = temp + self.__rankDup[i] + "\t"
-        return temp
+        global _finalPatterns
+        periods = {}
+        for i in self._Database:
+            for x, y in _finalPatterns.items():
+                if len(x) == 1:
+                    periods[x] = y
+                else:
+                    s = 1
+                    check = self._check(i, x)
+                    if check == 1:
+                        for j in i:
+                            if j.item in x:
+                                s *= j.probability
+                        if x in periods:
+                            periods[x] += s
+                        else:
+                            periods[x] = s
+        for x, y in periods.items():
+            weight = 0
+            for i in x:
+                weight += self._weights[i]
+            weight = weight / len(x)
+            if weight * y >= self._expWSup:
+                sample = str()
+                for i in x:
+                    sample = sample + i + "\t"
+                self._finalPatterns[sample] = y
 
     @deprecated(
         "It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
-        main program to start the operation
-
-        :return: None
+        startMine() method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns.
         """
         self.mine()
 
     def mine(self) -> None:
         """
-        main program to start the operation
-
-        :return: None
+        mine() method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patternS
         """
-        global _minSup, _minWeight, _miniWeight, _maxWeight, _weights
-        self.__startTime = _fp._time.time()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        if self._minSup is None:
-            raise Exception("Please enter the Minimum Support")
-        self.__creatingItemSets()
+        global _expSup, _expWSup, _weights, _finalPatterns
+        self._startTime = _ab._time.time()
+        self._Database, self._weights = [], {}
+        self._creatingItemSets()
         self._scanningWeights()
-        _weights = {k: v for k, v in _weights.items() if v >= _minWeight}
-        _maxWeight = max([s for s in _weights.values()])
-        _miniWeight = min([s for s in _weights.values()])
-        self._minSup = self.__convert(self._minSup)
-        _minSup = self._minSup
-        itemSet = self.__frequentOneItem()
-        updatedTransactions = self.__updateTransactions(itemSet)
-        for x, y in self.__rank.items():
-            self.__rankDup[y] = x
-        info = {self.__rank[k]: v for k, v in self.__mapSupport.items()}
-        __Tree = self.__buildTree(updatedTransactions, info)
-        patterns = __Tree.generatePatterns([])
-        self.__finalPatterns = {}
-        for k in patterns:
-            s = self.__savePeriodic(k[0])
-            self.__finalPatterns[str(s)] = k[1]
-        print("Weighted Frequent patterns were generated successfully using basic algorithm")
-        self.__endTime = _fp._time.time()
-        self.__memoryUSS = float()
-        self.__memoryRSS = float()
-        process = _fp._psutil.Process(_fp._os.getpid())
-        self.__memoryUSS = process.memory_full_info().uss
-        self.__memoryRSS = process.memory_info().rss
-
+        _weights = self._weights
+        self._expSup = float(self._expSup)
+        self._expWSup = float(self._expWSup)
+        _expSup = self._expSup
+        _expWSup = self._expWSup
+        self._finalPatterns = {}
+        mapSupport, plist = self._frequentOneItem()
+        self.Database1 = self._updateTransactions(mapSupport)
+        info = {k: v for k, v in mapSupport.items()}
+        Tree1 = self._buildTree(self.Database1, info)
+        Tree1.generatePatterns([])
+        self._removeFalsePositives()
+        print("Weighted Frequent patterns were generated  successfully using basic algorithm")
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self.memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self.memoryRSS = process.memory_info().rss
 
     def getMemoryUSS(self) -> float:
         """
 
         Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
-        return self.__memoryUSS
+        return self._memoryUSS
 
     def getMemoryRSS(self) -> float:
         """
 
-        Total amount of RSS memory consumed by the mining process will be retrieved from this function.
+        Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
-
-        return self.__memoryRSS
+        return self.memoryRSS
 
     def getRuntime(self) -> float:
         """
 
-        Calculating the total amount of runtime taken by the mining process.
+        Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
-        return self.__endTime - self.__startTime
+        return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self) -> pd.DataFrame:
         """
 
-        Storing final frequent patterns in a dataframe.
+        Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
-
         dataframe = {}
         data = []
-        for a, b in self.__finalPatterns.items():
-            data.append([a.replace('\t', ' '), b])
-            dataframe = _fp._pd.DataFrame(data, columns=['Patterns', 'Support'])
+        for a, b in self._finalPatterns.items():
+            s = str()
+            for i in a:
+                s = s + i + " "
+            data.append([s, b])
+            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataframe
 
     def save(self, outFile: str) -> None:
         """
+        Complete set of frequent patterns will be loaded in to an output file
 
-        Complete set of frequent patterns will be loaded in to an output file.
-
-        :param outFile: name of the output file
+        :param outFile: Specify name of the output file
         :type outFile: csv file
         :return: None
         """
-        self._oFile = outFile
-        writer = open(self._oFile, 'w+')
-        for x, y in self.__finalPatterns.items():
-            s1 = x.strip() + ":" + str(y)
+        self.oFile = outFile
+        writer = open(self.oFile, 'w+')
+        for x, y in self._finalPatterns.items():
+            s = str()
+            for i in x:
+                s = s + i + "\t"
+            s1 = s.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
-    def getPatterns(self) -> Dict[str, int]:
+    def getPatterns(self) -> dict:
         """
 
-        Function to send the set of frequent patterns after completion of the mining process.
+        Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
-        return self.__finalPatterns
+        return self._finalPatterns
 
     def printResults(self) -> None:
         """
         This function is used to print the results
         """
-        print("Total number of  Weighted Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of  Weighted Uncertain Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:",  self.getRuntime())
 
-        
-
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_fp._sys.argv) == 6 or len(_fp._sys.argv) == 7:
-        if len(_fp._sys.argv) == 7:
-            _ap = WFIM(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4], _fp._sys.argv[5], _fp._sys.argv[6])
-        if len(_fp._sys.argv) == 6:
-            _ap = WFIM(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4], _fp._sys.argv[5])
+    if len(_ab._sys.argv) == 6 or len(_ab._sys.argv) == 7:
+        if len(_ab._sys.argv) == 7:
+            _ap = WUFIM(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5], _ab._sys.argv[6])
+        if len(_ab._sys.argv) == 6:
+            _ap = WUFIM(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
         _ap.startMine()
         _ap.mine()
-        print("Total number of Weighted Frequent Patterns:", len(_ap.getPatterns()))
-        _ap.save(_fp._sys.argv[2])
-        print("Total Memory in USS:",  _ap.getMemoryUSS())
+        print("Total number of Weighted Uncertain Frequent Patterns:", len(_ap.getPatterns()))
+        _ap.save(_ab._sys.argv[2])
+        print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
+        for k in [120, 140, 160, 180, 200]:
+            _ap = WUFIM('/Users/likhitha/Downloads/uncertainTransaction_T10I4D200K.csv', '/Users/likhitha/Downloads/T10_weights.txt',
+                        k, 500, '\t')
+            _ap.startMine()
+            print("Total number of Weighted Uncertain Frequent Patterns:", len(_ap.getPatterns()))
+            _ap.save('/Users/likhitha/Downloads/WUFIM_output.txt')
+            print("Total Memory in USS:", _ap.getMemoryUSS())
+            print("Total Memory in RSS", _ap.getMemoryRSS())
+            print("Total ExecutionTime in ms:", _ap.getRuntime())
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `pami-2024.5.28.1/PAMI/weightedFrequentPattern/basic/abstract.py` & `pami-2024.5.7.1/PAMI/weightedFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/weightedFrequentRegularPattern/basic/abstract.py` & `pami-2024.5.7.1/PAMI/weightedFrequentRegularPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PAMI/weightedUncertainFrequentPattern/basic/abstract.py` & `pami-2024.5.7.1/PAMI/weightedUncertainFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.5.28.1/PKG-INFO` & `pami-2024.5.7.1/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: pami
-Version: 2024.5.28.1
+Version: 2024.5.7.1
 Summary: This software is being developed at the University of Aizu, Aizu-Wakamatsu, Fukushima, Japan
 Home-page: https://github.com/udayLab/PAMI
 Author: Rage Uday Kiran
 Author-email: uday.rage@gmail.com
 License: GPLv3
 Classifier: Development Status :: 5 - Production/Stable
 Classifier: Programming Language :: Python :: 3
@@ -111,20 +111,17 @@
 
 ![PAMI's production process](./images/pamiDevelopmentSteps.png?raw=true)
 
 <!--- ![alt text](https://github.com/[username]/[reponame]/blob/[branch]/image.jpg?raw=true) ---> 
 ***
 # Recent Updates
 
-- **Version 2024.05.01:** 
-In this latest version, the following updates have been made:
-  - Included two new algorithms, **Gspan and TKG**, for frequent subgraph mining.
-  - Updated three Synthetic Data Generator, **transactional database, temporal database, and geo-referenced transactional database**.
-  - Optimized the following frequent pattern mining algorithms: **Apriori, Aprioribitset, ECLAT, ECLATbitset, FPGrowth, and CHARM**.
-  - startMine() function has been deprecated to mine() function.
+- Version 2023.07.07: New algorithms: cuApriroi, cuAprioriBit, cuEclat, cuEclatBit, gPPMiner, cuGPFMiner, FPStream, HUPMS, SHUPGrowth New codes to generate synthetic databases
+- Version 2023.06.20: Fuzzy Partial Periodic, Periodic Patterns in High Utility, Code Documentation, help() function Update 
+- Version 2023.03.01: prefixSpan and SPADE   
 
 Total number of algorithms: 83
 
 ***
 # Features
 
 -  Well-tested and production-ready
@@ -272,22 +269,25 @@
 ***
 # Contribution to PAMI
 
 We invite and encourage all community members to contribute, report bugs, fix bugs, enhance documentation, propose improvements, and share their creative ideas.
 
 ***
 # Tutorials
+
 ### 0. Association Rule Mining
 
 | Basic                                                                                                                                                                                                                                                |
 |------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
 | Confidence <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/associationRules/basic/confidence.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a> |
 | Lift <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/associationRules/basic/lift.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>             |
 | Leverage <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/associationRules/basic/leverage.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>     |
 
+
+
 ### 1. Pattern mining in binary transactional databases
 
 #### 1.1. Frequent pattern mining: [Sample](https://udaylab.github.io/PAMI/frequentPatternMining.html)
 
 | Basic                                                                                                                                                                                                                                                      | Closed                                                                                                                                                                                                                                       | Maximal                                                                                                                                                                                                                                                     | Top-k                                                                                                                                                                                                                                  | CUDA           | pyspark                                                                                                                                                                                                                                                             |
 |------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
 | Apriori <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/frequentPattern/basic/Apriori.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>              | CHARM <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/frequentPattern/closed/CHARM.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> | maxFP-growth  <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/frequentPattern/maximal/MaxFPGrowth.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> | FAE <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/frequentPattern/topk/FAE.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> | cudaAprioriGCT | parallelApriori <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/frequentPattern/pyspark/parallelApriori.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a>   |
@@ -590,40 +590,34 @@
 #### 12.1. Creation of synthetic databases
 
 | Database type                                                                                                                                                                                                                                                                        |
 |--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
 | Transactional database <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/extras/syntheticDataGenerators/TransactionalDatabase.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a> | |
 | Temporal database <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/extras/syntheticDataGenerators/TemporalDatabase.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>           |
 | Utility database (coming soon)                                                                                                                                                                                                                                                       |
-| spatio-transactional database (coming soon)                                                                                                                                                                                                                                          |
-| spatio-temporal database (coming soon)                                                                                                                                                                                                                                               |
-| fuzzy transactional database (coming soon)                                                                                                                                                                                                                                           |
-| fuzzy temporal database (coming soon)                                                                                                                                                                                                                                                |
-| Sequence database generator (coming soon)                                                                                                                                                                                                                                            |
-
 
 #### 12.2. Converting a dataframe into a specific database type
 | Approaches                                  |
 |---------------------------------------------|
 | Dense dataframe to databases (coming soon)  |
 | Sparse dataframe to databases (coming soon) |
 
 #### 12.3. Gathering the statistical details of a database
-| Approaches                                                                                                                                                                                                                                                        |
-|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
-| Transactional database <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/extras/stats/TransactionalDatabase.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a> |
-| Temporal database <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/extras/stats/TemporalDatabase.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>           |
-| Utility database (coming soon)                                                                                                                                                                                                                                    |
+| Approaches                           |
+|--------------------------------------|
+| Transactional database (coming soon) |
+| Temporal database (coming soon)      |
+| Utility database (coming soon)       |
 
 #### 12.4. Generating Latex code for the experimental results
 | Approaches               |
 |--------------------------|
 | Latex code (coming soon) |
 
 ***
 
 # Real World Case Studies
 
-1. Air pollution analytics <a target="_blank" href="https://colab.research.google.com/github/udayLab/PAMI/blob/main/notebooks/airPollutionAnalytics.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
+1. Air pollution analytics <a target="_blank" href="https://colab.research.google.com/github/vanithakattumuri/PAMI/blob/main/notebooks/airPollutionAnalytics.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
 
 
 [Go to Top](#table-of-contents)
```

### Comparing `pami-2024.5.28.1/README.md` & `pami-2024.5.7.1/README.md`

 * *Files 1% similar despite different names*

```diff
@@ -64,20 +64,17 @@
 
 ![PAMI's production process](./images/pamiDevelopmentSteps.png?raw=true)
 
 <!--- ![alt text](https://github.com/[username]/[reponame]/blob/[branch]/image.jpg?raw=true) ---> 
 ***
 # Recent Updates
 
-- **Version 2024.05.01:** 
-In this latest version, the following updates have been made:
-  - Included two new algorithms, **Gspan and TKG**, for frequent subgraph mining.
-  - Updated three Synthetic Data Generator, **transactional database, temporal database, and geo-referenced transactional database**.
-  - Optimized the following frequent pattern mining algorithms: **Apriori, Aprioribitset, ECLAT, ECLATbitset, FPGrowth, and CHARM**.
-  - startMine() function has been deprecated to mine() function.
+- Version 2023.07.07: New algorithms: cuApriroi, cuAprioriBit, cuEclat, cuEclatBit, gPPMiner, cuGPFMiner, FPStream, HUPMS, SHUPGrowth New codes to generate synthetic databases
+- Version 2023.06.20: Fuzzy Partial Periodic, Periodic Patterns in High Utility, Code Documentation, help() function Update 
+- Version 2023.03.01: prefixSpan and SPADE   
 
 Total number of algorithms: 83
 
 ***
 # Features
 
 -  Well-tested and production-ready
@@ -225,22 +222,25 @@
 ***
 # Contribution to PAMI
 
 We invite and encourage all community members to contribute, report bugs, fix bugs, enhance documentation, propose improvements, and share their creative ideas.
 
 ***
 # Tutorials
+
 ### 0. Association Rule Mining
 
 | Basic                                                                                                                                                                                                                                                |
 |------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
 | Confidence <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/associationRules/basic/confidence.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a> |
 | Lift <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/associationRules/basic/lift.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>             |
 | Leverage <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/associationRules/basic/leverage.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>     |
 
+
+
 ### 1. Pattern mining in binary transactional databases
 
 #### 1.1. Frequent pattern mining: [Sample](https://udaylab.github.io/PAMI/frequentPatternMining.html)
 
 | Basic                                                                                                                                                                                                                                                      | Closed                                                                                                                                                                                                                                       | Maximal                                                                                                                                                                                                                                                     | Top-k                                                                                                                                                                                                                                  | CUDA           | pyspark                                                                                                                                                                                                                                                             |
 |------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
 | Apriori <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/frequentPattern/basic/Apriori.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>              | CHARM <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/frequentPattern/closed/CHARM.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> | maxFP-growth  <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/frequentPattern/maximal/MaxFPGrowth.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> | FAE <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/frequentPattern/topk/FAE.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> | cudaAprioriGCT | parallelApriori <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/frequentPattern/pyspark/parallelApriori.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a>   |
@@ -543,40 +543,34 @@
 #### 12.1. Creation of synthetic databases
 
 | Database type                                                                                                                                                                                                                                                                        |
 |--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
 | Transactional database <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/extras/syntheticDataGenerators/TransactionalDatabase.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a> | |
 | Temporal database <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/extras/syntheticDataGenerators/TemporalDatabase.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>           |
 | Utility database (coming soon)                                                                                                                                                                                                                                                       |
-| spatio-transactional database (coming soon)                                                                                                                                                                                                                                          |
-| spatio-temporal database (coming soon)                                                                                                                                                                                                                                               |
-| fuzzy transactional database (coming soon)                                                                                                                                                                                                                                           |
-| fuzzy temporal database (coming soon)                                                                                                                                                                                                                                                |
-| Sequence database generator (coming soon)                                                                                                                                                                                                                                            |
-
 
 #### 12.2. Converting a dataframe into a specific database type
 | Approaches                                  |
 |---------------------------------------------|
 | Dense dataframe to databases (coming soon)  |
 | Sparse dataframe to databases (coming soon) |
 
 #### 12.3. Gathering the statistical details of a database
-| Approaches                                                                                                                                                                                                                                                        |
-|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
-| Transactional database <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/extras/stats/TransactionalDatabase.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a> |
-| Temporal database <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/extras/stats/TemporalDatabase.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>           |
-| Utility database (coming soon)                                                                                                                                                                                                                                    |
+| Approaches                           |
+|--------------------------------------|
+| Transactional database (coming soon) |
+| Temporal database (coming soon)      |
+| Utility database (coming soon)       |
 
 #### 12.4. Generating Latex code for the experimental results
 | Approaches               |
 |--------------------------|
 | Latex code (coming soon) |
 
 ***
 
 # Real World Case Studies
 
-1. Air pollution analytics <a target="_blank" href="https://colab.research.google.com/github/udayLab/PAMI/blob/main/notebooks/airPollutionAnalytics.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
+1. Air pollution analytics <a target="_blank" href="https://colab.research.google.com/github/vanithakattumuri/PAMI/blob/main/notebooks/airPollutionAnalytics.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
 
 
 [Go to Top](#table-of-contents)
```

### Comparing `pami-2024.5.28.1/pami.egg-info/PKG-INFO` & `pami-2024.5.7.1/pami.egg-info/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: pami
-Version: 2024.5.28.1
+Version: 2024.5.7.1
 Summary: This software is being developed at the University of Aizu, Aizu-Wakamatsu, Fukushima, Japan
 Home-page: https://github.com/udayLab/PAMI
 Author: Rage Uday Kiran
 Author-email: uday.rage@gmail.com
 License: GPLv3
 Classifier: Development Status :: 5 - Production/Stable
 Classifier: Programming Language :: Python :: 3
@@ -111,20 +111,17 @@
 
 ![PAMI's production process](./images/pamiDevelopmentSteps.png?raw=true)
 
 <!--- ![alt text](https://github.com/[username]/[reponame]/blob/[branch]/image.jpg?raw=true) ---> 
 ***
 # Recent Updates
 
-- **Version 2024.05.01:** 
-In this latest version, the following updates have been made:
-  - Included two new algorithms, **Gspan and TKG**, for frequent subgraph mining.
-  - Updated three Synthetic Data Generator, **transactional database, temporal database, and geo-referenced transactional database**.
-  - Optimized the following frequent pattern mining algorithms: **Apriori, Aprioribitset, ECLAT, ECLATbitset, FPGrowth, and CHARM**.
-  - startMine() function has been deprecated to mine() function.
+- Version 2023.07.07: New algorithms: cuApriroi, cuAprioriBit, cuEclat, cuEclatBit, gPPMiner, cuGPFMiner, FPStream, HUPMS, SHUPGrowth New codes to generate synthetic databases
+- Version 2023.06.20: Fuzzy Partial Periodic, Periodic Patterns in High Utility, Code Documentation, help() function Update 
+- Version 2023.03.01: prefixSpan and SPADE   
 
 Total number of algorithms: 83
 
 ***
 # Features
 
 -  Well-tested and production-ready
@@ -272,22 +269,25 @@
 ***
 # Contribution to PAMI
 
 We invite and encourage all community members to contribute, report bugs, fix bugs, enhance documentation, propose improvements, and share their creative ideas.
 
 ***
 # Tutorials
+
 ### 0. Association Rule Mining
 
 | Basic                                                                                                                                                                                                                                                |
 |------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
 | Confidence <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/associationRules/basic/confidence.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a> |
 | Lift <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/associationRules/basic/lift.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>             |
 | Leverage <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/associationRules/basic/leverage.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>     |
 
+
+
 ### 1. Pattern mining in binary transactional databases
 
 #### 1.1. Frequent pattern mining: [Sample](https://udaylab.github.io/PAMI/frequentPatternMining.html)
 
 | Basic                                                                                                                                                                                                                                                      | Closed                                                                                                                                                                                                                                       | Maximal                                                                                                                                                                                                                                                     | Top-k                                                                                                                                                                                                                                  | CUDA           | pyspark                                                                                                                                                                                                                                                             |
 |------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
 | Apriori <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/frequentPattern/basic/Apriori.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>              | CHARM <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/frequentPattern/closed/CHARM.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> | maxFP-growth  <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/frequentPattern/maximal/MaxFPGrowth.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> | FAE <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/frequentPattern/topk/FAE.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> | cudaAprioriGCT | parallelApriori <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/frequentPattern/pyspark/parallelApriori.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a>   |
@@ -590,40 +590,34 @@
 #### 12.1. Creation of synthetic databases
 
 | Database type                                                                                                                                                                                                                                                                        |
 |--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
 | Transactional database <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/extras/syntheticDataGenerators/TransactionalDatabase.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a> | |
 | Temporal database <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/extras/syntheticDataGenerators/TemporalDatabase.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>           |
 | Utility database (coming soon)                                                                                                                                                                                                                                                       |
-| spatio-transactional database (coming soon)                                                                                                                                                                                                                                          |
-| spatio-temporal database (coming soon)                                                                                                                                                                                                                                               |
-| fuzzy transactional database (coming soon)                                                                                                                                                                                                                                           |
-| fuzzy temporal database (coming soon)                                                                                                                                                                                                                                                |
-| Sequence database generator (coming soon)                                                                                                                                                                                                                                            |
-
 
 #### 12.2. Converting a dataframe into a specific database type
 | Approaches                                  |
 |---------------------------------------------|
 | Dense dataframe to databases (coming soon)  |
 | Sparse dataframe to databases (coming soon) |
 
 #### 12.3. Gathering the statistical details of a database
-| Approaches                                                                                                                                                                                                                                                        |
-|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
-| Transactional database <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/extras/stats/TransactionalDatabase.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a> |
-| Temporal database <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/extras/stats/TemporalDatabase.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>           |
-| Utility database (coming soon)                                                                                                                                                                                                                                    |
+| Approaches                           |
+|--------------------------------------|
+| Transactional database (coming soon) |
+| Temporal database (coming soon)      |
+| Utility database (coming soon)       |
 
 #### 12.4. Generating Latex code for the experimental results
 | Approaches               |
 |--------------------------|
 | Latex code (coming soon) |
 
 ***
 
 # Real World Case Studies
 
-1. Air pollution analytics <a target="_blank" href="https://colab.research.google.com/github/udayLab/PAMI/blob/main/notebooks/airPollutionAnalytics.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
+1. Air pollution analytics <a target="_blank" href="https://colab.research.google.com/github/vanithakattumuri/PAMI/blob/main/notebooks/airPollutionAnalytics.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
 
 
 [Go to Top](#table-of-contents)
```

### Comparing `pami-2024.5.28.1/pami.egg-info/SOURCES.txt` & `pami-2024.5.7.1/pami.egg-info/SOURCES.txt`

 * *Files 0% similar despite different names*

```diff
@@ -10,15 +10,14 @@
 PAMI/AssociationRules/basic/abstract.py
 PAMI/AssociationRules/basic/confidence.py
 PAMI/AssociationRules/basic/leverage.py
 PAMI/AssociationRules/basic/lift.py
 PAMI/correlatedPattern/__init__.py
 PAMI/correlatedPattern/basic/CoMine.py
 PAMI/correlatedPattern/basic/CoMinePlus.py
-PAMI/correlatedPattern/basic/_CoMine.py
 PAMI/correlatedPattern/basic/__init__.py
 PAMI/correlatedPattern/basic/abstract.py
 PAMI/coveragePattern/__init__.py
 PAMI/coveragePattern/basic/CMine.py
 PAMI/coveragePattern/basic/CPPG.py
 PAMI/coveragePattern/basic/__init__.py
 PAMI/coveragePattern/basic/abstract.py
@@ -118,15 +117,14 @@
 PAMI/frequentPattern/basic/Apriori.py
 PAMI/frequentPattern/basic/Aprioribitset.py
 PAMI/frequentPattern/basic/ECLAT.py
 PAMI/frequentPattern/basic/ECLATDiffset.py
 PAMI/frequentPattern/basic/ECLATbitset.py
 PAMI/frequentPattern/basic/FPGrowth.py
 PAMI/frequentPattern/basic/_Apriori.py
-PAMI/frequentPattern/basic/_ECLATDiffset.py
 PAMI/frequentPattern/basic/_FPGrowth.py
 PAMI/frequentPattern/basic/__init__.py
 PAMI/frequentPattern/basic/abstract.py
 PAMI/frequentPattern/closed/CHARM.py
 PAMI/frequentPattern/closed/__init__.py
 PAMI/frequentPattern/closed/abstract.py
 PAMI/frequentPattern/cuda/__init__.py
@@ -143,15 +141,14 @@
 PAMI/frequentPattern/maximal/abstract.py
 PAMI/frequentPattern/pyspark/__init__.py
 PAMI/frequentPattern/pyspark/abstract.py
 PAMI/frequentPattern/pyspark/parallelApriori.py
 PAMI/frequentPattern/pyspark/parallelECLAT.py
 PAMI/frequentPattern/pyspark/parallelFPGrowth.py
 PAMI/frequentPattern/topk/FAE.py
-PAMI/frequentPattern/topk/_FAE.py
 PAMI/frequentPattern/topk/__init__.py
 PAMI/frequentPattern/topk/abstract.py
 PAMI/fuzzyCorrelatedPattern/__init__.py
 PAMI/fuzzyCorrelatedPattern/basic/FCPGrowth.py
 PAMI/fuzzyCorrelatedPattern/basic/__init__.py
 PAMI/fuzzyCorrelatedPattern/basic/abstract.py
 PAMI/fuzzyFrequentPattern/__init__.py
```

### Comparing `pami-2024.5.28.1/setup.py` & `pami-2024.5.7.1/setup.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 import setuptools
 
 with open('README.md', 'r') as fh:
     long_description = fh.read()
 
 setuptools.setup(
     name='pami',
-    version='2024.5.28.1',
+    version='2024.5.7.1',
     author='Rage Uday Kiran',
     author_email='uday.rage@gmail.com',
     description='This software is being developed at the University of Aizu, Aizu-Wakamatsu, Fukushima, Japan',
     long_description=long_description,
     long_description_content_type='text/markdown',
     packages=setuptools.find_packages(),
     url='https://github.com/udayLab/PAMI',
```

