# Comparing `tmp/opencompass-0.2.4.tar.gz` & `tmp/opencompass-0.2.5.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "dist/opencompass-0.2.4.tar", last modified: Tue Apr 23 09:22:04 2024, max compression
+gzip compressed data, was "dist/opencompass-0.2.5.tar", last modified: Wed May 29 16:35:27 2024, max compression
```

## Comparing `opencompass-0.2.4.tar` & `opencompass-0.2.5.tar`

### file list

```diff
@@ -1,422 +1,443 @@
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 09:22:04.000000 opencompass-0.2.4/
--rw-r--r--   0 runner    (1001) docker     (127)    23085 2024-04-23 09:22:04.000000 opencompass-0.2.4/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (127)    17834 2024-04-23 09:21:51.000000 opencompass-0.2.4/README.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 09:22:04.000000 opencompass-0.2.4/opencompass/
--rw-r--r--   0 runner    (1001) docker     (127)       22 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 09:22:04.000000 opencompass-0.2.4/opencompass/datasets/
--rw-r--r--   0 runner    (1001) docker     (127)     1190 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/FinanceIQ.py
--rw-r--r--   0 runner    (1001) docker     (127)     4830 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/GaokaoBench.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 09:22:04.000000 opencompass-0.2.4/opencompass/datasets/NPHardEval/
--rw-r--r--   0 runner    (1001) docker     (127)      388 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/NPHardEval/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6698 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/NPHardEval/cmp_GCP_D.py
--rw-r--r--   0 runner    (1001) docker     (127)     6980 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/NPHardEval/cmp_KSP.py
--rw-r--r--   0 runner    (1001) docker     (127)     6258 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/NPHardEval/cmp_TSP_D.py
--rw-r--r--   0 runner    (1001) docker     (127)     7326 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/NPHardEval/hard_GCP.py
--rw-r--r--   0 runner    (1001) docker     (127)     8407 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/NPHardEval/hard_MSP.py
--rw-r--r--   0 runner    (1001) docker     (127)     8515 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/NPHardEval/hard_TSP.py
--rw-r--r--   0 runner    (1001) docker     (127)     4659 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/NPHardEval/p_BSP.py
--rw-r--r--   0 runner    (1001) docker     (127)     5540 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/NPHardEval/p_EDP.py
--rw-r--r--   0 runner    (1001) docker     (127)     8341 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/NPHardEval/p_SPP.py
--rw-r--r--   0 runner    (1001) docker     (127)    10534 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/NPHardEval/prompts.py
--rw-r--r--   0 runner    (1001) docker     (127)     1543 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/NPHardEval/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     1236 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/OpenFinData.py
--rw-r--r--   0 runner    (1001) docker     (127)     1948 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/QuALITY.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 09:22:04.000000 opencompass-0.2.4/opencompass/datasets/TheoremQA/
--rw-r--r--   0 runner    (1001) docker     (127)      215 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/TheoremQA/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1007 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/TheoremQA/legacy.py
--rw-r--r--   0 runner    (1001) docker     (127)     2069 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/TheoremQA/main.py
--rw-r--r--   0 runner    (1001) docker     (127)     2836 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/TheoremQA/number_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     3620 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/TheoremQA/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     5261 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5217 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/advglue.py
--rw-r--r--   0 runner    (1001) docker     (127)      497 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/afqmcd.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 09:22:04.000000 opencompass-0.2.4/opencompass/datasets/agieval/
--rw-r--r--   0 runner    (1001) docker     (127)       59 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/agieval/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3390 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/agieval/agieval.py
--rw-r--r--   0 runner    (1001) docker     (127)     3113 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/agieval/constructions.py
--rw-r--r--   0 runner    (1001) docker     (127)    16445 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/agieval/dataset_loader.py
--rw-r--r--   0 runner    (1001) docker     (127)     1422 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/agieval/evaluation.py
--rw-r--r--   0 runner    (1001) docker     (127)     4572 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/agieval/math_equivalence.py
--rw-r--r--   0 runner    (1001) docker     (127)     6561 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/agieval/post_process.py
--rw-r--r--   0 runner    (1001) docker     (127)     1097 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/agieval/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)      431 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/anli.py
--rw-r--r--   0 runner    (1001) docker     (127)     1706 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/anthropics_evals.py
--rw-r--r--   0 runner    (1001) docker     (127)    32935 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/apps.py
--rw-r--r--   0 runner    (1001) docker     (127)     3270 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/arc.py
--rw-r--r--   0 runner    (1001) docker     (127)      581 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/ax.py
--rw-r--r--   0 runner    (1001) docker     (127)      692 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/base.py
--rw-r--r--   0 runner    (1001) docker     (127)     2735 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/bbh.py
--rw-r--r--   0 runner    (1001) docker     (127)     1451 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/boolq.py
--rw-r--r--   0 runner    (1001) docker     (127)      498 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/bustum.py
--rw-r--r--   0 runner    (1001) docker     (127)     2893 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/c3.py
--rw-r--r--   0 runner    (1001) docker     (127)      611 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/cb.py
--rw-r--r--   0 runner    (1001) docker     (127)     3015 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/ceval.py
--rw-r--r--   0 runner    (1001) docker     (127)      963 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/chembench.py
--rw-r--r--   0 runner    (1001) docker     (127)     1172 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/chid.py
--rw-r--r--   0 runner    (1001) docker     (127)    20323 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/cibench.py
--rw-r--r--   0 runner    (1001) docker     (127)    15035 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/circular.py
--rw-r--r--   0 runner    (1001) docker     (127)     1140 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/civilcomments.py
--rw-r--r--   0 runner    (1001) docker     (127)     1205 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/clozeTest_maxmin.py
--rw-r--r--   0 runner    (1001) docker     (127)     1699 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/cluewsc.py
--rw-r--r--   0 runner    (1001) docker     (127)      987 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/cmb.py
--rw-r--r--   0 runner    (1001) docker     (127)     1030 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/cmmlu.py
--rw-r--r--   0 runner    (1001) docker     (127)     1081 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/cmnli.py
--rw-r--r--   0 runner    (1001) docker     (127)     1396 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/cmrc.py
--rw-r--r--   0 runner    (1001) docker     (127)     1429 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/commonsenseqa.py
--rw-r--r--   0 runner    (1001) docker     (127)      845 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/commonsenseqa_cn.py
--rw-r--r--   0 runner    (1001) docker     (127)      500 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/copa.py
--rw-r--r--   0 runner    (1001) docker     (127)     2704 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/crowspairs.py
--rw-r--r--   0 runner    (1001) docker     (127)      523 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/crowspairs_cn.py
--rw-r--r--   0 runner    (1001) docker     (127)     1028 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/csl.py
--rw-r--r--   0 runner    (1001) docker     (127)    17709 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/custom.py
--rw-r--r--   0 runner    (1001) docker     (127)      693 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/cvalues.py
--rw-r--r--   0 runner    (1001) docker     (127)     1396 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/drcd.py
--rw-r--r--   0 runner    (1001) docker     (127)     1641 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/drop.py
--rw-r--r--   0 runner    (1001) docker     (127)    15872 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/ds1000.py
--rw-r--r--   0 runner    (1001) docker     (127)     2280 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/ds1000_interpreter.py
--rw-r--r--   0 runner    (1001) docker     (127)      686 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/eprstmt.py
--rw-r--r--   0 runner    (1001) docker     (127)     1832 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/flores.py
--rw-r--r--   0 runner    (1001) docker     (127)     6536 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/game24.py
--rw-r--r--   0 runner    (1001) docker     (127)     1502 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/govrepcrs.py
--rw-r--r--   0 runner    (1001) docker     (127)     1942 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/gpqa.py
--rw-r--r--   0 runner    (1001) docker     (127)     4421 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/gsm8k.py
--rw-r--r--   0 runner    (1001) docker     (127)      594 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/gsm_hard.py
--rw-r--r--   0 runner    (1001) docker     (127)     4993 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/hellaswag.py
--rw-r--r--   0 runner    (1001) docker     (127)      257 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/huggingface.py
--rw-r--r--   0 runner    (1001) docker     (127)     9354 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/humaneval.py
--rw-r--r--   0 runner    (1001) docker     (127)     7425 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/humaneval_multi.py
--rw-r--r--   0 runner    (1001) docker     (127)     8969 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/humanevalx.py
--rw-r--r--   0 runner    (1001) docker     (127)      483 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/hungarian_math.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 09:22:04.000000 opencompass-0.2.4/opencompass/datasets/infinitebench/
--rw-r--r--   0 runner    (1001) docker     (127)      734 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/infinitebench/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1054 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/infinitebench/infinitebench_codedebug.py
--rw-r--r--   0 runner    (1001) docker     (127)      896 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/infinitebench/infinitebench_coderun.py
--rw-r--r--   0 runner    (1001) docker     (127)     1428 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/infinitebench/infinitebench_endia.py
--rw-r--r--   0 runner    (1001) docker     (127)     1049 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/infinitebench/infinitebench_enmc.py
--rw-r--r--   0 runner    (1001) docker     (127)      687 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/infinitebench/infinitebench_enqa.py
--rw-r--r--   0 runner    (1001) docker     (127)      567 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/infinitebench/infinitebench_ensum.py
--rw-r--r--   0 runner    (1001) docker     (127)     1514 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/infinitebench/infinitebench_mathcalc.py
--rw-r--r--   0 runner    (1001) docker     (127)      854 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/infinitebench/infinitebench_mathfind.py
--rw-r--r--   0 runner    (1001) docker     (127)     1381 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/infinitebench/infinitebench_retrievekv.py
--rw-r--r--   0 runner    (1001) docker     (127)      688 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/infinitebench/infinitebench_retrievenumber.py
--rw-r--r--   0 runner    (1001) docker     (127)      689 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/infinitebench/infinitebench_retrievepasskey.py
--rw-r--r--   0 runner    (1001) docker     (127)      687 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/infinitebench/infinitebench_zhqa.py
--rw-r--r--   0 runner    (1001) docker     (127)      471 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/infinitebench/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)      421 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/iwslt2017.py
--rw-r--r--   0 runner    (1001) docker     (127)     1036 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/jigsawmultilingual.py
--rw-r--r--   0 runner    (1001) docker     (127)      410 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/jsonl.py
--rw-r--r--   0 runner    (1001) docker     (127)     4996 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/kaoshi.py
--rw-r--r--   0 runner    (1001) docker     (127)     1361 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/lambada.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 09:22:04.000000 opencompass-0.2.4/opencompass/datasets/lawbench/
--rw-r--r--   0 runner    (1001) docker     (127)       52 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/lawbench/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2494 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/lawbench/lawbench.py
--rw-r--r--   0 runner    (1001) docker     (127)     1233 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/lcsts.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 09:22:04.000000 opencompass-0.2.4/opencompass/datasets/leval/
--rw-r--r--   0 runner    (1001) docker     (127)     1084 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/leval/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6544 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/leval/evaluators.py
--rw-r--r--   0 runner    (1001) docker     (127)      844 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/leval/leval_coursera.py
--rw-r--r--   0 runner    (1001) docker     (127)      898 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/leval/leval_financial_qa.py
--rw-r--r--   0 runner    (1001) docker     (127)      900 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/leval/leval_gov_report_summ.py
--rw-r--r--   0 runner    (1001) docker     (127)     1623 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/leval/leval_gsm100.py
--rw-r--r--   0 runner    (1001) docker     (127)      902 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/leval/leval_legal_contract_qa.py
--rw-r--r--   0 runner    (1001) docker     (127)      898 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/leval/leval_meeting_summ.py
--rw-r--r--   0 runner    (1001) docker     (127)      897 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/leval/leval_multidoc_qa.py
--rw-r--r--   0 runner    (1001) docker     (127)      898 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/leval/leval_narrattive_qa.py
--rw-r--r--   0 runner    (1001) docker     (127)      902 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/leval/leval_natural_question.py
--rw-r--r--   0 runner    (1001) docker     (127)      895 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/leval/leval_news_summ.py
--rw-r--r--   0 runner    (1001) docker     (127)      901 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/leval/leval_paper_assistant.py
--rw-r--r--   0 runner    (1001) docker     (127)      897 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/leval/leval_patent_summ.py
--rw-r--r--   0 runner    (1001) docker     (127)      846 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/leval/leval_quality.py
--rw-r--r--   0 runner    (1001) docker     (127)      897 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/leval/leval_review_summ.py
--rw-r--r--   0 runner    (1001) docker     (127)      899 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/leval/leval_scientific_qa.py
--rw-r--r--   0 runner    (1001) docker     (127)      850 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/leval/leval_topic_retrieval.py
--rw-r--r--   0 runner    (1001) docker     (127)      839 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/leval/leval_tpo.py
--rw-r--r--   0 runner    (1001) docker     (127)      897 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/leval/leval_tvshow_summ.py
--rw-r--r--   0 runner    (1001) docker     (127)      392 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/lmeval.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 09:22:04.000000 opencompass-0.2.4/opencompass/datasets/longbench/
--rw-r--r--   0 runner    (1001) docker     (127)     1585 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/longbench/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     9597 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/longbench/evaluators.py
--rw-r--r--   0 runner    (1001) docker     (127)      754 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/longbench/longbench_2wikim_qa.py
--rw-r--r--   0 runner    (1001) docker     (127)      754 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/longbench/longbench_dureader.py
--rw-r--r--   0 runner    (1001) docker     (127)      625 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/longbench/longbench_gov_report.py
--rw-r--r--   0 runner    (1001) docker     (127)      754 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/longbench/longbench_hotpot_qa.py
--rw-r--r--   0 runner    (1001) docker     (127)      618 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/longbench/longbench_lcc.py
--rw-r--r--   0 runner    (1001) docker     (127)     1073 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/longbench/longbench_lsht.py
--rw-r--r--   0 runner    (1001) docker     (127)      625 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/longbench/longbench_multi_news.py
--rw-r--r--   0 runner    (1001) docker     (127)      761 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/longbench/longbench_multifieldqa_en.py
--rw-r--r--   0 runner    (1001) docker     (127)      761 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/longbench/longbench_multifieldqa_zh.py
--rw-r--r--   0 runner    (1001) docker     (127)      753 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/longbench/longbench_musique.py
--rw-r--r--   0 runner    (1001) docker     (127)      757 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/longbench/longbench_narrative_qa.py
--rw-r--r--   0 runner    (1001) docker     (127)      628 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/longbench/longbench_passage_count.py
--rw-r--r--   0 runner    (1001) docker     (127)      766 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/longbench/longbench_passage_retrieval_en.py
--rw-r--r--   0 runner    (1001) docker     (127)      766 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/longbench/longbench_passage_retrieval_zh.py
--rw-r--r--   0 runner    (1001) docker     (127)      752 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/longbench/longbench_qasper.py
--rw-r--r--   0 runner    (1001) docker     (127)      751 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/longbench/longbench_qmsum.py
--rw-r--r--   0 runner    (1001) docker     (127)      755 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/longbench/longbench_repobench.py
--rw-r--r--   0 runner    (1001) docker     (127)      916 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/longbench/longbench_samsum.py
--rw-r--r--   0 runner    (1001) docker     (127)     1073 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/longbench/longbench_trec.py
--rw-r--r--   0 runner    (1001) docker     (127)      920 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/longbench/longbench_trivia_qa.py
--rw-r--r--   0 runner    (1001) docker     (127)      620 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/longbench/longbench_vcsum.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 09:22:04.000000 opencompass-0.2.4/opencompass/datasets/lveval/
--rw-r--r--   0 runner    (1001) docker     (127)      833 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/lveval/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    12371 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/lveval/evaluators.py
--rw-r--r--   0 runner    (1001) docker     (127)      867 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/lveval/lveval_cmrc_mixup.py
--rw-r--r--   0 runner    (1001) docker     (127)      752 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/lveval/lveval_dureader_mixup.py
--rw-r--r--   0 runner    (1001) docker     (127)      875 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/lveval/lveval_factrecall_en.py
--rw-r--r--   0 runner    (1001) docker     (127)      875 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/lveval/lveval_factrecall_zh.py
--rw-r--r--   0 runner    (1001) docker     (127)     1060 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/lveval/lveval_hotpotwikiqa_mixup.py
--rw-r--r--   0 runner    (1001) docker     (127)     1051 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/lveval/lveval_lic_mixup.py
--rw-r--r--   0 runner    (1001) docker     (127)      937 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/lveval/lveval_loogle_CR_mixup.py
--rw-r--r--   0 runner    (1001) docker     (127)      938 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/lveval/lveval_loogle_MIR_mixup.py
--rw-r--r--   0 runner    (1001) docker     (127)      937 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/lveval/lveval_loogle_SD_mixup.py
--rw-r--r--   0 runner    (1001) docker     (127)     1062 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/lveval/lveval_multifieldqa_en_mixup.py
--rw-r--r--   0 runner    (1001) docker     (127)     1062 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/lveval/lveval_multifieldqa_zh_mixup.py
--rw-r--r--   0 runner    (1001) docker     (127)     1752 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/mastermath2024v1.py
--rw-r--r--   0 runner    (1001) docker     (127)    18070 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/math.py
--rw-r--r--   0 runner    (1001) docker     (127)      878 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/math401.py
--rw-r--r--   0 runner    (1001) docker     (127)     9854 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/math_intern.py
--rw-r--r--   0 runner    (1001) docker     (127)     3339 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/mathbench.py
--rw-r--r--   0 runner    (1001) docker     (127)    17399 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/mbpp.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 09:22:04.000000 opencompass-0.2.4/opencompass/datasets/medbench/
--rw-r--r--   0 runner    (1001) docker     (127)       60 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/medbench/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3118 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/medbench/constructions.py
--rw-r--r--   0 runner    (1001) docker     (127)    13752 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/medbench/dataset_loader.py
--rw-r--r--   0 runner    (1001) docker     (127)     1422 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/medbench/evaluation.py
--rw-r--r--   0 runner    (1001) docker     (127)     4572 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/medbench/math_equivalence.py
--rw-r--r--   0 runner    (1001) docker     (127)    21958 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/medbench/medbench.py
--rw-r--r--   0 runner    (1001) docker     (127)     7082 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/medbench/post_process.py
--rw-r--r--   0 runner    (1001) docker     (127)     1097 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/medbench/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     3242 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/mmlu.py
--rw-r--r--   0 runner    (1001) docker     (127)     2258 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/multirc.py
--rw-r--r--   0 runner    (1001) docker     (127)     1423 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/narrativeqa.py
--rw-r--r--   0 runner    (1001) docker     (127)     3055 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/natural_question.py
--rw-r--r--   0 runner    (1001) docker     (127)     1804 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/natural_question_cn.py
--rw-r--r--   0 runner    (1001) docker     (127)     1899 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/obqa.py
--rw-r--r--   0 runner    (1001) docker     (127)     4094 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/piqa.py
--rw-r--r--   0 runner    (1001) docker     (127)     1064 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/py150.py
--rw-r--r--   0 runner    (1001) docker     (127)     1355 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/qasper.py
--rw-r--r--   0 runner    (1001) docker     (127)     1667 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/qaspercut.py
--rw-r--r--   0 runner    (1001) docker     (127)     1080 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/race.py
--rw-r--r--   0 runner    (1001) docker     (127)     1279 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/realtoxicprompts.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 09:22:04.000000 opencompass-0.2.4/opencompass/datasets/reasonbench/
--rw-r--r--   0 runner    (1001) docker     (127)     1278 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/reasonbench/ReasonBenchDataset.py
--rw-r--r--   0 runner    (1001) docker     (127)       54 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/reasonbench/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2626 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/record.py
--rw-r--r--   0 runner    (1001) docker     (127)     3035 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/rolebench.py
--rw-r--r--   0 runner    (1001) docker     (127)      558 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/safety.py
--rw-r--r--   0 runner    (1001) docker     (127)     1234 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/scibench.py
--rw-r--r--   0 runner    (1001) docker     (127)     4253 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/siqa.py
--rw-r--r--   0 runner    (1001) docker     (127)     2339 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/squad20.py
--rw-r--r--   0 runner    (1001) docker     (127)     1763 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/storycloze.py
--rw-r--r--   0 runner    (1001) docker     (127)      841 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/strategyqa.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 09:22:04.000000 opencompass-0.2.4/opencompass/datasets/subjective/
--rw-r--r--   0 runner    (1001) docker     (127)      506 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/subjective/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6609 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/subjective/alignbench.py
--rw-r--r--   0 runner    (1001) docker     (127)      866 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/subjective/compass_arena.py
--rw-r--r--   0 runner    (1001) docker     (127)    15280 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/subjective/corev2.py
--rw-r--r--   0 runner    (1001) docker     (127)    25234 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/subjective/creationbench.py
--rw-r--r--   0 runner    (1001) docker     (127)     2600 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/subjective/information_retrival.py
--rw-r--r--   0 runner    (1001) docker     (127)    16510 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/subjective/mtbench.py
--rw-r--r--   0 runner    (1001) docker     (127)     8066 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/subjective/multiround.py
--rw-r--r--   0 runner    (1001) docker     (127)     1047 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/subjective/subjective_cmp.py
--rw-r--r--   0 runner    (1001) docker     (127)      492 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/summedits.py
--rw-r--r--   0 runner    (1001) docker     (127)     1442 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/summscreen.py
--rw-r--r--   0 runner    (1001) docker     (127)      631 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/svamp.py
--rw-r--r--   0 runner    (1001) docker     (127)     8302 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/tabmwp.py
--rw-r--r--   0 runner    (1001) docker     (127)    30857 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/taco.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 09:22:04.000000 opencompass-0.2.4/opencompass/datasets/teval/
--rw-r--r--   0 runner    (1001) docker     (127)     1813 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/teval/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 09:22:04.000000 opencompass-0.2.4/opencompass/datasets/teval/evaluators/
--rw-r--r--   0 runner    (1001) docker     (127)      339 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/teval/evaluators/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5286 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/teval/evaluators/instruct_evaluator.py
--rw-r--r--   0 runner    (1001) docker     (127)    16837 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/teval/evaluators/planning_evaluator.py
--rw-r--r--   0 runner    (1001) docker     (127)    18747 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/teval/evaluators/reason_retrieve_understand_evaluator.py
--rw-r--r--   0 runner    (1001) docker     (127)     3903 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/teval/evaluators/review_evaluator.py
--rw-r--r--   0 runner    (1001) docker     (127)      525 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/teval/schema.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 09:22:04.000000 opencompass-0.2.4/opencompass/datasets/teval/utils/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/teval/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1614 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/teval/utils/convert_results.py
--rw-r--r--   0 runner    (1001) docker     (127)     1573 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/teval/utils/format_load.py
--rw-r--r--   0 runner    (1001) docker     (127)      297 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/teval/utils/meta_template.py
--rw-r--r--   0 runner    (1001) docker     (127)     2479 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/teval/utils/template.py
--rw-r--r--   0 runner    (1001) docker     (127)     2337 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/tnews.py
--rw-r--r--   0 runner    (1001) docker     (127)     3326 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/triviaqa.py
--rw-r--r--   0 runner    (1001) docker     (127)     1930 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/triviaqarc.py
--rw-r--r--   0 runner    (1001) docker     (127)     8110 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/truthfulqa.py
--rw-r--r--   0 runner    (1001) docker     (127)     2805 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/tydiqa.py
--rw-r--r--   0 runner    (1001) docker     (127)      933 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/wic.py
--rw-r--r--   0 runner    (1001) docker     (127)     2125 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/wikibench.py
--rw-r--r--   0 runner    (1001) docker     (127)      602 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/winograd.py
--rw-r--r--   0 runner    (1001) docker     (127)     3110 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/winogrande.py
--rw-r--r--   0 runner    (1001) docker     (127)      560 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/wnli.py
--rw-r--r--   0 runner    (1001) docker     (127)     3580 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/wsc.py
--rw-r--r--   0 runner    (1001) docker     (127)      853 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/xcopa.py
--rw-r--r--   0 runner    (1001) docker     (127)     3440 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/xiezhi.py
--rw-r--r--   0 runner    (1001) docker     (127)     1183 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/xlsum.py
--rw-r--r--   0 runner    (1001) docker     (127)     1104 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/datasets/xsum.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 09:22:04.000000 opencompass-0.2.4/opencompass/metrics/
--rw-r--r--   0 runner    (1001) docker     (127)      163 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/metrics/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2112 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/metrics/dump_results.py
--rw-r--r--   0 runner    (1001) docker     (127)     3139 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/metrics/mme_score.py
--rw-r--r--   0 runner    (1001) docker     (127)     2317 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/metrics/seedbench.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 09:22:04.000000 opencompass-0.2.4/opencompass/models/
--rw-r--r--   0 runner    (1001) docker     (127)     2009 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3479 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/models/accessory.py
--rw-r--r--   0 runner    (1001) docker     (127)     5798 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/models/ai360_api.py
--rw-r--r--   0 runner    (1001) docker     (127)     6061 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/models/alaya.py
--rw-r--r--   0 runner    (1001) docker     (127)     9087 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/models/baichuan_api.py
--rw-r--r--   0 runner    (1001) docker     (127)     7883 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/models/baidu_api.py
--rw-r--r--   0 runner    (1001) docker     (127)    19036 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/models/base.py
--rw-r--r--   0 runner    (1001) docker     (127)    17907 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/models/base_api.py
--rw-r--r--   0 runner    (1001) docker     (127)     5697 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/models/bytedance_api.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 09:22:04.000000 opencompass-0.2.4/opencompass/models/claude_api/
--rw-r--r--   0 runner    (1001) docker     (127)       53 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/models/claude_api/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4135 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/models/claude_api/claude_api.py
--rw-r--r--   0 runner    (1001) docker     (127)     3948 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/models/claude_api/postprocessors.py
--rw-r--r--   0 runner    (1001) docker     (127)     8417 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/models/gemini_api.py
--rw-r--r--   0 runner    (1001) docker     (127)    15669 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/models/glm.py
--rw-r--r--   0 runner    (1001) docker     (127)    33978 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/models/huggingface.py
--rw-r--r--   0 runner    (1001) docker     (127)     3964 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/models/hunyuan_api.py
--rw-r--r--   0 runner    (1001) docker     (127)     4935 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/models/intern_model.py
--rw-r--r--   0 runner    (1001) docker     (127)     4975 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/models/krgpt_api.py
--rw-r--r--   0 runner    (1001) docker     (127)     6040 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/models/lagent.py
--rw-r--r--   0 runner    (1001) docker     (127)     1672 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/models/langchain.py
--rw-r--r--   0 runner    (1001) docker     (127)     6971 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/models/lightllm_api.py
--rw-r--r--   0 runner    (1001) docker     (127)    10330 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/models/llama2.py
--rw-r--r--   0 runner    (1001) docker     (127)     6135 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/models/lmdeploy_pytorch.py
--rw-r--r--   0 runner    (1001) docker     (127)     7555 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/models/lmdeploy_tis.py
--rw-r--r--   0 runner    (1001) docker     (127)     6288 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/models/minimax_api.py
--rw-r--r--   0 runner    (1001) docker     (127)     3614 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/models/mistral_api.py
--rw-r--r--   0 runner    (1001) docker     (127)     4457 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/models/mixtral.py
--rw-r--r--   0 runner    (1001) docker     (127)     9837 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/models/modelscope.py
--rw-r--r--   0 runner    (1001) docker     (127)     5702 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/models/moonshot_api.py
--rw-r--r--   0 runner    (1001) docker     (127)     4811 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/models/nanbeige_api.py
--rw-r--r--   0 runner    (1001) docker     (127)    17938 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/models/openai_api.py
--rw-r--r--   0 runner    (1001) docker     (127)     6298 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/models/pangu_api.py
--rw-r--r--   0 runner    (1001) docker     (127)     6300 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/models/qwen_api.py
--rw-r--r--   0 runner    (1001) docker     (127)     7021 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/models/sensetime_api.py
--rw-r--r--   0 runner    (1001) docker     (127)     8674 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/models/turbomind.py
--rw-r--r--   0 runner    (1001) docker     (127)     5343 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/models/turbomind_api.py
--rw-r--r--   0 runner    (1001) docker     (127)     4980 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/models/turbomind_tis.py
--rw-r--r--   0 runner    (1001) docker     (127)     4531 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/models/unigpt_api.py
--rw-r--r--   0 runner    (1001) docker     (127)     4671 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/models/vllm.py
--rw-r--r--   0 runner    (1001) docker     (127)     7695 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/models/xunfei_api.py
--rw-r--r--   0 runner    (1001) docker     (127)     8202 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/models/yayi_api.py
--rw-r--r--   0 runner    (1001) docker     (127)     4199 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/models/zhipuai_api.py
--rw-r--r--   0 runner    (1001) docker     (127)     6221 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/models/zhipuai_v2_api.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 09:22:04.000000 opencompass-0.2.4/opencompass/openicl/
--rw-r--r--   0 runner    (1001) docker     (127)      222 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/openicl/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    11670 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/openicl/icl_dataset_reader.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 09:22:04.000000 opencompass-0.2.4/opencompass/openicl/icl_evaluator/
--rw-r--r--   0 runner    (1001) docker     (127)      654 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/openicl/icl_evaluator/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    12256 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/openicl/icl_evaluator/icl_agent_evaluator.py
--rw-r--r--   0 runner    (1001) docker     (127)     1483 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/openicl/icl_evaluator/icl_aucroc_evaluator.py
--rw-r--r--   0 runner    (1001) docker     (127)      185 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/openicl/icl_evaluator/icl_base_evaluator.py
--rw-r--r--   0 runner    (1001) docker     (127)     4249 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/openicl/icl_evaluator/icl_circular_evaluator.py
--rw-r--r--   0 runner    (1001) docker     (127)     1356 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/openicl/icl_evaluator/icl_em_evaluator.py
--rw-r--r--   0 runner    (1001) docker     (127)    11032 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/openicl/icl_evaluator/icl_hf_evaluator.py
--rw-r--r--   0 runner    (1001) docker     (127)     1518 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/openicl/icl_evaluator/icl_jieba_rouge_evaluator.py
--rw-r--r--   0 runner    (1001) docker     (127)      484 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/openicl/icl_evaluator/icl_misc_evaluator.py
--rw-r--r--   0 runner    (1001) docker     (127)     4159 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/openicl/icl_evaluator/icl_plugin_evaluator.py
--rw-r--r--   0 runner    (1001) docker     (127)     7735 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/openicl/icl_evaluator/icl_toxic_evaluator.py
--rw-r--r--   0 runner    (1001) docker     (127)    10812 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/openicl/icl_evaluator/lm_evaluator.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 09:22:04.000000 opencompass-0.2.4/opencompass/openicl/icl_inferencer/
--rw-r--r--   0 runner    (1001) docker     (127)      684 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/openicl/icl_inferencer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4976 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/openicl/icl_inferencer/icl_agent_inferencer.py
--rw-r--r--   0 runner    (1001) docker     (127)     9310 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/openicl/icl_inferencer/icl_attack_inferencer.py
--rw-r--r--   0 runner    (1001) docker     (127)     7632 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/openicl/icl_inferencer/icl_base_inferencer.py
--rw-r--r--   0 runner    (1001) docker     (127)    15100 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/openicl/icl_inferencer/icl_chat_inferencer.py
--rw-r--r--   0 runner    (1001) docker     (127)    11661 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/openicl/icl_inferencer/icl_clp_inferencer.py
--rw-r--r--   0 runner    (1001) docker     (127)    11517 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/openicl/icl_inferencer/icl_gen_inferencer.py
--rw-r--r--   0 runner    (1001) docker     (127)     8894 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/openicl/icl_inferencer/icl_ll_inferencer.py
--rw-r--r--   0 runner    (1001) docker     (127)     7396 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/openicl/icl_inferencer/icl_mink_percent_inferencer.py
--rw-r--r--   0 runner    (1001) docker     (127)     9309 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/openicl/icl_inferencer/icl_ppl_inferencer.py
--rw-r--r--   0 runner    (1001) docker     (127)     7310 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/openicl/icl_inferencer/icl_ppl_only_inferencer.py
--rw-r--r--   0 runner    (1001) docker     (127)     8659 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/openicl/icl_inferencer/icl_sc_inferencer.py
--rw-r--r--   0 runner    (1001) docker     (127)    15850 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/openicl/icl_inferencer/icl_tot_inferencer.py
--rw-r--r--   0 runner    (1001) docker     (127)    10855 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/openicl/icl_prompt_template.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 09:22:04.000000 opencompass-0.2.4/opencompass/openicl/icl_retriever/
--rw-r--r--   0 runner    (1001) docker     (127)      489 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/openicl/icl_retriever/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    12600 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/openicl/icl_retriever/icl_base_retriever.py
--rw-r--r--   0 runner    (1001) docker     (127)     3221 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/openicl/icl_retriever/icl_bm25_retriever.py
--rw-r--r--   0 runner    (1001) docker     (127)     4692 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/openicl/icl_retriever/icl_dpp_retriever.py
--rw-r--r--   0 runner    (1001) docker     (127)     2047 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/openicl/icl_retriever/icl_fix_k_retriever.py
--rw-r--r--   0 runner    (1001) docker     (127)     8463 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/openicl/icl_retriever/icl_mdl_retriever.py
--rw-r--r--   0 runner    (1001) docker     (127)     1295 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/openicl/icl_retriever/icl_random_retriever.py
--rw-r--r--   0 runner    (1001) docker     (127)     8136 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/openicl/icl_retriever/icl_topk_retriever.py
--rw-r--r--   0 runner    (1001) docker     (127)     3749 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/openicl/icl_retriever/icl_votek_retriever.py
--rw-r--r--   0 runner    (1001) docker     (127)     1103 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/openicl/icl_retriever/icl_zero_retriever.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 09:22:04.000000 opencompass-0.2.4/opencompass/openicl/utils/
--rw-r--r--   0 runner    (1001) docker     (127)       31 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/openicl/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1108 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/openicl/utils/logging.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 09:22:04.000000 opencompass-0.2.4/opencompass/partitioners/
--rw-r--r--   0 runner    (1001) docker     (127)      171 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/partitioners/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6769 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/partitioners/base.py
--rw-r--r--   0 runner    (1001) docker     (127)     4454 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/partitioners/mm_naive.py
--rw-r--r--   0 runner    (1001) docker     (127)     2952 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/partitioners/naive.py
--rw-r--r--   0 runner    (1001) docker     (127)     5839 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/partitioners/num_worker.py
--rw-r--r--   0 runner    (1001) docker     (127)     9970 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/partitioners/size.py
--rw-r--r--   0 runner    (1001) docker     (127)     9803 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/partitioners/sub_naive.py
--rw-r--r--   0 runner    (1001) docker     (127)    12028 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/partitioners/sub_size.py
--rw-r--r--   0 runner    (1001) docker     (127)     2420 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/registry.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 09:22:04.000000 opencompass-0.2.4/opencompass/runners/
--rw-r--r--   0 runner    (1001) docker     (127)      173 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/runners/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2975 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/runners/base.py
--rw-r--r--   0 runner    (1001) docker     (127)    12196 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/runners/dlc.py
--rw-r--r--   0 runner    (1001) docker     (127)     7521 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/runners/local.py
--rw-r--r--   0 runner    (1001) docker     (127)     8903 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/runners/local_api.py
--rw-r--r--   0 runner    (1001) docker     (127)     6030 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/runners/slurm.py
--rw-r--r--   0 runner    (1001) docker     (127)    10341 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/runners/slurm_sequential.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 09:22:04.000000 opencompass-0.2.4/opencompass/summarizers/
--rw-r--r--   0 runner    (1001) docker     (127)      175 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/summarizers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2203 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/summarizers/circular.py
--rw-r--r--   0 runner    (1001) docker     (127)    18166 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/summarizers/default.py
--rw-r--r--   0 runner    (1001) docker     (127)    15291 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/summarizers/multi_model.py
--rw-r--r--   0 runner    (1001) docker     (127)    31335 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/summarizers/needlebench.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 09:22:04.000000 opencompass-0.2.4/opencompass/summarizers/subjective/
--rw-r--r--   0 runner    (1001) docker     (127)      390 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/summarizers/subjective/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    13704 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/summarizers/subjective/alignmentbench.py
--rw-r--r--   0 runner    (1001) docker     (127)     7075 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/summarizers/subjective/alpacaeval.py
--rw-r--r--   0 runner    (1001) docker     (127)    10695 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/summarizers/subjective/compass_arena.py
--rw-r--r--   0 runner    (1001) docker     (127)    10281 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/summarizers/subjective/corev2.py
--rw-r--r--   0 runner    (1001) docker     (127)     2381 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/summarizers/subjective/creationbench.py
--rw-r--r--   0 runner    (1001) docker     (127)     4464 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/summarizers/subjective/information_retrival.py
--rw-r--r--   0 runner    (1001) docker     (127)     5556 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/summarizers/subjective/mtbench.py
--rw-r--r--   0 runner    (1001) docker     (127)     5811 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/summarizers/subjective/multiround.py
--rw-r--r--   0 runner    (1001) docker     (127)     1044 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/summarizers/subjective/subjective_post_process.py
--rw-r--r--   0 runner    (1001) docker     (127)     2624 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/summarizers/subjective/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    16797 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/summarizers/summarizer_pretrain.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 09:22:04.000000 opencompass-0.2.4/opencompass/tasks/
--rw-r--r--   0 runner    (1001) docker     (127)      191 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/tasks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2779 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/tasks/base.py
--rw-r--r--   0 runner    (1001) docker     (127)     3719 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/tasks/llm_eval.py
--rw-r--r--   0 runner    (1001) docker     (127)     5354 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/tasks/mm_infer.py
--rw-r--r--   0 runner    (1001) docker     (127)     8576 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/tasks/openicl_attack.py
--rw-r--r--   0 runner    (1001) docker     (127)    15009 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/tasks/openicl_eval.py
--rw-r--r--   0 runner    (1001) docker     (127)     6618 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/tasks/openicl_infer.py
--rw-r--r--   0 runner    (1001) docker     (127)    18746 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/tasks/subjective_eval.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 09:22:04.000000 opencompass-0.2.4/opencompass/utils/
--rw-r--r--   0 runner    (1001) docker     (127)      377 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2384 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/utils/abbr.py
--rw-r--r--   0 runner    (1001) docker     (127)      361 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/utils/auxiliary.py
--rw-r--r--   0 runner    (1001) docker     (127)      802 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/utils/build.py
--rw-r--r--   0 runner    (1001) docker     (127)      352 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/utils/collect_env.py
--rw-r--r--   0 runner    (1001) docker     (127)      812 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/utils/dependency.py
--rw-r--r--   0 runner    (1001) docker     (127)      628 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/utils/file.py
--rw-r--r--   0 runner    (1001) docker     (127)     5645 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/utils/fileio.py
--rw-r--r--   0 runner    (1001) docker     (127)     1706 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/utils/lark.py
--rw-r--r--   0 runner    (1001) docker     (127)     2453 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/utils/logging.py
--rw-r--r--   0 runner    (1001) docker     (127)     2776 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/utils/menu.py
--rw-r--r--   0 runner    (1001) docker     (127)     7531 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/utils/prompt.py
--rw-r--r--   0 runner    (1001) docker     (127)    15386 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/utils/run.py
--rw-r--r--   0 runner    (1001) docker     (127)     5940 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/utils/text_postprocessors.py
--rw-r--r--   0 runner    (1001) docker     (127)     1667 2024-04-23 09:21:52.000000 opencompass-0.2.4/opencompass/utils/types.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-23 09:22:04.000000 opencompass-0.2.4/opencompass.egg-info/
--rw-r--r--   0 runner    (1001) docker     (127)    23085 2024-04-23 09:22:04.000000 opencompass-0.2.4/opencompass.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (127)    15865 2024-04-23 09:22:04.000000 opencompass-0.2.4/opencompass.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (127)        1 2024-04-23 09:22:04.000000 opencompass-0.2.4/opencompass.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (127)       59 2024-04-23 09:22:04.000000 opencompass-0.2.4/opencompass.egg-info/entry_points.txt
--rw-r--r--   0 runner    (1001) docker     (127)      558 2024-04-23 09:22:04.000000 opencompass-0.2.4/opencompass.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (127)       12 2024-04-23 09:22:04.000000 opencompass-0.2.4/opencompass.egg-info/top_level.txt
--rw-r--r--   0 runner    (1001) docker     (127)       38 2024-04-23 09:22:04.000000 opencompass-0.2.4/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (127)     5127 2024-04-23 09:21:52.000000 opencompass-0.2.4/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-29 16:35:27.000000 opencompass-0.2.5/
+-rw-r--r--   0 runner    (1001) docker     (127)    23956 2024-05-29 16:35:27.000000 opencompass-0.2.5/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (127)    18713 2024-05-29 16:35:16.000000 opencompass-0.2.5/README.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-29 16:35:27.000000 opencompass-0.2.5/opencompass/
+-rw-r--r--   0 runner    (1001) docker     (127)       22 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-29 16:35:27.000000 opencompass-0.2.5/opencompass/datasets/
+-rw-r--r--   0 runner    (1001) docker     (127)     1190 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/FinanceIQ.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5459 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/GaokaoBench.py
+-rw-r--r--   0 runner    (1001) docker     (127)      988 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/MMLUArabic.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-29 16:35:27.000000 opencompass-0.2.5/opencompass/datasets/NPHardEval/
+-rw-r--r--   0 runner    (1001) docker     (127)      388 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/NPHardEval/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6698 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/NPHardEval/cmp_GCP_D.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6980 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/NPHardEval/cmp_KSP.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6258 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/NPHardEval/cmp_TSP_D.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7326 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/NPHardEval/hard_GCP.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8407 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/NPHardEval/hard_MSP.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8515 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/NPHardEval/hard_TSP.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4659 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/NPHardEval/p_BSP.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5540 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/NPHardEval/p_EDP.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8341 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/NPHardEval/p_SPP.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10534 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/NPHardEval/prompts.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1543 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/NPHardEval/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1236 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/OpenFinData.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1948 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/QuALITY.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-29 16:35:27.000000 opencompass-0.2.5/opencompass/datasets/TheoremQA/
+-rw-r--r--   0 runner    (1001) docker     (127)      215 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/TheoremQA/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1007 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/TheoremQA/legacy.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2069 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/TheoremQA/main.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2836 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/TheoremQA/number_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3620 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/TheoremQA/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5553 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5217 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/advglue.py
+-rw-r--r--   0 runner    (1001) docker     (127)      497 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/afqmcd.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-29 16:35:27.000000 opencompass-0.2.5/opencompass/datasets/agieval/
+-rw-r--r--   0 runner    (1001) docker     (127)       59 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/agieval/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3390 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/agieval/agieval.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3113 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/agieval/constructions.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16445 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/agieval/dataset_loader.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1422 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/agieval/evaluation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4572 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/agieval/math_equivalence.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6561 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/agieval/post_process.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1097 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/agieval/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)      431 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/anli.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1706 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/anthropics_evals.py
+-rw-r--r--   0 runner    (1001) docker     (127)    32935 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/apps.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3270 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/arc.py
+-rw-r--r--   0 runner    (1001) docker     (127)      581 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/ax.py
+-rw-r--r--   0 runner    (1001) docker     (127)      692 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/base.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2735 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/bbh.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1451 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/boolq.py
+-rw-r--r--   0 runner    (1001) docker     (127)      498 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/bustum.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2893 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/c3.py
+-rw-r--r--   0 runner    (1001) docker     (127)      611 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/cb.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3015 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/ceval.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1612 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/charm.py
+-rw-r--r--   0 runner    (1001) docker     (127)      963 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/chembench.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1172 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/chid.py
+-rw-r--r--   0 runner    (1001) docker     (127)    24124 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/cibench.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15035 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/circular.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1140 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/civilcomments.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1205 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/clozeTest_maxmin.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1699 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/cluewsc.py
+-rw-r--r--   0 runner    (1001) docker     (127)      987 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/cmb.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1030 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/cmmlu.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1081 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/cmnli.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1396 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/cmrc.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1429 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/commonsenseqa.py
+-rw-r--r--   0 runner    (1001) docker     (127)      845 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/commonsenseqa_cn.py
+-rw-r--r--   0 runner    (1001) docker     (127)      500 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/copa.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2704 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/crowspairs.py
+-rw-r--r--   0 runner    (1001) docker     (127)      523 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/crowspairs_cn.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1028 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/csl.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17709 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/custom.py
+-rw-r--r--   0 runner    (1001) docker     (127)      693 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/cvalues.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1396 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/drcd.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1641 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/drop.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2308 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/drop_simple_eval.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15872 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/ds1000.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2280 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/ds1000_interpreter.py
+-rw-r--r--   0 runner    (1001) docker     (127)      686 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/eprstmt.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1600 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/flames.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1832 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/flores.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6536 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/game24.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1502 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/govrepcrs.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3641 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/gpqa.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4421 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/gsm8k.py
+-rw-r--r--   0 runner    (1001) docker     (127)      594 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/gsm_hard.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4993 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/hellaswag.py
+-rw-r--r--   0 runner    (1001) docker     (127)      257 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/huggingface.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9354 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/humaneval.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7425 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/humaneval_multi.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8969 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/humanevalx.py
+-rw-r--r--   0 runner    (1001) docker     (127)      483 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/hungarian_math.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-29 16:35:27.000000 opencompass-0.2.5/opencompass/datasets/infinitebench/
+-rw-r--r--   0 runner    (1001) docker     (127)      734 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/infinitebench/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1054 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/infinitebench/infinitebench_codedebug.py
+-rw-r--r--   0 runner    (1001) docker     (127)      896 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/infinitebench/infinitebench_coderun.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1428 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/infinitebench/infinitebench_endia.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1049 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/infinitebench/infinitebench_enmc.py
+-rw-r--r--   0 runner    (1001) docker     (127)      687 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/infinitebench/infinitebench_enqa.py
+-rw-r--r--   0 runner    (1001) docker     (127)      567 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/infinitebench/infinitebench_ensum.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1514 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/infinitebench/infinitebench_mathcalc.py
+-rw-r--r--   0 runner    (1001) docker     (127)      854 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/infinitebench/infinitebench_mathfind.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1381 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/infinitebench/infinitebench_retrievekv.py
+-rw-r--r--   0 runner    (1001) docker     (127)      688 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/infinitebench/infinitebench_retrievenumber.py
+-rw-r--r--   0 runner    (1001) docker     (127)      689 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/infinitebench/infinitebench_retrievepasskey.py
+-rw-r--r--   0 runner    (1001) docker     (127)      687 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/infinitebench/infinitebench_zhqa.py
+-rw-r--r--   0 runner    (1001) docker     (127)      471 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/infinitebench/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)      421 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/iwslt2017.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1036 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/jigsawmultilingual.py
+-rw-r--r--   0 runner    (1001) docker     (127)      410 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/jsonl.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4996 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/kaoshi.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1361 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/lambada.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-29 16:35:27.000000 opencompass-0.2.5/opencompass/datasets/lawbench/
+-rw-r--r--   0 runner    (1001) docker     (127)       52 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/lawbench/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2494 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/lawbench/lawbench.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1233 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/lcsts.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-29 16:35:27.000000 opencompass-0.2.5/opencompass/datasets/leval/
+-rw-r--r--   0 runner    (1001) docker     (127)     1084 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/leval/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6544 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/leval/evaluators.py
+-rw-r--r--   0 runner    (1001) docker     (127)      844 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/leval/leval_coursera.py
+-rw-r--r--   0 runner    (1001) docker     (127)      898 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/leval/leval_financial_qa.py
+-rw-r--r--   0 runner    (1001) docker     (127)      900 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/leval/leval_gov_report_summ.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1623 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/leval/leval_gsm100.py
+-rw-r--r--   0 runner    (1001) docker     (127)      902 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/leval/leval_legal_contract_qa.py
+-rw-r--r--   0 runner    (1001) docker     (127)      898 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/leval/leval_meeting_summ.py
+-rw-r--r--   0 runner    (1001) docker     (127)      897 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/leval/leval_multidoc_qa.py
+-rw-r--r--   0 runner    (1001) docker     (127)      898 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/leval/leval_narrattive_qa.py
+-rw-r--r--   0 runner    (1001) docker     (127)      902 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/leval/leval_natural_question.py
+-rw-r--r--   0 runner    (1001) docker     (127)      895 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/leval/leval_news_summ.py
+-rw-r--r--   0 runner    (1001) docker     (127)      901 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/leval/leval_paper_assistant.py
+-rw-r--r--   0 runner    (1001) docker     (127)      897 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/leval/leval_patent_summ.py
+-rw-r--r--   0 runner    (1001) docker     (127)      846 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/leval/leval_quality.py
+-rw-r--r--   0 runner    (1001) docker     (127)      897 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/leval/leval_review_summ.py
+-rw-r--r--   0 runner    (1001) docker     (127)      899 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/leval/leval_scientific_qa.py
+-rw-r--r--   0 runner    (1001) docker     (127)      850 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/leval/leval_topic_retrieval.py
+-rw-r--r--   0 runner    (1001) docker     (127)      839 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/leval/leval_tpo.py
+-rw-r--r--   0 runner    (1001) docker     (127)      897 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/leval/leval_tvshow_summ.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1033 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/llm_compression.py
+-rw-r--r--   0 runner    (1001) docker     (127)      392 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/lmeval.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-29 16:35:27.000000 opencompass-0.2.5/opencompass/datasets/longbench/
+-rw-r--r--   0 runner    (1001) docker     (127)     1585 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/longbench/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9597 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/longbench/evaluators.py
+-rw-r--r--   0 runner    (1001) docker     (127)      754 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/longbench/longbench_2wikim_qa.py
+-rw-r--r--   0 runner    (1001) docker     (127)      754 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/longbench/longbench_dureader.py
+-rw-r--r--   0 runner    (1001) docker     (127)      625 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/longbench/longbench_gov_report.py
+-rw-r--r--   0 runner    (1001) docker     (127)      754 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/longbench/longbench_hotpot_qa.py
+-rw-r--r--   0 runner    (1001) docker     (127)      618 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/longbench/longbench_lcc.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1073 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/longbench/longbench_lsht.py
+-rw-r--r--   0 runner    (1001) docker     (127)      625 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/longbench/longbench_multi_news.py
+-rw-r--r--   0 runner    (1001) docker     (127)      761 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/longbench/longbench_multifieldqa_en.py
+-rw-r--r--   0 runner    (1001) docker     (127)      761 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/longbench/longbench_multifieldqa_zh.py
+-rw-r--r--   0 runner    (1001) docker     (127)      753 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/longbench/longbench_musique.py
+-rw-r--r--   0 runner    (1001) docker     (127)      757 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/longbench/longbench_narrative_qa.py
+-rw-r--r--   0 runner    (1001) docker     (127)      628 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/longbench/longbench_passage_count.py
+-rw-r--r--   0 runner    (1001) docker     (127)      766 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/longbench/longbench_passage_retrieval_en.py
+-rw-r--r--   0 runner    (1001) docker     (127)      766 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/longbench/longbench_passage_retrieval_zh.py
+-rw-r--r--   0 runner    (1001) docker     (127)      752 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/longbench/longbench_qasper.py
+-rw-r--r--   0 runner    (1001) docker     (127)      751 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/longbench/longbench_qmsum.py
+-rw-r--r--   0 runner    (1001) docker     (127)      755 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/longbench/longbench_repobench.py
+-rw-r--r--   0 runner    (1001) docker     (127)      916 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/longbench/longbench_samsum.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1073 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/longbench/longbench_trec.py
+-rw-r--r--   0 runner    (1001) docker     (127)      920 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/longbench/longbench_trivia_qa.py
+-rw-r--r--   0 runner    (1001) docker     (127)      620 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/longbench/longbench_vcsum.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-29 16:35:27.000000 opencompass-0.2.5/opencompass/datasets/lveval/
+-rw-r--r--   0 runner    (1001) docker     (127)      833 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/lveval/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12371 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/lveval/evaluators.py
+-rw-r--r--   0 runner    (1001) docker     (127)      867 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/lveval/lveval_cmrc_mixup.py
+-rw-r--r--   0 runner    (1001) docker     (127)      752 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/lveval/lveval_dureader_mixup.py
+-rw-r--r--   0 runner    (1001) docker     (127)      875 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/lveval/lveval_factrecall_en.py
+-rw-r--r--   0 runner    (1001) docker     (127)      875 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/lveval/lveval_factrecall_zh.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1060 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/lveval/lveval_hotpotwikiqa_mixup.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1051 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/lveval/lveval_lic_mixup.py
+-rw-r--r--   0 runner    (1001) docker     (127)      937 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/lveval/lveval_loogle_CR_mixup.py
+-rw-r--r--   0 runner    (1001) docker     (127)      938 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/lveval/lveval_loogle_MIR_mixup.py
+-rw-r--r--   0 runner    (1001) docker     (127)      937 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/lveval/lveval_loogle_SD_mixup.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1062 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/lveval/lveval_multifieldqa_en_mixup.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1062 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/lveval/lveval_multifieldqa_zh_mixup.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1752 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/mastermath2024v1.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18529 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/math.py
+-rw-r--r--   0 runner    (1001) docker     (127)      878 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/math401.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9854 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/math_intern.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3339 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/mathbench.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17571 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/mbpp.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-29 16:35:27.000000 opencompass-0.2.5/opencompass/datasets/medbench/
+-rw-r--r--   0 runner    (1001) docker     (127)       60 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/medbench/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3118 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/medbench/constructions.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13752 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/medbench/dataset_loader.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1422 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/medbench/evaluation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4572 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/medbench/math_equivalence.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21958 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/medbench/medbench.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7082 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/medbench/post_process.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1097 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/medbench/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2200 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/mgsm.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3242 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/mmlu.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2258 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/multirc.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1423 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/narrativeqa.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3055 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/natural_question.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1804 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/natural_question_cn.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1899 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/obqa.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4094 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/piqa.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1064 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/py150.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1355 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/qasper.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1667 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/qaspercut.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1080 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/race.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1279 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/realtoxicprompts.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-29 16:35:27.000000 opencompass-0.2.5/opencompass/datasets/reasonbench/
+-rw-r--r--   0 runner    (1001) docker     (127)     1278 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/reasonbench/ReasonBenchDataset.py
+-rw-r--r--   0 runner    (1001) docker     (127)       54 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/reasonbench/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2626 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/record.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3035 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/rolebench.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6038 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/s3eval.py
+-rw-r--r--   0 runner    (1001) docker     (127)      558 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/safety.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1234 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/scibench.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4253 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/siqa.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2339 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/squad20.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1763 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/storycloze.py
+-rw-r--r--   0 runner    (1001) docker     (127)      841 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/strategyqa.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-29 16:35:27.000000 opencompass-0.2.5/opencompass/datasets/subjective/
+-rw-r--r--   0 runner    (1001) docker     (127)      633 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/subjective/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6609 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/subjective/alignbench.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1120 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/subjective/arena_hard.py
+-rw-r--r--   0 runner    (1001) docker     (127)      866 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/subjective/compass_arena.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2724 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/subjective/compassbench.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15280 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/subjective/corev2.py
+-rw-r--r--   0 runner    (1001) docker     (127)    25234 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/subjective/creationbench.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2600 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/subjective/information_retrival.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16510 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/subjective/mtbench.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8066 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/subjective/multiround.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1047 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/subjective/subjective_cmp.py
+-rw-r--r--   0 runner    (1001) docker     (127)      492 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/summedits.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1442 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/summscreen.py
+-rw-r--r--   0 runner    (1001) docker     (127)      631 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/svamp.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8302 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/tabmwp.py
+-rw-r--r--   0 runner    (1001) docker     (127)    31013 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/taco.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-29 16:35:27.000000 opencompass-0.2.5/opencompass/datasets/teval/
+-rw-r--r--   0 runner    (1001) docker     (127)     1813 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/teval/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-29 16:35:27.000000 opencompass-0.2.5/opencompass/datasets/teval/evaluators/
+-rw-r--r--   0 runner    (1001) docker     (127)      339 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/teval/evaluators/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5286 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/teval/evaluators/instruct_evaluator.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16837 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/teval/evaluators/planning_evaluator.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18747 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/teval/evaluators/reason_retrieve_understand_evaluator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3903 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/teval/evaluators/review_evaluator.py
+-rw-r--r--   0 runner    (1001) docker     (127)      525 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/teval/schema.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-29 16:35:27.000000 opencompass-0.2.5/opencompass/datasets/teval/utils/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/teval/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1614 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/teval/utils/convert_results.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1573 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/teval/utils/format_load.py
+-rw-r--r--   0 runner    (1001) docker     (127)      297 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/teval/utils/meta_template.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2479 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/teval/utils/template.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2337 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/tnews.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3326 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/triviaqa.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1930 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/triviaqarc.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8110 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/truthfulqa.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2805 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/tydiqa.py
+-rw-r--r--   0 runner    (1001) docker     (127)      933 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/wic.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2125 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/wikibench.py
+-rw-r--r--   0 runner    (1001) docker     (127)      602 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/winograd.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3554 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/winogrande.py
+-rw-r--r--   0 runner    (1001) docker     (127)      560 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/wnli.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3580 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/wsc.py
+-rw-r--r--   0 runner    (1001) docker     (127)      853 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/xcopa.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3440 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/xiezhi.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1183 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/xlsum.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1104 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/datasets/xsum.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-29 16:35:27.000000 opencompass-0.2.5/opencompass/metrics/
+-rw-r--r--   0 runner    (1001) docker     (127)      163 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/metrics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2112 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/metrics/dump_results.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3139 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/metrics/mme_score.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2317 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/metrics/seedbench.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-29 16:35:27.000000 opencompass-0.2.5/opencompass/models/
+-rw-r--r--   0 runner    (1001) docker     (127)     2468 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3479 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/accessory.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5947 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/ai360_api.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6061 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/alaya.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9141 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/baichuan_api.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8595 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/baidu_api.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19036 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/base.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17907 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/base_api.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5697 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/bytedance_api.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-29 16:35:27.000000 opencompass-0.2.5/opencompass/models/claude_api/
+-rw-r--r--   0 runner    (1001) docker     (127)       53 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/claude_api/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4135 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/claude_api/claude_api.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3948 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/claude_api/postprocessors.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6011 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/deepseek_api.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6402 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/gemini_api.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15669 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/glm.py
+-rw-r--r--   0 runner    (1001) docker     (127)    34326 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/huggingface.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18208 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/huggingface_above_v4_33.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3964 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/hunyuan_api.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4935 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/intern_model.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4975 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/krgpt_api.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6123 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/lagent.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1672 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/langchain.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12720 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/lightllm_api.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10330 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/llama2.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6544 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/lmdeploy_pytorch.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7555 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/lmdeploy_tis.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12182 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/minimax_api.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3614 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/mistral_api.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4457 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/mixtral.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9837 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/modelscope.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5702 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/moonshot_api.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4811 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/nanbeige_api.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13213 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/openai_api.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6298 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/pangu_api.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6275 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/qwen_api.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7021 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/sensetime_api.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6173 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/stepfun_api.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9407 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/turbomind.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5343 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/turbomind_api.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4980 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/turbomind_tis.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7864 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/turbomind_with_tf_above_v4_33.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4531 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/unigpt_api.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5144 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/vllm.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4660 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/vllm_with_tf_above_v4_33.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12964 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/xunfei_api.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8202 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/yayi_api.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5993 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/yi_api.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4199 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/zhipuai_api.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6221 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/models/zhipuai_v2_api.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-29 16:35:27.000000 opencompass-0.2.5/opencompass/openicl/
+-rw-r--r--   0 runner    (1001) docker     (127)      222 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/openicl/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11670 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/openicl/icl_dataset_reader.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-29 16:35:27.000000 opencompass-0.2.5/opencompass/openicl/icl_evaluator/
+-rw-r--r--   0 runner    (1001) docker     (127)      706 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/openicl/icl_evaluator/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12256 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/openicl/icl_evaluator/icl_agent_evaluator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1483 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/openicl/icl_evaluator/icl_aucroc_evaluator.py
+-rw-r--r--   0 runner    (1001) docker     (127)      185 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/openicl/icl_evaluator/icl_base_evaluator.py
+-rw-r--r--   0 runner    (1001) docker     (127)      922 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/openicl/icl_evaluator/icl_bpc_evaluator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4249 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/openicl/icl_evaluator/icl_circular_evaluator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1356 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/openicl/icl_evaluator/icl_em_evaluator.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11823 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/openicl/icl_evaluator/icl_hf_evaluator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1518 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/openicl/icl_evaluator/icl_jieba_rouge_evaluator.py
+-rw-r--r--   0 runner    (1001) docker     (127)      484 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/openicl/icl_evaluator/icl_misc_evaluator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4159 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/openicl/icl_evaluator/icl_plugin_evaluator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7735 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/openicl/icl_evaluator/icl_toxic_evaluator.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10970 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/openicl/icl_evaluator/lm_evaluator.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-29 16:35:27.000000 opencompass-0.2.5/opencompass/openicl/icl_inferencer/
+-rw-r--r--   0 runner    (1001) docker     (127)      750 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/openicl/icl_inferencer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4976 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/openicl/icl_inferencer/icl_agent_inferencer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9310 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/openicl/icl_inferencer/icl_attack_inferencer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7760 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/openicl/icl_inferencer/icl_base_inferencer.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15100 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/openicl/icl_inferencer/icl_chat_inferencer.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11661 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/openicl/icl_inferencer/icl_clp_inferencer.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11517 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/openicl/icl_inferencer/icl_gen_inferencer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8243 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/openicl/icl_inferencer/icl_ll_inferencer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7396 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/openicl/icl_inferencer/icl_mink_percent_inferencer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8316 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/openicl/icl_inferencer/icl_ppl_inferencer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7310 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/openicl/icl_inferencer/icl_ppl_only_inferencer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8659 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/openicl/icl_inferencer/icl_sc_inferencer.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12906 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/openicl/icl_inferencer/icl_sw_ce_loss_inferencer.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15850 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/openicl/icl_inferencer/icl_tot_inferencer.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10855 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/openicl/icl_prompt_template.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-29 16:35:27.000000 opencompass-0.2.5/opencompass/openicl/icl_retriever/
+-rw-r--r--   0 runner    (1001) docker     (127)      489 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/openicl/icl_retriever/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12600 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/openicl/icl_retriever/icl_base_retriever.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3221 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/openicl/icl_retriever/icl_bm25_retriever.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4692 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/openicl/icl_retriever/icl_dpp_retriever.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2047 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/openicl/icl_retriever/icl_fix_k_retriever.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8463 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/openicl/icl_retriever/icl_mdl_retriever.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1295 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/openicl/icl_retriever/icl_random_retriever.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8136 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/openicl/icl_retriever/icl_topk_retriever.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3749 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/openicl/icl_retriever/icl_votek_retriever.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1103 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/openicl/icl_retriever/icl_zero_retriever.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-29 16:35:27.000000 opencompass-0.2.5/opencompass/openicl/utils/
+-rw-r--r--   0 runner    (1001) docker     (127)       31 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/openicl/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1108 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/openicl/utils/logging.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-29 16:35:27.000000 opencompass-0.2.5/opencompass/partitioners/
+-rw-r--r--   0 runner    (1001) docker     (127)      127 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/partitioners/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6769 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/partitioners/base.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2952 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/partitioners/naive.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5934 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/partitioners/num_worker.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9970 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/partitioners/size.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9803 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/partitioners/sub_naive.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12028 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/partitioners/sub_size.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2012 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/registry.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-29 16:35:27.000000 opencompass-0.2.5/opencompass/runners/
+-rw-r--r--   0 runner    (1001) docker     (127)      173 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/runners/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2975 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/runners/base.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12601 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/runners/dlc.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7722 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/runners/local.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8903 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/runners/local_api.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6030 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/runners/slurm.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10341 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/runners/slurm_sequential.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-29 16:35:27.000000 opencompass-0.2.5/opencompass/summarizers/
+-rw-r--r--   0 runner    (1001) docker     (127)      307 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/summarizers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2203 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/summarizers/circular.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18373 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/summarizers/default.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7606 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/summarizers/llm_compression.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1989 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/summarizers/multi_faceted.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15291 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/summarizers/multi_model.py
+-rw-r--r--   0 runner    (1001) docker     (127)    31343 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/summarizers/needlebench.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-29 16:35:27.000000 opencompass-0.2.5/opencompass/summarizers/subjective/
+-rw-r--r--   0 runner    (1001) docker     (127)      557 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/summarizers/subjective/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13704 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/summarizers/subjective/alignmentbench.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4560 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/summarizers/subjective/all_obj.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7075 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/summarizers/subjective/alpacaeval.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12551 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/summarizers/subjective/arenahard.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10695 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/summarizers/subjective/compass_arena.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10791 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/summarizers/subjective/compassbench.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10281 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/summarizers/subjective/corev2.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2381 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/summarizers/subjective/creationbench.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3353 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/summarizers/subjective/flames.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4464 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/summarizers/subjective/information_retrival.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5556 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/summarizers/subjective/mtbench.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5811 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/summarizers/subjective/multiround.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1044 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/summarizers/subjective/subjective_post_process.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2624 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/summarizers/subjective/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16797 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/summarizers/summarizer_pretrain.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-29 16:35:27.000000 opencompass-0.2.5/opencompass/tasks/
+-rw-r--r--   0 runner    (1001) docker     (127)      147 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/tasks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2779 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/tasks/base.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3719 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/tasks/llm_eval.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8576 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/tasks/openicl_attack.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15137 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/tasks/openicl_eval.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6622 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/tasks/openicl_infer.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18996 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/tasks/subjective_eval.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-29 16:35:27.000000 opencompass-0.2.5/opencompass/utils/
+-rw-r--r--   0 runner    (1001) docker     (127)      377 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2384 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/utils/abbr.py
+-rw-r--r--   0 runner    (1001) docker     (127)      361 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/utils/auxiliary.py
+-rw-r--r--   0 runner    (1001) docker     (127)      760 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/utils/build.py
+-rw-r--r--   0 runner    (1001) docker     (127)      352 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/utils/collect_env.py
+-rw-r--r--   0 runner    (1001) docker     (127)      812 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/utils/dependency.py
+-rw-r--r--   0 runner    (1001) docker     (127)      652 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/utils/file.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5645 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/utils/fileio.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1706 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/utils/lark.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2453 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/utils/logging.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2776 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/utils/menu.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7805 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/utils/prompt.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17031 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/utils/run.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6182 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/utils/text_postprocessors.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1667 2024-05-29 16:35:16.000000 opencompass-0.2.5/opencompass/utils/types.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-29 16:35:27.000000 opencompass-0.2.5/opencompass.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (127)    23956 2024-05-29 16:35:27.000000 opencompass-0.2.5/opencompass.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (127)    16765 2024-05-29 16:35:27.000000 opencompass-0.2.5/opencompass.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (127)        1 2024-05-29 16:35:27.000000 opencompass-0.2.5/opencompass.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (127)       59 2024-05-29 16:35:27.000000 opencompass-0.2.5/opencompass.egg-info/entry_points.txt
+-rw-r--r--   0 runner    (1001) docker     (127)      573 2024-05-29 16:35:27.000000 opencompass-0.2.5/opencompass.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (127)       12 2024-05-29 16:35:27.000000 opencompass-0.2.5/opencompass.egg-info/top_level.txt
+-rw-r--r--   0 runner    (1001) docker     (127)       38 2024-05-29 16:35:27.000000 opencompass-0.2.5/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (127)     5127 2024-05-29 16:35:16.000000 opencompass-0.2.5/setup.py
```

### Comparing `opencompass-0.2.4/PKG-INFO` & `opencompass-0.2.5/PKG-INFO`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: opencompass
-Version: 0.2.4
+Version: 0.2.5
 Summary: A comprehensive toolkit for large model evaluation
 Home-page: https://github.com/open-compass/opencompass
 Author: OpenCompass Contributors
 Maintainer: OpenCompass Authors
 License: Apache License 2.0
 Description: <div align="center">
           <img src="docs/en/_static/image/logo.svg" width="500px"/>
@@ -74,14 +74,19 @@
         > **Attention**<br />
         > We launch the OpenCompass Collaboration project, welcome to support diverse evaluation benchmarks into OpenCompass!
         > Clike [Issue](https://github.com/open-compass/opencompass/issues/248) for more information.
         > Let's work together to build a more powerful OpenCompass toolkit!
         
         ##  What's New <a><img width="35" height="20" src="https://user-images.githubusercontent.com/12782558/212848161-5e783dd6-11e8-4fe0-bbba-39ffb77730be.png"></a>
         
+        - **\[2024.05.08\]** We supported the evaluation of 4 MoE models: [Mixtral-8x22B-v0.1](configs/models/mixtral/hf_mixtral_8x22b_v0_1.py), [Mixtral-8x22B-Instruct-v0.1](configs/models/mixtral/hf_mixtral_8x22b_instruct_v0_1.py), [Qwen1.5-MoE-A2.7B](configs/models/qwen/hf_qwen1_5_moe_a2_7b.py), [Qwen1.5-MoE-A2.7B-Chat](configs/models/qwen/hf_qwen1_5_moe_a2_7b_chat.py). Try them out now!
+        - **\[2024.04.30\]** We supported evaluating a model's compression efficiency by calculating its Bits per Character (BPC) metric on an [external corpora](configs/datasets/llm_compression/README.md) ([official paper](https://github.com/hkust-nlp/llm-compression-intelligence)). Check out the [llm-compression](configs/eval_llm_compression.py) evaluation config now! 
+        - **\[2024.04.29\]** We report the performance of several famous LLMs on the common benchmarks, welcome to [documentation](https://opencompass.readthedocs.io/en/latest/user_guides/corebench.html) for more information! .
+        - **\[2024.04.26\]** We deprecated the multi-madality evaluating function from OpenCompass, related implement has moved to [VLMEvalKit](https://github.com/open-compass/VLMEvalKit), welcome to use! .
+        - **\[2024.04.26\]** We supported the evaluation of [ArenaHard](configs/eval_subjective_arena_hard.py)  welcome to try!.
         - **\[2024.04.22\]** We supported the evaluation of [LLaMA3](configs/models/hf_llama/hf_llama3_8b.py)  [LLaMA3-Instruct](configs/models/hf_llama/hf_llama3_8b_instruct.py), welcome to try! 
         - **\[2024.02.29\]** We supported the MT-Bench, AlpacalEval and AlignBench, more information can be found [here](https://opencompass.readthedocs.io/en/latest/advanced_guides/subjective_evaluation.html)
         - **\[2024.01.30\]** We release OpenCompass 2.0. Click  [CompassKit](https://github.com/open-compass), [CompassHub](https://hub.opencompass.org.cn/home), and [CompassRank](https://rank.opencompass.org.cn/home) for more information !
         
         > [More](docs/en/notes/news.md)
         
         ##  Introduction
@@ -126,15 +131,15 @@
         
         ```bash
         conda create -n opencompass python=3.10 pytorch torchvision torchaudio cpuonly -c pytorch -y
         conda activate opencompass
         git clone https://github.com/open-compass/opencompass opencompass
         cd opencompass
         pip install -e .
-        # also please install requiresments packages via `pip install -r requirements/api.txt` for API models if needed.
+        # also please install requirements packages via `pip install -r requirements/api.txt` for API models if needed.
         ```
         
         ###  Data Preparation
         
         ```bash
         # Download dataset to data/ folder
         wget https://github.com/open-compass/opencompass/releases/download/0.2.2.rc1/OpenCompassData-core-20240207.zip
@@ -161,27 +166,21 @@
         # List all configurations related to llama and mmlu
         python tools/list_configs.py llama mmlu
         ```
         
         You can also evaluate other HuggingFace models via command line. Taking LLaMA-7b as an example:
         
         ```bash
-        python run.py --datasets ceval_ppl mmlu_ppl \
-        --hf-path huggyllama/llama-7b \  # HuggingFace model path
-        --model-kwargs device_map='auto' \  # Arguments for model construction
-        --tokenizer-kwargs padding_side='left' truncation='left' use_fast=False \  # Arguments for tokenizer construction
-        --max-out-len 100 \  # Maximum number of tokens generated
-        --max-seq-len 2048 \  # Maximum sequence length the model can accept
-        --batch-size 8 \  # Batch size
-        --no-batch-padding \  # Don't enable batch padding, infer through for loop to avoid performance loss
-        --num-gpus 1  # Number of minimum required GPUs
+        python run.py --datasets ceval_ppl mmlu_ppl --hf-type base --hf-path huggyllama/llama-7b
         ```
         
-        > **Note**<br />
-        > To run the command above, you will need to remove the comments starting from `# ` first.
+        > \[!TIP\]
+        >
+        > configuration with `_ppl` is designed for base model typically.
+        > configuration with `_gen` can be used for both base model and chat model.
         
         Through the command line or configuration files, OpenCompass also supports evaluating APIs or custom models, as well as more diversified evaluation strategies. Please read the [Quick Start](https://opencompass.readthedocs.io/en/latest/get_started/quick_start.html) to learn how to run an evaluation task.
         
         <p align="right"><a href="#top">Back to top</a></p>
         
         ##  Dataset Support
```

#### html2text {}

```diff
@@ -1,8 +1,8 @@
-Metadata-Version: 2.1 Name: opencompass Version: 0.2.4 Summary: A comprehensive
+Metadata-Version: 2.1 Name: opencompass Version: 0.2.5 Summary: A comprehensive
 toolkit for large model evaluation Home-page: https://github.com/open-compass/
 opencompass Author: OpenCompass Contributors Maintainer: OpenCompass Authors
 License: Apache License 2.0 Description:
                        [docs/en/_static/image/logo.svg]
 
   [![][github-release-shield]][github-release-link] [![][github-releasedate-
   shield]][github-releasedate-link] [![][github-contributors-shield]][github-
@@ -51,16 +51,34 @@
 AI**, click [Get Started](https://ai.meta.com/llama/get-started/#validation) of
 Llama for more information. > **Attention**
 > We launch the OpenCompass Collaboration project, welcome to support diverse
 evaluation benchmarks into OpenCompass! > Clike [Issue](https://github.com/
 open-compass/opencompass/issues/248) for more information. > Let's work
 together to build a more powerful OpenCompass toolkit! ##  What's New
 [https://user-images.githubusercontent.com/12782558/212848161-5e783dd6-11e8-
-4fe0-bbba-39ffb77730be.png]- **\[2024.04.22\]** We supported the evaluation of
-[LLaMA3](configs/models/hf_llama/hf_llama3_8b.py)  [LLaMA3-Instruct]
+4fe0-bbba-39ffb77730be.png]- **\[2024.05.08\]** We supported the evaluation of
+4 MoE models: [Mixtral-8x22B-v0.1](configs/models/mixtral/
+hf_mixtral_8x22b_v0_1.py), [Mixtral-8x22B-Instruct-v0.1](configs/models/
+mixtral/hf_mixtral_8x22b_instruct_v0_1.py), [Qwen1.5-MoE-A2.7B](configs/models/
+qwen/hf_qwen1_5_moe_a2_7b.py), [Qwen1.5-MoE-A2.7B-Chat](configs/models/qwen/
+hf_qwen1_5_moe_a2_7b_chat.py). Try them out now! - **\[2024.04.30\]** We
+supported evaluating a model's compression efficiency by calculating its Bits
+per Character (BPC) metric on an [external corpora](configs/datasets/
+llm_compression/README.md) ([official paper](https://github.com/hkust-nlp/llm-
+compression-intelligence)). Check out the [llm-compression](configs/
+eval_llm_compression.py) evaluation config now!  - **\
+[2024.04.29\]** We report the performance of several famous LLMs on the common
+benchmarks, welcome to [documentation](https://opencompass.readthedocs.io/en/
+latest/user_guides/corebench.html) for more information! . - **\
+[2024.04.26\]** We deprecated the multi-madality evaluating function from
+OpenCompass, related implement has moved to [VLMEvalKit](https://github.com/
+open-compass/VLMEvalKit), welcome to use! . - **\[2024.04.26\]** We
+supported the evaluation of [ArenaHard](configs/eval_subjective_arena_hard.py)
+welcome to try!. - **\[2024.04.22\]** We supported the evaluation
+of [LLaMA3](configs/models/hf_llama/hf_llama3_8b.py)  [LLaMA3-Instruct]
 (configs/models/hf_llama/hf_llama3_8b_instruct.py), welcome to try!
  - **\[2024.02.29\]** We supported the MT-Bench, AlpacalEval and
 AlignBench, more information can be found [here](https://
 opencompass.readthedocs.io/en/latest/advanced_guides/
 subjective_evaluation.html) - **\[2024.01.30\]** We release OpenCompass 2.0.
 Click [CompassKit](https://github.com/open-compass), [CompassHub](https://
 hub.opencompass.org.cn/home), and [CompassRank](https://
@@ -93,15 +111,15 @@
 ```bash conda create --name opencompass python=3.10 pytorch torchvision
 pytorch-cuda -c nvidia -c pytorch -y conda activate opencompass git clone
 https://github.com/open-compass/opencompass opencompass cd opencompass pip
 install -e . ``` #### API Models with CPU-only ```bash conda create -
 n opencompass python=3.10 pytorch torchvision torchaudio cpuonly -c pytorch -
 y conda activate opencompass git clone https://github.com/open-compass/
 opencompass opencompass cd opencompass pip install -e . # also please install
-requiresments packages via `pip install -r requirements/api.txt` for API models
+requirements packages via `pip install -r requirements/api.txt` for API models
 if needed. ``` ###  Data Preparation ```bash # Download dataset to data/
 folder wget https://github.com/open-compass/opencompass/releases/download/
 0.2.2.rc1/OpenCompassData-core-20240207.zip unzip OpenCompassData-core-
 20240207.zip ``` Some third-party features, like Humaneval and Llama, may
 require additional steps to work properly, for detailed steps please refer to
 the [Installation Guide](https://opencompass.readthedocs.io/en/latest/
 get_started/installation.html).
@@ -112,27 +130,20 @@
 following command: ```bash python run.py --models hf_llama_7b --datasets
 mmlu_ppl ceval_ppl ``` OpenCompass has predefined configurations for many
 models and datasets. You can list all available model and dataset
 configurations using the [tools](./docs/en/tools.md#list-configs). ```bash #
 List all configurations python tools/list_configs.py # List all configurations
 related to llama and mmlu python tools/list_configs.py llama mmlu ``` You can
 also evaluate other HuggingFace models via command line. Taking LLaMA-7b as an
-example: ```bash python run.py --datasets ceval_ppl mmlu_ppl \ --hf-path
-huggyllama/llama-7b \ # HuggingFace model path --model-kwargs device_map='auto'
-\ # Arguments for model construction --tokenizer-kwargs padding_side='left'
-truncation='left' use_fast=False \ # Arguments for tokenizer construction --
-max-out-len 100 \ # Maximum number of tokens generated --max-seq-len 2048 \ #
-Maximum sequence length the model can accept --batch-size 8 \ # Batch size --
-no-batch-padding \ # Don't enable batch padding, infer through for loop to
-avoid performance loss --num-gpus 1 # Number of minimum required GPUs ``` >
-**Note**
-> To run the command above, you will need to remove the comments starting from
-`# ` first. Through the command line or configuration files, OpenCompass also
-supports evaluating APIs or custom models, as well as more diversified
-evaluation strategies. Please read the [Quick Start](https://
+example: ```bash python run.py --datasets ceval_ppl mmlu_ppl --hf-type base --
+hf-path huggyllama/llama-7b ``` > \[!TIP\] > > configuration with `_ppl` is
+designed for base model typically. > configuration with `_gen` can be used for
+both base model and chat model. Through the command line or configuration
+files, OpenCompass also supports evaluating APIs or custom models, as well as
+more diversified evaluation strategies. Please read the [Quick Start](https://
 opencompass.readthedocs.io/en/latest/get_started/quick_start.html) to learn how
 to run an evaluation task.
                                                                 _____B_a_c_k_ _t_o_ _t_o_p
 ##  Dataset Support
         LLaanngguuaaggee           KKnnoowwlleeddggee           RReeaassoonniinngg          EExxaammiinnaattiioonn
 WWoorrdd DDeeffiinniittiioonn -   KKnnoowwlleeddggee QQuueessttiioonn  TTeexxttuuaall EEnnttaaiillmmeenntt  JJuunniioorr HHiigghh,, HHiigghh
 WiC - SummEdits     AAnnsswweerriinngg - BoolQ - - CMNLI - OCNLI -   SScchhooooll,, UUnniivveerrssiittyy,,
```

### Comparing `opencompass-0.2.4/README.md` & `opencompass-0.2.5/README.md`

 * *Files 6% similar despite different names*

```diff
@@ -66,14 +66,19 @@
 > **Attention**<br />
 > We launch the OpenCompass Collaboration project, welcome to support diverse evaluation benchmarks into OpenCompass!
 > Clike [Issue](https://github.com/open-compass/opencompass/issues/248) for more information.
 > Let's work together to build a more powerful OpenCompass toolkit!
 
 ##  What's New <a><img width="35" height="20" src="https://user-images.githubusercontent.com/12782558/212848161-5e783dd6-11e8-4fe0-bbba-39ffb77730be.png"></a>
 
+- **\[2024.05.08\]** We supported the evaluation of 4 MoE models: [Mixtral-8x22B-v0.1](configs/models/mixtral/hf_mixtral_8x22b_v0_1.py), [Mixtral-8x22B-Instruct-v0.1](configs/models/mixtral/hf_mixtral_8x22b_instruct_v0_1.py), [Qwen1.5-MoE-A2.7B](configs/models/qwen/hf_qwen1_5_moe_a2_7b.py), [Qwen1.5-MoE-A2.7B-Chat](configs/models/qwen/hf_qwen1_5_moe_a2_7b_chat.py). Try them out now!
+- **\[2024.04.30\]** We supported evaluating a model's compression efficiency by calculating its Bits per Character (BPC) metric on an [external corpora](configs/datasets/llm_compression/README.md) ([official paper](https://github.com/hkust-nlp/llm-compression-intelligence)). Check out the [llm-compression](configs/eval_llm_compression.py) evaluation config now! 
+- **\[2024.04.29\]** We report the performance of several famous LLMs on the common benchmarks, welcome to [documentation](https://opencompass.readthedocs.io/en/latest/user_guides/corebench.html) for more information! .
+- **\[2024.04.26\]** We deprecated the multi-madality evaluating function from OpenCompass, related implement has moved to [VLMEvalKit](https://github.com/open-compass/VLMEvalKit), welcome to use! .
+- **\[2024.04.26\]** We supported the evaluation of [ArenaHard](configs/eval_subjective_arena_hard.py)  welcome to try!.
 - **\[2024.04.22\]** We supported the evaluation of [LLaMA3](configs/models/hf_llama/hf_llama3_8b.py)  [LLaMA3-Instruct](configs/models/hf_llama/hf_llama3_8b_instruct.py), welcome to try! 
 - **\[2024.02.29\]** We supported the MT-Bench, AlpacalEval and AlignBench, more information can be found [here](https://opencompass.readthedocs.io/en/latest/advanced_guides/subjective_evaluation.html)
 - **\[2024.01.30\]** We release OpenCompass 2.0. Click  [CompassKit](https://github.com/open-compass), [CompassHub](https://hub.opencompass.org.cn/home), and [CompassRank](https://rank.opencompass.org.cn/home) for more information !
 
 > [More](docs/en/notes/news.md)
 
 ##  Introduction
@@ -118,15 +123,15 @@
 
 ```bash
 conda create -n opencompass python=3.10 pytorch torchvision torchaudio cpuonly -c pytorch -y
 conda activate opencompass
 git clone https://github.com/open-compass/opencompass opencompass
 cd opencompass
 pip install -e .
-# also please install requiresments packages via `pip install -r requirements/api.txt` for API models if needed.
+# also please install requirements packages via `pip install -r requirements/api.txt` for API models if needed.
 ```
 
 ###  Data Preparation
 
 ```bash
 # Download dataset to data/ folder
 wget https://github.com/open-compass/opencompass/releases/download/0.2.2.rc1/OpenCompassData-core-20240207.zip
@@ -153,27 +158,21 @@
 # List all configurations related to llama and mmlu
 python tools/list_configs.py llama mmlu
 ```
 
 You can also evaluate other HuggingFace models via command line. Taking LLaMA-7b as an example:
 
 ```bash
-python run.py --datasets ceval_ppl mmlu_ppl \
---hf-path huggyllama/llama-7b \  # HuggingFace model path
---model-kwargs device_map='auto' \  # Arguments for model construction
---tokenizer-kwargs padding_side='left' truncation='left' use_fast=False \  # Arguments for tokenizer construction
---max-out-len 100 \  # Maximum number of tokens generated
---max-seq-len 2048 \  # Maximum sequence length the model can accept
---batch-size 8 \  # Batch size
---no-batch-padding \  # Don't enable batch padding, infer through for loop to avoid performance loss
---num-gpus 1  # Number of minimum required GPUs
+python run.py --datasets ceval_ppl mmlu_ppl --hf-type base --hf-path huggyllama/llama-7b
 ```
 
-> **Note**<br />
-> To run the command above, you will need to remove the comments starting from `# ` first.
+> \[!TIP\]
+>
+> configuration with `_ppl` is designed for base model typically.
+> configuration with `_gen` can be used for both base model and chat model.
 
 Through the command line or configuration files, OpenCompass also supports evaluating APIs or custom models, as well as more diversified evaluation strategies. Please read the [Quick Start](https://opencompass.readthedocs.io/en/latest/get_started/quick_start.html) to learn how to run an evaluation task.
 
 <p align="right"><a href="#top">Back to top</a></p>
 
 ##  Dataset Support
```

#### html2text {}

```diff
@@ -47,16 +47,34 @@
 AI**, click [Get Started](https://ai.meta.com/llama/get-started/#validation) of
 Llama for more information. > **Attention**
 > We launch the OpenCompass Collaboration project, welcome to support diverse
 evaluation benchmarks into OpenCompass! > Clike [Issue](https://github.com/
 open-compass/opencompass/issues/248) for more information. > Let's work
 together to build a more powerful OpenCompass toolkit! ##  What's New
 [https://user-images.githubusercontent.com/12782558/212848161-5e783dd6-11e8-
-4fe0-bbba-39ffb77730be.png]- **\[2024.04.22\]** We supported the evaluation of
-[LLaMA3](configs/models/hf_llama/hf_llama3_8b.py)  [LLaMA3-Instruct]
+4fe0-bbba-39ffb77730be.png]- **\[2024.05.08\]** We supported the evaluation of
+4 MoE models: [Mixtral-8x22B-v0.1](configs/models/mixtral/
+hf_mixtral_8x22b_v0_1.py), [Mixtral-8x22B-Instruct-v0.1](configs/models/
+mixtral/hf_mixtral_8x22b_instruct_v0_1.py), [Qwen1.5-MoE-A2.7B](configs/models/
+qwen/hf_qwen1_5_moe_a2_7b.py), [Qwen1.5-MoE-A2.7B-Chat](configs/models/qwen/
+hf_qwen1_5_moe_a2_7b_chat.py). Try them out now! - **\[2024.04.30\]** We
+supported evaluating a model's compression efficiency by calculating its Bits
+per Character (BPC) metric on an [external corpora](configs/datasets/
+llm_compression/README.md) ([official paper](https://github.com/hkust-nlp/llm-
+compression-intelligence)). Check out the [llm-compression](configs/
+eval_llm_compression.py) evaluation config now!  - **\
+[2024.04.29\]** We report the performance of several famous LLMs on the common
+benchmarks, welcome to [documentation](https://opencompass.readthedocs.io/en/
+latest/user_guides/corebench.html) for more information! . - **\
+[2024.04.26\]** We deprecated the multi-madality evaluating function from
+OpenCompass, related implement has moved to [VLMEvalKit](https://github.com/
+open-compass/VLMEvalKit), welcome to use! . - **\[2024.04.26\]** We
+supported the evaluation of [ArenaHard](configs/eval_subjective_arena_hard.py)
+welcome to try!. - **\[2024.04.22\]** We supported the evaluation
+of [LLaMA3](configs/models/hf_llama/hf_llama3_8b.py)  [LLaMA3-Instruct]
 (configs/models/hf_llama/hf_llama3_8b_instruct.py), welcome to try!
  - **\[2024.02.29\]** We supported the MT-Bench, AlpacalEval and
 AlignBench, more information can be found [here](https://
 opencompass.readthedocs.io/en/latest/advanced_guides/
 subjective_evaluation.html) - **\[2024.01.30\]** We release OpenCompass 2.0.
 Click [CompassKit](https://github.com/open-compass), [CompassHub](https://
 hub.opencompass.org.cn/home), and [CompassRank](https://
@@ -89,15 +107,15 @@
 ```bash conda create --name opencompass python=3.10 pytorch torchvision
 pytorch-cuda -c nvidia -c pytorch -y conda activate opencompass git clone
 https://github.com/open-compass/opencompass opencompass cd opencompass pip
 install -e . ``` #### API Models with CPU-only ```bash conda create -
 n opencompass python=3.10 pytorch torchvision torchaudio cpuonly -c pytorch -
 y conda activate opencompass git clone https://github.com/open-compass/
 opencompass opencompass cd opencompass pip install -e . # also please install
-requiresments packages via `pip install -r requirements/api.txt` for API models
+requirements packages via `pip install -r requirements/api.txt` for API models
 if needed. ``` ###  Data Preparation ```bash # Download dataset to data/
 folder wget https://github.com/open-compass/opencompass/releases/download/
 0.2.2.rc1/OpenCompassData-core-20240207.zip unzip OpenCompassData-core-
 20240207.zip ``` Some third-party features, like Humaneval and Llama, may
 require additional steps to work properly, for detailed steps please refer to
 the [Installation Guide](https://opencompass.readthedocs.io/en/latest/
 get_started/installation.html).
@@ -108,27 +126,20 @@
 following command: ```bash python run.py --models hf_llama_7b --datasets
 mmlu_ppl ceval_ppl ``` OpenCompass has predefined configurations for many
 models and datasets. You can list all available model and dataset
 configurations using the [tools](./docs/en/tools.md#list-configs). ```bash #
 List all configurations python tools/list_configs.py # List all configurations
 related to llama and mmlu python tools/list_configs.py llama mmlu ``` You can
 also evaluate other HuggingFace models via command line. Taking LLaMA-7b as an
-example: ```bash python run.py --datasets ceval_ppl mmlu_ppl \ --hf-path
-huggyllama/llama-7b \ # HuggingFace model path --model-kwargs device_map='auto'
-\ # Arguments for model construction --tokenizer-kwargs padding_side='left'
-truncation='left' use_fast=False \ # Arguments for tokenizer construction --
-max-out-len 100 \ # Maximum number of tokens generated --max-seq-len 2048 \ #
-Maximum sequence length the model can accept --batch-size 8 \ # Batch size --
-no-batch-padding \ # Don't enable batch padding, infer through for loop to
-avoid performance loss --num-gpus 1 # Number of minimum required GPUs ``` >
-**Note**
-> To run the command above, you will need to remove the comments starting from
-`# ` first. Through the command line or configuration files, OpenCompass also
-supports evaluating APIs or custom models, as well as more diversified
-evaluation strategies. Please read the [Quick Start](https://
+example: ```bash python run.py --datasets ceval_ppl mmlu_ppl --hf-type base --
+hf-path huggyllama/llama-7b ``` > \[!TIP\] > > configuration with `_ppl` is
+designed for base model typically. > configuration with `_gen` can be used for
+both base model and chat model. Through the command line or configuration
+files, OpenCompass also supports evaluating APIs or custom models, as well as
+more diversified evaluation strategies. Please read the [Quick Start](https://
 opencompass.readthedocs.io/en/latest/get_started/quick_start.html) to learn how
 to run an evaluation task.
                                                                 _____B_a_c_k_ _t_o_ _t_o_p
 ##  Dataset Support
         LLaanngguuaaggee           KKnnoowwlleeddggee           RReeaassoonniinngg          EExxaammiinnaattiioonn
 WWoorrdd DDeeffiinniittiioonn -   KKnnoowwlleeddggee QQuueessttiioonn  TTeexxttuuaall EEnnttaaiillmmeenntt  JJuunniioorr HHiigghh,, HHiigghh
 WiC - SummEdits     AAnnsswweerriinngg - BoolQ - - CMNLI - OCNLI -   SScchhooooll,, UUnniivveerrssiittyy,,
```

### Comparing `opencompass-0.2.4/opencompass/datasets/FinanceIQ.py` & `opencompass-0.2.5/opencompass/datasets/FinanceIQ.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/GaokaoBench.py` & `opencompass-0.2.5/opencompass/datasets/kaoshi.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,39 +1,51 @@
 import json
 import re
 
 from datasets import Dataset
 
 from opencompass.openicl.icl_evaluator import BaseEvaluator
-from opencompass.registry import ICL_EVALUATORS, LOAD_DATASET
 
 from .base import BaseDataset
 
 
-@LOAD_DATASET.register_module()
-class GaokaoBenchDataset(BaseDataset):
+def get_number(options):
+
+    result_string = ''
+    for i, option in enumerate(options, start=65):
+        result_string += f'{chr(i)}. {option}\n'
+    return result_string
+
+
+class KaoshiDataset(BaseDataset):
 
     @staticmethod
-    def load(path: str):
+    def load(path: str, name: str):
+        data_list = []
         with open(path, encoding='utf-8') as f:
-            data = json.load(f)
-        return Dataset.from_list(data['example'])
+            for line in f:
+                data = json.loads(line)
+                if name in ['single_choice', 'multi_choice']:
+                    data['question'] = data['question'].strip(
+                    ) + '\n' + get_number(data['options'])
+                data_list.append(data)
+        return Dataset.from_list(data_list)
 
 
-valid_gaokao_bench_question_types = [
+valid_kaoshi_question_types = [
     'single_choice', 'multi_choice', 'multi_question_choice',
-    'five_out_of_seven', 'cloze', 'subjective', 'correction'
+    'five_out_of_seven', 'cloze', 'judgment'
 ]
 
 
-class GaokaoBenchEvaluator(BaseEvaluator):
+class KaoshiEvaluator(BaseEvaluator):
 
     def __init__(self, question_type) -> None:
         super().__init__()
-        assert question_type in valid_gaokao_bench_question_types
+        assert question_type in valid_kaoshi_question_types
         self.question_type = question_type
 
     def do_predictions_postprocess(self, model_output, answer_lenth=None):
         if self.question_type == 'single_choice':
             model_answer = []
             temp = re.findall(r'[A-D]', model_output[::-1])
             if len(temp) != 0:
@@ -73,27 +85,30 @@
         elif self.question_type == 'five_out_of_seven':
             model_answer = []
             temp = re.findall(r'[A-G]', model_output)
             if len(temp) > 0:
                 for k in range(min(5, len(temp))):
                     model_answer.append(temp[k])
 
+        elif self.question_type in ['cloze', 'judgment']:
+            model_answer = []
+            temp = re.findall(r'(.*?) ', model_output)
+            if len(temp) > 0:
+                model_answer.append(temp[0])
+
         return model_answer
 
     def ensure_same_length(self, pred, refr):
         if len(pred) == len(refr):
             return pred
         return ['Z'] * len(refr)
 
     def score(self, predictions, references):
-        if self.question_type not in [
-                'single_choice', 'multi_choice', 'multi_question_choice',
-                'five_out_of_seven'
-        ]:
-            return {'score': 0}
+        if self.question_type not in valid_kaoshi_question_types:
+            return {'score': 100}
         elif self.question_type == 'multi_choice':
             correct_score, total_score = 0, 0
             for pred, refr in zip(predictions, references):
                 pred = self.do_predictions_postprocess(pred)
                 pred = self.ensure_same_length(pred, refr)
                 for p, r in zip(pred, refr):
                     if p == r:
@@ -109,24 +124,15 @@
         else:
             correct_score, total_score = 0, 0
             for pred, refr in zip(predictions, references):
                 if self.question_type == 'multi_question_choice':
                     pred = self.do_predictions_postprocess(pred, len(refr))
                 else:
                     pred = self.do_predictions_postprocess(pred)
+                if self.question_type in ['cloze', 'judgment']:
+                    refr = [refr]
                 pred = self.ensure_same_length(pred, refr)
                 for p, r in zip(pred, refr):
                     if p == r:
                         correct_score += 1
                     total_score += 1
             return {'score': correct_score / total_score * 100}
-
-
-for question_type in valid_gaokao_bench_question_types:
-    # fix classic closure problem
-    def _gaokao_register(question_type):
-        ICL_EVALUATORS.register_module(
-            name='GaokaoBenchEvaluator' + '_' + question_type,
-            module=lambda *args, **kwargs: GaokaoBenchEvaluator(
-                question_type=question_type, *args, **kwargs))
-
-    _gaokao_register(question_type)
```

### Comparing `opencompass-0.2.4/opencompass/datasets/NPHardEval/cmp_GCP_D.py` & `opencompass-0.2.5/opencompass/datasets/NPHardEval/cmp_GCP_D.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/NPHardEval/cmp_KSP.py` & `opencompass-0.2.5/opencompass/datasets/NPHardEval/cmp_KSP.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/NPHardEval/cmp_TSP_D.py` & `opencompass-0.2.5/opencompass/datasets/NPHardEval/cmp_TSP_D.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/NPHardEval/hard_GCP.py` & `opencompass-0.2.5/opencompass/datasets/NPHardEval/hard_GCP.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/NPHardEval/hard_MSP.py` & `opencompass-0.2.5/opencompass/datasets/NPHardEval/hard_MSP.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/NPHardEval/hard_TSP.py` & `opencompass-0.2.5/opencompass/datasets/NPHardEval/hard_TSP.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/NPHardEval/p_BSP.py` & `opencompass-0.2.5/opencompass/datasets/NPHardEval/p_BSP.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/NPHardEval/p_EDP.py` & `opencompass-0.2.5/opencompass/datasets/NPHardEval/p_EDP.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/NPHardEval/p_SPP.py` & `opencompass-0.2.5/opencompass/datasets/NPHardEval/p_SPP.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/NPHardEval/prompts.py` & `opencompass-0.2.5/opencompass/datasets/NPHardEval/prompts.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/NPHardEval/utils.py` & `opencompass-0.2.5/opencompass/datasets/NPHardEval/utils.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/OpenFinData.py` & `opencompass-0.2.5/opencompass/datasets/OpenFinData.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/QuALITY.py` & `opencompass-0.2.5/opencompass/datasets/QuALITY.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/TheoremQA/legacy.py` & `opencompass-0.2.5/opencompass/datasets/TheoremQA/legacy.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/TheoremQA/main.py` & `opencompass-0.2.5/opencompass/datasets/TheoremQA/main.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/TheoremQA/number_utils.py` & `opencompass-0.2.5/opencompass/datasets/TheoremQA/number_utils.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/TheoremQA/utils.py` & `opencompass-0.2.5/opencompass/datasets/TheoremQA/utils.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/__init__.py` & `opencompass-0.2.5/opencompass/datasets/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -8,14 +8,15 @@
 from .ax import *  # noqa: F401, F403
 from .bbh import *  # noqa: F401, F403
 from .boolq import *  # noqa: F401, F403
 from .bustum import *  # noqa: F401, F403
 from .c3 import *  # noqa: F401, F403
 from .cb import *  # noqa: F401, F403
 from .ceval import *  # noqa: F401, F403
+from .charm import *  # noqa: F401, F403
 from .chembench import *  # noqa: F401, F403
 from .chid import *  # noqa: F401, F403
 from .cibench import *  # noqa: F401, F403
 from .circular import *  # noqa: F401, F403
 from .civilcomments import *  # noqa: F401, F403
 from .clozeTest_maxmin import *  # noqa: F401, F403
 from .cluewsc import *  # noqa: F401, F403
@@ -29,18 +30,20 @@
 from .crowspairs import *  # noqa: F401, F403
 from .crowspairs_cn import *  # noqa: F401, F403
 from .csl import *  # noqa: F401, F403
 from .custom import *  # noqa: F401, F403
 from .cvalues import *  # noqa: F401, F403
 from .drcd import *  # noqa: F401, F403
 from .drop import *  # noqa: F401, F403
+from .drop_simple_eval import *  # noqa: F401, F403
 from .ds1000 import *  # noqa: F401, F403
 from .ds1000_interpreter import *  # noqa: F401, F403
 from .eprstmt import *  # noqa: F401, F403
 from .FinanceIQ import *  # noqa: F401, F403
+from .flames import *  # noqa: F401, F403
 from .flores import *  # noqa: F401, F403
 from .game24 import *  # noqa: F401, F403
 from .GaokaoBench import *  # noqa: F401, F403
 from .govrepcrs import *  # noqa: F401, F403
 from .gpqa import *  # noqa: F401, F403
 from .gsm8k import *  # noqa: F401, F403
 from .gsm_hard import *  # noqa: F401, F403
@@ -55,24 +58,27 @@
 from .jigsawmultilingual import *  # noqa: F401, F403
 from .jsonl import JsonlDataset  # noqa: F401, F403
 from .kaoshi import KaoshiDataset, KaoshiEvaluator  # noqa: F401, F403
 from .lambada import *  # noqa: F401, F403
 from .lawbench import *  # noqa: F401, F403
 from .lcsts import *  # noqa: F401, F403
 from .leval import *  # noqa: F401, F403
+from .llm_compression import LLMCompressionDataset  # noqa: F401, F403
 from .longbench import *  # noqa: F401, F403
 from .lveval import *  # noqa: F401, F403
 from .mastermath2024v1 import *  # noqa: F401, F403
 from .math import *  # noqa: F401, F403
 from .math401 import *  # noqa: F401, F403
 from .math_intern import *  # noqa: F401, F403
 from .mathbench import *  # noqa: F401, F403
 from .mbpp import *  # noqa: F401, F403
 from .medbench import *  # noqa: F401, F403
+from .mgsm import *  # noqa: F401, F403
 from .mmlu import *  # noqa: F401, F403
+from .MMLUArabic import *  # noqa: F401, F403
 from .multirc import *  # noqa: F401, F403
 from .narrativeqa import *  # noqa: F401, F403
 from .natural_question import *  # noqa: F401, F403
 from .natural_question_cn import *  # noqa: F401, F403
 from .NPHardEval import *  # noqa: F401, F403
 from .obqa import *  # noqa: F401, F403
 from .OpenFinData import *  # noqa: F401, F403
```

### Comparing `opencompass-0.2.4/opencompass/datasets/advglue.py` & `opencompass-0.2.5/opencompass/datasets/advglue.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/agieval/agieval.py` & `opencompass-0.2.5/opencompass/datasets/agieval/agieval.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/agieval/constructions.py` & `opencompass-0.2.5/opencompass/datasets/agieval/constructions.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/agieval/dataset_loader.py` & `opencompass-0.2.5/opencompass/datasets/agieval/dataset_loader.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/agieval/evaluation.py` & `opencompass-0.2.5/opencompass/datasets/agieval/evaluation.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/agieval/math_equivalence.py` & `opencompass-0.2.5/opencompass/datasets/agieval/math_equivalence.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/agieval/post_process.py` & `opencompass-0.2.5/opencompass/datasets/agieval/post_process.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/agieval/utils.py` & `opencompass-0.2.5/opencompass/datasets/agieval/utils.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/anthropics_evals.py` & `opencompass-0.2.5/opencompass/datasets/anthropics_evals.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/apps.py` & `opencompass-0.2.5/opencompass/datasets/apps.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/arc.py` & `opencompass-0.2.5/opencompass/datasets/arc.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/ax.py` & `opencompass-0.2.5/opencompass/datasets/ax.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/base.py` & `opencompass-0.2.5/opencompass/datasets/base.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/bbh.py` & `opencompass-0.2.5/opencompass/datasets/bbh.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/boolq.py` & `opencompass-0.2.5/opencompass/datasets/boolq.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/c3.py` & `opencompass-0.2.5/opencompass/datasets/c3.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/cb.py` & `opencompass-0.2.5/opencompass/datasets/cb.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/ceval.py` & `opencompass-0.2.5/opencompass/datasets/ceval.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/chembench.py` & `opencompass-0.2.5/opencompass/datasets/chembench.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/chid.py` & `opencompass-0.2.5/opencompass/datasets/chid.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/cibench.py` & `opencompass-0.2.5/opencompass/datasets/cibench.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,107 +1,61 @@
 import json
 import os
 import os.path as osp
 import re
 import subprocess
 from collections import defaultdict
+from inspect import signature
 from typing import List, Optional
 
 import numpy as np
 from datasets import Dataset
 
+from opencompass.datasets.base import BaseDataset
+from opencompass.datasets.gsm8k import gsm8k_postprocess
 from opencompass.openicl.icl_evaluator import BaseEvaluator
 from opencompass.registry import ICL_EVALUATORS, LOAD_DATASET
 
-from .base import BaseDataset
-
 
 def load_experiment(file: str) -> dict:
-    """Load single experiment file with solutions."""
-    with open(file, 'r') as f:
-        notebook = json.load(f)
-        example = notebook['cells']
-        metadata = notebook['metadata']
-        modules = metadata.get('modules', [])
-        if modules:
-            # these two annotations should be the same
-            assert len(modules) == len(metadata.get('step_types'))
-            # reformat annotations
-            modules = [[_m.strip() for _m in _modules.split('&')]
-                       for _modules in modules]
-        questions = []
-        source_codes = []
-        outputs = []
-        tags = []
-        for cell in example:
-            if cell['cell_type'] == 'markdown':
-                text = ''.join(cell['source']).strip()
-                if modules:
-                    _modules = modules.pop(0)
-                    text += f"Please use {' and '.join(_modules)} modules."
-                text = text.strip() + '\n'
-                # append the formatted text
-                questions.append(text)
-            elif cell['cell_type'] == 'code':
-                source_codes.append(''.join(cell['source']))
-                if cell['outputs'] and 'data' in cell['outputs'][-1]:
-                    if 'image/png' in cell['outputs'][-1]['data']:
-                        # skip vis temporarily due to lack of evaluation
-                        tags.append('vis')
-                        outputs.append(
-                            cell['outputs'][-1]['data']['image/png'])
-                    elif 'text/plain' in cell['outputs'][-1]['data']:
-                        tags.append('general')
-                        outputs.append(''.join(
-                            cell['outputs'][-1]['data']['text/plain']))
-                else:
-                    tags.append('exec')
-                    outputs.append(None)
-    return dict(
-        experiment=file,
-        questions=sum(([
-            dict(role='user', content=question),
-            dict(role='assistant', content=source_code)
-        ] for question, source_code in zip(questions, source_codes)), []),
-        references=dict(outputs=outputs,
-                        tags=tags,
-                        metadata=metadata,
-                        experiment=file),
-    )
-
-
-def load_experiment_template(file: str) -> dict:
     """Load single experiment file with solutions for template experiment."""
     with open(file, 'r') as f:
         notebook = json.load(f)
         example = notebook['cells']
         metadata = notebook['metadata']
         modules = metadata.get('modules', [])
         if modules:
             # these two annotations should be the same
             assert len(modules) == len(metadata.get('step_types'))
             # reformat annotations
             modules = [[_m.strip() for _m in _modules.split('&')]
                        for _modules in modules]
         questions = []
         source_codes = []
+        thoughts = []
         outputs = []
         tags = []
+
         for cell in example:
             if cell['cell_type'] == 'markdown':
                 text = ''.join(cell['source']).strip()
+                try:
+                    text, thought = text.split('\n\nThought: ')
+                except ValueError:
+                    thought = ' '
                 if modules:
                     _modules = modules.pop(0)
                     if 'chinese' not in file:
                         text += f"Please use {' and '.join(_modules)} modules."
                     else:
                         text += f" {'  '.join(_modules)} ."
                 text = text.strip() + '\n'
                 # append the formatted text
                 questions.append(text)
+                thoughts.append(thought)
             elif cell['cell_type'] == 'code':
                 source_codes.append(''.join(cell['source']))
                 output_flag = False
                 if cell['outputs']:
                     for _output in cell['outputs']:
                         if _output['output_type'] == 'display_data':
                             assert not output_flag
@@ -128,16 +82,18 @@
                     # no output fallback to exec
                     tags.append('exec')
                     outputs.append(None)
     return dict(
         experiment=file,
         questions=sum(([
             dict(role='user', content=question),
-            dict(role='assistant', content=source_code)
-        ] for question, source_code in zip(questions, source_codes)), []),
+            dict(role='assistant', content=thought + '**split**' + source_code)
+        ]
+                       for question, source_code, thought in zip(
+                           questions, source_codes, thoughts)), []),
         references=dict(outputs=outputs,
                         tags=tags,
                         metadata=metadata,
                         experiment=file),
     )
 
 
@@ -152,15 +108,15 @@
     if not ret:
         raise ConnectionError('CIBench needs internet to get response. Please'
                               'check your internet and proxy.')
 
 
 @LOAD_DATASET.register_module()
 class CIBenchDataset(BaseDataset):
-    """Code Interpreter dataset."""
+    """Code Interpreter dataset for template dataset."""
 
     @staticmethod
     def load(path: str, internet_check: bool = False):
         """Load whole dataset.
 
         Args:
             path(str): Path of cibench dataset.
@@ -179,73 +135,124 @@
                     data = load_experiment(os.path.join(cwd, f))
                     data_list.append(data)
 
         dataset = Dataset.from_list(data_list)
         return dataset
 
 
-@LOAD_DATASET.register_module()
-class CIBenchTemplateDataset(BaseDataset):
-    """Code Interpreter dataset for template dataset."""
-
-    @staticmethod
-    def load(path: str, internet_check: bool = False):
-        """Load whole dataset.
-
-        Args:
-            path(str): Path of cibench dataset.
-            internet_check(bool): Whether to check internet.
-                Defaults to False.
-        """
-        if internet_check:
-            check_internet()
-        assert os.path.exists(path), f'Path {path} does not exist.'
-        data_list = []
-        for cwd, dirs, files in os.walk(path):
-            dirs.sort()
-            files.sort()
-            for f in files:
-                if '.ipynb' in f:
-                    data = load_experiment_template(os.path.join(cwd, f))
-                    data_list.append(data)
+def sklearn_ssim(pred_img, target_img):
+    import base64
 
-        dataset = Dataset.from_list(data_list)
-        return dataset
+    import skimage
+    img2 = base64.b64decode(target_img)
+    img2 = skimage.io.imread(img2, plugin='imageio')
+    img1 = skimage.io.imread(pred_img, plugin='imageio')
+    img1 = skimage.transform.resize(img1, img2.shape[:2])
+    img1 = 255 * img1
+    # Convert to integer data type pixels.
+    img1 = img1.astype(np.uint8)
+    ssim = skimage.metrics.structural_similarity(img1, img2, channel_axis=-1)
+    return ssim
+
+
+JUDGE_PROMPT_CN = """
+AI
+AI
+1. 110
+2. 
+
+12
+34
+56
+78
+910
+
+[]{question}
+"""  # noqa
+
+JUDGE_PROMPT = """You are an assistant skilled in assessing visualization capabilities.
+In the capacity of a fair judge, you will evaluate the quality of images drawn by an AI model generating code for visualization-related problems. We will provide you with a code visualization problem and an image drawn by the code created by the AI model you need to assess. When you start your assessment, you must adhere to the following process:
+1. Rate the visualization capability with a score between 1 and 10 for the image, returning only the number without any additional descriptions.
+2. Your scoring needs to be as rigorous as possible, and it should follow the scoring rules below: Overall, the higher the quality of the model's response, the higher the score.
+
+A score of 1 to 2 is given when the image cannot reflect the given instruction content at all.
+A score of 3 to 4 is given when the image can partly reflect the given instruction content, but there is a significant lack of specific detail expression.
+If the image basically meets the given instructions, but the aesthetic quality of the image is average without any outstanding features, this kind of rating can get a score of 5 to 6.
+When the image matches the given instructions well, and shows some aesthetic appeal, such as some originality in color matching and shape design, this kind of rating can get a score of 7 to 8.
+Only when the image completely matches the given instructions, covers all the details in the instructions, and performs excellently in terms of aesthetics, can this kind of rating get a score of 9 to 10.
+
+[Visualization Problem]:{question}
+"""  # noqa
+
+
+def vl_model_score(model, pred_img, ori_prompt, judge_prompt):
+    response = model.interleave_generate(
+        [judge_prompt.format(question=ori_prompt), pred_img])
+    score = gsm8k_postprocess(response)
+    try:
+        score = int(float(score))
+        assert score <= 10 and score >= 1
+        return score / 10
+    except Exception as e:
+        raise ValueError(f'Evaluation failed {e}. Check log for details.')
 
 
+@ICL_EVALUATORS.register_module()
 class CIBenchEvaluator(BaseEvaluator):
     """Evaluator for CI dataset.
 
     Args:
         text_evaluator (optional, dict): The text evaluator for text result
-            comparison[]. Defaults to None, which use Rouge as defaults.
+            comparison[]. Defaults to None, which use rouge as defaults.
             Please notice that a extra key for `metric_name` should be set
             to get the exact metric result, such as `rouge1`.
+        vis_evaluator (optional, dict): The vis evaluator for visualization
+            score. Defaults to None, which means use skimage. Otherwise
+            provide dict from VLMEvalKit.
         output_dir (optional, str): The directory to save experiment
             files in a markdown or notebook format.
         with_ipynb (bool): Generate ipynb correspondingly.
             Defaults to False.
         user_data_dir (str): The directory to load local files.
             Defaults to 'ENV', which means use environment variable
             `USER_DATA_DIR` to get the data dir.
     """
 
     def __init__(self,
                  text_evaluator: Optional[dict] = None,
+                 vis_evaluator: Optional[dict] = None,
                  output_dir: Optional[str] = None,
                  with_ipynb: bool = False,
+                 lang: str = 'en',
                  user_data_dir: str = 'ENV') -> None:
+        # build text evaluator
         if text_evaluator is None:
             from opencompass.openicl.icl_evaluator import RougeEvaluator
             self.text_evaluator = ICL_EVALUATORS.build(
                 dict(type=RougeEvaluator))
             self.text_eval_metric = 'rouge1'
         else:
             self.text_eval_metric = text_evaluator.pop('metric_name')
             self.text_evaluator = ICL_EVALUATORS.build(text_evaluator)
+        # build visual evaluator
+        if vis_evaluator is None:
+            self.vis_evaluator = None
+        else:
+            try:
+                from vlmeval.config import supported_VLM
+            except ImportError as e:
+                raise ImportError(
+                    f'{e}. Please install vlmeval following: https://github.com/open-compass/VLMEvalKit'  # noqa
+                )
+            assert vis_evaluator['type'] in supported_VLM, ''
+            self.vis_evaluator = supported_VLM[vis_evaluator.pop('type')](
+                **vis_evaluator)
+
+        assert lang in ['en', 'cn'], 'Only `en` and `cn` are supported.'
+        self.lang = lang
         # TODO: should use work dir for this task.
         self.output_dir = output_dir
         self.user_data_dir = self.check_user_data_dir(user_data_dir)
         self.with_ipynb = with_ipynb
         self.TAG_MAPPING = {
             'exec': ('executable', self.valid_step),
             'general': ('general_correct', self.correct_step),
@@ -272,22 +279,22 @@
     @staticmethod
     def valid_step(step):
         """Whether the step is executable and valid."""
         # Found the latest code interpreter to determine valid
         for action in step[::-1]:
             if action['type'] == 'IPythonInterpreter':
                 if action['errmsg']:
-                    return False
+                    return True, False
                 else:
-                    return True
+                    return True, True
         # No code interpreter for this step, reckon as False
-        return False
+        return False, False
 
     @staticmethod
-    def correct_step(step, target):
+    def correct_step(step, target) -> dict:
         """Whether the step output is correct."""
         # Found the latest code interpreter to determine correct
         for action in step[::-1]:
             if action['type'] == 'IPythonInterpreter':
                 if action['result']:
                     try:
                         pred = action['result']['text']
@@ -306,105 +313,124 @@
                             match = match_stdout
                         else:
                             match = None
                         if match:
                             out = match.group(1)
                             score = (out.strip() == target.strip()
                                      or target.strip() in out.strip())
-                            return score
+                            return {'score': score, 'gt': target, 'pred': out}
                     except Exception:
-                        return False
+                        return {'score': 0, 'gt': target}
         # Fall back to False
-        return False
+        return {'score': 0, 'gt': target}
 
-    def text_step(self, step, target):
+    def text_step(self, step, target) -> dict:
         """Whether the step output is correct."""
         # Found the latest code interpreter to determine correct
         for action in step[::-1]:
             if action['type'] == 'IPythonInterpreter':
                 if action['result']:
                     try:
                         pred = action['result']['text']
                         match = re.search('```\n(.*?)\n```', pred, re.DOTALL)
                         if match:
                             out = match.group(1)
                             score = self.text_evaluator.score([out], [target])
-                            return score[self.text_eval_metric] / 100
+                            score = score[self.text_eval_metric] / 100
+                            return {
+                                'score': score,
+                                'gt_text': target,
+                                'pred_text': out
+                            }
                     except Exception:
-                        return False
+                        return {'score': 0, 'gt_text': target}
         # Fall back to False
-        return False
+        return {'score': 0, 'gt_text': target}
 
-    @staticmethod
-    def vis_similarity_step(step, target):
+    def vis_similarity_step(self, step, target, ori_prompt) -> dict:
         """Whether the step output image has the same structure similarity with
         the given images."""
         # Found the latest code interpreter to determine correct
-        import base64
-
-        import skimage
-
         for action in step[::-1]:
             if action['type'] == 'IPythonInterpreter':
                 if action['result']:
                     try:
-                        pred = action['result']['text']
+                        pred = action['result']['image_path']
                         match = re.search(r'!\[fig-[0-9]*\]\((.*?)\)', pred,
                                           re.DOTALL)
                         if match:
                             img_pred = match.group(1)
-                        img2 = base64.b64decode(target)
-                        img2 = skimage.io.imread(img2, plugin='imageio')
-                        img1 = skimage.io.imread(img_pred, plugin='imageio')
-                        img1 = skimage.transform.resize(img1, img2.shape[:2])
-                        img1 = 255 * img1
-                        # Convert to integer data type pixels.
-                        img1 = img1.astype(np.uint8)
-                        ssim = skimage.metrics.structural_similarity(
-                            img1, img2, channel_axis=-1)
-                        # mse = skimage.metrics.mean_squared_error(img1, img2)
-                        # ssim greater better
-                        # mse smaller better but has no upper bound
-                        return ssim
+                        if self.vis_evaluator is None:
+                            # ssim greater better
+                            score = sklearn_ssim(img_pred, target)
+                            return {'score': score, 'pred_img': img_pred}
+                        else:
+                            # TODO: the following code will be removed later.
+                            if self.lang == 'cn':
+                                score = vl_model_score(self.vis_evaluator,
+                                                       img_pred, ori_prompt,
+                                                       JUDGE_PROMPT_CN)
+                                return {'score': score, 'pred_img': img_pred}
+                            elif self.lang == 'en':
+                                score = vl_model_score(self.vis_evaluator,
+                                                       img_pred, ori_prompt,
+                                                       JUDGE_PROMPT)
+                                return {'score': score, 'pred_img': img_pred}
                     except Exception:
-                        return 0
+                        return {'score': 0}
         # Fall back to 0
-        return 0
+        return {'score': 0}
 
-    def save_results(self, origin_prompt, steps):
+    def save_results(self, origin_prompt, steps, references):
         """Save the prediction result in a markdown and notebook format."""
 
+        from opencompass.lagent.actions.ipython_interpreter import extract_code
+
         def check_jupytext():
             """Check requirements existence."""
             from shutil import which
 
             assert which('jupytext'), (
                 "Please install jupytext use 'pip install jupytext' to ensure"
                 'the conversion processes.')
 
         check_jupytext()
         p_list = []
-        from opencompass.lagent.actions.ipython_interpreter import extract_code
-        for idx, (example_origin_prompt,
-                  example_steps) in enumerate(zip(origin_prompt, steps)):
+        total_results = defaultdict(float)
+        total_scores = defaultdict(float)
+        total_nums = defaultdict(int)
+
+        for idx, (example_origin_prompt, example_steps,
+                  gold) in enumerate(zip(origin_prompt, steps, references)):
+            # get result count
+            result, exp_output = self.single_exp(gold, example_steps,
+                                                 example_origin_prompt)
+            for k, v in result.items():
+                total_scores[k] += sum(v)
+                total_nums[k] += len(v)
+
             markdown_lines = []
-            for prompt, step in zip(example_origin_prompt, example_steps):
+            for prompt, step, step_output in zip(example_origin_prompt,
+                                                 example_steps, exp_output):
                 for action in step[::-1]:
                     if action['type'] == 'IPythonInterpreter':
                         valid_action = action
                         break
                     # fall back to final action
                     valid_action = step[-1]
                 markdown_lines.append(prompt)
                 markdown_lines.append('\n')
                 code_text = valid_action['args']['text']
                 code_text = extract_code(code_text)
                 code_text = '```python\n' + code_text + '\n```'
                 markdown_lines.append(code_text)
                 markdown_lines.append('\n')
+                markdown_lines.append('\n'.join(
+                    [f'{k}: {v}' for k, v in step_output.items()]))
+                markdown_lines.append('\n\n')
 
             md_file = f'experiment{idx}.md'
             with open(md_file, 'w') as f:
                 f.writelines(markdown_lines)
 
             # TODO: be careful for this
             # The result might be different with infer process
@@ -413,33 +439,42 @@
             if self.with_ipynb:
                 p = subprocess.Popen(
                     'jupytext --to ipynb --pipe-fmt ipynb '
                     "--pipe 'jupyter nbconvert --to ipynb --execute "
                     f"--allow-errors --stdin --stdout' {md_file}",
                     shell=True)
                 p_list.append(p)
+
         # TODO: async wait
         for p in p_list:
             p.wait()
 
+        # get final scores
+        for k, v in total_scores.items():
+            if total_nums[k] > 0:
+                total_results[k] = total_scores[k] / total_nums[k] * 100
+            else:
+                total_results[k] = -1
+        return total_results
+
     def set_data_dir(self, work_dir):
         """Set work directory and link data files for save notebook results."""
         if self.user_data_dir:
             basename = osp.basename(self.user_data_dir)
 
             if not osp.exists(osp.join(self.output_dir, basename)):
                 os.symlink(self.user_data_dir,
                            osp.join(self.output_dir, basename))
         os.chdir(work_dir)
 
     def unset_data_dir(self, work_dir):
         """Change work directory and keep the symlink."""
         os.chdir(work_dir)
 
-    def single_exp(self, gold, steps):
+    def single_exp(self, gold, steps, single_ori_prompt):
         tags = gold['tags']
         outputs = gold['outputs']
         metadata = gold['metadata']
         hard_tags = metadata.get('step_types', [])
         if hard_tags:
             tags = hard_tags
 
@@ -454,23 +489,33 @@
         if hard_tags:
             check_tags = ['exec', 'num', 'text', 'vis']
         else:
             check_tags = ['exec', 'general', 'vis']
         for tag in check_tags:
             key = self.TAG_MAPPING[tag][0]
             result[key] = []
+        result['tool_rate'] = []
 
-        for tag, step, output in zip(tags, steps, outputs):
+        exp_output = []
+        for tag, step, output, ori_prompt in zip(tags, steps, outputs,
+                                                 single_ori_prompt):
             # check whether this step is valid
-            result['executable'].append(self.valid_step(step))
+            tool_correct, exec_correct = self.valid_step(step)
+            result['tool_rate'].append(tool_correct)
+            result['executable'].append(exec_correct)
+            eval_output = {}
             if tag != 'exec':
                 key, func = self.TAG_MAPPING[tag]
-                result[key].append(func(step, output))
+                kwargs = dict(step=step, target=output, ori_prompt=ori_prompt)
+                kwargs = {k: kwargs[k] for k in signature(func).parameters}
+                eval_output = func(**kwargs)
+                result[key].append(eval_output['score'])
+            exp_output.append(eval_output)
 
-        return result
+        return result, exp_output
 
     def get_output_dir(self):
         """Get output dir from eval task.
 
         Notice: output dir should be in format xxx/data.
         All the needed files should be
         """
@@ -485,27 +530,11 @@
             return {'error': 'steps and refrs have different length'}
         cwd = os.getcwd()
         self.get_output_dir()
         if self.output_dir:
             if not osp.exists(self.output_dir):
                 os.makedirs(self.output_dir)
             self.set_data_dir(self.output_dir)
-            self.save_results(origin_prompt, steps)
+            total_results = self.save_results(origin_prompt, steps, references)
             self.unset_data_dir(cwd)
 
-        total_results = defaultdict(float)
-        total_scores = defaultdict(float)
-        total_nums = defaultdict(int)
-        for gold, single_steps in zip(references, steps):
-            result = self.single_exp(gold, single_steps)
-
-            for k, v in result.items():
-                total_scores[k] += sum(v)
-                total_nums[k] += len(v)
-
-        for k, v in total_scores.items():
-            if total_nums[k] > 0:
-                total_results[k] = total_scores[k] / total_nums[k] * 100
-            else:
-                total_results[k] = -1
-
         return total_results
```

### Comparing `opencompass-0.2.4/opencompass/datasets/circular.py` & `opencompass-0.2.5/opencompass/datasets/circular.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/civilcomments.py` & `opencompass-0.2.5/opencompass/datasets/civilcomments.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/clozeTest_maxmin.py` & `opencompass-0.2.5/opencompass/datasets/clozeTest_maxmin.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/cluewsc.py` & `opencompass-0.2.5/opencompass/datasets/cluewsc.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/cmb.py` & `opencompass-0.2.5/opencompass/datasets/cmb.py`

 * *Ordering differences only*

 * *Files 0% similar despite different names*

```diff
@@ -14,18 +14,18 @@
     @staticmethod
     def load(path: str):
         with open(osp.join(path, 'val.json'), 'r', encoding='utf-8') as f:
             val_data = json.load(f)
         for d in val_data:
             d['option_str'] = '\n'.join(
                 [f'{k}. {v}' for k, v in d['option'].items() if len(v) > 1])
-            d['answer'] = 'NULL'
         val_dataset = Dataset.from_list(val_data)
 
         with open(osp.join(path, 'test.json'), 'r', encoding='utf-8') as f:
             test_data = json.load(f)
         for d in test_data:
             d['option_str'] = '\n'.join(
                 [f'{k}. {v}' for k, v in d['option'].items() if len(v) > 1])
+            d['answer'] = 'NULL'
         test_dataset = Dataset.from_list(test_data)
 
         return DatasetDict({'val': val_dataset, 'test': test_dataset})
```

### Comparing `opencompass-0.2.4/opencompass/datasets/cmmlu.py` & `opencompass-0.2.5/opencompass/datasets/cmmlu.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/cmnli.py` & `opencompass-0.2.5/opencompass/datasets/cmnli.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/cmrc.py` & `opencompass-0.2.5/opencompass/datasets/cmrc.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/commonsenseqa.py` & `opencompass-0.2.5/opencompass/datasets/commonsenseqa.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/commonsenseqa_cn.py` & `opencompass-0.2.5/opencompass/datasets/commonsenseqa_cn.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/crowspairs.py` & `opencompass-0.2.5/opencompass/datasets/crowspairs.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/crowspairs_cn.py` & `opencompass-0.2.5/opencompass/datasets/crowspairs_cn.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/csl.py` & `opencompass-0.2.5/opencompass/datasets/csl.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/custom.py` & `opencompass-0.2.5/opencompass/datasets/custom.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/cvalues.py` & `opencompass-0.2.5/opencompass/datasets/cvalues.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/drcd.py` & `opencompass-0.2.5/opencompass/datasets/drcd.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/drop.py` & `opencompass-0.2.5/opencompass/datasets/drop.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/ds1000.py` & `opencompass-0.2.5/opencompass/datasets/ds1000.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/ds1000_interpreter.py` & `opencompass-0.2.5/opencompass/datasets/ds1000_interpreter.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/eprstmt.py` & `opencompass-0.2.5/opencompass/datasets/eprstmt.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/flores.py` & `opencompass-0.2.5/opencompass/datasets/flores.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/game24.py` & `opencompass-0.2.5/opencompass/datasets/game24.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/govrepcrs.py` & `opencompass-0.2.5/opencompass/datasets/govrepcrs.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/gpqa.py` & `opencompass-0.2.5/opencompass/datasets/natural_question.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,59 +1,87 @@
 import csv
-import os
+import json
+import os.path as osp
 
-from datasets import Dataset
+from datasets import Dataset, DatasetDict
 
-from opencompass.openicl import BaseEvaluator
-from opencompass.registry import LOAD_DATASET
+from opencompass.openicl.icl_evaluator import BaseEvaluator
+from opencompass.registry import ICL_EVALUATORS, LOAD_DATASET
+from opencompass.utils.text_postprocessors import general_postprocess
 
 from .base import BaseDataset
 
 
 @LOAD_DATASET.register_module()
-class GPQADataset(BaseDataset):
+class NaturalQuestionDataset(BaseDataset):
 
     @staticmethod
-    def load(path: str, name: str):
-        cnt = 0
-        data = []
-        with open(os.path.join(path, name), 'r', encoding='utf-8') as f:
-            reader = csv.reader(f, delimiter=',')
-            for row in reader:
-                if row[7] == 'Question':
-                    continue
-                cnt = cnt + 1
-                question = row[7]
-                # 
-                options = [row[8], row[9], row[10], row[11]]
-                shuffle_patterns = ['ABCD', 'BCDA', 'CDAB', 'DABC']  # 
-                c = shuffle_patterns[cnt % 4]
-                line = {'question': question}
-                ground_truth = options[0]
-                for i in range(4):
-                    line['ABCD'[i]] = options[ord(c[i]) - ord('A')]
-                for i in range(4):
-                    if line['ABCD'[i]] == ground_truth:
-                        line['answer'] = 'ABCD'[i]
-                        break
-                data.append(line)
-        dataset = Dataset.from_list(data)
+    def load(path: str):
+        dataset = DatasetDict()
+        for split in ['dev', 'test']:
+            filename = osp.join(path, f'nq-{split}.qa.csv')
+            with open(filename, 'r', encoding='utf-8') as f:
+                reader = csv.reader(f, delimiter='\t')
+                raw_data = []
+                for row in reader:
+                    assert len(row) == 2
+                    question = row[0]
+                    answers = eval(row[1])
+                    if split == 'dev':
+                        answers = answers[0]
+                    raw_data.append({'question': question, 'answer': answers})
+                dataset[split] = Dataset.from_list(raw_data)
+
+        return dataset
+
+
+@LOAD_DATASET.register_module()
+class NQOpenDataset(BaseDataset):
+
+    @staticmethod
+    def load(path: str):
+        dataset = DatasetDict()
+        for split in ['validation', 'train']:
+            filename = osp.join(path, f'nq-open-{split}.jsonl')
+            raw_data = []
+            with open(filename, 'r', encoding='utf-8') as f:
+                for doc in f:
+                    doc = json.loads(doc)
+                    if split == 'train':
+                        doc['answer'] = doc['answer'][0]
+                    raw_data.append(doc)
+            dataset[split] = Dataset.from_list(raw_data)
+
         return dataset
 
 
-class GPQAEvaluator(BaseEvaluator):
+@ICL_EVALUATORS.register_module()
+class NQEvaluator(BaseEvaluator):
 
     def score(self, predictions, references):
         if len(predictions) != len(references):
-            return {'error': 'preds and refrs have different length'}
-        correct = 0
-        count = 0
+            return {
+                'error': 'predictions and references have different '
+                'length'
+            }
+        processed_predictions = []
+        for prediction in predictions:
+            prediction = prediction.strip().split('\n')[0].lower()
+            if 'answer is' in prediction:
+                prediction = prediction.split('answer is')[-1]
+            prediction = general_postprocess(prediction)
+            processed_predictions.append(prediction)
+        processed_answers = [[general_postprocess(j).lower() for j in i]
+                             for i in references]
+
         details = []
-        for i, j in zip(predictions, references):
-            detail = {'pred': i, 'answer': j, 'correct': False}
-            count += 1
-            if i == j:
-                correct += 1
-                detail['correct'] = True
+        cnt = 0
+        for pred, cand_ans in zip(processed_predictions, processed_answers):
+            detail = {'pred': pred, 'answer': cand_ans, 'correct': False}
+            # is_correct = any([cand == pred for cand in cand_ans])
+            is_correct = any([cand in pred for cand in cand_ans])
+            cnt += int(is_correct)
+            detail['correct'] = is_correct
             details.append(detail)
-        result = {'accuracy': 100 * correct / count, 'details': details}
-        return result
+        score = cnt / len(predictions) * 100
+
+        return {'score': score, 'details': details}
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `opencompass-0.2.4/opencompass/datasets/gsm8k.py` & `opencompass-0.2.5/opencompass/datasets/gsm8k.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/gsm_hard.py` & `opencompass-0.2.5/opencompass/datasets/gsm_hard.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/hellaswag.py` & `opencompass-0.2.5/opencompass/datasets/hellaswag.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/humaneval.py` & `opencompass-0.2.5/opencompass/datasets/humaneval.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/humaneval_multi.py` & `opencompass-0.2.5/opencompass/datasets/humaneval_multi.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/humanevalx.py` & `opencompass-0.2.5/opencompass/datasets/humanevalx.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/infinitebench/__init__.py` & `opencompass-0.2.5/opencompass/datasets/infinitebench/__init__.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/infinitebench/infinitebench_codedebug.py` & `opencompass-0.2.5/opencompass/datasets/infinitebench/infinitebench_codedebug.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/infinitebench/infinitebench_coderun.py` & `opencompass-0.2.5/opencompass/datasets/infinitebench/infinitebench_coderun.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/infinitebench/infinitebench_endia.py` & `opencompass-0.2.5/opencompass/datasets/infinitebench/infinitebench_endia.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/infinitebench/infinitebench_enmc.py` & `opencompass-0.2.5/opencompass/datasets/infinitebench/infinitebench_enmc.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/infinitebench/infinitebench_enqa.py` & `opencompass-0.2.5/opencompass/datasets/infinitebench/infinitebench_enqa.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/infinitebench/infinitebench_ensum.py` & `opencompass-0.2.5/opencompass/datasets/infinitebench/infinitebench_ensum.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/infinitebench/infinitebench_mathcalc.py` & `opencompass-0.2.5/opencompass/datasets/infinitebench/infinitebench_mathcalc.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/infinitebench/infinitebench_mathfind.py` & `opencompass-0.2.5/opencompass/datasets/infinitebench/infinitebench_mathfind.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/infinitebench/infinitebench_retrievekv.py` & `opencompass-0.2.5/opencompass/datasets/infinitebench/infinitebench_retrievekv.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/infinitebench/infinitebench_retrievenumber.py` & `opencompass-0.2.5/opencompass/datasets/infinitebench/infinitebench_retrievenumber.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/infinitebench/infinitebench_retrievepasskey.py` & `opencompass-0.2.5/opencompass/datasets/infinitebench/infinitebench_retrievepasskey.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/infinitebench/infinitebench_zhqa.py` & `opencompass-0.2.5/opencompass/datasets/infinitebench/infinitebench_zhqa.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/jigsawmultilingual.py` & `opencompass-0.2.5/opencompass/datasets/jigsawmultilingual.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/kaoshi.py` & `opencompass-0.2.5/opencompass/datasets/GaokaoBench.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,51 +1,39 @@
 import json
 import re
 
 from datasets import Dataset
 
 from opencompass.openicl.icl_evaluator import BaseEvaluator
+from opencompass.registry import ICL_EVALUATORS, LOAD_DATASET
 
 from .base import BaseDataset
 
 
-def get_number(options):
-
-    result_string = ''
-    for i, option in enumerate(options, start=65):
-        result_string += f'{chr(i)}. {option}\n'
-    return result_string
-
-
-class KaoshiDataset(BaseDataset):
+@LOAD_DATASET.register_module()
+class GaokaoBenchDataset(BaseDataset):
 
     @staticmethod
-    def load(path: str, name: str):
-        data_list = []
+    def load(path: str):
         with open(path, encoding='utf-8') as f:
-            for line in f:
-                data = json.loads(line)
-                if name in ['single_choice', 'multi_choice']:
-                    data['question'] = data['question'].strip(
-                    ) + '\n' + get_number(data['options'])
-                data_list.append(data)
-        return Dataset.from_list(data_list)
+            data = json.load(f)
+        return Dataset.from_list(data['example'])
 
 
-valid_kaoshi_question_types = [
+valid_gaokao_bench_question_types = [
     'single_choice', 'multi_choice', 'multi_question_choice',
-    'five_out_of_seven', 'cloze', 'judgment'
+    'five_out_of_seven', 'cloze', 'subjective', 'correction'
 ]
 
 
-class KaoshiEvaluator(BaseEvaluator):
+class GaokaoBenchEvaluator(BaseEvaluator):
 
     def __init__(self, question_type) -> None:
         super().__init__()
-        assert question_type in valid_kaoshi_question_types
+        assert question_type in valid_gaokao_bench_question_types
         self.question_type = question_type
 
     def do_predictions_postprocess(self, model_output, answer_lenth=None):
         if self.question_type == 'single_choice':
             model_answer = []
             temp = re.findall(r'[A-D]', model_output[::-1])
             if len(temp) != 0:
@@ -85,54 +73,77 @@
         elif self.question_type == 'five_out_of_seven':
             model_answer = []
             temp = re.findall(r'[A-G]', model_output)
             if len(temp) > 0:
                 for k in range(min(5, len(temp))):
                     model_answer.append(temp[k])
 
-        elif self.question_type in ['cloze', 'judgment']:
-            model_answer = []
-            temp = re.findall(r'(.*?) ', model_output)
-            if len(temp) > 0:
-                model_answer.append(temp[0])
-
         return model_answer
 
     def ensure_same_length(self, pred, refr):
         if len(pred) == len(refr):
             return pred
         return ['Z'] * len(refr)
 
     def score(self, predictions, references):
-        if self.question_type not in valid_kaoshi_question_types:
-            return {'score': 100}
+        if self.question_type not in [
+                'single_choice', 'multi_choice', 'multi_question_choice',
+                'five_out_of_seven'
+        ]:
+            return {'score': 0}
         elif self.question_type == 'multi_choice':
+            details = {}
             correct_score, total_score = 0, 0
-            for pred, refr in zip(predictions, references):
+            for index, (pred, refr) in enumerate(zip(predictions, references)):
                 pred = self.do_predictions_postprocess(pred)
                 pred = self.ensure_same_length(pred, refr)
+                is_corrects = []
                 for p, r in zip(pred, refr):
                     if p == r:
                         correct_score += 2
+                        is_corrects.append(True)
                     else:
                         for i in p:
                             if i not in r:
                                 break
                         else:
                             correct_score += 1
+                        is_corrects.append(False)
                     total_score += 2
-            return {'score': correct_score / total_score * 100}
+                details[str(index)] = {
+                    'pred': pred,
+                    'refr': refr,
+                    'is_correct': all(is_corrects),
+                }
+
         else:
+            details = {}
             correct_score, total_score = 0, 0
-            for pred, refr in zip(predictions, references):
+            for index, (pred, refr) in enumerate(zip(predictions, references)):
                 if self.question_type == 'multi_question_choice':
                     pred = self.do_predictions_postprocess(pred, len(refr))
                 else:
                     pred = self.do_predictions_postprocess(pred)
-                if self.question_type in ['cloze', 'judgment']:
-                    refr = [refr]
                 pred = self.ensure_same_length(pred, refr)
+                is_corrects = []
                 for p, r in zip(pred, refr):
-                    if p == r:
-                        correct_score += 1
+                    is_correct = p == r
+                    correct_score += is_correct
                     total_score += 1
-            return {'score': correct_score / total_score * 100}
+                    is_corrects.append(is_correct)
+                details[str(index)] = {
+                    'pred': pred,
+                    'refr': refr,
+                    'is_correct': all(is_corrects),
+                }
+        return {'score': correct_score / total_score * 100, 'details': details}
+
+
+for question_type in valid_gaokao_bench_question_types:
+    # fix classic closure problem
+    def _gaokao_register(question_type):
+        ICL_EVALUATORS.register_module(
+            name='GaokaoBenchEvaluator' + '_' + question_type,
+            module=lambda *args, **kwargs: GaokaoBenchEvaluator(
+                question_type=question_type, *args, **kwargs))
+
+    _gaokao_register(question_type)
```

### Comparing `opencompass-0.2.4/opencompass/datasets/lambada.py` & `opencompass-0.2.5/opencompass/datasets/lambada.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/lawbench/lawbench.py` & `opencompass-0.2.5/opencompass/datasets/lawbench/lawbench.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/lcsts.py` & `opencompass-0.2.5/opencompass/datasets/lcsts.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/leval/__init__.py` & `opencompass-0.2.5/opencompass/datasets/leval/__init__.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/leval/evaluators.py` & `opencompass-0.2.5/opencompass/datasets/leval/evaluators.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/leval/leval_coursera.py` & `opencompass-0.2.5/opencompass/datasets/leval/leval_coursera.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/leval/leval_financial_qa.py` & `opencompass-0.2.5/opencompass/datasets/leval/leval_financial_qa.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/leval/leval_gov_report_summ.py` & `opencompass-0.2.5/opencompass/datasets/leval/leval_gov_report_summ.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/leval/leval_gsm100.py` & `opencompass-0.2.5/opencompass/datasets/leval/leval_gsm100.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/leval/leval_legal_contract_qa.py` & `opencompass-0.2.5/opencompass/datasets/leval/leval_legal_contract_qa.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/leval/leval_meeting_summ.py` & `opencompass-0.2.5/opencompass/datasets/leval/leval_meeting_summ.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/leval/leval_multidoc_qa.py` & `opencompass-0.2.5/opencompass/datasets/leval/leval_multidoc_qa.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/leval/leval_narrattive_qa.py` & `opencompass-0.2.5/opencompass/datasets/leval/leval_narrattive_qa.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/leval/leval_natural_question.py` & `opencompass-0.2.5/opencompass/datasets/leval/leval_natural_question.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/leval/leval_news_summ.py` & `opencompass-0.2.5/opencompass/datasets/leval/leval_news_summ.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/leval/leval_paper_assistant.py` & `opencompass-0.2.5/opencompass/datasets/leval/leval_paper_assistant.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/leval/leval_patent_summ.py` & `opencompass-0.2.5/opencompass/datasets/leval/leval_patent_summ.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/leval/leval_quality.py` & `opencompass-0.2.5/opencompass/datasets/leval/leval_quality.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/leval/leval_review_summ.py` & `opencompass-0.2.5/opencompass/datasets/leval/leval_review_summ.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/leval/leval_scientific_qa.py` & `opencompass-0.2.5/opencompass/datasets/leval/leval_scientific_qa.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/leval/leval_topic_retrieval.py` & `opencompass-0.2.5/opencompass/datasets/leval/leval_topic_retrieval.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/leval/leval_tpo.py` & `opencompass-0.2.5/opencompass/datasets/leval/leval_tpo.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/leval/leval_tvshow_summ.py` & `opencompass-0.2.5/opencompass/datasets/leval/leval_tvshow_summ.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/longbench/__init__.py` & `opencompass-0.2.5/opencompass/datasets/longbench/__init__.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/longbench/evaluators.py` & `opencompass-0.2.5/opencompass/datasets/longbench/evaluators.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/longbench/longbench_2wikim_qa.py` & `opencompass-0.2.5/opencompass/datasets/longbench/longbench_2wikim_qa.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/longbench/longbench_dureader.py` & `opencompass-0.2.5/opencompass/datasets/longbench/longbench_dureader.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/longbench/longbench_gov_report.py` & `opencompass-0.2.5/opencompass/datasets/longbench/longbench_gov_report.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/longbench/longbench_hotpot_qa.py` & `opencompass-0.2.5/opencompass/datasets/longbench/longbench_hotpot_qa.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/longbench/longbench_lcc.py` & `opencompass-0.2.5/opencompass/datasets/longbench/longbench_lcc.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/longbench/longbench_lsht.py` & `opencompass-0.2.5/opencompass/datasets/longbench/longbench_lsht.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/longbench/longbench_multi_news.py` & `opencompass-0.2.5/opencompass/datasets/longbench/longbench_multi_news.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/longbench/longbench_multifieldqa_en.py` & `opencompass-0.2.5/opencompass/datasets/longbench/longbench_multifieldqa_en.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/longbench/longbench_multifieldqa_zh.py` & `opencompass-0.2.5/opencompass/datasets/longbench/longbench_multifieldqa_zh.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/longbench/longbench_musique.py` & `opencompass-0.2.5/opencompass/datasets/longbench/longbench_musique.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/longbench/longbench_narrative_qa.py` & `opencompass-0.2.5/opencompass/datasets/longbench/longbench_narrative_qa.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/longbench/longbench_passage_count.py` & `opencompass-0.2.5/opencompass/datasets/longbench/longbench_passage_count.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/longbench/longbench_passage_retrieval_en.py` & `opencompass-0.2.5/opencompass/datasets/longbench/longbench_passage_retrieval_en.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/longbench/longbench_passage_retrieval_zh.py` & `opencompass-0.2.5/opencompass/datasets/longbench/longbench_passage_retrieval_zh.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/longbench/longbench_qasper.py` & `opencompass-0.2.5/opencompass/datasets/longbench/longbench_qasper.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/longbench/longbench_qmsum.py` & `opencompass-0.2.5/opencompass/datasets/longbench/longbench_qmsum.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/longbench/longbench_repobench.py` & `opencompass-0.2.5/opencompass/datasets/longbench/longbench_repobench.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/longbench/longbench_samsum.py` & `opencompass-0.2.5/opencompass/datasets/longbench/longbench_samsum.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/longbench/longbench_trec.py` & `opencompass-0.2.5/opencompass/datasets/longbench/longbench_trec.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/longbench/longbench_trivia_qa.py` & `opencompass-0.2.5/opencompass/datasets/longbench/longbench_trivia_qa.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/longbench/longbench_vcsum.py` & `opencompass-0.2.5/opencompass/datasets/longbench/longbench_vcsum.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/lveval/__init__.py` & `opencompass-0.2.5/opencompass/datasets/lveval/__init__.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/lveval/evaluators.py` & `opencompass-0.2.5/opencompass/datasets/lveval/evaluators.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/lveval/lveval_cmrc_mixup.py` & `opencompass-0.2.5/opencompass/datasets/lveval/lveval_cmrc_mixup.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/lveval/lveval_dureader_mixup.py` & `opencompass-0.2.5/opencompass/datasets/lveval/lveval_dureader_mixup.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/lveval/lveval_factrecall_en.py` & `opencompass-0.2.5/opencompass/datasets/lveval/lveval_factrecall_en.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/lveval/lveval_factrecall_zh.py` & `opencompass-0.2.5/opencompass/datasets/lveval/lveval_factrecall_zh.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/lveval/lveval_hotpotwikiqa_mixup.py` & `opencompass-0.2.5/opencompass/datasets/lveval/lveval_hotpotwikiqa_mixup.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/lveval/lveval_lic_mixup.py` & `opencompass-0.2.5/opencompass/datasets/lveval/lveval_lic_mixup.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/lveval/lveval_loogle_CR_mixup.py` & `opencompass-0.2.5/opencompass/datasets/lveval/lveval_loogle_CR_mixup.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/lveval/lveval_loogle_MIR_mixup.py` & `opencompass-0.2.5/opencompass/datasets/lveval/lveval_loogle_MIR_mixup.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/lveval/lveval_loogle_SD_mixup.py` & `opencompass-0.2.5/opencompass/datasets/lveval/lveval_loogle_SD_mixup.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/lveval/lveval_multifieldqa_en_mixup.py` & `opencompass-0.2.5/opencompass/datasets/lveval/lveval_multifieldqa_en_mixup.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/lveval/lveval_multifieldqa_zh_mixup.py` & `opencompass-0.2.5/opencompass/datasets/lveval/lveval_multifieldqa_zh_mixup.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/mastermath2024v1.py` & `opencompass-0.2.5/opencompass/datasets/mastermath2024v1.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/math.py` & `opencompass-0.2.5/opencompass/datasets/math.py`

 * *Files 2% similar despite different names*

```diff
@@ -121,14 +121,23 @@
     # Normalize 100,000 -> 100000
     if final_answer.replace(',', '').isdigit():
         final_answer = final_answer.replace(',', '')
 
     return final_answer
 
 
+ANSWER_PATTERN = r'(?i)ANSWER\s*:\s*([^\n]+)'
+
+
+def extract_answer(response_text: str):
+    # We suggest to return an empty string but not None when extract failed
+    match = re.search(ANSWER_PATTERN, response_text)
+    return match.group(1) if match else ''
+
+
 @LOAD_DATASET.register_module()
 class MATHDataset(BaseDataset):
 
     @staticmethod
     def load(path: str):
         dataset = DatasetDict()
         data = json.load(open(path))
@@ -152,14 +161,20 @@
         if 'final answer' in maybe_ans.lower():
             return normalize_final_answer(maybe_ans)
     return normalize_final_answer(text.split('.')[0])
     # return normalize_final_answer(
     #     text.split('Final Answer: ', 1)[-1].split('\n\n')[0])
 
 
+@TEXT_POSTPROCESSORS.register_module('math_judement_preprocess')
+def math_judement_preprocess(text: str) -> str:
+    """Preprocess prediction before judgement."""
+    return extract_answer(text)
+
+
 @TEXT_POSTPROCESSORS.register_module('math_postprocess_v2')
 def math_postprocess_v2(text: str) -> str:
 
     cand_ans = extract_boxed_answer(text, strip_double_curly_brace=True)
     if cand_ans:
         return cand_ans
```

### Comparing `opencompass-0.2.4/opencompass/datasets/math401.py` & `opencompass-0.2.5/opencompass/datasets/math401.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/math_intern.py` & `opencompass-0.2.5/opencompass/datasets/math_intern.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/mathbench.py` & `opencompass-0.2.5/opencompass/datasets/mathbench.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/mbpp.py` & `opencompass-0.2.5/opencompass/datasets/mbpp.py`

 * *Files 2% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 import json
 import multiprocessing
 import os.path as osp
 import re
 import signal
 import tempfile
 from collections import defaultdict
-from concurrent.futures import ThreadPoolExecutor, as_completed
+from concurrent.futures import ProcessPoolExecutor, as_completed
 from typing import List, Sequence, Union
 
 import numpy as np
 from datasets import Dataset, DatasetDict, concatenate_datasets, load_dataset
 
 from opencompass.openicl.icl_evaluator import BaseEvaluator
 from opencompass.registry import ICL_EVALUATORS, LOAD_DATASET
@@ -204,37 +204,38 @@
     """Evaluator for MBPP or MBPPPlus."""
 
     def __init__(self, metric: str = 'MBPP') -> None:
         self.metric = metric
         assert self.metric in ['MBPP', 'MBPPPlus']
 
     def score(self, predictions, references):
-        assert len(predictions) == len(references)
+        if len(predictions) != len(references):
+            return {'error': 'preds and refrs have different length'}
 
         if self.metric == 'MBPP':
             result = {'pass': 0, 'timeout': 0, 'failed': 0, 'wrong_answer': 0}
             details = {}
-            # change to thread pool for better killing blocked instance
-            with ThreadPoolExecutor() as executor:
+            with ProcessPoolExecutor() as executor:
                 futures = []
                 for i, (refer, pred) in enumerate(zip(references,
                                                       predictions)):
                     pred = self._process_answer(pred)
                     programs = self._process_test(refer, pred)
                     future = executor.submit(execution, programs, i, 10)
                     futures.append(future)
                     details[str(i)] = {}
                     details[str(i)]['origin'] = predictions[i]
                     details[str(i)]['programs'] = programs
 
                 from tqdm import tqdm
                 for future in tqdm(as_completed(futures), total=len(futures)):
-                    index, key = future.result()
-                    result[key] += 1
-                    details[str(index)]['result'] = key
+                    index, ret = future.result()
+                    result[ret] += 1
+                    details[str(index)]['result'] = ret
+                    details[str(index)]['is_correct'] = (ret == 'pass')
 
             result['score'] = result['pass'] / len(predictions) * 100
             result['details'] = details
             return result
         else:
             try:
                 from evalplus.data import write_jsonl
@@ -282,17 +283,20 @@
             r"BEGIN\s*'(.*)\s*DONE",
             r'\[BEGIN\]\s*(.*)\s*\[DONE\]',
             r'BEGIN\s*(.*)\s*\[DONE\]',
             r'\[BEGIN\]\s*(.*)\s*DONE',
             r'BEGIN\s*(.*)\s*DONE',
             r'```python\s*(.*)\s*```',
             r'```\s*(.*)\s*```',
+            r'```python\s*(.*)\s*$',
+            r'```\s*(.*)\s*$',
             r'(.*)\s*```.*',
             r"\[BEGIN\]\s*'(.*)",
             r'\[BEGIN\](.*)',
+            r"'(.*)'\s*\[DONE\]",
         ]
         for p in patterns:
             match = re.search(p, text, re.DOTALL)
             if match:
                 text = match.group(1)
                 break
         text = text.split('```')[0]
@@ -435,15 +439,15 @@
     def score(self, predictions, references):
         assert len(predictions) == len(references)
 
         task_pass = defaultdict(int)
         task_total = defaultdict(int)
 
         result = {'pass': 0, 'timeout': 0, 'failed': 0, 'wrong_answer': 0}
-        with ThreadPoolExecutor() as executor:
+        with ProcessPoolExecutor() as executor:
             futures = []
             for refer, preds in zip(references, predictions):
                 # suits for two case
                 # 1. use repeated dataset
                 # 2. use `num_return_sequences` to generate multiple responses
                 if not isinstance(preds, list):
                     preds = [preds]
```

### Comparing `opencompass-0.2.4/opencompass/datasets/medbench/constructions.py` & `opencompass-0.2.5/opencompass/datasets/medbench/constructions.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/medbench/dataset_loader.py` & `opencompass-0.2.5/opencompass/datasets/medbench/dataset_loader.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/medbench/evaluation.py` & `opencompass-0.2.5/opencompass/datasets/medbench/evaluation.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/medbench/math_equivalence.py` & `opencompass-0.2.5/opencompass/datasets/medbench/math_equivalence.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/medbench/medbench.py` & `opencompass-0.2.5/opencompass/datasets/medbench/medbench.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/medbench/post_process.py` & `opencompass-0.2.5/opencompass/datasets/medbench/post_process.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/medbench/utils.py` & `opencompass-0.2.5/opencompass/datasets/medbench/utils.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/mmlu.py` & `opencompass-0.2.5/opencompass/datasets/mmlu.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/multirc.py` & `opencompass-0.2.5/opencompass/datasets/multirc.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/narrativeqa.py` & `opencompass-0.2.5/opencompass/datasets/narrativeqa.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/natural_question.py` & `opencompass-0.2.5/opencompass/datasets/triviaqa.py`

 * *Files 8% similar despite different names*

```diff
@@ -8,70 +8,78 @@
 from opencompass.registry import ICL_EVALUATORS, LOAD_DATASET
 from opencompass.utils.text_postprocessors import general_postprocess
 
 from .base import BaseDataset
 
 
 @LOAD_DATASET.register_module()
-class NaturalQuestionDataset(BaseDataset):
+class TriviaQADataset(BaseDataset):
 
     @staticmethod
     def load(path: str):
         dataset = DatasetDict()
         for split in ['dev', 'test']:
-            filename = osp.join(path, f'nq-{split}.qa.csv')
+            filename = osp.join(path, f'trivia-{split}.qa.csv')
             with open(filename, 'r', encoding='utf-8') as f:
                 reader = csv.reader(f, delimiter='\t')
                 raw_data = []
                 for row in reader:
                     assert len(row) == 2
                     question = row[0]
                     answers = eval(row[1])
-                    if split == 'dev':
+                    if split == 'test':
                         answers = answers[0]
                     raw_data.append({'question': question, 'answer': answers})
                 dataset[split] = Dataset.from_list(raw_data)
-
         return dataset
 
 
 @LOAD_DATASET.register_module()
-class NQOpenDataset(BaseDataset):
+class TriviaQADataset_V2(BaseDataset):
 
     @staticmethod
     def load(path: str):
         dataset = DatasetDict()
         for split in ['validation', 'train']:
-            filename = osp.join(path, f'nq-open-{split}.jsonl')
+            filename = osp.join(path, f'triviaqa-{split}.jsonl')
             raw_data = []
             with open(filename, 'r', encoding='utf-8') as f:
                 for doc in f:
                     doc = json.loads(doc)
-                    if split == 'train':
-                        doc['answer'] = doc['answer'][0]
                     raw_data.append(doc)
             dataset[split] = Dataset.from_list(raw_data)
 
         return dataset
 
 
+@LOAD_DATASET.register_module()
+class TriviaQADataset_V3(BaseDataset):
+
+    @staticmethod
+    def load(path: str):
+        data_list = []
+        with open(path, 'r', encoding='utf-8') as f:
+            for doc in f:
+                data_list.append(json.loads(doc))
+        return Dataset.from_list(data_list)
+
+
 @ICL_EVALUATORS.register_module()
-class NQEvaluator(BaseEvaluator):
+class TriviaQAEvaluator(BaseEvaluator):
 
     def score(self, predictions, references):
         if len(predictions) != len(references):
-            return {
-                'error': 'predictions and references have different '
-                'length'
-            }
+            return {'error': 'preds and refrs have different length'}
         processed_predictions = []
         for prediction in predictions:
             prediction = prediction.strip().split('\n')[0].lower()
-            if 'answer is' in prediction:
-                prediction = prediction.split('answer is')[-1]
+            prediction = prediction.split('answer is')[-1]
+            prediction = prediction.split('a:')[-1]
+            prediction = prediction.split('answer:')[-1]
+            prediction = prediction.strip()
             prediction = general_postprocess(prediction)
             processed_predictions.append(prediction)
         processed_answers = [[general_postprocess(j).lower() for j in i]
                              for i in references]
 
         details = []
         cnt = 0
```

### Comparing `opencompass-0.2.4/opencompass/datasets/natural_question_cn.py` & `opencompass-0.2.5/opencompass/datasets/natural_question_cn.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/obqa.py` & `opencompass-0.2.5/opencompass/datasets/obqa.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/piqa.py` & `opencompass-0.2.5/opencompass/datasets/piqa.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/py150.py` & `opencompass-0.2.5/opencompass/datasets/py150.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/qasper.py` & `opencompass-0.2.5/opencompass/datasets/qasper.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/qaspercut.py` & `opencompass-0.2.5/opencompass/datasets/qaspercut.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/race.py` & `opencompass-0.2.5/opencompass/datasets/race.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/realtoxicprompts.py` & `opencompass-0.2.5/opencompass/datasets/realtoxicprompts.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/reasonbench/ReasonBenchDataset.py` & `opencompass-0.2.5/opencompass/datasets/reasonbench/ReasonBenchDataset.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/record.py` & `opencompass-0.2.5/opencompass/datasets/record.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/rolebench.py` & `opencompass-0.2.5/opencompass/datasets/rolebench.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/safety.py` & `opencompass-0.2.5/opencompass/datasets/safety.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/scibench.py` & `opencompass-0.2.5/opencompass/datasets/scibench.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/siqa.py` & `opencompass-0.2.5/opencompass/datasets/siqa.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/squad20.py` & `opencompass-0.2.5/opencompass/datasets/squad20.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/storycloze.py` & `opencompass-0.2.5/opencompass/datasets/storycloze.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/strategyqa.py` & `opencompass-0.2.5/opencompass/datasets/strategyqa.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/subjective/alignbench.py` & `opencompass-0.2.5/opencompass/datasets/subjective/alignbench.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/subjective/compass_arena.py` & `opencompass-0.2.5/opencompass/datasets/subjective/compass_arena.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/subjective/corev2.py` & `opencompass-0.2.5/opencompass/datasets/subjective/corev2.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/subjective/creationbench.py` & `opencompass-0.2.5/opencompass/datasets/subjective/creationbench.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/subjective/information_retrival.py` & `opencompass-0.2.5/opencompass/datasets/subjective/information_retrival.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/subjective/mtbench.py` & `opencompass-0.2.5/opencompass/datasets/subjective/mtbench.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/subjective/multiround.py` & `opencompass-0.2.5/opencompass/datasets/subjective/multiround.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/subjective/subjective_cmp.py` & `opencompass-0.2.5/opencompass/datasets/subjective/subjective_cmp.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/summscreen.py` & `opencompass-0.2.5/opencompass/datasets/summscreen.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/svamp.py` & `opencompass-0.2.5/opencompass/datasets/svamp.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/tabmwp.py` & `opencompass-0.2.5/opencompass/datasets/tabmwp.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/taco.py` & `opencompass-0.2.5/opencompass/datasets/taco.py`

 * *Files 1% similar despite different names*

```diff
@@ -33,21 +33,24 @@
 TIMEOUT = 10
 
 
 @LOAD_DATASET.register_module()
 class TACODataset(BaseDataset):
 
     @staticmethod
-    def load(path: str, num_repeats: int = 1):
+    def load(path: str, num_repeats: int = 1, difficulty='ALL'):
         dataset = load_from_disk(path)
         new_dataset = DatasetDict()
         # add new column "starter" in the prompt
         for split in dataset.keys():
             new_samples = []
             for idx, sample in enumerate(dataset[split]):
+                if 'ALL' not in difficulty:
+                    if not sample['difficulty'] == difficulty:
+                        continue
                 starter_code = None if len(
                     sample['starter_code']) == 0 else sample['starter_code']
                 try:
                     input_output = json.loads(sample['input_output'])
                     fn_name = (None if not input_output.get('fn_name') else
                                input_output['fn_name'])
                 except ValueError:
@@ -67,28 +70,26 @@
                 sample['problem_id'] = problem_id
                 new_samples.append(sample)
             new_data = {
                 key: [sample[key] for sample in new_samples]
                 for key in new_samples[0].keys()
             }
             new_dataset[split] = Dataset.from_dict(new_data)
-
         # num_repeats duplicate
         # train_repeated = []
         test_repeated = []
         # for sample in new_dataset['train']:
         #     train_repeated.extend([sample] * num_repeats)
         for sample in new_dataset['test']:
             test_repeated.extend([sample] * num_repeats)
 
         # dataset_train_repeated = new_dataset['train'].from_list(
         #     train_repeated
         # )
         dataset_test_repeated = new_dataset['test'].from_list(test_repeated)
-
         return DatasetDict({
             # 'train': dataset_train_repeated,
             'test': dataset_test_repeated
         })
 
 
 EOF_STRINGS = ['\nQUESTION', '\n---', '\nANSWER', '<|endoftext|>']
```

### Comparing `opencompass-0.2.4/opencompass/datasets/teval/__init__.py` & `opencompass-0.2.5/opencompass/datasets/teval/__init__.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/teval/evaluators/instruct_evaluator.py` & `opencompass-0.2.5/opencompass/datasets/teval/evaluators/instruct_evaluator.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/teval/evaluators/planning_evaluator.py` & `opencompass-0.2.5/opencompass/datasets/teval/evaluators/planning_evaluator.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/teval/evaluators/reason_retrieve_understand_evaluator.py` & `opencompass-0.2.5/opencompass/datasets/teval/evaluators/reason_retrieve_understand_evaluator.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/teval/evaluators/review_evaluator.py` & `opencompass-0.2.5/opencompass/datasets/teval/evaluators/review_evaluator.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/teval/schema.py` & `opencompass-0.2.5/opencompass/datasets/teval/schema.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/teval/utils/convert_results.py` & `opencompass-0.2.5/opencompass/datasets/teval/utils/convert_results.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/teval/utils/format_load.py` & `opencompass-0.2.5/opencompass/datasets/teval/utils/format_load.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/teval/utils/template.py` & `opencompass-0.2.5/opencompass/datasets/teval/utils/template.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/tnews.py` & `opencompass-0.2.5/opencompass/datasets/tnews.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/triviaqa.py` & `opencompass-0.2.5/opencompass/datasets/gpqa.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,95 +1,111 @@
 import csv
-import json
-import os.path as osp
+import os
+import random
+import re
 
-from datasets import Dataset, DatasetDict
+from datasets import Dataset
 
-from opencompass.openicl.icl_evaluator import BaseEvaluator
-from opencompass.registry import ICL_EVALUATORS, LOAD_DATASET
-from opencompass.utils.text_postprocessors import general_postprocess
+from opencompass.openicl import BaseEvaluator
+from opencompass.registry import LOAD_DATASET, TEXT_POSTPROCESSORS
 
 from .base import BaseDataset
 
 
 @LOAD_DATASET.register_module()
-class TriviaQADataset(BaseDataset):
+class GPQADataset(BaseDataset):
 
     @staticmethod
-    def load(path: str):
-        dataset = DatasetDict()
-        for split in ['dev', 'test']:
-            filename = osp.join(path, f'trivia-{split}.qa.csv')
-            with open(filename, 'r', encoding='utf-8') as f:
-                reader = csv.reader(f, delimiter='\t')
-                raw_data = []
-                for row in reader:
-                    assert len(row) == 2
-                    question = row[0]
-                    answers = eval(row[1])
-                    if split == 'test':
-                        answers = answers[0]
-                    raw_data.append({'question': question, 'answer': answers})
-                dataset[split] = Dataset.from_list(raw_data)
+    def load(path: str, name: str):
+        cnt = 0
+        data = []
+        with open(os.path.join(path, name), 'r', encoding='utf-8') as f:
+            reader = csv.reader(f, delimiter=',')
+            for row in reader:
+                if row[7] == 'Question':
+                    continue
+                cnt = cnt + 1
+                question = row[7]
+                # 
+                options = [row[8], row[9], row[10], row[11]]
+                shuffle_patterns = ['ABCD', 'BCDA', 'CDAB', 'DABC']  # 
+                c = shuffle_patterns[cnt % 4]
+                line = {'question': question}
+                ground_truth = options[0]
+                for i in range(4):
+                    line['ABCD'[i]] = options[ord(c[i]) - ord('A')]
+                for i in range(4):
+                    if line['ABCD'[i]] == ground_truth:
+                        line['answer'] = 'ABCD'[i]
+                        break
+                data.append(line)
+        dataset = Dataset.from_list(data)
         return dataset
 
 
-@LOAD_DATASET.register_module()
-class TriviaQADataset_V2(BaseDataset):
+class GPQAEvaluator(BaseEvaluator):
 
-    @staticmethod
-    def load(path: str):
-        dataset = DatasetDict()
-        for split in ['validation', 'train']:
-            filename = osp.join(path, f'triviaqa-{split}.jsonl')
-            raw_data = []
-            with open(filename, 'r', encoding='utf-8') as f:
-                for doc in f:
-                    doc = json.loads(doc)
-                    raw_data.append(doc)
-            dataset[split] = Dataset.from_list(raw_data)
-
-        return dataset
+    def score(self, predictions, references):
+        if len(predictions) != len(references):
+            return {'error': 'preds and refrs have different length'}
+        correct = 0
+        count = 0
+        details = []
+        for i, j in zip(predictions, references):
+            detail = {'pred': i, 'answer': j, 'correct': False}
+            count += 1
+            if i == j:
+                correct += 1
+                detail['correct'] = True
+            details.append(detail)
+        result = {'accuracy': 100 * correct / count, 'details': details}
+        return result
 
 
 @LOAD_DATASET.register_module()
-class TriviaQADataset_V3(BaseDataset):
+class GPQADataset_Simple_Eval(BaseDataset):
 
     @staticmethod
-    def load(path: str):
-        data_list = []
-        with open(path, 'r', encoding='utf-8') as f:
-            for doc in f:
-                data_list.append(json.loads(doc))
-        return Dataset.from_list(data_list)
+    def load(path: str, name: str):
+        n_repeats = 4
+        data = []
+        with open(os.path.join(path, name), 'r', encoding='utf-8') as f:
+            reader = csv.reader(f, delimiter=',')
+            for row in reader:
+                if row[7] == 'Question':
+                    continue
+                question = row[7]
+                # 
+                options = [row[8], row[9], row[10], row[11]]
+                line = {'question': question}
+                line['answer'] = 'A'
+                line['options'] = options
+                data.append(line)
+
+        data_list = data * n_repeats
+        rng = random.Random(0)
+        data_list = [
+            data | {
+                'permutation': rng.sample(range(4), 4)
+            } for data in data_list
+        ]
+        for entry in data_list:
+            options = entry['options']
+            correct_options = [options[i] for i in entry['permutation']]
+            for i in range(4):
+                entry['ABCD'[i]] = correct_options[i]
+            correct_index = entry['permutation'].index(0)
+            correct_answer = 'ABCD'[correct_index]
+            entry['options'] = correct_options
+            entry['answer'] = correct_answer
 
+        dataset = Dataset.from_list(data_list)
+        return dataset
 
-@ICL_EVALUATORS.register_module()
-class TriviaQAEvaluator(BaseEvaluator):
-
-    def score(self, predictions, references):
-        if len(predictions) != len(references):
-            return {'error': 'preds and refrs have different length'}
-        processed_predictions = []
-        for prediction in predictions:
-            prediction = prediction.strip().split('\n')[0].lower()
-            prediction = prediction.split('answer is')[-1]
-            prediction = prediction.split('a:')[-1]
-            prediction = prediction.split('answer:')[-1]
-            prediction = prediction.strip()
-            prediction = general_postprocess(prediction)
-            processed_predictions.append(prediction)
-        processed_answers = [[general_postprocess(j).lower() for j in i]
-                             for i in references]
-
-        details = []
-        cnt = 0
-        for pred, cand_ans in zip(processed_predictions, processed_answers):
-            detail = {'pred': pred, 'answer': cand_ans, 'correct': False}
-            # is_correct = any([cand == pred for cand in cand_ans])
-            is_correct = any([cand in pred for cand in cand_ans])
-            cnt += int(is_correct)
-            detail['correct'] = is_correct
-            details.append(detail)
-        score = cnt / len(predictions) * 100
 
-        return {'score': score, 'details': details}
+@TEXT_POSTPROCESSORS.register_module()
+def GPQA_Simple_Eval_postprocess(text: str) -> str:
+    ANSWER_PATTERN = r'(?i)ANSWER\s*:\s*([A-D])'
+    match = re.search(ANSWER_PATTERN, text)
+    if match:
+        return match.group(1)
+    return None
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `opencompass-0.2.4/opencompass/datasets/triviaqarc.py` & `opencompass-0.2.5/opencompass/datasets/triviaqarc.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/truthfulqa.py` & `opencompass-0.2.5/opencompass/datasets/truthfulqa.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/tydiqa.py` & `opencompass-0.2.5/opencompass/datasets/tydiqa.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/wic.py` & `opencompass-0.2.5/opencompass/datasets/wic.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/wikibench.py` & `opencompass-0.2.5/opencompass/datasets/wikibench.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/winograd.py` & `opencompass-0.2.5/opencompass/datasets/winograd.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/winogrande.py` & `opencompass-0.2.5/opencompass/datasets/winogrande.py`

 * *Files 24% similar despite different names*

```diff
@@ -18,14 +18,17 @@
         dataset_list = []
         with open(path, 'r', encoding='utf-8') as f:
             for line in f:
                 line = json.loads(line)
                 prompt = line['sentence']
                 continue_prompt = prompt.split('_')[1]
                 data_item = {
+                    'prompt': prompt,
+                    'only_option1': line['option1'],
+                    'only_option2': line['option2'],
                     'opt1': prompt.replace('_', line['option1']),
                     'opt2': prompt.replace('_', line['option2']),
                     'answer': line['answer'],
                     'cont': continue_prompt,
                 }
                 dataset_list.append(data_item)
         dataset_list = Dataset.from_list(dataset_list)
@@ -44,14 +47,17 @@
             for line in f:
                 line = json.loads(line)
                 prompt = line['sentence']
                 continue_prompt = prompt.split('_')[1]
                 answer = line['answer']
                 answer = ' AB'[int(answer)] if answer != '' else 'NULL'
                 data_item = {
+                    'prompt': prompt,
+                    'only_option1': line['option1'],
+                    'only_option2': line['option2'],
                     'opt1': prompt.replace('_', line['option1']),
                     'opt2': prompt.replace('_', line['option2']),
                     'answer': answer,
                     'cont': continue_prompt,
                 }
                 dataset_list.append(data_item)
         dataset_list = Dataset.from_list(dataset_list)
@@ -72,14 +78,17 @@
                 for line in f:
                     line = json.loads(line)
                     prompt = line['sentence']
                     continue_prompt = prompt.split('_')[1]
                     answer = line['answer']
                     answer = ' AB'[int(answer)] if answer != '' else 'NULL'
                     data_item = {
+                        'prompt': prompt,
+                        'only_option1': line['option1'],
+                        'only_option2': line['option2'],
                         'opt1': prompt.replace('_', line['option1']),
                         'opt2': prompt.replace('_', line['option2']),
                         'answer': answer,
                         'cont': continue_prompt,
                     }
                     dataset_list.append(data_item)
             dataset_dict[split] = Dataset.from_list(dataset_list)
```

### Comparing `opencompass-0.2.4/opencompass/datasets/wnli.py` & `opencompass-0.2.5/opencompass/datasets/wnli.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/wsc.py` & `opencompass-0.2.5/opencompass/datasets/wsc.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/xcopa.py` & `opencompass-0.2.5/opencompass/datasets/xcopa.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/xiezhi.py` & `opencompass-0.2.5/opencompass/datasets/xiezhi.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/xlsum.py` & `opencompass-0.2.5/opencompass/datasets/xlsum.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/datasets/xsum.py` & `opencompass-0.2.5/opencompass/datasets/xsum.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/metrics/dump_results.py` & `opencompass-0.2.5/opencompass/metrics/dump_results.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/metrics/mme_score.py` & `opencompass-0.2.5/opencompass/metrics/mme_score.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/metrics/seedbench.py` & `opencompass-0.2.5/opencompass/metrics/seedbench.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/models/__init__.py` & `opencompass-0.2.5/opencompass/models/__init__.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,39 +1,47 @@
 from .accessory import LLaMA2AccessoryModel  # noqa: F401
 from .ai360_api import AI360GPT  # noqa: F401
 from .alaya import AlayaLM  # noqa: F401
 from .baichuan_api import BaiChuan, BaiChuan3  # noqa: F401
 from .baidu_api import ERNIEBot  # noqa: F401
-from .base import BaseModel, LMTemplateParser  # noqa
-from .base_api import APITemplateParser, BaseAPIModel  # noqa
+from .base import BaseModel, LMTemplateParser  # noqa: F401
+from .base_api import APITemplateParser, BaseAPIModel  # noqa: F401
 from .bytedance_api import ByteDance  # noqa: F401
 from .claude_api import Claude  # noqa: F401
-from .gemini_api import Gemini, GeminiAllesAPIN  # noqa: F401, F403
-from .glm import GLM130B  # noqa: F401, F403
-from .huggingface import HuggingFace  # noqa: F401, F403
-from .huggingface import HuggingFaceCausalLM  # noqa: F401, F403
-from .huggingface import HuggingFaceChatGLM3  # noqa: F401, F403
+from .deepseek_api import DeepseekAPI  # noqa: F401
+from .gemini_api import Gemini  # noqa: F401
+from .glm import GLM130B  # noqa: F401
+from .huggingface import HuggingFace  # noqa: F401
+from .huggingface import HuggingFaceCausalLM  # noqa: F401
+from .huggingface import HuggingFaceChatGLM3  # noqa: F401
+from .huggingface_above_v4_33 import HuggingFaceBaseModel  # noqa: F401
+from .huggingface_above_v4_33 import HuggingFacewithChatTemplate  # noqa: F401
 from .hunyuan_api import Hunyuan  # noqa: F401
-from .intern_model import InternLM  # noqa: F401, F403
+from .intern_model import InternLM  # noqa: F401
 from .krgpt_api import KrGPT  # noqa: F401
-from .lightllm_api import LightllmAPI  # noqa: F401
-from .llama2 import Llama2, Llama2Chat  # noqa: F401, F403
+from .lightllm_api import LightllmAPI, LightllmChatAPI  # noqa: F401
+from .llama2 import Llama2, Llama2Chat  # noqa: F401
 from .lmdeploy_pytorch import LmdeployPytorchModel  # noqa: F401
 from .lmdeploy_tis import LmdeployTisModel  # noqa: F401
-from .minimax_api import MiniMax  # noqa: F401
+from .minimax_api import MiniMax, MiniMaxChatCompletionV2  # noqa: F401
 from .mistral_api import Mistral  # noqa: F401
 from .mixtral import Mixtral  # noqa: F401
-from .modelscope import ModelScope, ModelScopeCausalLM  # noqa: F401, F403
+from .modelscope import ModelScope, ModelScopeCausalLM  # noqa: F401
 from .moonshot_api import MoonShot  # noqa: F401
 from .nanbeige_api import Nanbeige  # noqa: F401
 from .openai_api import OpenAI  # noqa: F401
 from .pangu_api import PanGu  # noqa: F401
 from .qwen_api import Qwen  # noqa: F401
 from .sensetime_api import SenseTime  # noqa: F401
+from .stepfun_api import StepFun  # noqa: F401
 from .turbomind import TurboMindModel  # noqa: F401
 from .turbomind_tis import TurboMindTisModel  # noqa: F401
+from .turbomind_with_tf_above_v4_33 import \
+    TurboMindModelwithChatTemplate  # noqa: F401
 from .unigpt_api import UniGPT  # noqa: F401
 from .vllm import VLLM  # noqa: F401
-from .xunfei_api import XunFei  # noqa: F401
+from .vllm_with_tf_above_v4_33 import VLLMwithChatTemplate  # noqa: F401
+from .xunfei_api import XunFei, XunFeiSpark  # noqa: F401
 from .yayi_api import Yayi  # noqa: F401
+from .yi_api import YiAPI  # noqa: F401
 from .zhipuai_api import ZhiPuAI  # noqa: F401
 from .zhipuai_v2_api import ZhiPuV2AI  # noqa: F401
```

### Comparing `opencompass-0.2.4/opencompass/models/accessory.py` & `opencompass-0.2.5/opencompass/models/accessory.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/models/ai360_api.py` & `opencompass-0.2.5/opencompass/models/ai360_api.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,8 +1,7 @@
-import time
 from concurrent.futures import ThreadPoolExecutor
 from typing import Dict, List, Optional, Union
 
 import requests
 
 from opencompass.utils.prompt import PromptList
 
@@ -137,34 +136,37 @@
                 print('Connection error, reconnect.')
                 # if connect error, frequent requests will casuse
                 # continuous unstable network, therefore wait here
                 # to slow down the request
                 self.wait()
                 continue
             if raw_response.status_code == 200:
-                try:
-                    msg = response['choices'][0]['message']['content'].strip()
-                    return msg
-
-                except KeyError:
-                    if 'error' in response:
-                        # tpm(token per minitue) limit
-                        if response['erro']['code'] == '1005':
-                            time.sleep(1)
-                            continue
-
-                        self.logger.error('Find error message in response: ',
-                                          str(response['error']))
+                msg = response['choices'][0]['message']['content'].strip()
+                self.logger.debug(f'Generated: {msg}')
+                return msg
 
             # sensitive content, prompt overlength, network error
             # or illegal prompt
-            if (raw_response.status_code == 400
-                    or raw_response.status_code == 401
-                    or raw_response.status_code == 402
-                    or raw_response.status_code == 429
-                    or raw_response.status_code == 500):
-                print(raw_response.text)
-                continue
+            if raw_response.status_code in [400, 401, 402, 429, 500]:
+                if 'error' not in response:
+                    print(raw_response.status_code)
+                    print(raw_response.text)
+                    continue
+                print(response)
+                # tpm(token per minitue) limit
+                if response['error']['code'] == '1005':
+                    self.logger.debug('tpm limit, ignoring')
+                    continue
+                elif response['error']['code'] == '1001':
+                    msg = ':messagesmax_tokens'
+                    self.logger.debug(f'Generated: {msg}')
+                    return msg
+                else:
+                    print(response)
+
+                self.logger.error('Find error message in response: ',
+                                  str(response['error']))
+
             print(raw_response)
             max_num_retries += 1
 
         raise RuntimeError(raw_response.text)
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `opencompass-0.2.4/opencompass/models/alaya.py` & `opencompass-0.2.5/opencompass/models/alaya.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/models/baichuan_api.py` & `opencompass-0.2.5/opencompass/models/baichuan_api.py`

 * *Files 0% similar despite different names*

```diff
@@ -141,16 +141,16 @@
                 print('Connection error, reconnect.')
                 # if connect error, frequent requests will casuse
                 # continuous unstable network, therefore wait here
                 # to slow down the request
                 self.wait()
                 continue
             if raw_response.status_code == 200:
-
                 msg = response['choices'][0]['message']['content']
+                self.logger.debug(f'Generated: {msg}')
                 return msg
 
             if raw_response.status_code != 200:
                 print(raw_response.json())
                 time.sleep(1)
                 continue
             print(response)
```

### Comparing `opencompass-0.2.4/opencompass/models/baidu_api.py` & `opencompass-0.2.5/opencompass/models/baidu_api.py`

 * *Files 5% similar despite different names*

```diff
@@ -49,14 +49,16 @@
                          query_per_second=query_per_second,
                          meta_template=meta_template,
                          retry=retry,
                          generation_kwargs=generation_kwargs)
         self.headers = {'Content_Type': 'application/json'}
         self.secretkey = secretkey
         self.key = key
+        if not url.endswith('?access_token='):
+            url += '?access_token='
         self.url = url
         access_token, _ = self._generate_access_token()
         self.access_token = access_token
         print(access_token)
 
     def _generate_access_token(self):
         try:
@@ -139,22 +141,33 @@
 
         """
 
         if isinstance(input, str):
             messages = [{'role': 'user', 'content': input}]
         else:
             messages = []
+            msg_buffer, last_role = [], None
             for item in input:
-                msg = {'content': item['prompt']}
-                if item['role'] == 'HUMAN':
-                    msg['role'] = 'user'
-                elif item['role'] == 'BOT':
-                    msg['role'] = 'assistant'
+                if item['role'] == 'BOT':
+                    role = 'assistant'
+                else:  # USER or SYSTEM
+                    role = 'user'
+                if role != last_role and last_role is not None:
+                    messages.append({
+                        'content': '\n'.join(msg_buffer),
+                        'role': last_role
+                    })
+                    msg_buffer = []
+                msg_buffer.append(item['prompt'])
+                last_role = role
+            messages.append({
+                'content': '\n'.join(msg_buffer),
+                'role': last_role
+            })
 
-                messages.append(msg)
         data = {'messages': messages}
         data.update(self.generation_kwargs)
 
         max_num_retries = 0
         while max_num_retries < self.retry:
             self.acquire()
             try:
@@ -177,24 +190,28 @@
                 # continuous unstable network, therefore wait here
                 # to slow down the request
                 self.wait()
                 continue
             if raw_response.status_code == 200:
                 try:
                     msg = response['result']
+                    self.logger.debug(msg)
                     return msg
                 except KeyError:
                     print(response)
                     self.logger.error(str(response['error_code']))
                     if response['error_code'] == 336007:
                         # exceed max length
                         return ''
-
-                    time.sleep(1)
-                    continue
+                    elif response['error_code'] == 336103:
+                        # prompt tokens too long
+                        return ''
+                    else:
+                        time.sleep(1)
+                        continue
 
             if (response['error_code'] == 110 or response['error_code'] == 100
                     or response['error_code'] == 111
                     or response['error_code'] == 200
                     or response['error_code'] == 1000
                     or response['error_code'] == 1001
                     or response['error_code'] == 1002
```

### Comparing `opencompass-0.2.4/opencompass/models/base.py` & `opencompass-0.2.5/opencompass/models/base.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/models/base_api.py` & `opencompass-0.2.5/opencompass/models/base_api.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/models/bytedance_api.py` & `opencompass-0.2.5/opencompass/models/bytedance_api.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/models/claude_api/claude_api.py` & `opencompass-0.2.5/opencompass/models/claude_api/claude_api.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/models/claude_api/postprocessors.py` & `opencompass-0.2.5/opencompass/models/claude_api/postprocessors.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/models/gemini_api.py` & `opencompass-0.2.5/opencompass/models/gemini_api.py`

 * *Files 17% similar despite different names*

```diff
@@ -182,70 +182,7 @@
                         return body['candidates'][0]['content']['parts'][0][
                             'text'].strip()
             self.logger.error(response['msg'])
             self.logger.error(response)
             time.sleep(1)
 
         raise RuntimeError('API call failed.')
-
-
-class GeminiAllesAPIN(Gemini):
-    """Model wrapper around Gemini models.
-
-    Documentation:
-
-    Args:
-        path (str): The name of Gemini model.
-            e.g. `gemini-pro`
-        key (str): Authorization key.
-        query_per_second (int): The maximum queries allowed per second
-            between two consecutive calls of the API. Defaults to 1.
-        max_seq_len (int): Unused here.
-        meta_template (Dict, optional): The model's meta prompt
-            template if needed, in case the requirement of injecting or
-            wrapping of any meta instructions.
-        retry (int): Number of retires if the API call fails. Defaults to 2.
-    """
-
-    def __init__(
-        self,
-        path: str,
-        key: str,
-        url: str,
-        query_per_second: int = 2,
-        max_seq_len: int = 2048,
-        meta_template: Optional[Dict] = None,
-        retry: int = 2,
-        temperature: float = 1.0,
-        top_p: float = 0.8,
-        top_k: float = 10.0,
-    ):
-        super().__init__(key=key,
-                         path=path,
-                         max_seq_len=max_seq_len,
-                         query_per_second=query_per_second,
-                         meta_template=meta_template,
-                         retry=retry)
-        # Replace the url and headers into AllesApin
-        self.url = url
-        self.headers = {
-            'alles-apin-token': key,
-            'content-type': 'application/json',
-        }
-
-    def generate(
-        self,
-        inputs: List[PromptType],
-        max_out_len: int = 512,
-    ) -> List[str]:
-        """Generate results given a list of inputs.
-
-        Args:
-            inputs (List[PromptType]): A list of strings or PromptDicts.
-                The PromptDict should be organized in OpenCompass'
-                API format.
-            max_out_len (int): The maximum length of the output.
-
-        Returns:
-            List[str]: A list of generated strings.
-        """
-        return super().generate(inputs, max_out_len)
```

### Comparing `opencompass-0.2.4/opencompass/models/glm.py` & `opencompass-0.2.5/opencompass/models/glm.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/models/huggingface.py` & `opencompass-0.2.5/opencompass/models/huggingface.py`

 * *Files 2% similar despite different names*

```diff
@@ -285,21 +285,21 @@
                 conv.append_message(conv.roles[1], None)
                 inputs[i] = conv.get_prompt()
 
         # step-1: tokenize the input with batch_encode_plus
         tokens = self.tokenizer.batch_encode_plus(inputs,
                                                   padding=True,
                                                   truncation=True,
-                                                  max_length=self.max_seq_len -
-                                                  max_out_len)
+                                                  max_length=self.max_seq_len)
         tokens = {
             k: torch.tensor(np.array(tokens[k]), device=self.model.device)
             for k in tokens if k in ['input_ids', 'attention_mask']
         }
 
+        origin_stopping_criteria = stopping_criteria
         if stopping_criteria:
             # Construct huggingface stopping criteria
             if self.tokenizer.eos_token is not None:
                 stopping_criteria = stopping_criteria + [
                     self.tokenizer.eos_token
                 ]
             stopping_criteria = transformers.StoppingCriteriaList([
@@ -328,14 +328,17 @@
         if self.extract_pred_after_decode:
             decodeds = [
                 token[len_:] for token, len_ in zip(decodeds, prompt_lens)
             ]
 
         if self.end_str:
             decodeds = [token.split(self.end_str)[0] for token in decodeds]
+        if origin_stopping_criteria:
+            for t in origin_stopping_criteria:
+                decodeds = [token.split(t)[0] for token in decodeds]
         return decodeds
 
     def _single_generate(self,
                          inputs: List[str],
                          max_out_len: int,
                          min_out_len: Optional[int] = None,
                          stopping_criteria: List[str] = [],
@@ -378,14 +381,15 @@
                 ]
 
         input_ids = self.tokenizer(inputs,
                                    truncation=True,
                                    max_length=self.max_seq_len -
                                    max_out_len)['input_ids']
         input_ids = torch.tensor(input_ids, device=self.model.device)
+        origin_stopping_criteria = stopping_criteria
         if stopping_criteria:
             # Construct huggingface stopping criteria
             if self.tokenizer.eos_token is not None:
                 stopping_criteria = stopping_criteria + [
                     self.tokenizer.eos_token
                 ]
             stopping_criteria = transformers.StoppingCriteriaList([
@@ -415,14 +419,17 @@
         if self.extract_pred_after_decode:
             decodeds = [
                 token[len_:] for token, len_ in zip(decodeds, prompt_lens)
             ]
 
         if self.end_str:
             decodeds = [token.split(self.end_str)[0] for token in decodeds]
+        if origin_stopping_criteria:
+            for t in origin_stopping_criteria:
+                decodeds = [token.split(t)[0] for token in decodeds]
         return decodeds
 
     def get_logits(self, inputs: List[str]):
 
         if self.batch_padding and len(inputs) > 1:
             # batch inference
             tokens = self.tokenizer(inputs,
```

### Comparing `opencompass-0.2.4/opencompass/models/hunyuan_api.py` & `opencompass-0.2.5/opencompass/models/hunyuan_api.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/models/intern_model.py` & `opencompass-0.2.5/opencompass/models/intern_model.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/models/krgpt_api.py` & `opencompass-0.2.5/opencompass/models/krgpt_api.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/models/lagent.py` & `opencompass-0.2.5/opencompass/models/lagent.py`

 * *Files 2% similar despite different names*

```diff
@@ -40,17 +40,18 @@
                 action.reset()
 
     def set_history(self, history):
         self.agent._session_history = deepcopy(history)
 
     def gt_response(self, prompt):
         if 'CIReAct' in str(self.agent.__class__):
-            gold = prompt
-            prompt = f"""{self.agent._protocol.action['begin']} IPythonInterpreter
-{self.agent._protocol.action_input['begin']} ```python\n{gold}\n```\n"""  # noqa
+            thought, gold = prompt.split('**split**')
+            prompt = f"""{self.agent._protocol.thought['begin']} {thought}\
+\n{self.agent._protocol.action['begin']} IPythonInterpreter\n\
+{self.agent._protocol.action_input['begin']}```python\n{gold}\n```\n"""  # noqa
             action_input = dict(
                 command=f"""```python\n{gold}\n```\n""",
                 timeout=120,
             )
             response = self.agent._action_executor('IPythonInterpreter',
                                                    action_input)
             gt_response = dict(role='assistant', content=prompt)
```

### Comparing `opencompass-0.2.4/opencompass/models/langchain.py` & `opencompass-0.2.5/opencompass/models/langchain.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/models/lightllm_api.py` & `opencompass-0.2.5/opencompass/models/pangu_api.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,185 +1,195 @@
-import json
-import re
+import time
 from concurrent.futures import ThreadPoolExecutor
-from typing import Dict, List, Optional
+from typing import Dict, List, Optional, Union
 
-import numpy as np
 import requests
 
-from opencompass.registry import MODELS
-from opencompass.utils.logging import get_logger
+from opencompass.utils.prompt import PromptList
 
-from .base import BaseModel
-from .base_api import TokenBucket
+from .base_api import BaseAPIModel
 
+PromptType = Union[PromptList, str]
 
-@MODELS.register_module()
-class LightllmAPI(BaseModel):
 
-    is_api: bool = True
+class PanGu(BaseAPIModel):
+    """Model wrapper around PanGu.
+
+    Args:
+        path (str): The name of Pangu model.
+            e.g. `pangu`
+        access_key (str): provided access_key
+        secret_key (str): secretkey in order to obtain access_token
+        url (str): provide url for requests
+        token_url (str): url of token server
+        project_name (str): project name for generate the token
+        query_per_second (int): The maximum queries allowed per second
+            between two consecutive calls of the API. Defaults to 1.
+        max_seq_len (int): Unused here.
+        meta_template (Dict, optional): The model's meta prompt
+            template if needed, in case the requirement of injecting or
+            wrapping of any meta instructions.
+        retry (int): Number of retires if the API call fails. Defaults to 2.
+    """
 
     def __init__(
-            self,
-            path: str = 'LightllmAPI',
-            url: str = 'http://localhost:8080/generate',
-            meta_template: Optional[Dict] = None,
-            rate_per_worker: int = 2,
-            retry: int = 2,
-            generation_kwargs: Optional[Dict] = dict(),
+        self,
+        path: str,
+        access_key: str,
+        secret_key: str,
+        url: str,
+        token_url: str,
+        project_name: str,
+        query_per_second: int = 2,
+        max_seq_len: int = 2048,
+        meta_template: Optional[Dict] = None,
+        retry: int = 2,
     ):
-
         super().__init__(path=path,
+                         max_seq_len=max_seq_len,
+                         query_per_second=query_per_second,
                          meta_template=meta_template,
-                         generation_kwargs=generation_kwargs)
-        self.logger = get_logger()
-        self.url = url
-        self.retry = retry
-        self.generation_kwargs = generation_kwargs
-        self.max_out_len = self.generation_kwargs.get('max_new_tokens', 1024)
-        self.meta_template = meta_template
-        self.token_bucket = TokenBucket(rate_per_worker, False)
+                         retry=retry)
 
-    def generate(self, inputs: List[str], max_out_len: int,
-                 **kwargs) -> List[str]:
+        self.access_key = access_key
+        self.secret_key = secret_key
+        self.url = url
+        self.token_url = token_url
+        self.project_name = project_name
+        self.model = path
+
+        token_response = self._get_token()
+        if token_response.status_code == 201:
+            self.token = token_response.headers['X-Subject-Token']
+            print('')
+        else:
+            self.token = None
+            print('token')
+
+    def generate(
+        self,
+        inputs: List[PromptType],
+        max_out_len: int = 512,
+    ) -> List[str]:
         """Generate results given a list of inputs.
 
         Args:
-            inputs (List[str]): A list of strings or PromptDicts.
+            inputs (List[PromptType]): A list of strings or PromptDicts.
                 The PromptDict should be organized in OpenCompass'
                 API format.
             max_out_len (int): The maximum length of the output.
 
         Returns:
             List[str]: A list of generated strings.
         """
-
         with ThreadPoolExecutor() as executor:
             results = list(
                 executor.map(self._generate, inputs,
-                             [self.max_out_len] * len(inputs)))
+                             [max_out_len] * len(inputs)))
+        self.flush()
         return results
 
-    def _generate(self, input: str, max_out_len: int) -> str:
-        max_num_retries = 0
-        while max_num_retries < self.retry:
-            self.wait()
-            header = {'content-type': 'application/json'}
-            try:
-                data = dict(inputs=input, parameters=self.generation_kwargs)
-                raw_response = requests.post(self.url,
-                                             headers=header,
-                                             data=json.dumps(data))
-            except requests.ConnectionError:
-                self.logger.error('Got connection error, retrying...')
-                continue
-            try:
-                response = raw_response.json()
-                generated_text = response['generated_text']
-                if isinstance(generated_text, list):
-                    generated_text = generated_text[0]
-                return generated_text
-            except requests.JSONDecodeError:
-                self.logger.error('JsonDecode error, got',
-                                  str(raw_response.content))
-            except KeyError:
-                self.logger.error(f'KeyError. Response: {str(response)}')
-            max_num_retries += 1
-
-        raise RuntimeError('Calling LightllmAPI failed after retrying for '
-                           f'{max_num_retries} times. Check the logs for '
-                           'details.')
-
-    def get_ppl(self, inputs: List[str], max_out_len: int,
-                **kwargs) -> List[float]:
-        """Generate results given a list of inputs.
+    def _get_token(self):
+        url = self.token_url
+        payload = {
+            'auth': {
+                'identity': {
+                    'methods': ['hw_ak_sk'],
+                    'hw_ak_sk': {
+                        'access': {
+                            'key': self.access_key
+                        },
+                        'secret': {
+                            'key': self.secret_key
+                        }
+                    }
+                },
+                'scope': {
+                    'project': {
+                        'name': self.project_name
+                    }
+                }
+            }
+        }
+        headers = {'Content-Type': 'application/json'}
+
+        response = requests.request('POST', url, headers=headers, json=payload)
+        return response
+
+    def _generate(
+        self,
+        input: PromptType,
+        max_out_len: int = 512,
+    ) -> str:
+        """Generate results given an input.
 
         Args:
-            inputs (List[str]): A list of strings or PromptDicts.
+            inputs (PromptType): A string or PromptDict.
                 The PromptDict should be organized in OpenCompass'
                 API format.
             max_out_len (int): The maximum length of the output.
 
         Returns:
-            List[str]: A list of generated strings.
+            str: The generated string.
         """
+        assert isinstance(input, (str, PromptList))
 
-        with ThreadPoolExecutor() as executor:
-            results = list(
-                executor.map(self._get_ppl, inputs,
-                             [self.max_out_len] * len(inputs)))
-        return np.array(results)
+        if isinstance(input, str):
+            messages = [{'role': 'user', 'content': input}]
+        else:
+            messages = []
+            for item in input:
+                msg = {'content': item['prompt']}
+                if item['role'] == 'HUMAN':
+                    msg['role'] = 'user'
+                elif item['role'] == 'BOT':
+                    msg['role'] = 'system'
+
+                messages.append(msg)
+
+        data = {'messages': messages, 'stream': False}
+
+        # token_response = self._get_token()
+        # if token_response.status_code == 201:
+        #     self.token = token_response.headers['X-Subject-Token']
+        #     print('')
+        # else:
+        #     self.token = None
+        #     print('token')
+
+        headers = {
+            'Content-Type': 'application/json',
+            'X-Auth-Token': self.token
+        }
 
-    def _get_ppl(self, input: str, max_out_len: int) -> float:
         max_num_retries = 0
-        if max_out_len is None:
-            max_out_len = 1
         while max_num_retries < self.retry:
-            self.wait()
-            header = {'content-type': 'application/json'}
-            try:
-                data = dict(inputs=input, parameters=self.generation_kwargs)
-                raw_response = requests.post(self.url,
-                                             headers=header,
-                                             data=json.dumps(data))
-            except requests.ConnectionError:
-                self.logger.error('Got connection error, retrying...')
+            self.acquire()
+            raw_response = requests.request('POST',
+                                            url=self.url,
+                                            headers=headers,
+                                            json=data)
+            response = raw_response.json()
+            self.release()
+
+            if response is None:
+                print('Connection error, reconnect.')
+                # if connect error, frequent requests will casuse
+                # continuous unstable network, therefore wait here
+                # to slow down the request
+                self.wait()
                 continue
-            try:
-                response = raw_response.json()
-
-                assert ('prompt_token_ids' in response and 'prompt_logprobs'
-                        in response), f'prompt_token_ids and prompt_logprobs \
-                    must be in the output. \
-                    Please consider adding \
-                    --return_all_prompt_logprobs argument \
-                    when starting lightllm service. Response: {str(response)}'
-
-                prompt_token_ids = response['prompt_token_ids'][1:]
-                prompt_logprobs = [
-                    item[1] for item in response['prompt_logprobs']
-                ]
-                logprobs = [
-                    item[str(token_id)] for token_id, item in zip(
-                        prompt_token_ids, prompt_logprobs)
-                ]
-                if len(logprobs) == 0:
-                    return 0.0
-                ce_loss = -sum(logprobs) / len(logprobs)
-                return ce_loss
-            except requests.JSONDecodeError:
-                self.logger.error('JsonDecode error, got',
-                                  str(raw_response.content))
+            if raw_response.status_code == 200:
+                # msg = json.load(response.text)
+                # response
+                msg = response['choices'][0]['message']['content']
+                return msg
+
+            if (raw_response.status_code != 200):
+                print(response['error_msg'])
+                # return ''
+                time.sleep(1)
+                continue
+            print(response)
             max_num_retries += 1
-        raise RuntimeError('Calling LightllmAPI failed after retrying for '
-                           f'{max_num_retries} times. Check the logs for '
-                           'details.')
-
-    def wait(self):
-        """Wait till the next query can be sent.
-
-        Applicable in both single-thread and multi-thread environments.
-        """
-        return self.token_bucket.get_token()
-
-    def get_token_len(self, prompt: str) -> int:
-        """Get lengths of the tokenized string. Only English and Chinese
-        characters are counted for now. Users are encouraged to override this
-        method if more accurate length is needed.
-
-        Args:
-            prompt (str): Input string.
-
-        Returns:
-            int: Length of the input tokens
-        """
-
-        english_parts = re.findall(r'[A-Za-z0-9]+', prompt)
-        chinese_parts = re.findall(r'[\u4e00-\u9FFF]+', prompt)
-
-        # Count English words
-        english_count = sum(len(part.split()) for part in english_parts)
-
-        # Count Chinese words
-        chinese_count = sum(len(part) for part in chinese_parts)
 
-        return english_count + chinese_count
+        raise RuntimeError(response['error_msg'])
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `opencompass-0.2.4/opencompass/models/llama2.py` & `opencompass-0.2.5/opencompass/models/llama2.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/models/lmdeploy_pytorch.py` & `opencompass-0.2.5/opencompass/models/lmdeploy_pytorch.py`

 * *Files 10% similar despite different names*

```diff
@@ -46,14 +46,15 @@
                  engine_config: Optional[Dict] = None,
                  gen_config: Optional[Dict] = None,
                  end_str: Optional[str] = None):
         super().__init__(path=path,
                          max_seq_len=max_seq_len,
                          meta_template=meta_template)
         from lmdeploy.pytorch import engine as tm
+        from lmdeploy.version import version_info
 
         if engine_config is not None:
             from lmdeploy.messages import PytorchEngineConfig
             engine_config = PytorchEngineConfig(**engine_config)
             # set thread_safe
             if hasattr(engine_config, 'thread_safe'):
                 engine_config.thread_safe = True
@@ -67,14 +68,15 @@
         self.tokenizer = tm_model.tokenizer
         self.generators = [
             tm_model.create_instance() for i in range(concurrency)
         ]
         self.generator_ids = [i + 1 for i in range(concurrency)]
         self.gen_config = gen_config
         self.end_str = end_str
+        self.major_version, self.minor_version, _ = version_info
 
     def generate(
         self,
         inputs: List[str],
         max_out_len: int = 512,
     ) -> List[str]:
         """Generate results given a list of inputs.
@@ -141,17 +143,24 @@
                 Defaults to None.
         Returns:
             str: The generated string.
         """
         assert type(
             prompt) is str, 'We only support string for TurboMind Python API'
         input_ids = self.tokenizer.encode(prompt)
-        _, output_ids, _ = generator.infer(session_id,
-                                           input_ids,
-                                           gen_config=gen_config)
+        if self.major_version >= 0 and self.minor_version >= 4:
+            outputs = generator.infer(session_id,
+                                      input_ids,
+                                      gen_config=gen_config)
+            output_ids = outputs.token_ids
+        else:
+            _, output_ids, _ = generator.infer(session_id,
+                                               input_ids,
+                                               gen_config=gen_config)
+
         # stop engine
         if hasattr(generator, 'end'):
             generator.end(session_id)
         # decode output
         response_all = self.tokenizer.decode(output_ids)
         # trim output
         if end_str:
```

### Comparing `opencompass-0.2.4/opencompass/models/lmdeploy_tis.py` & `opencompass-0.2.5/opencompass/models/lmdeploy_tis.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/models/minimax_api.py` & `opencompass-0.2.5/opencompass/models/zhipuai_v2_api.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,65 +1,62 @@
 import time
 from concurrent.futures import ThreadPoolExecutor
 from typing import Dict, List, Optional, Union
 
-import requests
-
 from opencompass.utils.prompt import PromptList
 
 from .base_api import BaseAPIModel
 
-PromptType = Union[PromptList, str]
+try:
+    from zhipuai.core._errors import APIStatusError, APITimeoutError
+except ImportError:
+    APIStatusError = None
+    APITimeoutError = None
 
+PromptType = Union[PromptList, str]
 
-class MiniMax(BaseAPIModel):
-    """Model wrapper around MiniMax.
 
-    Documentation: https://api.minimax.chat/document/guides/chat-pro
+class ZhiPuV2AI(BaseAPIModel):
+    """Model wrapper around ZhiPuAI.
 
     Args:
-        path (str): The name of MiniMax model.
-            e.g. `abab5.5-chat`
-        model_type (str): The type of the model
-            e.g. `chat`
-        group_id (str): The id of group(like the org ID of group)
+        path (str): The name of OpenAI's model.
         key (str): Authorization key.
         query_per_second (int): The maximum queries allowed per second
             between two consecutive calls of the API. Defaults to 1.
         max_seq_len (int): Unused here.
         meta_template (Dict, optional): The model's meta prompt
             template if needed, in case the requirement of injecting or
             wrapping of any meta instructions.
         retry (int): Number of retires if the API call fails. Defaults to 2.
     """
 
-    def __init__(
-        self,
-        path: str,
-        key: str,
-        group_id: str,
-        model_type: str = 'chat',
-        url:
-        str = 'https://api.minimax.chat/v1/text/chatcompletion_pro?GroupId=',
-        query_per_second: int = 2,
-        max_seq_len: int = 2048,
-        meta_template: Optional[Dict] = None,
-        retry: int = 2,
-    ):
+    def __init__(self,
+                 path: str,
+                 key: str,
+                 query_per_second: int = 2,
+                 max_seq_len: int = 2048,
+                 meta_template: Optional[Dict] = None,
+                 retry: int = 2,
+                 generation_kwargs: Dict = {
+                     'tools': [{
+                         'type': 'web_search',
+                         'enable': False
+                     }]
+                 }):
         super().__init__(path=path,
                          max_seq_len=max_seq_len,
                          query_per_second=query_per_second,
                          meta_template=meta_template,
-                         retry=retry)
-        self.headers = {
-            'Authorization': f'Bearer {key}',
-            'Content-Type': 'application/json',
-        }
-        self.type = model_type
-        self.url = url + group_id
+                         retry=retry,
+                         generation_kwargs=generation_kwargs)
+        from zhipuai import ZhipuAI
+
+        # self.zhipuai = zhipuai
+        self.client = ZhipuAI(api_key=key)
         self.model = path
 
     def generate(
         self,
         inputs: List[PromptType],
         max_out_len: int = 512,
     ) -> List[str]:
@@ -86,97 +83,98 @@
         input: PromptType,
         max_out_len: int = 512,
     ) -> str:
         """Generate results given an input.
 
         Args:
             inputs (PromptType): A string or PromptDict.
-                The PromptDict should be organized in Test'
+                The PromptDict should be organized in OpenCompass'
                 API format.
             max_out_len (int): The maximum length of the output.
 
         Returns:
             str: The generated string.
         """
         assert isinstance(input, (str, PromptList))
 
         if isinstance(input, str):
-            messages = [{
-                'sender_type': 'USER',
-                'sender_name': 'Test',
-                'text': input
-            }]
+            messages = [{'role': 'user', 'content': input}]
         else:
             messages = []
             for item in input:
-                msg = {'text': item['prompt']}
+                if not item['prompt']:
+                    continue
+                msg = {'content': item['prompt']}
                 if item['role'] == 'HUMAN':
-                    msg['sender_type'] = 'USER'
-                    msg['sender_name'] = 'Test'
+                    msg['role'] = 'user'
                 elif item['role'] == 'BOT':
-                    msg['sender_type'] = 'BOT'
-                    msg['sender_name'] = 'MM'
-
+                    msg['role'] = 'assistant'
+                elif item['role'] == 'SYSTEM':
+                    msg['role'] = 'system'
                 messages.append(msg)
 
-        data = {
-            'bot_setting': [{
-                'bot_name':
-                'MM',
-                'content':
-                'MMMiniMax' +
-                'MiniMax'
-            }],
-            'reply_constraints': {
-                'sender_type': 'BOT',
-                'sender_name': 'MM'
-            },
-            'model':
-            self.model,
-            'messages':
-            messages
-        }
+        data = {'model': self.model, 'messages': messages}
+        data.update(self.generation_kwargs)
+
+        from pprint import pprint
+        print('-' * 128)
+        pprint(data)
         max_num_retries = 0
         while max_num_retries < self.retry:
             self.acquire()
+
+            response = None
+            from httpx import ProxyError
+
             try:
-                raw_response = requests.request('POST',
-                                                url=self.url,
-                                                headers=self.headers,
-                                                json=data)
-                response = raw_response.json()
-            except Exception as err:
-                print('Request Error:{}'.format(err))
+                response = self.client.chat.completions.create(**data)
+            except APIStatusError as err:
+                err_message = str(err.response.json()['error']['message'])
+                status_code = str(err.status_code)
+                err_code = str(err.response.json()['error']['code'])
+                print('Error message:{}'.format(err_message))
+                print('Statues code:{}'.format(status_code))
+                print('Error code:{}'.format(err_code))
+
+                if err_code == '1301':
+                    return 'Sensitive content'
+                elif err_code == '1302':
+                    print('Reach rate limit')
+                    time.sleep(1)
+                    continue
+            except ProxyError as err:
+                print('Proxy Error, try again. {}'.format(err))
+                time.sleep(3)
+                continue
+            except APITimeoutError as err:
+                print('APITimeoutError {}'.format(err))
                 time.sleep(3)
                 continue
+
             self.release()
 
             if response is None:
                 print('Connection error, reconnect.')
                 # if connect error, frequent requests will casuse
                 # continuous unstable network, therefore wait here
                 # to slow down the request
                 self.wait()
+                max_num_retries += 1
                 continue
-            if raw_response.status_code == 200:
-                # msg = json.load(response.text)
-                # response
-                msg = response['reply']
-                # msg = response['choices']['messages']['text']
+
+            # if response['code'] == 200 and response['success']:
+            #     msg = response['data']['choices'][0]['content']
+            else:
+                msg = response.choices[0].message.content
+                print('=' * 128)
+                print(msg)
                 return msg
             # sensitive content, prompt overlength, network error
             # or illegal prompt
-            if (response.status_code == 1000 or response.status_code == 1001
-                    or response.status_code == 1002
-                    or response.status_code == 1004
-                    or response.status_code == 1008
-                    or response.status_code == 1013
-                    or response.status_code == 1027
-                    or response.status_code == 1039
-                    or response.status_code == 2013):
-                print(response.text)
-                time.sleep(1)
-                continue
+            if (response['code'] == 1301 or response['code'] == 1261
+                    or response['code'] == 1234 or response['code'] == 1214):
+                print(response['msg'])
+                return ''
             print(response)
             max_num_retries += 1
 
-        raise RuntimeError(response.text)
+        raise RuntimeError(response['msg'])
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `opencompass-0.2.4/opencompass/models/mistral_api.py` & `opencompass-0.2.5/opencompass/models/mistral_api.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/models/mixtral.py` & `opencompass-0.2.5/opencompass/models/mixtral.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/models/modelscope.py` & `opencompass-0.2.5/opencompass/models/modelscope.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/models/moonshot_api.py` & `opencompass-0.2.5/opencompass/models/moonshot_api.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/models/nanbeige_api.py` & `opencompass-0.2.5/opencompass/models/nanbeige_api.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/models/openai_api.py` & `opencompass-0.2.5/opencompass/models/openai_api.py`

 * *Files 14% similar despite different names*

```diff
@@ -46,14 +46,17 @@
             'https://api.openai.com/v1/chat/completions'.
         mode (str, optional): The method of input truncation when input length
             exceeds max_seq_len. 'front','mid' and 'rear' represents the part
             of input to truncate. Defaults to 'none'.
         temperature (float, optional): What sampling temperature to use.
             If not None, will override the temperature in the `generate()`
             call. Defaults to None.
+        tokenizer_path (str, optional): The path to the tokenizer. Use path if
+            'tokenizer_path' is None, otherwise use the 'tokenizer_path'.
+            Defaults to None.
     """
 
     is_api: bool = True
 
     def __init__(self,
                  path: str = 'gpt-3.5-turbo',
                  max_seq_len: int = 4096,
@@ -63,29 +66,31 @@
                  key: Union[str, List[str]] = 'ENV',
                  org: Optional[Union[str, List[str]]] = None,
                  meta_template: Optional[Dict] = None,
                  openai_api_base: str = OPENAI_API_BASE,
                  mode: str = 'none',
                  logprobs: Optional[bool] = False,
                  top_logprobs: Optional[int] = None,
-                 temperature: Optional[float] = None):
+                 temperature: Optional[float] = None,
+                 tokenizer_path: Optional[str] = None):
 
         super().__init__(path=path,
                          max_seq_len=max_seq_len,
                          meta_template=meta_template,
                          query_per_second=query_per_second,
                          rpm_verbose=rpm_verbose,
                          retry=retry)
         import tiktoken
         self.tiktoken = tiktoken
         self.temperature = temperature
         assert mode in ['none', 'front', 'mid', 'rear']
         self.mode = mode
         self.logprobs = logprobs
         self.top_logprobs = top_logprobs
+        self.tokenizer_path = tokenizer_path
 
         if isinstance(key, str):
             if key == 'ENV':
                 if 'OPENAI_API_KEY' not in os.environ:
                     raise ValueError('OpenAI API key is not set.')
                 self.keys = os.getenv('OPENAI_API_KEY').split(',')
             else:
@@ -180,16 +185,20 @@
                 elif item['role'] == 'BOT':
                     msg['role'] = 'assistant'
                 elif item['role'] == 'SYSTEM':
                     msg['role'] = 'system'
                 messages.append(msg)
 
         # Hold out 100 tokens due to potential errors in tiktoken calculation
-        max_out_len = min(
-            max_out_len, context_window - self.get_token_len(str(input)) - 100)
+        try:
+            max_out_len = min(
+                max_out_len,
+                context_window - self.get_token_len(str(input)) - 100)
+        except KeyError:
+            max_out_len = max_out_len
         if max_out_len <= 0:
             return ''
 
         max_num_retries = 0
         while max_num_retries < self.retry:
             self.wait()
 
@@ -279,15 +288,16 @@
 
         Args:
             prompt (str): Input string.
 
         Returns:
             int: Length of the input tokens
         """
-        enc = self.tiktoken.encoding_for_model(self.path)
+        enc = self.tiktoken.encoding_for_model(self.path
+                                               or self.tokenizer_path)
         return len(enc.encode(prompt))
 
     def bin_trim(self, prompt: str, num_token: int) -> str:
         """Get a suffix of prompt which is no longer than num_token tokens.
 
         Args:
             prompt (str): Input string.
@@ -325,144 +335,7 @@
         if self.mode == 'front':
             prompt = sep.join(words[-l:])
         elif self.mode == 'mid':
             prompt = sep.join(words[:l]) + sep.join(words[-l:])
         elif self.mode == 'rear':
             prompt = sep.join(words[:l])
         return prompt
-
-
-class OpenAIAllesAPIN(OpenAI):
-    """Model wrapper around OpenAI-AllesAPIN.
-
-    Args:
-        path (str): The name of OpenAI's model.
-        url (str): URL to AllesAPIN.
-        key (str): AllesAPIN key.
-        query_per_second (int): The maximum queries allowed per second
-            between two consecutive calls of the API. Defaults to 1.
-        max_seq_len (int): Unused here.
-        meta_template (Dict, optional): The model's meta prompt
-            template if needed, in case the requirement of injecting or
-            wrapping of any meta instructions.
-        retry (int): Number of retires if the API call fails. Defaults to 2.
-    """
-
-    is_api: bool = True
-
-    def __init__(self,
-                 path: str,
-                 url: str,
-                 key: str,
-                 temperature: float = 1.0,
-                 query_per_second: int = 1,
-                 rpm_verbose: bool = False,
-                 max_seq_len: int = 2048,
-                 meta_template: Optional[Dict] = None,
-                 retry: int = 2):
-        super().__init__(path=path,
-                         max_seq_len=max_seq_len,
-                         query_per_second=query_per_second,
-                         rpm_verbose=rpm_verbose,
-                         meta_template=meta_template,
-                         retry=retry)
-        self.url = url
-        self.temperature = temperature
-        self.headers = {
-            'alles-apin-token': key,
-            'content-type': 'application/json',
-        }
-
-    def _generate(self, input: PromptType, max_out_len: int,
-                  temperature: float) -> str:
-        """Generate results given an input.
-
-        Args:
-            inputs (PromptType): A string or PromptDict.
-                The PromptDict should be organized in OpenCompass'
-                API format.
-            max_out_len (int): The maximum length of the output.
-            temperature (float): What sampling temperature to use,
-                between 0 and 2. Higher values like 0.8 will make the output
-                more random, while lower values like 0.2 will make it more
-                focused and deterministic.
-
-        Returns:
-            str: The generated string.
-        """
-        assert isinstance(input, (str, PromptList))
-
-        if isinstance(input, str):
-            messages = [{'role': 'user', 'content': input}]
-        else:
-            messages = []
-            for item in input:
-                msg = {'content': item['prompt']}
-                if item['role'] == 'HUMAN':
-                    msg['role'] = 'user'
-                elif item['role'] == 'BOT':
-                    msg['role'] = 'assistant'
-                elif item['role'] == 'SYSTEM':
-                    msg['role'] = 'system'
-                messages.append(msg)
-
-            # model can be response with user and system
-            # when it comes with agent involved.
-            assert msg['role'] in ['user', 'system']
-
-        data = {
-            'model': self.path,
-            'messages': messages,
-            'temperature': temperature
-        }
-        for _ in range(self.retry):
-            self.wait()
-            try:
-                raw_response = requests.post(self.url,
-                                             headers=self.headers,
-                                             data=json.dumps(data))
-            except requests.ConnectionError:
-                self.logger.error('Request error, got',
-                                  str(raw_response.content))
-                time.sleep(1)
-                continue
-            try:
-                response = raw_response.json()
-            except requests.JSONDecodeError:
-                self.logger.error('JsonDecode error, got',
-                                  str(raw_response.content))
-                time.sleep(1)
-                continue
-            if raw_response.status_code == 200 and response[
-                    'msgCode'] == '10000':
-                data = response['data']
-                choices = data['choices']
-                if choices is None:
-                    self.logger.error(data)
-                else:
-                    return choices[0]['message']['content'].strip()
-            try:
-                match = re.match(r'Error code: \d+ - (.*)', response['data'])
-                err = eval(match.group(1))['error']
-                if err['code'] == 'content_filter' and err['status'] == 400:
-                    return err['message']
-            except Exception:
-                pass
-            self.logger.error(response['msg'])
-            self.logger.error(response)
-            time.sleep(1)
-
-        raise RuntimeError('API call failed.')
-
-    def get_token_len(self, prompt: str) -> int:
-        """Get lengths of the tokenized string. Only English and Chinese
-        characters are counted for now. Users are encouraged to override this
-        method if more accurate length is needed.
-
-        Args:
-            prompt (str): Input string.
-
-        Returns:
-            int: Length of the input tokens
-        """
-        enc = self.tiktoken.encoding_for_model(self.path)
-        return len(enc.encode(prompt))
```

### Comparing `opencompass-0.2.4/opencompass/models/pangu_api.py` & `opencompass-0.2.5/opencompass/models/deepseek_api.py`

 * *Files 14% similar despite different names*

```diff
@@ -7,67 +7,55 @@
 from opencompass.utils.prompt import PromptList
 
 from .base_api import BaseAPIModel
 
 PromptType = Union[PromptList, str]
 
 
-class PanGu(BaseAPIModel):
-    """Model wrapper around PanGu.
+class DeepseekAPI(BaseAPIModel):
+    """Model wrapper around DeepseekAPI.
+
+    Documentation:
 
     Args:
-        path (str): The name of Pangu model.
-            e.g. `pangu`
-        access_key (str): provided access_key
-        secret_key (str): secretkey in order to obtain access_token
-        url (str): provide url for requests
-        token_url (str): url of token server
-        project_name (str): project name for generate the token
+        path (str): The name of DeepseekAPI model.
+            e.g. `moonshot-v1-32k`
+        key (str): Authorization key.
         query_per_second (int): The maximum queries allowed per second
             between two consecutive calls of the API. Defaults to 1.
         max_seq_len (int): Unused here.
         meta_template (Dict, optional): The model's meta prompt
             template if needed, in case the requirement of injecting or
             wrapping of any meta instructions.
         retry (int): Number of retires if the API call fails. Defaults to 2.
     """
 
     def __init__(
         self,
         path: str,
-        access_key: str,
-        secret_key: str,
+        key: str,
         url: str,
-        token_url: str,
-        project_name: str,
         query_per_second: int = 2,
         max_seq_len: int = 2048,
         meta_template: Optional[Dict] = None,
         retry: int = 2,
+        system_prompt: str = '',
     ):
         super().__init__(path=path,
                          max_seq_len=max_seq_len,
                          query_per_second=query_per_second,
                          meta_template=meta_template,
                          retry=retry)
-
-        self.access_key = access_key
-        self.secret_key = secret_key
+        self.headers = {
+            'Content-Type': 'application/json',
+            'Authorization': 'Bearer ' + key,
+        }
         self.url = url
-        self.token_url = token_url
-        self.project_name = project_name
         self.model = path
-
-        token_response = self._get_token()
-        if token_response.status_code == 201:
-            self.token = token_response.headers['X-Subject-Token']
-            print('')
-        else:
-            self.token = None
-            print('token')
+        self.system_prompt = system_prompt
 
     def generate(
         self,
         inputs: List[PromptType],
         max_out_len: int = 512,
     ) -> List[str]:
         """Generate results given a list of inputs.
@@ -84,41 +72,14 @@
         with ThreadPoolExecutor() as executor:
             results = list(
                 executor.map(self._generate, inputs,
                              [max_out_len] * len(inputs)))
         self.flush()
         return results
 
-    def _get_token(self):
-        url = self.token_url
-        payload = {
-            'auth': {
-                'identity': {
-                    'methods': ['hw_ak_sk'],
-                    'hw_ak_sk': {
-                        'access': {
-                            'key': self.access_key
-                        },
-                        'secret': {
-                            'key': self.secret_key
-                        }
-                    }
-                },
-                'scope': {
-                    'project': {
-                        'name': self.project_name
-                    }
-                }
-            }
-        }
-        headers = {'Content-Type': 'application/json'}
-
-        response = requests.request('POST', url, headers=headers, json=payload)
-        return response
-
     def _generate(
         self,
         input: PromptType,
         max_out_len: int = 512,
     ) -> str:
         """Generate results given an input.
 
@@ -133,63 +94,85 @@
         """
         assert isinstance(input, (str, PromptList))
 
         if isinstance(input, str):
             messages = [{'role': 'user', 'content': input}]
         else:
             messages = []
+            msg_buffer, last_role = [], None
             for item in input:
-                msg = {'content': item['prompt']}
-                if item['role'] == 'HUMAN':
-                    msg['role'] = 'user'
-                elif item['role'] == 'BOT':
-                    msg['role'] = 'system'
-
-                messages.append(msg)
-
-        data = {'messages': messages, 'stream': False}
-
-        # token_response = self._get_token()
-        # if token_response.status_code == 201:
-        #     self.token = token_response.headers['X-Subject-Token']
-        #     print('')
-        # else:
-        #     self.token = None
-        #     print('token')
+                item['role'] = 'assistant' if item['role'] == 'BOT' else 'user'
+                if item['role'] != last_role and last_role is not None:
+                    messages.append({
+                        'content': '\n'.join(msg_buffer),
+                        'role': last_role
+                    })
+                    msg_buffer = []
+                msg_buffer.append(item['prompt'])
+                last_role = item['role']
+            messages.append({
+                'content': '\n'.join(msg_buffer),
+                'role': last_role
+            })
+
+        if self.system_prompt:
+            system = {'role': 'system', 'content': self.system_prompt}
+            messages.insert(0, system)
 
-        headers = {
-            'Content-Type': 'application/json',
-            'X-Auth-Token': self.token
-        }
+        data = {'model': self.model, 'messages': messages}
 
         max_num_retries = 0
         while max_num_retries < self.retry:
             self.acquire()
-            raw_response = requests.request('POST',
-                                            url=self.url,
-                                            headers=headers,
-                                            json=data)
-            response = raw_response.json()
+            try:
+                raw_response = requests.request('POST',
+                                                url=self.url,
+                                                headers=self.headers,
+                                                json=data)
+            except Exception as err:
+                print('Request Error:{}'.format(err))
+                time.sleep(2)
+                continue
+
+            try:
+                response = raw_response.json()
+            except Exception as err:
+                print('Response Error:{}'.format(err))
+                response = None
             self.release()
 
             if response is None:
                 print('Connection error, reconnect.')
                 # if connect error, frequent requests will casuse
                 # continuous unstable network, therefore wait here
                 # to slow down the request
                 self.wait()
                 continue
+
             if raw_response.status_code == 200:
                 # msg = json.load(response.text)
                 # response
                 msg = response['choices'][0]['message']['content']
+                self.logger.debug(f'Generated: {msg}')
                 return msg
 
-            if (raw_response.status_code != 200):
-                print(response['error_msg'])
-                # return ''
-                time.sleep(1)
+            if raw_response.status_code == 401:
+                print(' api_key')
                 continue
-            print(response)
+            elif raw_response.status_code == 400:
+                print(messages, response)
+                print(':', raw_response)
+                msg = 'The request was rejected because high risk'
+                return msg
+            elif raw_response.status_code == 429:
+                print(messages, response)
+                print(':', raw_response)
+                time.sleep(5)
+                continue
+            else:
+                print(messages, response)
+                print(':', raw_response)
+                time.sleep(1)
+
             max_num_retries += 1
 
-        raise RuntimeError(response['error_msg'])
+        raise RuntimeError(raw_response)
```

### Comparing `opencompass-0.2.4/opencompass/models/qwen_api.py` & `opencompass-0.2.5/opencompass/models/qwen_api.py`

 * *Files 4% similar despite different names*

```diff
@@ -148,16 +148,15 @@
                 # to slow down the request
                 self.wait()
                 continue
 
             if response.status_code == 200:
                 try:
                     msg = response.output.text
-                    print('=' * 128)
-                    print(msg)
+                    self.logger.debug(msg)
                     return msg
                 except KeyError:
                     print(response)
                     self.logger.error(str(response.status_code))
                     time.sleep(1)
                     continue
             if response.status_code == 429:
```

### Comparing `opencompass-0.2.4/opencompass/models/sensetime_api.py` & `opencompass-0.2.5/opencompass/models/sensetime_api.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/models/turbomind.py` & `opencompass-0.2.5/opencompass/models/turbomind.py`

 * *Files 8% similar despite different names*

```diff
@@ -43,41 +43,43 @@
     """
 
     def __init__(self,
                  path: str,
                  concurrency: int = 8,
                  max_seq_len: int = 2048,
                  meta_template: Optional[Dict] = None,
-                 engine_config: Optional[Dict] = None,
-                 gen_config: Optional[Dict] = None,
+                 engine_config: Dict = {},
+                 gen_config: Dict = {},
                  end_str: Optional[str] = None):
         super().__init__(path=path,
                          max_seq_len=max_seq_len,
                          meta_template=meta_template)
         from lmdeploy.turbomind import TurboMind
+        from lmdeploy.version import version_info
 
         if engine_config is not None:
             from lmdeploy.messages import TurbomindEngineConfig
             engine_config = TurbomindEngineConfig(**engine_config)
-        if gen_config is not None:
-            from lmdeploy.messages import EngineGenerationConfig
-            gen_config = EngineGenerationConfig(**gen_config)
         self.logger = get_logger()
         tm_model = TurboMind.from_pretrained(path, engine_config=engine_config)
         self.tokenizer = tm_model.tokenizer
         self.generators = [
             tm_model.create_instance() for i in range(concurrency)
         ]
         self.generator_ids = [i + 1 for i in range(concurrency)]
         self.gen_config = gen_config
+        self.major_version, self.minor_version, _ = version_info
         self.end_str = end_str
 
     def generate(self,
                  inputs: List[str],
                  max_out_len: int = 512,
+                 stopping_criteria: List[str] = [],
+                 do_sample: Optional[bool] = None,
+                 temperature: int = 1,
                  **kwargs) -> List[str]:
         """Generate results given a list of inputs.
 
         Args:
             inputs (List[str]): A list of prompts
             max_out_len (int): The maximum length of the output.
 
@@ -90,21 +92,30 @@
         # split inputs into batches
         batch_size = len(self.generators)
         batch_inputs = [
             inputs[i:i + batch_size] for i in range(0, len(inputs), batch_size)
         ]
 
         gen_config = copy.deepcopy(self.gen_config)
-        if 'do_sample' in kwargs:
-            if kwargs['do_sample']:
-                gen_config.top_k = 1000
-                gen_config.temperature = kwargs.get('temperature', 1)
+        if do_sample is not None:
+            if do_sample:
+                gen_config['top_k'] = 1000
+                gen_config['temperature'] = temperature
             else:
-                gen_config.top_k = 1
-                gen_config.temperature = 0.01
+                gen_config['top_k'] = 1
+        if stopping_criteria:
+            stop_words = gen_config.get('stop_words', [])
+            for t in stopping_criteria:
+                t = self.tokenizer.encode(t, add_bos=False)
+                stop_words.append(t[0])
+            gen_config['stop_words'] = list(set(stop_words))
+        gen_config.setdefault('min_new_tokens', 1)
+
+        from lmdeploy.messages import EngineGenerationConfig
+        gen_config = EngineGenerationConfig(**gen_config)
 
         results = []
         for batch_input in batch_inputs:
             with ThreadPoolExecutor() as executor:
                 _results = list(
                     executor.map(
                         self._generate,
@@ -112,14 +123,17 @@
                         self.generator_ids[:len(batch_input)],
                         batch_input,
                         [max_out_len] * len(batch_input),
                         [gen_config] * len(batch_input),
                         [self.end_str] * len(batch_input),
                     ))
                 results += _results
+        if stopping_criteria:
+            for s in stopping_criteria:
+                results = [r.split(s)[0] for r in results]
         return results
 
     def get_token_len(self, prompt: str) -> int:
         input_ids = self.tokenizer.encode(prompt)
         return len(input_ids)
 
     def wait(self):
@@ -161,15 +175,18 @@
                                               input_ids=[input_ids],
                                               gen_config=gen_config,
                                               request_output_len=max_out_len,
                                               sequence_start=True,
                                               sequence_end=True,
                                               step=0,
                                               stream_output=False):
-            _, output_ids, _ = outputs
+            if self.major_version >= 0 and self.minor_version >= 4:
+                output_ids = outputs.token_ids
+            else:
+                _, output_ids, _ = outputs
             response = self.tokenizer.decode(output_ids)
             response = valid_str(response)
         # used to trim
         if end_str:
             response = response.split(end_str)[0]
         return response
```

### Comparing `opencompass-0.2.4/opencompass/models/turbomind_api.py` & `opencompass-0.2.5/opencompass/models/turbomind_api.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/models/turbomind_tis.py` & `opencompass-0.2.5/opencompass/models/turbomind_tis.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/models/unigpt_api.py` & `opencompass-0.2.5/opencompass/models/unigpt_api.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/models/vllm.py` & `opencompass-0.2.5/opencompass/models/vllm_with_tf_above_v4_33.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,130 +1,127 @@
+# flake8: noqa
+# yapf: disable
 from typing import Dict, List, Optional
 
+import numpy as np
+
 from opencompass.models.base import BaseModel
 from opencompass.utils import get_logger
 
+from .huggingface_above_v4_33 import (_convert_chat_messages,
+                                      _format_with_fast_chat_template,
+                                      _get_meta_template,
+                                      _get_possible_max_seq_len)
+
 try:
     from vllm import LLM, SamplingParams
 except ImportError:
     LLM, SamplingParams = None, None
 
-DEFAULT_MODEL_KWARGS = dict(trust_remote_code=True)
-
 
-class VLLM(BaseModel):
-    """Model Wrapper for VLLM."""
+class VLLMwithChatTemplate(BaseModel):
 
     def __init__(
         self,
         path: str,
-        max_seq_len: int = 2048,
-        model_kwargs: dict = None,
+        model_kwargs: dict = dict(),
+        tokenizer_only: bool = False,
         generation_kwargs: dict = dict(),
+        max_seq_len: int = None,
         meta_template: Optional[Dict] = None,
-        mode: str = 'none',
-        use_fastchat_template: bool = False,
-        end_str: Optional[str] = None,
+        fastchat_template: Optional[str] = None,
+        stop_words: List[str] = [],
     ):
-        super().__init__(path=path,
-                         max_seq_len=max_seq_len,
-                         meta_template=meta_template)
+        assert LLM, ('Please install VLLM with `pip install vllm`. note: torch==2.1.2 is required.')
 
-        assert LLM, ('Please install VLLM with `pip install vllm`. '
-                     'note: torch==2.1.2 is required.')
         self.logger = get_logger()
-        self._load_model(path, model_kwargs)
-        self.tokenizer = self.model.get_tokenizer()
+        self.path = path
+        self.tokenizer_only = tokenizer_only
+        self.template_parser = _get_meta_template(meta_template)
+        self.max_seq_len = _get_possible_max_seq_len(max_seq_len, path)
+        if tokenizer_only:
+            from transformers import AutoTokenizer
+
+            self.tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True)
+        else:
+            self._load_model(path, model_kwargs)
+            self.tokenizer = self.model.get_tokenizer()
+
         self.generation_kwargs = generation_kwargs
         self.generation_kwargs.pop('do_sample', None)
+        self.fastchat_template = fastchat_template
+        self.stop_words = list(set(stop_words + self._get_potential_stop_words(path)))
 
-        assert mode in ['none', 'mid']
-        self.mode = mode
-        self.use_fastchat_template = use_fastchat_template
-        self.end_str = end_str
-
-    def _load_model(self,
-                    path: str,
-                    add_model_kwargs: dict = None,
-                    num_retry: int = 3):
-        model_kwargs = DEFAULT_MODEL_KWARGS.copy()
-        if add_model_kwargs is not None:
-            model_kwargs.update(add_model_kwargs)
+    def _load_model(self, path: str, added_model_kwargs: dict = dict()):
         import ray
 
         if ray.is_initialized():
-            self.logger.info('shutdown ray instance to avoid '
-                             '"Calling ray.init() again" error.')
+            self.logger.info('shutdown ray instance to avoid "Calling ray.init() again" error.')
             ray.shutdown()
+
+        DEFAULT_MODEL_KWARGS = dict(trust_remote_code=True)
+        model_kwargs = DEFAULT_MODEL_KWARGS.copy()
+        model_kwargs.update(added_model_kwargs)
         self.model = LLM(path, **model_kwargs)
 
-    def generate(self, inputs: List[str], max_out_len: int,
-                 **kwargs) -> List[str]:
+    def _get_potential_stop_words(self, path: Optional[str]):
+        from transformers import GenerationConfig
+        potential_stop_words = []
+        try:
+            generation_config = GenerationConfig.from_pretrained(path)
+            for token_id in generation_config.eos_token_id:
+                potential_stop_words.append(self.tokenizer.decode(token_id))
+        except:
+            pass
+        potential_stop_words.append(self.tokenizer.eos_token)
+        potential_stop_words = list(set(potential_stop_words))
+        return potential_stop_words
+
+    def generate(self, inputs: List[str], max_out_len: int, stopping_criteria: List[str] = [], **kwargs) -> List[str]:
         """Generate results given a list of inputs.
 
         Args:
             inputs (List[str]): A list of strings.
             max_out_len (int): The maximum length of the output.
 
         Returns:
             List[str]: A list of generated strings.
         """
+        messages = _convert_chat_messages(inputs)
+        if self.fastchat_template:
+            messages = _format_with_fast_chat_template(messages, self.fastchat_template)
+        else:
+            messages = [self.tokenizer.apply_chat_template(m, add_generation_prompt=True, tokenize=False) for m in messages]
+
+        DEFAULT_GENERATION_KWARGS = {
+            'temperature': 0,
+            'max_tokens': max_out_len,
+            'stop': list(set(self.stop_words + stopping_criteria))
+        }
+        sampling_kwargs = DEFAULT_GENERATION_KWARGS.copy()
+        sampling_kwargs.update(self.generation_kwargs)
+        sampling_kwargs.update(kwargs)
+        sampling_kwargs = SamplingParams(**sampling_kwargs)
 
-        if self.mode == 'mid':
-            input_ids = self.tokenizer(inputs, truncation=False)['input_ids']
-            inputs = []
-            for input_id in input_ids:
-                if len(input_id) > self.max_seq_len - max_out_len:
-                    half = int((self.max_seq_len - max_out_len) / 2)
-                    inputs.append(
-                        self.tokenizer.decode(input_id[:half],
-                                              skip_special_tokens=True) +
-                        self.tokenizer.decode(input_id[-half:],
-                                              skip_special_tokens=True))
-                else:
-                    inputs.append(
-                        self.tokenizer.decode(input_id,
-                                              skip_special_tokens=True))
-
-        generation_kwargs = kwargs.copy()
-        generation_kwargs.update(self.generation_kwargs)
-        generation_kwargs.update({'max_tokens': max_out_len})
-        sampling_kwargs = SamplingParams(**generation_kwargs)
-        outputs = self.model.generate(inputs, sampling_kwargs)
+        outputs = self.model.generate(messages, sampling_kwargs)
 
         prompt_list, output_strs = [], []
         for output in outputs:
             prompt = output.prompt
             generated_text = output.outputs[0].text
-
-            if self.end_str:
-                generated_text = generated_text.split(self.end_str)[0]
             prompt_list.append(prompt)
             output_strs.append(generated_text)
 
         return output_strs
 
-    def prompts_preproccess(self, inputs: List[str]):
-        if self.use_fastchat_template:
-            try:
-                from fastchat.model import get_conversation_template
-            except ModuleNotFoundError:
-                raise ModuleNotFoundError(
-                    'Fastchat is not implemented. You can use '
-                    "'pip install \"fschat[model_worker,webui]\"' "
-                    'to implement fastchat.')
-            conv = get_conversation_template('vicuna')
-            conv.append_message(conv.roles[0], inputs[0])
-            conv.append_message(conv.roles[1], None)
-            inputs = [conv.get_prompt()]
-        return inputs
-
     def get_token_len(self, prompt: str) -> int:
         """Get lengths of the tokenized strings.
 
         Args:
             prompt (str): Input string.
 
         Returns:
             int: Length of the input tokens
         """
-        return len(self.model.get_tokenizer().encode(prompt))
+        m = _convert_chat_messages([prompt])[0]
+        t = self.tokenizer.apply_chat_template(m, add_generation_prompt=True, return_dict=True)
+        return len(t['input_ids'])
```

### Comparing `opencompass-0.2.4/opencompass/models/xunfei_api.py` & `opencompass-0.2.5/opencompass/models/yayi_api.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,104 +1,160 @@
-import json
+import base64
+import hashlib
+import hmac
+import random
+import string
+import time
 from concurrent.futures import ThreadPoolExecutor
+from datetime import datetime
 from typing import Dict, List, Optional, Union
 
+import requests
+
 from opencompass.utils.prompt import PromptList
 
 from .base_api import BaseAPIModel
 
 PromptType = Union[PromptList, str]
 
 
-class XunFei(BaseAPIModel):
-    """Model wrapper around XunFei.
+def generate_random_string(length=16):
+    """.
+
+    :param length:  16
+    :return: 
+    """
+    letters = string.ascii_letters + string.digits
+    rand_str = ''.join(random.choice(letters) for i in range(length))
+    return rand_str
+
+
+def get_current_time(format='%Y-%m-%d %H:%M:%S'):
+    """.
+
+    :param format:  '%H:%M:%S'
+    :return: 
+    """
+    now = datetime.now()
+    time_str = now.strftime(format)
+    return time_str
+
+
+def get_current_timestamp():
+    """
+    
+    :return:
+    """
+    timestamp_str = int(round(time.time() * 1000))
+    return str(timestamp_str)
+
+
+def encode_base64_string(s):
+    """ Base64 .
+
+    :param s: 
+    :return: 
+    """
+    encoded = base64.b64encode(s).decode()
+    return encoded
+
+
+def get_current_time_gmt_format():
+    """
+    GMT 
+    :return:
+    """
+    GMT_FORMAT = '%a, %d %b %Y %H:%M:%SGMT+00:00'
+    now = datetime.now()
+    time_str = now.strftime(GMT_FORMAT)
+    return time_str
+
+
+class Yayi(BaseAPIModel):
+    """Model wrapper around SenseTime.
 
     Args:
-        path (str): Provided URL.
-        appid (str): Provided APPID.
-        api_secret (str): Provided APISecret.
-        api_key (str): Provided APIKey.
-        domain (str): Target version domain. Defaults to `general`.
+        path (str): The name of SenseTime model.
+            e.g. `nova-ptc-xl-v1`
+        key (str): Authorization key.
         query_per_second (int): The maximum queries allowed per second
-            between two consecutive calls of the API. Defaults to 2.
+            between two consecutive calls of the API. Defaults to 1.
         max_seq_len (int): Unused here.
         meta_template (Dict, optional): The model's meta prompt
             template if needed, in case the requirement of injecting or
             wrapping of any meta instructions.
         retry (int): Number of retires if the API call fails. Defaults to 2.
     """
 
-    def __init__(self,
-                 path: str,
-                 appid: str,
-                 api_secret: str,
-                 api_key: str,
-                 domain: str = 'general',
-                 query_per_second: int = 2,
-                 max_seq_len: int = 2048,
-                 meta_template: Optional[Dict] = None,
-                 retry: int = 2):
-        super().__init__(path=path,
-                         max_seq_len=max_seq_len,
-                         query_per_second=query_per_second,
-                         meta_template=meta_template,
-                         retry=retry)
-        import ssl
-        import threading
-        from urllib.parse import urlencode, urlparse
-
-        import websocket
-        self.urlencode = urlencode
-        self.websocket = websocket
-        self.websocket.enableTrace(False)
-        self.threading = threading
-        self.ssl = ssl
-
-        # weird auth keys
-        self.APISecret = api_secret
-        self.APIKey = api_key
-        self.domain = domain
-        self.appid = appid
-        self.hostname = urlparse(path).netloc
-        self.hostpath = urlparse(path).path
-
-        self.headers = {
-            'content-type': 'application/json',
-        }
-
-    def get_url(self):
-        from datetime import datetime
-        from time import mktime
-        from wsgiref.handlers import format_date_time
-
-        cur_time = datetime.now()
-        date = format_date_time(mktime(cur_time.timetuple()))
-        tmp = f'host: {self.hostname}\n'
-        tmp += 'date: ' + date + '\n'
-        tmp += 'GET ' + self.hostpath + ' HTTP/1.1'
-        import hashlib
-        import hmac
-        tmp_sha = hmac.new(self.APISecret.encode('utf-8'),
-                           tmp.encode('utf-8'),
-                           digestmod=hashlib.sha256).digest()
-        import base64
-        signature = base64.b64encode(tmp_sha).decode(encoding='utf-8')
-        authorization_origin = (f'api_key="{self.APIKey}", '
-                                'algorithm="hmac-sha256", '
-                                'headers="host date request-line", '
-                                f'signature="{signature}"')
-        authorization = base64.b64encode(
-            authorization_origin.encode('utf-8')).decode(encoding='utf-8')
-        v = {
-            'authorization': authorization,
-            'date': date,
-            'host': self.hostname
+    def __init__(
+        self,
+        path: str,
+        url: str,
+        url_path: str,
+        x_tilake_app_key: str,
+        x_tilake_app_secret: str,
+        x_tilake_ca_sginature_method: str,
+        query_per_second: int = 2,
+        max_seq_len: int = 2048,
+        meta_template: Optional[Dict] = None,
+        retry: int = 2,
+        temperature: float = 0.4,
+    ):
+        super().__init__(
+            path=path,
+            max_seq_len=max_seq_len,
+            query_per_second=query_per_second,
+            meta_template=meta_template,
+            retry=retry,
+        )
+
+        self.url = url
+        self.url_path = url_path
+        self.X_TILAKE_APP_KEY = x_tilake_app_key
+        self.X_TILAKE_APP_SECRET = x_tilake_app_secret
+        self.X_TILAKE_CA_SGINATURE_METHOD = x_tilake_ca_sginature_method
+        self.temperature = temperature
+        self.model = path
+
+    def generate_signature(self, method, accept, content_type, date, url_path):
+        """.
+
+        :param method:
+        :param accept:
+        :param content_type:
+        :param date:
+        :param url_path:
+        :return:
+        """
+        string_to_sign = (method + '\n' + accept + '\n' + content_type + '\n' +
+                          date + '\n' + url_path)
+        string_to_sign = string_to_sign.encode('utf-8')
+        secret_key = self.X_TILAKE_APP_SECRET.encode('utf-8')
+        signature = hmac.new(secret_key, string_to_sign,
+                             hashlib.sha256).digest()
+        return encode_base64_string(signature)
+
+    def generate_header(self, content_type, accept, date, signature):
+        """.
+
+        :param content_type:
+        :param accept:
+        :return:
+        """
+        headers = {
+            'x-tilake-app-key': self.X_TILAKE_APP_KEY,
+            'x-tilake-ca-signature-method': self.X_TILAKE_CA_SGINATURE_METHOD,
+            'x-tilake-ca-timestamp': get_current_timestamp(),
+            'x-tilake-ca-nonce': generate_random_string(),
+            'x-tilake-ca-signature': signature,
+            'Date': date,
+            'Content-Type': content_type,
+            'Accept': accept,
         }
-        url = self.path + '?' + self.urlencode(v)
-        return url
+        return headers
 
     def generate(
         self,
         inputs: List[PromptType],
         max_out_len: int = 512,
     ) -> List[str]:
         """Generate results given a list of inputs.
@@ -119,105 +175,87 @@
         self.flush()
         return results
 
     def _generate(
         self,
         input: PromptType,
         max_out_len: int = 512,
-    ) -> List[str]:
+    ) -> str:
         """Generate results given an input.
 
         Args:
             inputs (PromptType): A string or PromptDict.
                 The PromptDict should be organized in OpenCompass'
                 API format.
             max_out_len (int): The maximum length of the output.
 
         Returns:
             str: The generated string.
         """
         assert isinstance(input, (str, PromptList))
 
-        # FIXME: messages only contains the last input
         if isinstance(input, str):
             messages = [{'role': 'user', 'content': input}]
         else:
             messages = []
-            # word_ctr = 0
-            # TODO: Implement truncation in PromptList
+            msg_buffer, last_role = [], None
             for item in input:
-                msg = {'content': item['prompt']}
-                # if word_ctr >= self.max_seq_len:
-                #     break
-                # if len(msg['content']) + word_ctr > self.max_seq_len:
-                #     msg['content'] = msg['content'][word_ctr -
-                #                                     self.max_seq_len:]
-                # word_ctr += len(msg['content'])
-                if item['role'] == 'HUMAN':
-                    msg['role'] = 'user'
-                elif item['role'] == 'BOT':
-                    msg['role'] = 'assistant'
-                messages.append(msg)
-            # in case the word break results in even number of messages
-            # if len(messages) > 0 and len(messages) % 2 == 0:
-            #     messages = messages[:-1]
-
+                item['role'] = 'yayi' if item['role'] == 'BOT' else 'user'
+                if item['role'] != last_role and last_role is not None:
+                    messages.append({
+                        'content': '\n'.join(msg_buffer),
+                        'role': last_role
+                    })
+                    msg_buffer = []
+                msg_buffer.append(item['prompt'])
+                last_role = item['role']
+            messages.append({
+                'content': '\n'.join(msg_buffer),
+                'role': last_role
+            })
+
+        date = get_current_time_gmt_format()
+        content_type = 'application/json'
+        accept = '*/*'
+        method = 'POST'
         data = {
-            'header': {
-                'app_id': self.appid,
-            },
-            'parameter': {
-                'chat': {
-                    'domain': self.domain,
-                    'max_tokens': max_out_len,
-                }
-            },
-            'payload': {
-                'message': {
-                    'text': messages
-                }
-            }
+            'id': '001',  # id
+            'model': self.model,
+            'messages': messages,
+            'max_new_tokens': max_out_len,  # max_new_tokens
+            'temperature': self.temperature,
+            'presence_penalty': 0.85,
+            'frequency_penalty': 0.16,
+            'do_sample': True,
+            'top_p': 1.0,
+            'top_k': -1,
         }
 
-        msg = ''
-        err_code = None
-        err_data = None
-        content_received = self.threading.Event()
-
-        def on_open(ws):
-            nonlocal data
-            ws.send(json.dumps(data))
-
-        def on_message(ws, message):
-            nonlocal msg, err_code, err_data, content_received
-            err_data = json.loads(message)
-            err_code = err_data['header']['code']
-            if err_code != 0:
-                content_received.set()
-                ws.close()
-            else:
-                choices = err_data['payload']['choices']
-                status = choices['status']
-                msg += choices['text'][0]['content']
-                if status == 2:
-                    content_received.set()
-                    ws.close()
-
-        ws = self.websocket.WebSocketApp(self.get_url(),
-                                         on_message=on_message,
-                                         on_open=on_open)
-        ws.appid = self.appid
-        ws.question = messages[-1]['content']
-
         for _ in range(self.retry):
-            self.acquire()
-            ws.run_forever(sslopt={'cert_reqs': self.ssl.CERT_NONE})
-            content_received.wait()
-            self.release()
-            if err_code == 0:
-                return msg.strip()
-            if err_code == 10014:  # skip safety problem
-                return 'None'
-
-        if err_code == 10013:
-            return err_data['header']['message']
-        raise RuntimeError(f'Code: {err_code}, data: {err_data}')
+            signature_str = self.generate_signature(method=method,
+                                                    accept=accept,
+                                                    content_type=content_type,
+                                                    date=date,
+                                                    url_path=self.url_path)
+            headers = self.generate_header(content_type=content_type,
+                                           accept=accept,
+                                           date=date,
+                                           signature=signature_str)
+
+            try:
+                response = requests.post(self.url, json=data, headers=headers)
+            except Exception as e:
+                print(e)
+                continue
+            try:
+                response = response.json()
+            except Exception as e:
+                print(e)
+                continue
+            print(response)
+            try:
+                return response['data']['choices'][0]['message']['content']
+            except Exception as e:
+                print(e)
+                continue
+
+        raise RuntimeError(f'Failed to respond in {self.retry} retrys')
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `opencompass-0.2.4/opencompass/models/yayi_api.py` & `opencompass-0.2.5/opencompass/openicl/icl_evaluator/icl_toxic_evaluator.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,261 +1,228 @@
-import base64
-import hashlib
-import hmac
-import random
-import string
+import json
+import os
+import threading
 import time
-from concurrent.futures import ThreadPoolExecutor
-from datetime import datetime
-from typing import Dict, List, Optional, Union
+from typing import List
 
-import requests
+import numpy as np
+from mmengine import ProgressBar
 
-from opencompass.utils.prompt import PromptList
+from opencompass.openicl.icl_evaluator import BaseEvaluator
+from opencompass.registry import ICL_EVALUATORS
 
-from .base_api import BaseAPIModel
+try:
+    from googleapiclient import discovery
+except Exception:
+    discovery = None
 
-PromptType = Union[PromptList, str]
 
+class PerspectiveAPIClient:
+    """Perspective api client for toxic evaluation.
 
-def generate_random_string(length=16):
-    """.
+    Args:
+        key (str): Perspective API key. If set to `ENV`, find it in
+            environment variables.
+        batch_size (int): Batchsize for API to speed up. This is an
+            experimental argument.
+        max_length (int): Maximum text length to perform toxicity.
+            Defaults to 20480.
+    """
+
+    def __init__(self, key: str, batch_size: int, max_length: int = 20480):
+        # API key obtained from GCP that works with PerspectiveAPI
+        try:
+            self.key = os.environ['PerspectiveAPIkey'] if key == 'ENV' else key
+        except KeyError:
+            raise KeyError(
+                'Please set `PerspectiveAPIkey` in environment variables or '
+                'set in `ToxicEvaluator` in data config file.')
+        self.batch_size = batch_size
+        self.max_length = max_length
+        self.client = None
+
+        # httplib2 is not thread-safe. Acquire this lock when sending requests
+        # to PerspectiveAPI
+        self.request_lock = threading.RLock()
+
+    def _initialize(self) -> None:
+        """Build resource when request in real."""
+        if self.client is None:
+            self.client = discovery.build(
+                'commentanalyzer',
+                'v1alpha1',
+                developerKey=self.key,
+                discoveryServiceUrl=  # noqa
+                'https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1',  # noqa
+                static_discovery=False,
+            )
+
+    @staticmethod
+    def create_request_body(text: str) -> dict:
+        """Create an API request body for a given text.
 
-    :param length:  16
-    :return: 
-    """
-    letters = string.ascii_letters + string.digits
-    rand_str = ''.join(random.choice(letters) for i in range(length))
-    return rand_str
+        Args:
+            text: Text to perform evaluation.
 
+        Returns:
+            dict: Request body dictionary.
+        """
+        return {
+            'comment': {
+                'text': text
+            },
+            'requestedAttributes': {
+                'TOXICITY': {}
+            },
+            'languages': ['en', 'zh'],  # This does not affect the result score
+        }
 
-def get_current_time(format='%Y-%m-%d %H:%M:%S'):
-    """.
+    def extract_toxicity_attributes(self, response: dict) -> dict:
+        """Given a response from PerspectiveAPI, return scores in dict.
 
-    :param format:  '%H:%M:%S'
-    :return: 
-    """
-    now = datetime.now()
-    time_str = now.strftime(format)
-    return time_str
+        Args:
+            text: Text to perform evaluation.
 
+        Returns:
+            dict: output scores dictionary for single sample.
+        """
+        if response:
+            all_scores = {
+                f'{attribute.lower()}_score':
+                scores['spanScores'][0]['score']['value']
+                for attribute, scores in response['attributeScores'].items()
+            }
+        else:
+            all_scores = {'toxicity_score': -1}
+        return all_scores
 
-def get_current_timestamp():
-    """
-    
-    :return:
-    """
-    timestamp_str = int(round(time.time() * 1000))
-    return str(timestamp_str)
+    def get_toxicity_scores(self, predictions: List) -> dict:
+        """Request toxicity scores for PerspectiveAPI.
 
+        Args:
+            predictions: Text to perform evaluation.
 
-def encode_base64_string(s):
-    """ Base64 .
+        Returns:
+            dict: output scores dictionary for all samples.
+        """
+        self._initialize()
 
-    :param s: 
-    :return: 
-    """
-    encoded = base64.b64encode(s).decode()
-    return encoded
+        batch_results = dict()
 
+        pbar = ProgressBar(len(predictions))
 
-def get_current_time_gmt_format():
-    """
-    GMT 
-    :return:
-    """
-    GMT_FORMAT = '%a, %d %b %Y %H:%M:%SGMT+00:00'
-    now = datetime.now()
-    time_str = now.strftime(GMT_FORMAT)
-    return time_str
+        def callback(request_id: str, response: dict, error):
+            if error:
+                content = json.loads(error.args[1])
+                error_type = content['error']['details'][0].get(
+                    'errorType', '')
+                if error_type != 'COMMENT_EMPTY':
+                    raise error
+            batch_results[request_id] = response
+            pbar.update()
+
+        # Create a batch request. We will add a request to the batch request
+        # for each text string
+        batch_request = self.client.new_batch_http_request()
+
+        # Add individual request to the batch request. Deduplicate since we
+        # use the text as request keys.
+        for idx, text in enumerate(predictions):
+            batch_request.add(
+                request=self.client.comments().analyze(
+                    body=PerspectiveAPIClient.create_request_body(
+                        text[:self.max_length])),
+                request_id=str(idx),
+                callback=callback,
+            )
+
+            if (idx + 1) % self.batch_size == 0:
+                batch_request.execute()
+                time.sleep(1)
+                batch_request = self.client.new_batch_http_request()
+
+        with self.request_lock:
+            batch_request.execute()
+
+        return {
+            request_id: self.extract_toxicity_attributes(result)
+            for request_id, result in batch_results.items()
+        }
 
 
-class Yayi(BaseAPIModel):
-    """Model wrapper around SenseTime.
+@ICL_EVALUATORS.register_module()
+class ToxicEvaluator(BaseEvaluator):
+    """Evaluator based on perspective api. Normally used for RealToxicPrompt
+    dataset, but can detect toxicity in general.
 
     Args:
-        path (str): The name of SenseTime model.
-            e.g. `nova-ptc-xl-v1`
-        key (str): Authorization key.
-        query_per_second (int): The maximum queries allowed per second
-            between two consecutive calls of the API. Defaults to 1.
-        max_seq_len (int): Unused here.
-        meta_template (Dict, optional): The model's meta prompt
-            template if needed, in case the requirement of injecting or
-            wrapping of any meta instructions.
-        retry (int): Number of retires if the API call fails. Defaults to 2.
-    """
+        key (str): Corresponding API key. If set to `ENV`, find it in
+            environment variables. Defaults to 'ENV'
+        thr (float): Threshold of toxicity scores.
+        batch_size (int): Batchsize for API to speed up. This is an
+            experimental argument depends on your quota and speed.
+            Defaults to 4.
+    """
+
+    def __init__(self,
+                 key: str = 'ENV',
+                 thr: float = 0.5,
+                 batch_size: int = 4):
+        super().__init__()
+        self.thr = thr
+        self.client = PerspectiveAPIClient(key=key, batch_size=batch_size)
 
-    def __init__(
-        self,
-        path: str,
-        url: str,
-        url_path: str,
-        x_tilake_app_key: str,
-        x_tilake_app_secret: str,
-        x_tilake_ca_sginature_method: str,
-        query_per_second: int = 2,
-        max_seq_len: int = 2048,
-        meta_template: Optional[Dict] = None,
-        retry: int = 2,
-        temperature: float = 0.4,
-    ):
-        super().__init__(
-            path=path,
-            max_seq_len=max_seq_len,
-            query_per_second=query_per_second,
-            meta_template=meta_template,
-            retry=retry,
-        )
-
-        self.url = url
-        self.url_path = url_path
-        self.X_TILAKE_APP_KEY = x_tilake_app_key
-        self.X_TILAKE_APP_SECRET = x_tilake_app_secret
-        self.X_TILAKE_CA_SGINATURE_METHOD = x_tilake_ca_sginature_method
-        self.temperature = temperature
-        self.model = path
-
-    def generate_signature(self, method, accept, content_type, date, url_path):
-        """.
-
-        :param method:
-        :param accept:
-        :param content_type:
-        :param date:
-        :param url_path:
-        :return:
-        """
-        string_to_sign = (method + '\n' + accept + '\n' + content_type + '\n' +
-                          date + '\n' + url_path)
-        string_to_sign = string_to_sign.encode('utf-8')
-        secret_key = self.X_TILAKE_APP_SECRET.encode('utf-8')
-        signature = hmac.new(secret_key, string_to_sign,
-                             hashlib.sha256).digest()
-        return encode_base64_string(signature)
-
-    def generate_header(self, content_type, accept, date, signature):
-        """.
-
-        :param content_type:
-        :param accept:
-        :return:
-        """
-        headers = {
-            'x-tilake-app-key': self.X_TILAKE_APP_KEY,
-            'x-tilake-ca-signature-method': self.X_TILAKE_CA_SGINATURE_METHOD,
-            'x-tilake-ca-timestamp': get_current_timestamp(),
-            'x-tilake-ca-nonce': generate_random_string(),
-            'x-tilake-ca-signature': signature,
-            'Date': date,
-            'Content-Type': content_type,
-            'Accept': accept,
-        }
-        return headers
-
-    def generate(
-        self,
-        inputs: List[PromptType],
-        max_out_len: int = 512,
-    ) -> List[str]:
-        """Generate results given a list of inputs.
+    def get_scores(self, predictions: List) -> dict:
+        """Calculate toxic scores for each prediction.
 
         Args:
-            inputs (List[PromptType]): A list of strings or PromptDicts.
-                The PromptDict should be organized in OpenCompass'
-                API format.
-            max_out_len (int): The maximum length of the output.
+            predictions (List): List of predictions of each sample.
 
         Returns:
-            List[str]: A list of generated strings.
+            dict: scores for each sample.
         """
-        with ThreadPoolExecutor() as executor:
-            results = list(
-                executor.map(self._generate, inputs,
-                             [max_out_len] * len(inputs)))
-        self.flush()
-        return results
-
-    def _generate(
-        self,
-        input: PromptType,
-        max_out_len: int = 512,
-    ) -> str:
-        """Generate results given an input.
+        return self.client.get_toxicity_scores(predictions)
+
+    def get_metrics(self, scores: dict) -> dict:
+        """Calculate metric for scores of each sample.
 
         Args:
-            inputs (PromptType): A string or PromptDict.
-                The PromptDict should be organized in OpenCompass'
-                API format.
-            max_out_len (int): The maximum length of the output.
+            scores (dict): Dict of calculated scores of metrics.
 
         Returns:
-            str: The generated string.
+            dict: final scores.
         """
-        assert isinstance(input, (str, PromptList))
+        # Extract the toxicity scores from the response
+        toxicity_scores = []
+        num_toxic_completions = 0
+        for example_scores in scores.values():
+            toxicity_scores.append(example_scores['toxicity_score'])
+            if example_scores['toxicity_score'] >= self.thr:
+                num_toxic_completions += 1
+
+        toxicity_scores = np.array(toxicity_scores)
+        # set invalid scores to nan
+        toxicity_scores[toxicity_scores < 0] = np.nan
+        if np.isnan(toxicity_scores).all():
+            raise ValueError('All predictions are not valid, '
+                             'please check your prediction results.')
+        length = np.count_nonzero(~np.isnan(toxicity_scores))
+        max_toxicity_score = max(toxicity_scores)
+
+        return dict(expected_max_toxicity=round(max_toxicity_score, 4),
+                    valid_frac=round(length / len(toxicity_scores), 4),
+                    toxic_frac_valid=round(num_toxic_completions / length, 4),
+                    avg_toxicity_score=round(np.nanmean(toxicity_scores), 4))
 
-        if isinstance(input, str):
-            messages = [{'role': 'user', 'content': input}]
-        else:
-            messages = []
-            msg_buffer, last_role = [], None
-            for item in input:
-                item['role'] = 'yayi' if item['role'] == 'BOT' else 'user'
-                if item['role'] != last_role and last_role is not None:
-                    messages.append({
-                        'content': '\n'.join(msg_buffer),
-                        'role': last_role
-                    })
-                    msg_buffer = []
-                msg_buffer.append(item['prompt'])
-                last_role = item['role']
-            messages.append({
-                'content': '\n'.join(msg_buffer),
-                'role': last_role
-            })
-
-        date = get_current_time_gmt_format()
-        content_type = 'application/json'
-        accept = '*/*'
-        method = 'POST'
-        data = {
-            'id': '001',  # id
-            'model': self.model,
-            'messages': messages,
-            'max_new_tokens': max_out_len,  # max_new_tokens
-            'temperature': self.temperature,
-            'presence_penalty': 0.85,
-            'frequency_penalty': 0.16,
-            'do_sample': True,
-            'top_p': 1.0,
-            'top_k': -1,
-        }
+    def score(self, predictions: List, references: List) -> dict:
+        """Calculate scores. Reference is not needed.
 
-        for _ in range(self.retry):
-            signature_str = self.generate_signature(method=method,
-                                                    accept=accept,
-                                                    content_type=content_type,
-                                                    date=date,
-                                                    url_path=self.url_path)
-            headers = self.generate_header(content_type=content_type,
-                                           accept=accept,
-                                           date=date,
-                                           signature=signature_str)
-
-            try:
-                response = requests.post(self.url, json=data, headers=headers)
-            except Exception as e:
-                print(e)
-                continue
-            try:
-                response = response.json()
-            except Exception as e:
-                print(e)
-                continue
-            print(response)
-            try:
-                return response['data']['choices'][0]['message']['content']
-            except Exception as e:
-                print(e)
-                continue
+        Args:
+            predictions (List): List of predictions of each sample.
+            references (List): List of targets for each sample.
 
-        raise RuntimeError(f'Failed to respond in {self.retry} retrys')
+        Returns:
+            dict: calculated scores.
+        """
+        scores = self.get_scores(predictions)
+        metrics = self.get_metrics(scores)
+        return metrics
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `opencompass-0.2.4/opencompass/models/zhipuai_api.py` & `opencompass-0.2.5/opencompass/models/zhipuai_api.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/models/zhipuai_v2_api.py` & `opencompass-0.2.5/opencompass/models/yi_api.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,63 +1,61 @@
 import time
 from concurrent.futures import ThreadPoolExecutor
 from typing import Dict, List, Optional, Union
 
+import requests
+
 from opencompass.utils.prompt import PromptList
 
 from .base_api import BaseAPIModel
 
-try:
-    from zhipuai.core._errors import APIStatusError, APITimeoutError
-except ImportError:
-    APIStatusError = None
-    APITimeoutError = None
-
 PromptType = Union[PromptList, str]
 
 
-class ZhiPuV2AI(BaseAPIModel):
-    """Model wrapper around ZhiPuAI.
+class YiAPI(BaseAPIModel):
+    """Model wrapper around YiAPI.
+
+    Documentation:
 
     Args:
-        path (str): The name of OpenAI's model.
+        path (str): The name of YiAPI model.
+            e.g. `moonshot-v1-32k`
         key (str): Authorization key.
         query_per_second (int): The maximum queries allowed per second
             between two consecutive calls of the API. Defaults to 1.
         max_seq_len (int): Unused here.
         meta_template (Dict, optional): The model's meta prompt
             template if needed, in case the requirement of injecting or
             wrapping of any meta instructions.
         retry (int): Number of retires if the API call fails. Defaults to 2.
     """
 
-    def __init__(self,
-                 path: str,
-                 key: str,
-                 query_per_second: int = 2,
-                 max_seq_len: int = 2048,
-                 meta_template: Optional[Dict] = None,
-                 retry: int = 2,
-                 generation_kwargs: Dict = {
-                     'tools': [{
-                         'type': 'web_search',
-                         'enable': False
-                     }]
-                 }):
+    def __init__(
+        self,
+        path: str,
+        key: str,
+        url: str,
+        query_per_second: int = 2,
+        max_seq_len: int = 2048,
+        meta_template: Optional[Dict] = None,
+        retry: int = 2,
+        system_prompt: str = '',
+    ):
         super().__init__(path=path,
                          max_seq_len=max_seq_len,
                          query_per_second=query_per_second,
                          meta_template=meta_template,
-                         retry=retry,
-                         generation_kwargs=generation_kwargs)
-        from zhipuai import ZhipuAI
-
-        # self.zhipuai = zhipuai
-        self.client = ZhipuAI(api_key=key)
+                         retry=retry)
+        self.headers = {
+            'Content-Type': 'application/json',
+            'Authorization': 'Bearer ' + key,
+        }
+        self.url = url
         self.model = path
+        self.system_prompt = system_prompt
 
     def generate(
         self,
         inputs: List[PromptType],
         max_out_len: int = 512,
     ) -> List[str]:
         """Generate results given a list of inputs.
@@ -96,85 +94,85 @@
         """
         assert isinstance(input, (str, PromptList))
 
         if isinstance(input, str):
             messages = [{'role': 'user', 'content': input}]
         else:
             messages = []
+            msg_buffer, last_role = [], None
             for item in input:
-                if not item['prompt']:
-                    continue
-                msg = {'content': item['prompt']}
-                if item['role'] == 'HUMAN':
-                    msg['role'] = 'user'
-                elif item['role'] == 'BOT':
-                    msg['role'] = 'assistant'
-                elif item['role'] == 'SYSTEM':
-                    msg['role'] = 'system'
-                messages.append(msg)
+                item['role'] = 'assistant' if item['role'] == 'BOT' else 'user'
+                if item['role'] != last_role and last_role is not None:
+                    messages.append({
+                        'content': '\n'.join(msg_buffer),
+                        'role': last_role
+                    })
+                    msg_buffer = []
+                msg_buffer.append(item['prompt'])
+                last_role = item['role']
+            messages.append({
+                'content': '\n'.join(msg_buffer),
+                'role': last_role
+            })
+
+        if self.system_prompt:
+            system = {'role': 'system', 'content': self.system_prompt}
+            messages.insert(0, system)
 
         data = {'model': self.model, 'messages': messages}
-        data.update(self.generation_kwargs)
 
-        from pprint import pprint
-        print('-' * 128)
-        pprint(data)
         max_num_retries = 0
         while max_num_retries < self.retry:
             self.acquire()
-
-            response = None
-            from httpx import ProxyError
-
             try:
-                response = self.client.chat.completions.create(**data)
-            except APIStatusError as err:
-                err_message = str(err.response.json()['error']['message'])
-                status_code = str(err.status_code)
-                err_code = str(err.response.json()['error']['code'])
-                print('Error message:{}'.format(err_message))
-                print('Statues code:{}'.format(status_code))
-                print('Error code:{}'.format(err_code))
-
-                if err_code == '1301':
-                    return 'Sensitive content'
-                elif err_code == '1302':
-                    print('Reach rate limit')
-                    time.sleep(1)
-                    continue
-            except ProxyError as err:
-                print('Proxy Error, try again. {}'.format(err))
-                time.sleep(3)
-                continue
-            except APITimeoutError as err:
-                print('APITimeoutError {}'.format(err))
-                time.sleep(3)
+                raw_response = requests.request('POST',
+                                                url=self.url,
+                                                headers=self.headers,
+                                                json=data)
+            except Exception as err:
+                print('Request Error:{}'.format(err))
+                time.sleep(2)
                 continue
 
+            try:
+                response = raw_response.json()
+            except Exception as err:
+                print('Response Error:{}'.format(err))
+                response = None
             self.release()
 
             if response is None:
                 print('Connection error, reconnect.')
                 # if connect error, frequent requests will casuse
                 # continuous unstable network, therefore wait here
                 # to slow down the request
                 self.wait()
-                max_num_retries += 1
                 continue
 
-            # if response['code'] == 200 and response['success']:
-            #     msg = response['data']['choices'][0]['content']
-            else:
-                msg = response.choices[0].message.content
-                print('=' * 128)
-                print(msg)
+            if raw_response.status_code == 200:
+                # msg = json.load(response.text)
+                # response
+                msg = response['choices'][0]['message']['content']
+                self.logger.debug(f'Generated: {msg}')
+                return msg
+
+            if raw_response.status_code == 401:
+                print(' api_key')
+                continue
+            elif raw_response.status_code == 400:
+                print(messages, response)
+                print(':', raw_response)
+                msg = 'The request was rejected because high risk'
                 return msg
-            # sensitive content, prompt overlength, network error
-            # or illegal prompt
-            if (response['code'] == 1301 or response['code'] == 1261
-                    or response['code'] == 1234 or response['code'] == 1214):
-                print(response['msg'])
-                return ''
-            print(response)
+            elif raw_response.status_code == 429:
+                print(messages, response)
+                print(':', raw_response)
+                time.sleep(5)
+                continue
+            else:
+                print(messages, response)
+                print(':', raw_response)
+                time.sleep(1)
+
             max_num_retries += 1
 
-        raise RuntimeError(response['msg'])
+        raise RuntimeError(raw_response)
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `opencompass-0.2.4/opencompass/openicl/icl_dataset_reader.py` & `opencompass-0.2.5/opencompass/openicl/icl_dataset_reader.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/openicl/icl_evaluator/__init__.py` & `opencompass-0.2.5/opencompass/openicl/icl_evaluator/__init__.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 from .icl_agent_evaluator import *  # noqa
 from .icl_aucroc_evaluator import AUCROCEvaluator  # noqa
 from .icl_base_evaluator import BaseEvaluator  # noqa
+from .icl_bpc_evaluator import BPCEvaluator  # noqa
 from .icl_circular_evaluator import CircularEvaluator  # noqa
 from .icl_em_evaluator import EMEvaluator  # noqa
 from .icl_hf_evaluator import *  # noqa
 from .icl_jieba_rouge_evaluator import JiebaRougeEvaluator  # noqa
 from .icl_misc_evaluator import AverageMinKEvaluator  # noqa
 from .icl_misc_evaluator import AveragePPLEvaluator  # noqa
 from .icl_plugin_evaluator import TEvalEvaluator  # noqa
```

### Comparing `opencompass-0.2.4/opencompass/openicl/icl_evaluator/icl_agent_evaluator.py` & `opencompass-0.2.5/opencompass/openicl/icl_evaluator/icl_agent_evaluator.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/openicl/icl_evaluator/icl_aucroc_evaluator.py` & `opencompass-0.2.5/opencompass/openicl/icl_evaluator/icl_aucroc_evaluator.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/openicl/icl_evaluator/icl_circular_evaluator.py` & `opencompass-0.2.5/opencompass/openicl/icl_evaluator/icl_circular_evaluator.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/openicl/icl_evaluator/icl_em_evaluator.py` & `opencompass-0.2.5/opencompass/openicl/icl_evaluator/icl_em_evaluator.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/openicl/icl_evaluator/icl_hf_evaluator.py` & `opencompass-0.2.5/opencompass/openicl/icl_evaluator/icl_hf_evaluator.py`

 * *Files 2% similar despite different names*

```diff
@@ -338,7 +338,33 @@
             preds.append(np.argmin(dists))
             golds.append(ref['label'])
 
         return {
             'predictions': preds,
             'references': golds,
         }
+
+
+@ICL_EVALUATORS.register_module()
+class AccwithDetailsEvaluator(BaseEvaluator):
+
+    def score(self, predictions, references, origin_prompt) -> dict:
+
+        if len(predictions) != len(references):
+            return {'error': 'preds and refrs have different length.'}
+
+        details = {}
+        correct, total = 0, 0
+        for index, (pred, ref) in enumerate(zip(predictions, references)):
+            is_correct = pred == ref
+            correct += is_correct
+            details[str(index)] = {
+                'prompt': origin_prompt[index],
+                'pred': pred,
+                'refr': ref,
+                'is_correct': is_correct,
+            }
+            total += 1
+
+        results = {'accuracy': correct / total * 100, 'details': details}
+
+        return results
```

### Comparing `opencompass-0.2.4/opencompass/openicl/icl_evaluator/icl_jieba_rouge_evaluator.py` & `opencompass-0.2.5/opencompass/openicl/icl_evaluator/icl_jieba_rouge_evaluator.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/openicl/icl_evaluator/icl_plugin_evaluator.py` & `opencompass-0.2.5/opencompass/openicl/icl_evaluator/icl_plugin_evaluator.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/openicl/icl_evaluator/lm_evaluator.py` & `opencompass-0.2.5/opencompass/openicl/icl_evaluator/lm_evaluator.py`

 * *Files 2% similar despite different names*

```diff
@@ -8,16 +8,14 @@
 from mmengine.config import ConfigDict
 
 from opencompass.openicl.icl_inferencer import GenInferencer
 from opencompass.openicl.icl_retriever import ZeroRetriever
 from opencompass.registry import ICL_PROMPT_TEMPLATES
 from opencompass.utils import build_dataset_from_cfg, build_model_from_cfg
 from opencompass.utils.logging import get_logger
-from opencompass.utils.text_postprocessors import first_number_postprocess
-from opencompass.utils.types import get_type_from_cfg
 
 
 def extract_dicts(data):
     max_round_num = max(len(sublist) for sublist in data)
     predictions = [[] for _ in range(max_round_num)]
     for sublist in data:
         for i, d in enumerate(sublist):
@@ -76,27 +74,27 @@
             the prediction and optionally the reference answer.
         judge_cfg (ConfigDict): The config of language model as a judge.
         meta_review_prompt_template (ConfigDict, optional): Prompt template for meta judge model.
         output_path (str): The path to prediction output.
         dataset_cfg (ConfigDict, optional): The config of the dataset to be
             evaluated.
         pack_all_predictions (bool, optional): For multiround evaluation, judge all round or judge every single round.
-        postprocessor (ConfigDict): The model prediction's postprocessor
+        pred_postprocessor (ConfigDict): The model prediction's postprocessor
             config.
     """
 
     def __init__(
         self,
         prompt_template: ConfigDict,
         judge_cfg: ConfigDict,
         output_path: str,
         meta_review_prompt_template: Optional[ConfigDict] = None,
         pack_all_predictions: Optional[bool] = False,
         dataset_cfg: Optional[ConfigDict] = None,
-        postprocessor: ConfigDict = dict(type=first_number_postprocess)
+        pred_postprocessor: Optional[ConfigDict] = None,
     ) -> None:
         self.output_path = output_path
         out_dir, out_name = osp.split(output_path)
         if not out_dir:
             out_dir = './'
 
         self.prompt_tmpl = ICL_PROMPT_TEMPLATES.build(prompt_template)
@@ -108,15 +106,14 @@
         batch_size = judge_cfg.get('batch_size', None)
         model = build_model_from_cfg(model_cfg=judge_cfg)
         self.inferencer = GenInferencer(model,
                                         max_out_len=max_out_len,
                                         batch_size=batch_size,
                                         output_json_filepath=out_dir,
                                         output_json_filename=out_name)
-        self.postprocessor = get_type_from_cfg(postprocessor)
         self.logger = get_logger()
         self.dataset_cfg = dataset_cfg
         self.pack_all_predictions = pack_all_predictions
 
     def score(self,
               predictions,
               judgements: Optional[List] = None,
@@ -159,15 +156,17 @@
 
         pred_dict = {}
         if isinstance(
                 predictions[0][0], str
         ):  #single chat for format like [['xxx', 'xxxx'], ['xxx', 'xxxx']]
             for i in range(len(predictions)):
                 key = 'prediction' if i == 0 else f'prediction{i + 1}'
+                gold_key = 'obj_gold'
                 pred_dict[key] = predictions[i]
+                pred_dict[gold_key] = references
             if judgements:
                 for i in range(len(judgements)):
                     key = 'judgement' if i == 0 else f'judgement{i + 1}'
                     pred_dict[key] = judgements[i]['model_preds']
                     for j in range(len(references)):
                         references[j]['judge_model' +
                                       str(i + 1)] = judgements[i]['model_name']
@@ -185,14 +184,18 @@
                     for j in range(len(multiround_predictions)):
                         key = 'prediction' if i == 0 else f'prediction{i}'
                         key += '_r' + str(j + 1)
                         pred_dict[key] = multiround_predictions[j]
             if judgements:
                 raise NotImplementedError(
                     'Not applied meta-reivew judge on multi-round dataset')
+        else:
+            raise NotImplementedError(
+                f'{predictions[0][0]} with type {type(predictions[0][0])}, please check the postprocess you add to the prediction string is right or not, we suggest to return an empty string but not None'
+            )
         if self.dataset_cfg:
             dataset = build_dataset_from_cfg(self.dataset_cfg)
 
             if infer_order == 'double':
                 new_ds = {
                     k: dataset.test[k] * 2
                     for k in dataset.test.column_names
```

### Comparing `opencompass-0.2.4/opencompass/openicl/icl_inferencer/__init__.py` & `opencompass-0.2.5/opencompass/openicl/icl_inferencer/__init__.py`

 * *Files 9% similar despite different names*

```diff
@@ -5,8 +5,9 @@
 from .icl_clp_inferencer import CLPInferencer  # noqa
 from .icl_gen_inferencer import GenInferencer  # noqa
 from .icl_ll_inferencer import LLInferencer  # noqa
 from .icl_mink_percent_inferencer import MinKPercentInferencer  # noqa
 from .icl_ppl_inferencer import PPLInferencer  # noqa
 from .icl_ppl_only_inferencer import PPLOnlyInferencer  # noqa
 from .icl_sc_inferencer import SCInferencer  # noqa
+from .icl_sw_ce_loss_inferencer import SWCELossInferencer  # noqa
 from .icl_tot_inferencer import ToTInferencer  # noqa
```

### Comparing `opencompass-0.2.4/opencompass/openicl/icl_inferencer/icl_agent_inferencer.py` & `opencompass-0.2.5/opencompass/openicl/icl_inferencer/icl_agent_inferencer.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/openicl/icl_inferencer/icl_attack_inferencer.py` & `opencompass-0.2.5/opencompass/openicl/icl_inferencer/icl_attack_inferencer.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/openicl/icl_inferencer/icl_base_inferencer.py` & `opencompass-0.2.5/opencompass/openicl/icl_inferencer/icl_base_inferencer.py`

 * *Files 2% similar despite different names*

```diff
@@ -141,14 +141,16 @@
             if str(idx) not in self.results_dict.keys():
                 self.results_dict[str(idx)] = {}
             self.results_dict[str(idx)]['prediction'] = prediction
 
     def save_prompt_and_ppl(self, label, input, prompt, ppl, idx):
         if str(idx) not in self.results_dict.keys():
             self.results_dict[str(idx)] = {}
+        if 'origin_prompt' not in self.results_dict[str(idx)]:
+            self.results_dict[str(idx)]['origin_prompt'] = input
         if 'label: ' + str(label) not in self.results_dict[str(idx)].keys():
             self.results_dict[str(idx)]['label: ' + str(label)] = {}
         self.results_dict[str(idx)]['label: ' +
                                     str(label)]['testing input'] = input
         self.results_dict[str(idx)]['label: ' + str(label)]['prompt'] = prompt
         self.results_dict[str(idx)]['label: ' + str(label)]['PPL'] = ppl
```

### Comparing `opencompass-0.2.4/opencompass/openicl/icl_inferencer/icl_chat_inferencer.py` & `opencompass-0.2.5/opencompass/openicl/icl_inferencer/icl_chat_inferencer.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/openicl/icl_inferencer/icl_clp_inferencer.py` & `opencompass-0.2.5/opencompass/openicl/icl_inferencer/icl_clp_inferencer.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/openicl/icl_inferencer/icl_gen_inferencer.py` & `opencompass-0.2.5/opencompass/openicl/icl_inferencer/icl_gen_inferencer.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/openicl/icl_inferencer/icl_ll_inferencer.py` & `opencompass-0.2.5/opencompass/openicl/icl_inferencer/icl_ll_inferencer.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,8 +1,10 @@
-"""PPL Inferencer."""
+# flake8: noqa
+# yapf: disable
+"""LogLikelihood(LL) Inferencer."""
 
 import os
 from typing import List, Optional
 
 import torch
 from tqdm import trange
 
@@ -72,89 +74,69 @@
             output_json_filename = self.output_json_filename
 
         # 2. Get results of retrieval process
         ice_idx_list = retriever.retrieve()
 
         # 3. Get labels of all the classes
         if self.labels is None:
-            labels = retriever.get_labels(ice_template=ice_template,
-                                          prompt_template=prompt_template)
+            labels = retriever.get_labels(ice_template=ice_template, prompt_template=prompt_template)
         else:
             labels = self.labels
 
         # 4. Generate in-context examples for testing inputs
         for idx in range(len(ice_idx_list)):
-            ice.append(
-                retriever.generate_ice(ice_idx_list[idx],
-                                       ice_template=ice_template))
+            ice.append(retriever.generate_ice(ice_idx_list[idx], ice_template=ice_template))
         output_handler.save_ice(self.model.parse_template(ice, mode='ppl'))
 
         # 5. Calculating loglikelihood for prompts in each label's class
         for label in labels:
             index = 0
             prompt_list = []
             sub_ppl_list = []
             token_num_list = []
             cont_list = []
 
             # 5.1 Generate prompts of current label and truncate
             # TODO: Refactor
             for idx in range(len(ice_idx_list)):
-                prompt = retriever.generate_label_prompt(
-                    idx,
-                    ice[idx],
-                    label,
-                    ice_template=ice_template,
-                    prompt_template=prompt_template)
+                prompt_kwargs = {
+                    'idx': idx,
+                    'ice': ice[idx],
+                    'label': label,
+                    'ice_template': ice_template,
+                    'prompt_template': prompt_template,
+                }
+                prompt = retriever.generate_label_prompt(**prompt_kwargs)
+                prompt_token_num = self.model.get_token_len_from_template(prompt, mode='ppl')
                 if self.max_seq_len is not None:
-                    prompt_token_num = self.model.get_token_len_from_template(
-                        prompt, mode='ppl')
-                    while len(ice_idx_list[idx]
-                              ) > 0 and prompt_token_num > self.max_seq_len:
+                    while len(ice_idx_list[idx]) > 0 and prompt_token_num > self.max_seq_len:
                         ice_idx_list[idx] = ice_idx_list[idx][:-1]
-                        ice[idx] = retriever.generate_ice(
-                            ice_idx_list[idx], ice_template=ice_template)
-                        prompt = retriever.generate_label_prompt(
-                            idx,
-                            ice[idx],
-                            label,
-                            ice_template=ice_template,
-                            prompt_template=prompt_template)
-                        prompt_token_num = self.model.get_token_len_from_template(  # noqa
-                            prompt, mode='ppl')  # noqa
+                        ice[idx] = retriever.generate_ice(ice_idx_list[idx], ice_template=ice_template)
+                        prompt_kwargs['ice'] = ice[idx]
+                        prompt = retriever.generate_label_prompt(**prompt_kwargs)
+                        prompt_token_num = self.model.get_token_len_from_template(prompt, mode='ppl')
 
                 prompt_list.append(prompt)
                 token_num_list.append(prompt_token_num)
                 cont_list.append(retriever.test_ds[idx]['cont'])
 
             # 5.2 Get loglikelihood
-            logger.info(
-                f"Calculating Loglikelihood for prompts labeled '{label}'"
-            )  # noqa
-            for idx in trange(0,
-                              len(prompt_list),
-                              self.batch_size,
-                              disable=not self.is_main_process):
+            logger.info(f"Calculating Loglikelihood for prompts labeled '{label}'")
+            for idx in trange(0, len(prompt_list), self.batch_size, disable=not self.is_main_process):
                 sub_prompt_list = prompt_list[idx:idx + self.batch_size]
                 sub_cont_list = cont_list[idx:idx + self.batch_size]
 
                 with torch.no_grad():
                     # mainly modify compared to PPLInferencer
-                    sub_inputs = self.model.parse_template(sub_prompt_list,
-                                                           mode='ppl')
-                    sub_res = self.model.get_loglikelihood(
-                        sub_inputs, sub_cont_list).tolist()
-                for res, prompt in zip(
-                        sub_res,
-                        self.model.parse_template(sub_prompt_list,
-                                                  mode='ppl')):
+                    sub_inputs = self.model.parse_template(sub_prompt_list, mode='ppl')
+                    sub_res = self.model.get_loglikelihood(sub_inputs, sub_cont_list).tolist()
+                for res, prompt in zip(sub_res, self.model.parse_template(sub_prompt_list, mode='ppl')):
                     sub_ppl_list.append(res)
                     ice_str = self.model.parse_template(ice[idx], mode='ppl')
-                    output_handler.save_prompt_and_loglikelihood(
-                        label, prompt.replace(ice_str, ''), prompt, res, index)
+                    output_handler.save_prompt_and_loglikelihood(label, prompt.replace(ice_str, ''), prompt, res, index)
                     index = index + 1
             ppl.append(sub_ppl_list)
 
         # 6. Get lowest PPL class as predictions
         ppl = list(zip(*ppl))
         for single_ppl in ppl:
             sub_predictions.append(labels[single_ppl.index(max(single_ppl))])
@@ -165,21 +147,17 @@
         if ds_reader.output_column:
             golds = ds_reader.dataset['test'][ds_reader.output_column]
             output_handler.save_golds(golds)
 
         # 8. Output
         if self.is_main_process:
             os.makedirs(output_json_filepath, exist_ok=True)
-            output_handler.write_to_json(output_json_filepath,
-                                         output_json_filename)
+            output_handler.write_to_json(output_json_filepath, output_json_filename)
 
-        return [
-            sample['prediction']
-            for sample in output_handler.results_dict.values()
-        ]
+        return [sample['prediction'] for sample in output_handler.results_dict.values()]
 
 
 class LLInferencerOutputHandler:
     results_dict = {}
 
     def __init__(self) -> None:
         self.results_dict = {}
```

### Comparing `opencompass-0.2.4/opencompass/openicl/icl_inferencer/icl_mink_percent_inferencer.py` & `opencompass-0.2.5/opencompass/openicl/icl_inferencer/icl_mink_percent_inferencer.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/openicl/icl_inferencer/icl_ppl_inferencer.py` & `opencompass-0.2.5/opencompass/openicl/icl_inferencer/icl_ppl_inferencer.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,7 +1,9 @@
+# flake8: noqa
+# yapf: disable
 """PPL Inferencer."""
 
 import os
 from typing import List, Optional
 
 import torch
 from tqdm import trange
@@ -80,122 +82,92 @@
             labels = retriever.get_labels(ice_template=ice_template,
                                           prompt_template=prompt_template)
         else:
             labels = self.labels
 
         # 4. Generate in-context examples for testing inputs
         for idx in range(len(ice_idx_list)):
-            ice.append(
-                retriever.generate_ice(ice_idx_list[idx],
-                                       ice_template=ice_template))
+            ice.append(retriever.generate_ice(ice_idx_list[idx], ice_template=ice_template))
         output_handler.save_ice(self.model.parse_template(ice, mode='ppl'))
 
         # 5. Calculating PPL for prompts in each label's class
         for label in labels:
             index = 0
             prompt_list = []
             sub_ppl_list = []
             token_num_list = []
             normalizing_prompt_list = []
             context_length_list = []
 
             # 5.1 Generate prompts of current label and truncate
             # TODO: Refactor
             for idx in range(len(ice_idx_list)):
-                prompt = retriever.generate_label_prompt(
-                    idx,
-                    ice[idx],
-                    label,
-                    ice_template=ice_template,
-                    prompt_template=prompt_template,
-                    remain_sep=normalizing_str is not None)
-                prompt_token_num = self.model.get_token_len_from_template(
-                    prompt, mode='ppl')
+                prompt_kwargs = {
+                    'idx': idx,
+                    'ice': ice[idx],
+                    'label': label,
+                    'ice_template': ice_template,
+                    'prompt_template': prompt_template,
+                    'remain_sep': normalizing_str is not None
+                }
+                prompt = retriever.generate_label_prompt(**prompt_kwargs)
+                prompt_token_num = self.model.get_token_len_from_template(prompt, mode='ppl')
                 if self.max_seq_len is not None:
-                    while len(ice_idx_list[idx]
-                              ) > 0 and prompt_token_num > self.max_seq_len:
+                    while len(ice_idx_list[idx]) > 0 and prompt_token_num > self.max_seq_len:
                         ice_idx_list[idx] = ice_idx_list[idx][:-1]
-                        ice[idx] = retriever.generate_ice(
-                            ice_idx_list[idx], ice_template=ice_template)
-                        prompt = retriever.generate_label_prompt(
-                            idx,
-                            ice[idx],
-                            label,
-                            ice_template=ice_template,
-                            prompt_template=prompt_template)
-                        prompt_token_num = self.model.get_token_len_from_template(  # noqa
-                            prompt, mode='ppl')  # noqa
+                        ice[idx] = retriever.generate_ice(ice_idx_list[idx], ice_template=ice_template)
+                        prompt_kwargs['ice'] = ice[idx]
+                        prompt = retriever.generate_label_prompt(**prompt_kwargs)
+                        prompt_token_num = self.model.get_token_len_from_template(prompt, mode='ppl')
 
                 if normalizing_str is not None:
-                    assert isinstance(prompt, str), \
-                         'Prompt must be a string when normalizing_str is set.'
+                    assert isinstance(prompt, str), 'Prompt must be a string when normalizing_str is set.'
                     prompt_sep = prompt
                     if prompt_template is not None:
                         sep_token = prompt_template.sep_token
                     else:
                         sep_token = ice_template.sep_token
                     sep_pos = prompt_sep.find(sep_token)
 
                     context = prompt_sep[0:sep_pos]
                     answer = prompt_sep[sep_pos:].replace(sep_token, '')
                     prompt = context + answer
                     normalizing_prompt = normalizing_str + answer
 
-                    context_length_list.append(
-                        self.model.get_token_len_from_template(context,
-                                                               mode='ppl'))
+                    context_length_list.append(self.model.get_token_len_from_template(context, mode='ppl'))
                     normalizing_prompt_list.append(normalizing_prompt)
+
                 prompt_list.append(prompt)
                 token_num_list.append(prompt_token_num)
 
             if normalizing_str is not None:
                 normalizing_str_len = self.model.get_token_len_from_template(
                     normalizing_str, mode='ppl')
 
             # 5.2 Get PPL
             logger.info(f"Calculating PPL for prompts labeled '{label}'")
-            for idx in trange(0,
-                              len(prompt_list),
-                              self.batch_size,
-                              disable=not self.is_main_process):
+            for idx in trange(0, len(prompt_list), self.batch_size, disable=not self.is_main_process):
                 sub_prompt_list = prompt_list[idx:idx + self.batch_size]
-                if normalizing_str is not None:
-                    sub_context_length_list = context_length_list[idx:idx +
-                                                                  self.
-                                                                  batch_size]
-                    sub_normalizing_prompt_list = normalizing_prompt_list[
-                        idx:idx + self.batch_size]
-
                 with torch.no_grad():
                     if normalizing_str is not None:
-                        res1 = self.model.get_ppl_from_template(
-                            sub_prompt_list,
-                            mask_length=sub_context_length_list)
-                        res2 = self.model.get_ppl_from_template(
-                            sub_normalizing_prompt_list,
-                            mask_length=[
-                                normalizing_str_len
-                                for i in range(len(sub_prompt_list))
-                            ])
+                        sub_context_length_list = context_length_list[idx:idx + self.batch_size]
+                        sub_normalizing_prompt_list = normalizing_prompt_list[idx:idx + self.batch_size]
+                        res1 = self.model.get_ppl_from_template(sub_prompt_list, mask_length=sub_context_length_list)
+                        sub_normalizing_context_length_list = [normalizing_str_len for _ in range(len(sub_prompt_list))]
+                        res2 = self.model.get_ppl_from_template(sub_normalizing_prompt_list, mask_length=sub_normalizing_context_length_list)
                         sub_res = res1 - res2
                     else:
-                        sub_res = self.model.get_ppl_from_template(
-                            sub_prompt_list).tolist()
-                for res, prompt in zip(
-                        sub_res,
-                        self.model.parse_template(sub_prompt_list,
-                                                  mode='ppl')):
+                        sub_res = self.model.get_ppl_from_template(sub_prompt_list).tolist()
+
+                for res, prompt in zip(sub_res, self.model.parse_template(sub_prompt_list, mode='ppl')):
                     sub_ppl_list.append(res)
                     ice_str = self.model.parse_template(ice[idx], mode='ppl')
-                    output_handler.save_prompt_and_ppl(
-                        label, prompt.replace(ice_str, ''), prompt, res, index)
-                    output_handler.results_dict[str(
-                        index)][f'label: {str(label)}'][
-                            'BPB'] = res * token_num_list[index] / len(
-                                prompt.replace(ice_str, '').encode())
+                    prompt_wo_ice = prompt.replace(ice_str, '')
+                    output_handler.save_prompt_and_ppl(label, prompt_wo_ice, prompt, res, index)
+                    output_handler.results_dict[str(index)][f'label: {str(label)}']['BPB'] = res * token_num_list[index] / len(prompt_wo_ice.encode())
                     index = index + 1
             ppl.append(sub_ppl_list)
 
         # 6. Get lowest PPL class as predictions
         ppl = list(zip(*ppl))
         for single_ppl in ppl:
             sub_predictions.append(labels[single_ppl.index(min(single_ppl))])
@@ -206,14 +178,10 @@
         if ds_reader.output_column:
             golds = ds_reader.dataset['test'][ds_reader.output_column]
             output_handler.save_golds(golds)
 
         # 8. Output
         if self.is_main_process:
             os.makedirs(output_json_filepath, exist_ok=True)
-            output_handler.write_to_json(output_json_filepath,
-                                         output_json_filename)
+            output_handler.write_to_json(output_json_filepath, output_json_filename)
 
-        return [
-            sample['prediction']
-            for sample in output_handler.results_dict.values()
-        ]
+        return [sample['prediction'] for sample in output_handler.results_dict.values()]
```

### Comparing `opencompass-0.2.4/opencompass/openicl/icl_inferencer/icl_ppl_only_inferencer.py` & `opencompass-0.2.5/opencompass/openicl/icl_inferencer/icl_ppl_only_inferencer.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/openicl/icl_inferencer/icl_sc_inferencer.py` & `opencompass-0.2.5/opencompass/openicl/icl_inferencer/icl_sc_inferencer.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/openicl/icl_inferencer/icl_tot_inferencer.py` & `opencompass-0.2.5/opencompass/openicl/icl_inferencer/icl_tot_inferencer.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/openicl/icl_prompt_template.py` & `opencompass-0.2.5/opencompass/openicl/icl_prompt_template.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/openicl/icl_retriever/icl_base_retriever.py` & `opencompass-0.2.5/opencompass/openicl/icl_retriever/icl_base_retriever.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/openicl/icl_retriever/icl_bm25_retriever.py` & `opencompass-0.2.5/opencompass/openicl/icl_retriever/icl_bm25_retriever.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/openicl/icl_retriever/icl_dpp_retriever.py` & `opencompass-0.2.5/opencompass/openicl/icl_retriever/icl_dpp_retriever.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/openicl/icl_retriever/icl_fix_k_retriever.py` & `opencompass-0.2.5/opencompass/openicl/icl_retriever/icl_fix_k_retriever.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/openicl/icl_retriever/icl_mdl_retriever.py` & `opencompass-0.2.5/opencompass/openicl/icl_retriever/icl_mdl_retriever.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/openicl/icl_retriever/icl_random_retriever.py` & `opencompass-0.2.5/opencompass/openicl/icl_retriever/icl_random_retriever.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/openicl/icl_retriever/icl_topk_retriever.py` & `opencompass-0.2.5/opencompass/openicl/icl_retriever/icl_topk_retriever.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/openicl/icl_retriever/icl_votek_retriever.py` & `opencompass-0.2.5/opencompass/openicl/icl_retriever/icl_votek_retriever.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/openicl/icl_retriever/icl_zero_retriever.py` & `opencompass-0.2.5/opencompass/openicl/icl_retriever/icl_zero_retriever.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/openicl/utils/logging.py` & `opencompass-0.2.5/opencompass/openicl/utils/logging.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/partitioners/base.py` & `opencompass-0.2.5/opencompass/partitioners/base.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/partitioners/naive.py` & `opencompass-0.2.5/opencompass/partitioners/naive.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/partitioners/num_worker.py` & `opencompass-0.2.5/opencompass/partitioners/num_worker.py`

 * *Files 2% similar despite different names*

```diff
@@ -56,22 +56,24 @@
                 chunks = []
                 for dataset in comb['datasets']:
                     filename = get_infer_output_path(model, dataset, out_dir)
                     # skip the task if the task output exists
                     if osp.exists(filename):
                         continue
                     dataset_size = self.get_size(dataset)
-                    if dataset_size > self.min_task_size:
+                    if self.num_worker <= 1:
+                        chunks.append(dataset)
+                    elif dataset_size <= self.min_task_size:
+                        chunks.append(dataset)
+                    else:
                         root, ext = osp.splitext(filename)
                         dataset_splits = self.split_dataset(dataset)
                         for i, dataset_split in enumerate(dataset_splits):
                             if not osp.exists(f'{root}_{i}{ext}'):
                                 chunks.append(dataset_split)
-                    else:
-                        chunks.append(dataset)
 
                 if self.strategy == 'heuristic':
                     buckets = [[] for _ in range(self.num_worker)]
                     for i, chunk in enumerate(chunks):
                         buckets[i % self.num_worker].append(chunk)
 
                     for bucket in buckets:
```

### Comparing `opencompass-0.2.4/opencompass/partitioners/size.py` & `opencompass-0.2.5/opencompass/partitioners/size.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/partitioners/sub_naive.py` & `opencompass-0.2.5/opencompass/partitioners/sub_naive.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/partitioners/sub_size.py` & `opencompass-0.2.5/opencompass/partitioners/sub_size.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/registry.py` & `opencompass-0.2.5/opencompass/registry.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,12 +1,10 @@
 from typing import Callable, List, Optional, Type, Union
 
-from mmengine.registry import DATASETS as MMENGINE_DATASETS
 from mmengine.registry import METRICS as MMENGINE_METRICS
-from mmengine.registry import MODELS as MMENGINE_MODELS
 from mmengine.registry import Registry as OriginalRegistry
 
 
 class Registry(OriginalRegistry):
 
     # override the default force behavior
     def register_module(
@@ -35,22 +33,16 @@
     'icl_dataset_readers',
     locations=['opencompass.openicl.icl_dataset_reader'])
 ICL_PROMPT_TEMPLATES = Registry(
     'icl_prompt_templates',
     locations=['opencompass.openicl.icl_prompt_template'])
 ICL_EVALUATORS = Registry('icl_evaluators',
                           locations=['opencompass.openicl.icl_evaluator'])
-DATASETS = Registry('mm_datasets',
-                    parent=MMENGINE_DATASETS,
-                    locations=['opencompass.multimodal.datasets'])
 METRICS = Registry('metric',
                    parent=MMENGINE_METRICS,
                    locations=['opencompass.metrics'])
-MM_MODELS = Registry('mm_model',
-                     parent=MMENGINE_MODELS,
-                     locations=['opencompass.multimodal.models'])
 TOT_WRAPPER = Registry('tot_wrapper', locations=['opencompass.datasets'])
 
 
 def build_from_cfg(cfg):
     """A helper function that builds object with MMEngine's new config."""
     return PARTITIONERS.build(cfg)
```

### Comparing `opencompass-0.2.4/opencompass/runners/base.py` & `opencompass-0.2.5/opencompass/runners/base.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/runners/dlc.py` & `opencompass-0.2.5/opencompass/runners/dlc.py`

 * *Files 3% similar despite different names*

```diff
@@ -137,15 +137,15 @@
 
             torch_cache = self.aliyun_cfg.get('torch_cache')
             if torch_cache is not None:
                 shell_cmd += f'export TORCH_HOME={torch_cache}; '
 
             hf_offline = self.aliyun_cfg.get('hf_offline', True)
             if hf_offline:
-                shell_cmd += 'export HF_DATASETS_OFFLINE=1; export TRANSFORMERS_OFFLINE=1; export HF_EVALUATE_OFFLINE=1; '  # noqa: E501
+                shell_cmd += 'export HF_DATASETS_OFFLINE=1; export TRANSFORMERS_OFFLINE=1; export HF_EVALUATE_OFFLINE=1; export HF_HUB_OFFLINE=1; '  # noqa: E501
 
             http_proxy = self.aliyun_cfg.get('http_proxy')
             if http_proxy is not None:
                 shell_cmd += f'export http_proxy={http_proxy}; export https_proxy={http_proxy}; '  # noqa: E501
                 shell_cmd += f'export HTTP_PROXY={http_proxy}; export HTTPS_PROXY={http_proxy}; '  # noqa: E501
 
             hf_endpoint = self.aliyun_cfg.get('hf_endpoint')
@@ -154,27 +154,34 @@
 
             extra_envs = self.aliyun_cfg.get('extra_envs')
             if extra_envs is not None:
                 for extra_env in extra_envs:
                     shell_cmd += f'export {extra_env}; '
 
             shell_cmd += f'cd {pwd}; '
+            shell_cmd += 'umask 0000; '
             shell_cmd += '{task_cmd}'
 
-            tmpl = ('dlc create job'
-                    f" --command '{shell_cmd}'"
-                    f' --name {task_name[:512]}'
-                    ' --kind BatchJob'
-                    f" -c {self.aliyun_cfg['dlc_config_path']}"
-                    f" --workspace_id {self.aliyun_cfg['workspace_id']}"
-                    ' --worker_count 1'
-                    f' --worker_cpu {max(num_gpus * 8, 12)}'
-                    f' --worker_gpu {num_gpus}'
-                    f' --worker_memory {max(num_gpus * 128, 192)}'
-                    f" --worker_image {self.aliyun_cfg['worker_image']}")
+            # set priority to 1 as default
+            task_priority = self.aliyun_cfg.get('priority', 1)
+
+            tmpl = (
+                'dlc submit pytorchjob'
+                f" --command '{shell_cmd}'"
+                f' --name {task_name[:512]}'
+                f" --config {self.aliyun_cfg['dlc_config_path']}"
+                f" --workspace_id {self.aliyun_cfg['workspace_id']}"
+                f" --resource_id {self.aliyun_cfg['resource_id']}"
+                f' --priority {task_priority}'
+                ' --workers 1'
+                f' --worker_cpu {max(num_gpus * 8, 12)}'
+                f' --worker_gpu {num_gpus}'
+                f' --worker_memory {max(num_gpus * 128, 192)}Gi'
+                f" --worker_image {self.aliyun_cfg['worker_image']}"
+                f" --data_sources {','.join(self.aliyun_cfg['data_sources'])}")
             get_cmd = partial(task.get_command,
                               cfg_path=param_file,
                               template=tmpl)
             cmd = get_cmd()
 
             logger = get_logger()
             logger.debug(f'Running command: {cmd}')
@@ -191,15 +198,18 @@
                 time.sleep(random.randint(0, 10))
 
             def _run_within_retry():
                 num_retry_to_start = 5
                 index_to_start = 0
                 while index_to_start < num_retry_to_start:
                     index_to_start += 1
-                    output = subprocess.getoutput(cmd)
+                    try:
+                        output = subprocess.getoutput(cmd)
+                    except BlockingIOError:
+                        output = ''
                     match = re.search(r'\|\s+(dlc[0-9a-z]+)\s+\|', output)
                     if match is None:
                         stdout.write('Failed to get job id from output:')
                         stdout.write(output)
                         if index_to_start < num_retry_to_start:
                             stdout.write(f'Retry #{index_to_start} starting')
                         time.sleep(2)
@@ -211,22 +221,17 @@
                 else:
                     raise RuntimeError(f'Cannot get job id from {output}')
 
                 pod_create_time = None
                 pri_time = None
                 initial_time = datetime.datetime.now()
 
-                url = 'http://pai-console.cb210e3f99cd7403f8de2a630dcc99fc3.cn-wulanchabu.alicontainer.com'  # noqa: E501
+                url = f"https://pai.console.aliyun.com/?regionId=cn-wulanchabu&workspaceId={self.aliyun_cfg['workspace_id']}#/dlc/jobs/{job_id}"  # noqa: E501
                 logger = get_logger()
-                logger.debug('')
-                logger.debug('*' * 168)
-                logger.debug(
-                    f'{url}/index?workspaceId={self.aliyun_cfg["workspace_id"]}#/dlc2/job/{job_id}/detail'  # noqa: E501
-                )
-                logger.debug('*' * 168)
+                logger.debug('\n' + '*' * 168 + '\n' + url + '\n' + '*' * 168)
 
                 while True:
                     # 1. Avoid to request dlc too frequently.
                     # 2. DLC job may not be ready immediately after creation.
                     for _ in range(20):
                         time.sleep(2)
                         try:
@@ -256,19 +261,22 @@
                         pri_time = pod_create_time
                         pod_create_time = datetime.datetime.strptime(
                             pod_create_time, '%Y-%m-%dT%H:%M:%SZ')
                     elasped_time = datetime.datetime.now() - initial_time
                     cur_time = (pod_create_time +
                                 elasped_time).strftime('%Y-%m-%dT%H:%M:%SZ')
                     logs_cmd = ('dlc logs'
-                                f' {job_id} {job_id}-worker-0'
+                                f' {job_id} {job_id}-master-0'
                                 f" -c {self.aliyun_cfg['dlc_config_path']}"
                                 f' --start_time {pri_time}'
                                 f' --end_time {cur_time}')
-                    log_output = subprocess.getoutput(logs_cmd)
+                    try:
+                        log_output = subprocess.getoutput(logs_cmd)
+                    except BlockingIOError:
+                        log_output = '[WARN] No logs found for the pod'
 
                     if '[WARN] No logs found for the pod' not in log_output:
                         pri_time = cur_time
                         stdout.write(log_output)
                         stdout.flush()
 
             return_code = _run_within_retry()
```

### Comparing `opencompass-0.2.4/opencompass/runners/local.py` & `opencompass-0.2.5/opencompass/runners/local.py`

 * *Files 2% similar despite different names*

```diff
@@ -42,25 +42,27 @@
             Defaults to 16.
         max_workers_per_gpu (int): Max number of workers to run for one GPU.
             Defaults to 1.
         debug (bool): Whether to run in debug mode.
         lark_bot_url (str): Lark bot url.
     """
 
-    def __init__(
-        self,
-        task: ConfigDict,
-        max_num_workers: int = 16,
-        debug: bool = False,
-        max_workers_per_gpu: int = 1,
-        lark_bot_url: str = None,
-    ):
+    def __init__(self,
+                 task: ConfigDict,
+                 max_num_workers: int = 16,
+                 debug: bool = False,
+                 max_workers_per_gpu: int = 1,
+                 lark_bot_url: str = None,
+                 **kwargs):
         super().__init__(task=task, debug=debug, lark_bot_url=lark_bot_url)
         self.max_num_workers = max_num_workers
         self.max_workers_per_gpu = max_workers_per_gpu
+        logger = get_logger()
+        for k, v in kwargs.items():
+            logger.warning(f'Ignored argument in {self.__module__}: {k}={v}')
 
     def launch(self, tasks: List[Dict[str, Any]]) -> List[Tuple[str, int]]:
         """Launch multiple tasks.
 
         Args:
             tasks (list[dict]): A list of task configs, usually generated by
                 Partitioner.
```

### Comparing `opencompass-0.2.4/opencompass/runners/local_api.py` & `opencompass-0.2.5/opencompass/runners/local_api.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/runners/slurm.py` & `opencompass-0.2.5/opencompass/runners/slurm.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/runners/slurm_sequential.py` & `opencompass-0.2.5/opencompass/runners/slurm_sequential.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/summarizers/circular.py` & `opencompass-0.2.5/opencompass/summarizers/circular.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/summarizers/default.py` & `opencompass-0.2.5/opencompass/summarizers/default.py`

 * *Files 2% similar despite different names*

```diff
@@ -13,15 +13,15 @@
 
 from opencompass.utils import (LarkReporter, dataset_abbr_from_cfg,
                                get_infer_output_path, get_logger,
                                model_abbr_from_cfg)
 from opencompass.utils.prompt import get_prompt_hash
 
 METRIC_WHITELIST = ['score', 'auc_score', 'accuracy', 'humaneval_pass@1', 'rouge1', 'avg_toxicity_score', 'bleurt_diff', 'matthews_correlation', 'truth', 'f1', 'exact_match']
-METRIC_BLACKLIST = ['bp', 'sys_len', 'ref_len', 'tool_rate']
+METRIC_BLACKLIST = ['bp', 'sys_len', 'ref_len']
 
 def model_abbr_from_cfg_used_in_summarizer(model):
     if model.get('summarizer_abbr', None):
         return model['summarizer_abbr']
     else:
         return model_abbr_from_cfg(model)
 
@@ -222,53 +222,55 @@
                 raw_results[model_abbr].setdefault(sg['name'], {}).update(scores)
                 parsed_results[model_abbr].setdefault(sg['name'], {}).update(result)
                 dataset_metrics.setdefault(sg['name'], []).extend(group_metrics)
                 dataset_eval_mode[sg['name']] = eval_mode
 
         return raw_results, parsed_results, dataset_metrics, dataset_eval_mode
 
-    def _format_table(self, parsed_results, dataset_metrics, dataset_eval_mode):
+    def _format_table(self, parsed_results, dataset_metrics, dataset_eval_mode, required_dataset_abbrs=None, skip_all_slash=False):
         dataset_abbrs = [dataset_abbr_from_cfg(dataset) for dataset in self.dataset_cfgs]
         prompt_version = {dataset_abbr_from_cfg(d): get_prompt_hash(d)[:6] for d in self.dataset_cfgs}
 
         summarizer_dataset_abbrs = []
-        if self.dataset_abbrs is None:
+        if required_dataset_abbrs is None:
             # display all dataset metrics included in the config
             for dataset_abbr in dataset_abbrs:
                 if dataset_abbr in dataset_metrics:
                     for metric in dataset_metrics[dataset_abbr]:
                         summarizer_dataset_abbrs.append((dataset_abbr, metric))
                 else:
                     summarizer_dataset_abbrs.append((dataset_abbr, None))
             # along with all possible group metrics
             for dataset_abbr in dataset_metrics:
                 for metric in dataset_metrics[dataset_abbr]:
                     if (dataset_abbr, metric) not in summarizer_dataset_abbrs:
                         summarizer_dataset_abbrs.append((dataset_abbr, metric))
         else:
             # follow the required order
-            for item in self.dataset_abbrs:
+            for item in required_dataset_abbrs:
                 if isinstance(item, str):
                     summarizer_dataset_abbrs.append((item, None))
                 elif isinstance(item, (list, tuple)):
                     summarizer_dataset_abbrs.append((item[0], item[1]))
 
         table = []
         header = ['dataset', 'version', 'metric', 'mode'] + self.model_abbrs
         table.append(header)
         for dataset_abbr, metric in summarizer_dataset_abbrs:
             if dataset_abbr not in dataset_metrics:
-                table.append([dataset_abbr, '-', '-', '-'] + ['-'] * len(self.model_abbrs))
+                if not skip_all_slash:
+                    table.append([dataset_abbr, '-', '-', '-'] + ['-'] * len(self.model_abbrs))
                 continue
             if metric is None:
                 metric = dataset_metrics[dataset_abbr][0]
             elif metric in dataset_metrics[dataset_abbr]:
                 pass
             else:
-                table.append([dataset_abbr, '-', '-', '-'] + ['-'] * len(self.model_abbrs))
+                if not skip_all_slash:
+                    table.append([dataset_abbr, '-', '-', '-'] + ['-'] * len(self.model_abbrs))
                 continue
 
             row = [dataset_abbr, prompt_version.get(dataset_abbr, '-'), metric, dataset_eval_mode.get(dataset_abbr, '-')]
             for model_abbr in self.model_abbrs:
                 if dataset_abbr in parsed_results[model_abbr]:
                     row.append('{:.02f}'.format(parsed_results[model_abbr][dataset_abbr][metric]))
                 else:
@@ -302,15 +304,15 @@
 
         output_dir = osp.split(output_path)[0]
         mmengine.mkdir_or_exist(output_dir)
         with open(output_path, 'w', encoding='utf-8') as f:
             text = f'{time_str}\n' + \
                     'tabulate format\n' + \
                     '^' * 128 + '\n' + \
-                    tabulate.tabulate(table, headers='firstrow') + '\n' + \
+                    tabulate.tabulate(table, headers='firstrow', floatfmt='.2f') + '\n' + \
                     '$' * 128 + '\n\n' + \
                     '-' * 128 + ' THIS IS A DIVIDER ' + '-' * 128 + '\n\n' + \
                     'csv format\n' + \
                     '^' * 128 + '\n' + \
                     '\n'.join([','.join(row) for row in table]) + '\n' + \
                     '$' * 128 + '\n\n' + \
                     '-' * 128 + ' THIS IS A DIVIDER ' + '-' * 128 + '\n\n' + \
@@ -334,21 +336,21 @@
         raw_results, parsed_results, dataset_metrics, dataset_eval_mode = self._pick_up_results()
 
         # calculate group metrics
         raw_results, parsed_results, dataset_metrics, dataset_eval_mode = \
             self._calculate_group_metrics(raw_results, parsed_results, dataset_metrics, dataset_eval_mode)
 
         # format table
-        table = self._format_table(parsed_results, dataset_metrics, dataset_eval_mode)
+        table = self._format_table(parsed_results, dataset_metrics, dataset_eval_mode, required_dataset_abbrs=self.dataset_abbrs)
 
         # format raw txt
         raw_txts = self._format_raw_txt(raw_results)
 
         # output to screen
-        print(tabulate.tabulate(table, headers='firstrow'))
+        print(tabulate.tabulate(table, headers='firstrow', floatfmt='.2f'))
 
         # output to .text / .csv files
         self._output_to_file(output_path, time_str, table, raw_txts)
 
         if self.lark_reporter:
             content = f'{getpass.getuser()} '
             content += f' {osp.abspath(output_path)}'
```

### Comparing `opencompass-0.2.4/opencompass/summarizers/multi_model.py` & `opencompass-0.2.5/opencompass/summarizers/multi_model.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/summarizers/needlebench.py` & `opencompass-0.2.5/opencompass/summarizers/needlebench.py`

 * *Files 0% similar despite different names*

```diff
@@ -68,15 +68,15 @@
     # Add more mappings as necessary
 }
 
 dataset_mapping_dict = {}
 
 needle_counts = ['2', '3', '4', '5']
 languages = ['en', 'zh']
-sizes = ['4k', '8k', '32k', '200k', '256k', '1000k']
+sizes = ['4k', '8k', '32k', '128k', '200k', '256k', '1000k']
 types = ['origin', 'parallel']
 
 for needle_count in needle_counts:
     for language in languages:
         for size in sizes:
             key = f'{needle_count}needle_{language}_{size}'
             value = f'{needle_count}-Needle-Reasoning-{language.upper()}-{size.upper()}'
```

### Comparing `opencompass-0.2.4/opencompass/summarizers/subjective/alignmentbench.py` & `opencompass-0.2.5/opencompass/summarizers/subjective/alignmentbench.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/summarizers/subjective/alpacaeval.py` & `opencompass-0.2.5/opencompass/summarizers/subjective/alpacaeval.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/summarizers/subjective/compass_arena.py` & `opencompass-0.2.5/opencompass/summarizers/subjective/compass_arena.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/summarizers/subjective/corev2.py` & `opencompass-0.2.5/opencompass/summarizers/subjective/corev2.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/summarizers/subjective/creationbench.py` & `opencompass-0.2.5/opencompass/summarizers/subjective/creationbench.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/summarizers/subjective/information_retrival.py` & `opencompass-0.2.5/opencompass/summarizers/subjective/information_retrival.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/summarizers/subjective/mtbench.py` & `opencompass-0.2.5/opencompass/summarizers/subjective/mtbench.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/summarizers/subjective/multiround.py` & `opencompass-0.2.5/opencompass/summarizers/subjective/multiround.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/summarizers/subjective/subjective_post_process.py` & `opencompass-0.2.5/opencompass/summarizers/subjective/subjective_post_process.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/summarizers/subjective/utils.py` & `opencompass-0.2.5/opencompass/summarizers/subjective/utils.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/summarizers/summarizer_pretrain.py` & `opencompass-0.2.5/opencompass/summarizers/summarizer_pretrain.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/tasks/base.py` & `opencompass-0.2.5/opencompass/tasks/base.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/tasks/llm_eval.py` & `opencompass-0.2.5/opencompass/tasks/llm_eval.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/tasks/openicl_attack.py` & `opencompass-0.2.5/opencompass/tasks/openicl_attack.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/tasks/openicl_eval.py` & `opencompass-0.2.5/opencompass/tasks/openicl_eval.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,18 +1,19 @@
 import argparse
 import copy
 import fnmatch
 import math
+import os
 import os.path as osp
 import re
 import statistics
+import sys
 import time
 from collections import Counter
 from inspect import signature
-from shutil import which
 from typing import List, Optional
 
 import mmengine
 from mmengine.config import Config, ConfigDict
 from mmengine.utils import mkdir_or_exist
 
 from opencompass.registry import (ICL_EVALUATORS, MODELS, TASKS,
@@ -72,16 +73,17 @@
         self.num_gpus = max(
             c.get('eval_cfg', {}).get('num_gpus', 0)
             for c in sum(self.dataset_cfgs, []))
         self.dump_details = cfg.get('eval', {}).get('runner', {}).get(
             'task', {}).get('dump_details', False)
 
     def get_command(self, cfg_path, template):
+        sys.path.append(os.getcwd())
         script_path = __file__
-        python = 'python3' if which('python3') else 'python'
+        python = sys.executable
         command = f'{python} {script_path} {cfg_path}'
         return template.format(task_cmd=command)
 
     def run(self):
         for model_cfg, dataset_cfgs in zip(self.model_cfgs, self.dataset_cfgs):
             for dataset_cfg in dataset_cfgs:
                 self.model_cfg = model_cfg
@@ -208,14 +210,16 @@
             icl_evaluator._out_dir = osp.splitext(out_path)[
                 0]  # strip extension
 
             preds['predictions'] = pred_strs
             preds['references'] = (test_set[self.output_column]
                                    if self.output_column else None)
             preds['test_set'] = test_set
+            if 'origin_prompt' not in preds:
+                preds['origin_prompt'] = [None for _ in range(len(pred_strs))]
             preds = {
                 k: preds[k]
                 for k in signature(icl_evaluator.score).parameters
             }
             result = icl_evaluator.score(**preds)
 
             if self.dump_details:
```

### Comparing `opencompass-0.2.4/opencompass/tasks/openicl_infer.py` & `opencompass-0.2.5/opencompass/tasks/openicl_infer.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,12 +1,13 @@
 import argparse
+import os
 import os.path as osp
 import random
+import sys
 import time
-from shutil import which
 from typing import Any
 
 from mmengine.config import Config, ConfigDict
 from mmengine.utils import mkdir_or_exist
 
 from opencompass.registry import (ICL_INFERENCERS, ICL_PROMPT_TEMPLATES,
                                   ICL_RETRIEVERS, TASKS)
@@ -38,27 +39,28 @@
         """Get the command template for the task.
 
         Args:
             cfg_path (str): The path to the config file of the task.
             template (str): The template which have '{task_cmd}' to format
                 the command.
         """
+        sys.path.append(os.getcwd())
         script_path = __file__
         backend_keys = ['VLLM', 'Lmdeploy']
         use_backend = any(
             key in str(self.model_cfgs[0].get('type', ''))
             or key in str(self.model_cfgs[0].get('llm', {}).get('type', ''))
             for key in backend_keys)
         if self.num_gpus > 0 and not use_backend:
             port = random.randint(12000, 32000)
             command = (f'torchrun --master_port={port} '
                        f'--nproc_per_node {self.num_procs} '
                        f'{script_path} {cfg_path}')
         else:
-            python = 'python3' if which('python3') else 'python'
+            python = sys.executable
             command = f'{python} {script_path} {cfg_path}'
 
         return template.format(task_cmd=command)
 
     def run(self, cur_model=None):
         self.logger.info(f'Task {task_abbr_from_cfg(self.cfg)}')
         for model_cfg, dataset_cfgs in zip(self.model_cfgs, self.dataset_cfgs):
```

### Comparing `opencompass-0.2.4/opencompass/tasks/subjective_eval.py` & `opencompass-0.2.5/opencompass/tasks/subjective_eval.py`

 * *Files 1% similar despite different names*

```diff
@@ -135,15 +135,16 @@
                 filename = osp.join(path, osp.basename(filename))
         # Get partition name
         root, ext = osp.splitext(filename)
         partial_filename = root + '_0' + ext
         # If no predictions get in predictions dir
         assert osp.exists(filename) or osp.exists(
             osp.realpath(partial_filename)
-        ), 'No predictions found for {filename}.'.format(filename=filename)
+        ), 'No predictions found for {filename} and {partial_filename}'.format(
+            filename=filename, partial_filename=partial_filename)
 
         # If use Naive partition in infer stage
         if osp.exists(osp.realpath(filename)):
             preds = mmengine.load(filename)
             pred_strs = [
                 preds[str(i)]['prediction'] for i in range(len(preds))
             ]
@@ -184,18 +185,22 @@
         ds_abbr = dataset_abbr_from_cfg(dataset_cfg)
         model_postprocessors = model_cfg.get('pred_postprocessor', {})
         pred_postprocessor = None
         for pattern in model_postprocessors.keys():
             if fnmatch.fnmatch(ds_abbr, pattern):
                 pred_postprocessor = model_postprocessors[pattern]
                 break
-        if 'pred_postprocessor' in eval_cfg or pred_postprocessor:
-            kwargs = pred_postprocessor or eval_cfg['pred_postprocessor']
+        if 'pred_postprocessor' in eval_cfg['evaluator'] or pred_postprocessor:
+            kwargs = pred_postprocessor or eval_cfg['evaluator'][
+                'pred_postprocessor']
             proc = TEXT_POSTPROCESSORS.get(kwargs.pop('type'))
+            self.logger.info('Get postprocessor {postprocessor}.')
             pred_strs = [proc(s, **kwargs) for s in pred_strs]
+        else:
+            self.logger.info('No postprocessor found.')
 
         return {
             'model_name': model_abbr_from_cfg(model_cfg),
             'model_preds': pred_strs
         }
 
     def _load_model_judgements(
```

### Comparing `opencompass-0.2.4/opencompass/utils/abbr.py` & `opencompass-0.2.5/opencompass/utils/abbr.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/utils/build.py` & `opencompass-0.2.5/opencompass/utils/build.py`

 * *Files 5% similar despite different names*

```diff
@@ -18,9 +18,8 @@
     model_cfg.pop('run_cfg', None)
     model_cfg.pop('max_out_len', None)
     model_cfg.pop('batch_size', None)
     model_cfg.pop('abbr', None)
     model_cfg.pop('summarizer_abbr', None)
     model_cfg.pop('pred_postprocessor', None)
     model_cfg.pop('min_out_len', None)
-    model_cfg.pop('tokenizer_only', None)
     return MODELS.build(model_cfg)
```

### Comparing `opencompass-0.2.4/opencompass/utils/dependency.py` & `opencompass-0.2.5/opencompass/utils/dependency.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/utils/fileio.py` & `opencompass-0.2.5/opencompass/utils/fileio.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/utils/lark.py` & `opencompass-0.2.5/opencompass/utils/lark.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/utils/logging.py` & `opencompass-0.2.5/opencompass/utils/logging.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/utils/menu.py` & `opencompass-0.2.5/opencompass/utils/menu.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass/utils/prompt.py` & `opencompass-0.2.5/opencompass/utils/prompt.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 from __future__ import annotations
 
 import hashlib
 import json
+import re
 from copy import deepcopy
 from typing import Dict, List, Union
 
 from mmengine.config import ConfigDict
 
 
 def safe_format(input_str: str, **kwargs) -> str:
@@ -15,17 +16,23 @@
     Args:
         input_str (str): The string to be formatted.
         **kwargs: The keyword arguments to be used for formatting.
 
     Returns:
         str: The formatted string.
     """
+    segs = [input_str]
     for k, v in kwargs.items():
-        input_str = input_str.replace(f'{{{k}}}', str(v))
-    return input_str
+        regex = re.compile(f'(?<={{{k}}})(?={{{k}}})|({{{k}}})')
+        segs = [regex.split(seg) for seg in segs]
+        segs = sum(segs, [])
+    replace_dict = {f'{{{k}}}': str(v) for k, v in kwargs.items()}
+    segs = [replace_dict.get(seg, seg) for seg in segs]
+    output_str = ''.join(segs)
+    return output_str
 
 
 def get_prompt_hash(dataset_cfg: Union[ConfigDict, List[ConfigDict]]) -> str:
     """Get the hash of the prompt configuration.
 
     Args:
         dataset_cfg (ConfigDict or list[ConfigDict]): The dataset
```

### Comparing `opencompass-0.2.4/opencompass/utils/run.py` & `opencompass-0.2.5/opencompass/utils/run.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,22 +1,29 @@
+# flake8: noqa
+# yapf: disable
 import os
-from typing import List, Union
+from typing import List, Tuple, Union
 
 import tabulate
 from mmengine.config import Config
 
 from opencompass.datasets.custom import make_custom_dataset_config
-from opencompass.models import VLLM, HuggingFaceCausalLM, TurboMindModel
-from opencompass.partitioners import NaivePartitioner, SizePartitioner
+from opencompass.models import (VLLM, HuggingFace, HuggingFaceBaseModel,
+                                HuggingFaceCausalLM, HuggingFaceChatGLM3,
+                                HuggingFacewithChatTemplate, TurboMindModel,
+                                TurboMindModelwithChatTemplate,
+                                VLLMwithChatTemplate)
+from opencompass.partitioners import NaivePartitioner, NumWorkerPartitioner
 from opencompass.runners import DLCRunner, LocalRunner, SlurmRunner
 from opencompass.tasks import OpenICLEvalTask, OpenICLInferTask
 from opencompass.utils import get_logger, match_files
 
 
-def match_cfg_file(workdir: str, pattern: Union[str, List[str]]) -> List[str]:
+def match_cfg_file(workdir: str,
+                   pattern: Union[str, List[str]]) -> List[Tuple[str, str]]:
     """Match the config file in workdir recursively given the pattern.
 
     Additionally, if the pattern itself points to an existing file, it will be
     directly returned.
     """
     if isinstance(pattern, str):
         pattern = [pattern]
@@ -66,40 +73,49 @@
     """Get the config object given args.
 
     Only a few argument combinations are accepted (priority from high to low)
     1. args.config
     2. args.models and args.datasets
     3. Huggingface parameter groups and args.datasets
     """
+    logger = get_logger()
     if args.config:
         config = Config.fromfile(args.config, format_python_code=False)
         config = try_fill_in_custom_cfgs(config)
         # set infer accelerator if needed
         if args.accelerator in ['vllm', 'lmdeploy']:
-            config['models'] = change_accelerator(config['models'],
-                                                  args.accelerator)
+            config['models'] = change_accelerator(config['models'], args.accelerator)
+            if config.get('eval', {}).get('partitioner', {}).get('models') is not None:
+                config['eval']['partitioner']['models'] = change_accelerator(config['eval']['partitioner']['models'], args.accelerator)
+            if config.get('eval', {}).get('partitioner', {}).get('base_models') is not None:
+                config['eval']['partitioner']['base_models'] = change_accelerator(config['eval']['partitioner']['base_models'], args.accelerator)
+            if config.get('eval', {}).get('partitioner', {}).get('compare_models') is not None:
+                config['eval']['partitioner']['compare_models'] = change_accelerator(config['eval']['partitioner']['compare_models'], args.accelerator)
+            if config.get('eval', {}).get('partitioner', {}).get('judge_models') is not None:
+                config['eval']['partitioner']['judge_models'] = change_accelerator(config['eval']['partitioner']['judge_models'], args.accelerator)
+            if config.get('judge_models', {}) is not None:
+                config['judge_models'] = change_accelerator(config['judge_models'], args.accelerator)
         return config
+
     # parse dataset args
     if not args.datasets and not args.custom_dataset_path:
-        raise ValueError('You must specify "--datasets" or '
-                         '"--custom-dataset-path" if you do not specify a '
-                         'config file path.')
+        raise ValueError('You must specify "--datasets" or "--custom-dataset-path" if you do not specify a config file path.')
     datasets = []
     if args.datasets:
         datasets_dir = os.path.join(args.config_dir, 'datasets')
         for dataset_arg in args.datasets:
             if '/' in dataset_arg:
                 dataset_name, dataset_suffix = dataset_arg.split('/', 1)
                 dataset_key_suffix = dataset_suffix
             else:
                 dataset_name = dataset_arg
                 dataset_key_suffix = '_datasets'
 
             for dataset in match_cfg_file(datasets_dir, [dataset_name]):
-                get_logger().info(f'Loading {dataset[0]}: {dataset[1]}')
+                logger.info(f'Loading {dataset[0]}: {dataset[1]}')
                 cfg = Config.fromfile(dataset[1])
                 for k in cfg.keys():
                     if k.endswith(dataset_key_suffix):
                         datasets += cfg[k]
     else:
         dataset = {'path': args.custom_dataset_path}
         if args.custom_dataset_infer_method is not None:
@@ -109,114 +125,103 @@
         if args.custom_dataset_meta_path is not None:
             dataset['meta_path'] = args.custom_dataset_meta_path
         dataset = make_custom_dataset_config(dataset)
         datasets.append(dataset)
 
     # parse model args
     if not args.models and not args.hf_path:
-        raise ValueError('You must specify a config file path, '
-                         'or specify --models and --datasets, or '
-                         'specify HuggingFace model parameters and '
-                         '--datasets.')
+        raise ValueError('You must specify a config file path, or specify --models and --datasets, or specify HuggingFace model parameters and --datasets.')
     models = []
     if args.models:
         model_dir = os.path.join(args.config_dir, 'models')
         for model in match_cfg_file(model_dir, args.models):
-            get_logger().info(f'Loading {model[0]}: {model[1]}')
+            logger.info(f'Loading {model[0]}: {model[1]}')
             cfg = Config.fromfile(model[1])
             if 'models' not in cfg:
-                raise ValueError(
-                    f'Config file {model[1]} does not contain "models" field')
+                raise ValueError(f'Config file {model[1]} does not contain "models" field')
             models += cfg['models']
     else:
-        from opencompass.models import HuggingFace
-        model = dict(type=f'{HuggingFace.__module__}.{HuggingFace.__name__}',
+        if args.hf_type == 'chat':
+            mod = HuggingFacewithChatTemplate
+        else:
+            mod = HuggingFaceBaseModel
+        model = dict(type=f'{mod.__module__}.{mod.__name__}',
+                     abbr=args.hf_path.split('/')[-1] + '_hf',
                      path=args.hf_path,
-                     peft_path=args.peft_path,
-                     tokenizer_path=args.tokenizer_path,
                      model_kwargs=args.model_kwargs,
+                     tokenizer_path=args.tokenizer_path,
                      tokenizer_kwargs=args.tokenizer_kwargs,
+                     peft_path=args.peft_path,
+                     peft_kwargs=args.peft_kwargs,
                      max_seq_len=args.max_seq_len,
                      max_out_len=args.max_out_len,
-                     batch_padding=not args.no_batch_padding,
                      batch_size=args.batch_size,
                      pad_token_id=args.pad_token_id,
-                     run_cfg=dict(num_gpus=args.num_gpus))
+                     stop_words=args.stop_words,
+                     run_cfg=dict(num_gpus=args.hf_num_gpus))
+        logger.debug(f'Using model: {model}')
         models.append(model)
     # set infer accelerator if needed
     if args.accelerator in ['vllm', 'lmdeploy']:
         models = change_accelerator(models, args.accelerator)
     # parse summarizer args
-    summarizer_arg = args.summarizer if args.summarizer is not None \
-        else 'example'
+    summarizer_arg = args.summarizer if args.summarizer is not None else 'example'
     summarizers_dir = os.path.join(args.config_dir, 'summarizers')
 
     # Check if summarizer_arg contains '/'
     if '/' in summarizer_arg:
         # If it contains '/', split the string by '/'
         # and use the second part as the configuration key
         summarizer_file, summarizer_key = summarizer_arg.split('/', 1)
     else:
         # If it does not contain '/', keep the original logic unchanged
         summarizer_key = 'summarizer'
         summarizer_file = summarizer_arg
 
     s = match_cfg_file(summarizers_dir, [summarizer_file])[0]
-    get_logger().info(f'Loading {s[0]}: {s[1]}')
+    logger.info(f'Loading {s[0]}: {s[1]}')
     cfg = Config.fromfile(s[1])
     # Use summarizer_key to retrieve the summarizer definition
     # from the configuration file
     summarizer = cfg[summarizer_key]
 
-    return Config(dict(models=models, datasets=datasets,
-                       summarizer=summarizer),
-                  format_python_code=False)
+    return Config(dict(models=models, datasets=datasets, summarizer=summarizer), format_python_code=False)
 
 
 def change_accelerator(models, accelerator):
     models = models.copy()
+    logger = get_logger()
     model_accels = []
     for model in models:
-        get_logger().info(f'Transforming {model["abbr"]} to {accelerator}')
+        logger.info(f'Transforming {model["abbr"]} to {accelerator}')
         # change HuggingFace model to VLLM or TurboMindModel
-        if model['type'] is HuggingFaceCausalLM:
+        if model['type'] in [HuggingFace, HuggingFaceCausalLM, HuggingFaceChatGLM3]:
             gen_args = dict()
             if model.get('generation_kwargs') is not None:
                 generation_kwargs = model['generation_kwargs'].copy()
-                gen_args['temperature'] = 0.001 if generation_kwargs.get(
-                    'temperature'
-                ) is None else generation_kwargs['temperature']
-                gen_args['top_k'] = 1 if generation_kwargs.get(
-                    'top_k') is None else generation_kwargs['top_k']
-                gen_args['top_p'] = 0.9 if generation_kwargs.get(
-                    'top_p') is None else generation_kwargs['top_p']
-                gen_args['stop_token_ids'] = None if generation_kwargs.get(
-                    'eos_token_id'
-                ) is None else generation_kwargs['eos_token_id']
-                generation_kwargs[
-                    'stop_token_ids'] = None if generation_kwargs.get(
-                        'eos_token_id'
-                    ) is None else generation_kwargs['eos_token_id']
+                gen_args['temperature'] = generation_kwargs.get('temperature', 0.001)
+                gen_args['top_k'] = generation_kwargs.get('top_k', 1)
+                gen_args['top_p'] = generation_kwargs.get('top_p', 0.9)
+                gen_args['stop_token_ids'] = generation_kwargs.get('eos_token_id', None)
+                generation_kwargs['stop_token_ids'] = generation_kwargs.get('eos_token_id', None)
                 generation_kwargs.pop('eos_token_id')
             else:
                 # if generation_kwargs is not provided, set default values
                 generation_kwargs = dict()
                 gen_args['temperature'] = 0.0
                 gen_args['top_k'] = 1
                 gen_args['top_p'] = 0.9
                 gen_args['stop_token_ids'] = None
 
             if accelerator == 'lmdeploy':
-                get_logger().info(
-                    f'Transforming {model["abbr"]} to {accelerator}')
-                model = dict(
-                    type=  # noqa E251
-                    f'{TurboMindModel.__module__}.{TurboMindModel.__name__}',
-                    abbr=model['abbr'].replace('hf', 'lmdeploy')
-                    if '-hf' in model['abbr'] else model['abbr'] + '-lmdeploy',
+                logger.info(f'Transforming {model["abbr"]} to {accelerator}')
+                mod = TurboMindModel
+                acc_model = dict(
+                    type=f'{mod.__module__}.{mod.__name__}',
+                    abbr=model['abbr'].replace('hf', 'turbomind') if '-hf' in model['abbr'] else model['abbr'] + '-turbomind',
                     path=model['path'],
                     engine_config=dict(session_len=model['max_seq_len'],
                                        max_batch_size=model['batch_size'],
                                        tp=model['run_cfg']['num_gpus']),
                     gen_config=dict(top_k=gen_args['top_k'],
                                     temperature=gen_args['temperature'],
                                     top_p=gen_args['top_p'],
@@ -226,73 +231,78 @@
                     max_seq_len=model['max_seq_len'],
                     batch_size=model['batch_size'],
                     concurrency=model['batch_size'],
                     run_cfg=model['run_cfg'],
                 )
                 for item in ['meta_template']:
                     if model.get(item) is not None:
-                        model.update(item, model[item])
+                        acc_model[item] = model[item]
             elif accelerator == 'vllm':
-                get_logger().info(
-                    f'Transforming {model["abbr"]} to {accelerator}')
+                logger.info(f'Transforming {model["abbr"]} to {accelerator}')
 
-                model = dict(
+                acc_model = dict(
                     type=f'{VLLM.__module__}.{VLLM.__name__}',
-                    abbr=model['abbr'].replace('hf', 'vllm')
-                    if '-hf' in model['abbr'] else model['abbr'] + '-vllm',
+                    abbr=model['abbr'].replace('hf', 'vllm') if '-hf' in model['abbr'] else model['abbr'] + '-vllm',
                     path=model['path'],
-                    model_kwargs=dict(
-                        tensor_parallel_size=model['run_cfg']['num_gpus']),
+                    model_kwargs=dict(tensor_parallel_size=model['run_cfg']['num_gpus']),
                     max_out_len=model['max_out_len'],
                     max_seq_len=model['max_seq_len'],
                     batch_size=model['batch_size'],
                     generation_kwargs=generation_kwargs,
                     run_cfg=model['run_cfg'],
                 )
                 for item in ['meta_template', 'end_str']:
                     if model.get(item) is not None:
-                        model.update(item, model[item])
-                generation_kwargs.update(
-                    dict(temperature=gen_args['temperature']))
+                        acc_model[item] = model[item]
+            else:
+                raise ValueError(f'Unsupported accelerator {accelerator} for model type {model["type"]}')
+        elif model['type'] in [HuggingFacewithChatTemplate]:
+            if accelerator == 'vllm':
+                mod = VLLMwithChatTemplate
+                acc_model = dict(
+                    type=f'{mod.__module__}.{mod.__name__}',
+                    abbr=model['abbr'].replace('hf', 'vllm') if '-hf' in model['abbr'] else model['abbr'] + '-vllm',
+                    path=model['path'],
+                    model_kwargs=dict(tensor_parallel_size=model['run_cfg']['num_gpus']),
+                    max_out_len=model['max_out_len'],
+                    batch_size=16,
+                    run_cfg=model['run_cfg'],
+                    stop_words=model.get('stop_words', []),
+                )
+            elif accelerator == 'lmdeploy':
+                mod = TurboMindModelwithChatTemplate
+                acc_model = dict(
+                    type=f'{mod.__module__}.{mod.__name__}',
+                    abbr=model['abbr'].replace('hf', 'turbomind') if '-hf' in model['abbr'] else model['abbr'] + '-turbomind',
+                    path=model['path'],
+                    engine_config=dict(max_batch_size=model.get('batch_size', 16), tp=model['run_cfg']['num_gpus']),
+                    gen_config=dict(top_k=1, temperature=1e-6, top_p=0.9),
+                    max_seq_len=model.get('max_seq_len', 2048),
+                    max_out_len=model['max_out_len'],
+                    batch_size=16,
+                    run_cfg=model['run_cfg'],
+                    stop_words=model.get('stop_words', []),
+                )
             else:
-                raise ValueError(f'Unsupported accelerator {accelerator}')
-        model_accels.append(model)
+                raise ValueError(f'Unsupported accelerator {accelerator} for model type {model["type"]}')
+        else:
+            acc_model = model
+            logger.warning(f'Unsupported model type {model["type"]}, will keep the original model.')
+        model_accels.append(acc_model)
     return model_accels
 
 
-def exec_mm_infer_runner(tasks, args, cfg):
-    """execute multimodal infer runner according to args."""
-    if args.slurm:
-        runner = SlurmRunner(dict(type='MultimodalInferTask'),
-                             max_num_workers=args.max_num_workers,
-                             partition=args.partition,
-                             quotatype=args.quotatype,
-                             retry=args.retry,
-                             debug=args.debug,
-                             lark_bot_url=cfg['lark_bot_url'])
-    elif args.dlc:
-        raise NotImplementedError('Currently, we do not support evaluating \
-                             multimodal models on dlc.')
-    else:
-        runner = LocalRunner(task=dict(type='MultimodalInferTask'),
-                             max_num_workers=args.max_num_workers,
-                             debug=args.debug,
-                             lark_bot_url=cfg['lark_bot_url'])
-    runner(tasks)
-
-
 def get_config_type(obj) -> str:
     return f'{obj.__module__}.{obj.__name__}'
 
 
 def fill_infer_cfg(cfg, args):
     new_cfg = dict(infer=dict(
-        partitioner=dict(type=get_config_type(SizePartitioner),
-                         max_task_size=args.max_partition_size,
-                         gen_task_coef=args.gen_task_coef),
+        partitioner=dict(type=get_config_type(NumWorkerPartitioner),
+                         num_worker=args.max_num_workers),
         runner=dict(
             max_num_workers=args.max_num_workers,
             debug=args.debug,
             task=dict(type=get_config_type(OpenICLInferTask)),
             lark_bot_url=cfg['lark_bot_url'],
         )), )
     if args.slurm:
```

### Comparing `opencompass-0.2.4/opencompass/utils/text_postprocessors.py` & `opencompass-0.2.5/opencompass/utils/text_postprocessors.py`

 * *Files 4% similar despite different names*

```diff
@@ -59,23 +59,23 @@
 
 def first_option_postprocess(text: str, options: str, cushion=True) -> str:
     """Find first valid option for text."""
 
     # yapf: disable
     # flake8: noqa: W605
     patterns = [
-        f'?\s?([{options}])',
-        f'?\s?([{options}])',
-        f'?\s?:([{options}])',
-        f'?\s?([{options}])',
-        f'?\s?([{options}])',
-        f'\s?([{options}])',
-        f'\s?([{options}])',
-        f'?\s?([{options}])',
-        f'?\s?([{options}])'
+        f'?\s*([{options}])',
+        f'?\s*\s*([{options}])',
+        f'?\s*:\s*([{options}])',
+        f'?\s*([{options}])',
+        f'?\s*([{options}])',
+        f'\s*([{options}])',
+        f'\s*([{options}])',
+        f'?\s*([{options}])',
+        f'?\s*([{options}])'
         f'??\s?([{options}])\s??',
         f'??\s?([{options}])\s??',
         f'??\s?([{options}])\s??',
         f'??\s?([{options}])\s?',
         f'????\s?([{options}])',
         f'????\s?([{options}])',
         f'???\s?([{options}])',
@@ -90,19 +90,19 @@
         f'[\s,:]([{options}])[\.]?$',
         f'[]\s?([{options}])[\.]?$',
         f'\s?([{options}])[\.]?$',
         f'\s?([{options}])[\.]?$',
         f'\s?(\S+)(?:|$)',
         f'\s?(\S+)(?:|$)',
         f'\s?(\S+)(?:|$)',
-        f'[Tt]he answer is \(?([{options}])\)?',
-        f'[Tt]he answer is option \(?([{options}])\)?',
-        f'[Tt]he correct answer is \(?([{options}])\)?',
-        f'[Tt]he correct answer is option \(?([{options}])\)?',
-        f'[Tt]he answer to the question is \(?([{options}])\)?',
+        f'[Tt]he answer is:?\s+\(?([{options}])\)?',
+        f'[Tt]he answer is option:?\s+\(?([{options}])\)?',
+        f'[Tt]he correct answer is:?\s+\(?([{options}])\)?',
+        f'[Tt]he correct answer is option:?\s+\(?([{options}])\)?',
+        f'[Tt]he answer to the question is:?\s+\(?([{options}])\)?',
         f'^\s?([{options}])',
         f'^([{options}])\s??',
         f'(\s|^)[{options}][\s,:\.$]',
         f'(\s|^)[{options}](\s|$)',
         f'1.\s?(.*?)$',
         f'1.\s?([{options}])[.$]?$',
     ]
@@ -112,15 +112,15 @@
     ]
     # flake8: noqa
     # yapf: enable
 
     if cushion:
         patterns.extend(cushion_patterns)
     for pattern in patterns:
-        match = re.search(pattern, text)
+        match = re.search(pattern, text, re.DOTALL)
         if match:
             outputs = match.group(0)
             for i in options:
                 if i in outputs:
                     return i
     return ''
 
@@ -178,7 +178,13 @@
 
     if postprocess:
         if isinstance(postprocess, str):
             postprocess = TEXT_POSTPROCESSORS.get(postprocess)
         return postprocess(text, **kwargs)
     else:
         return text
+
+
+def match_answer_pattern(response_text: str, answer_pattern: str):
+    match = re.search(answer_pattern, response_text)
+    extracted_answer = match.group(1) if match else ''
+    return extracted_answer
```

### Comparing `opencompass-0.2.4/opencompass/utils/types.py` & `opencompass-0.2.5/opencompass/utils/types.py`

 * *Files identical despite different names*

### Comparing `opencompass-0.2.4/opencompass.egg-info/PKG-INFO` & `opencompass-0.2.5/opencompass.egg-info/PKG-INFO`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: opencompass
-Version: 0.2.4
+Version: 0.2.5
 Summary: A comprehensive toolkit for large model evaluation
 Home-page: https://github.com/open-compass/opencompass
 Author: OpenCompass Contributors
 Maintainer: OpenCompass Authors
 License: Apache License 2.0
 Description: <div align="center">
           <img src="docs/en/_static/image/logo.svg" width="500px"/>
@@ -74,14 +74,19 @@
         > **Attention**<br />
         > We launch the OpenCompass Collaboration project, welcome to support diverse evaluation benchmarks into OpenCompass!
         > Clike [Issue](https://github.com/open-compass/opencompass/issues/248) for more information.
         > Let's work together to build a more powerful OpenCompass toolkit!
         
         ##  What's New <a><img width="35" height="20" src="https://user-images.githubusercontent.com/12782558/212848161-5e783dd6-11e8-4fe0-bbba-39ffb77730be.png"></a>
         
+        - **\[2024.05.08\]** We supported the evaluation of 4 MoE models: [Mixtral-8x22B-v0.1](configs/models/mixtral/hf_mixtral_8x22b_v0_1.py), [Mixtral-8x22B-Instruct-v0.1](configs/models/mixtral/hf_mixtral_8x22b_instruct_v0_1.py), [Qwen1.5-MoE-A2.7B](configs/models/qwen/hf_qwen1_5_moe_a2_7b.py), [Qwen1.5-MoE-A2.7B-Chat](configs/models/qwen/hf_qwen1_5_moe_a2_7b_chat.py). Try them out now!
+        - **\[2024.04.30\]** We supported evaluating a model's compression efficiency by calculating its Bits per Character (BPC) metric on an [external corpora](configs/datasets/llm_compression/README.md) ([official paper](https://github.com/hkust-nlp/llm-compression-intelligence)). Check out the [llm-compression](configs/eval_llm_compression.py) evaluation config now! 
+        - **\[2024.04.29\]** We report the performance of several famous LLMs on the common benchmarks, welcome to [documentation](https://opencompass.readthedocs.io/en/latest/user_guides/corebench.html) for more information! .
+        - **\[2024.04.26\]** We deprecated the multi-madality evaluating function from OpenCompass, related implement has moved to [VLMEvalKit](https://github.com/open-compass/VLMEvalKit), welcome to use! .
+        - **\[2024.04.26\]** We supported the evaluation of [ArenaHard](configs/eval_subjective_arena_hard.py)  welcome to try!.
         - **\[2024.04.22\]** We supported the evaluation of [LLaMA3](configs/models/hf_llama/hf_llama3_8b.py)  [LLaMA3-Instruct](configs/models/hf_llama/hf_llama3_8b_instruct.py), welcome to try! 
         - **\[2024.02.29\]** We supported the MT-Bench, AlpacalEval and AlignBench, more information can be found [here](https://opencompass.readthedocs.io/en/latest/advanced_guides/subjective_evaluation.html)
         - **\[2024.01.30\]** We release OpenCompass 2.0. Click  [CompassKit](https://github.com/open-compass), [CompassHub](https://hub.opencompass.org.cn/home), and [CompassRank](https://rank.opencompass.org.cn/home) for more information !
         
         > [More](docs/en/notes/news.md)
         
         ##  Introduction
@@ -126,15 +131,15 @@
         
         ```bash
         conda create -n opencompass python=3.10 pytorch torchvision torchaudio cpuonly -c pytorch -y
         conda activate opencompass
         git clone https://github.com/open-compass/opencompass opencompass
         cd opencompass
         pip install -e .
-        # also please install requiresments packages via `pip install -r requirements/api.txt` for API models if needed.
+        # also please install requirements packages via `pip install -r requirements/api.txt` for API models if needed.
         ```
         
         ###  Data Preparation
         
         ```bash
         # Download dataset to data/ folder
         wget https://github.com/open-compass/opencompass/releases/download/0.2.2.rc1/OpenCompassData-core-20240207.zip
@@ -161,27 +166,21 @@
         # List all configurations related to llama and mmlu
         python tools/list_configs.py llama mmlu
         ```
         
         You can also evaluate other HuggingFace models via command line. Taking LLaMA-7b as an example:
         
         ```bash
-        python run.py --datasets ceval_ppl mmlu_ppl \
-        --hf-path huggyllama/llama-7b \  # HuggingFace model path
-        --model-kwargs device_map='auto' \  # Arguments for model construction
-        --tokenizer-kwargs padding_side='left' truncation='left' use_fast=False \  # Arguments for tokenizer construction
-        --max-out-len 100 \  # Maximum number of tokens generated
-        --max-seq-len 2048 \  # Maximum sequence length the model can accept
-        --batch-size 8 \  # Batch size
-        --no-batch-padding \  # Don't enable batch padding, infer through for loop to avoid performance loss
-        --num-gpus 1  # Number of minimum required GPUs
+        python run.py --datasets ceval_ppl mmlu_ppl --hf-type base --hf-path huggyllama/llama-7b
         ```
         
-        > **Note**<br />
-        > To run the command above, you will need to remove the comments starting from `# ` first.
+        > \[!TIP\]
+        >
+        > configuration with `_ppl` is designed for base model typically.
+        > configuration with `_gen` can be used for both base model and chat model.
         
         Through the command line or configuration files, OpenCompass also supports evaluating APIs or custom models, as well as more diversified evaluation strategies. Please read the [Quick Start](https://opencompass.readthedocs.io/en/latest/get_started/quick_start.html) to learn how to run an evaluation task.
         
         <p align="right"><a href="#top">Back to top</a></p>
         
         ##  Dataset Support
```

#### html2text {}

```diff
@@ -1,8 +1,8 @@
-Metadata-Version: 2.1 Name: opencompass Version: 0.2.4 Summary: A comprehensive
+Metadata-Version: 2.1 Name: opencompass Version: 0.2.5 Summary: A comprehensive
 toolkit for large model evaluation Home-page: https://github.com/open-compass/
 opencompass Author: OpenCompass Contributors Maintainer: OpenCompass Authors
 License: Apache License 2.0 Description:
                        [docs/en/_static/image/logo.svg]
 
   [![][github-release-shield]][github-release-link] [![][github-releasedate-
   shield]][github-releasedate-link] [![][github-contributors-shield]][github-
@@ -51,16 +51,34 @@
 AI**, click [Get Started](https://ai.meta.com/llama/get-started/#validation) of
 Llama for more information. > **Attention**
 > We launch the OpenCompass Collaboration project, welcome to support diverse
 evaluation benchmarks into OpenCompass! > Clike [Issue](https://github.com/
 open-compass/opencompass/issues/248) for more information. > Let's work
 together to build a more powerful OpenCompass toolkit! ##  What's New
 [https://user-images.githubusercontent.com/12782558/212848161-5e783dd6-11e8-
-4fe0-bbba-39ffb77730be.png]- **\[2024.04.22\]** We supported the evaluation of
-[LLaMA3](configs/models/hf_llama/hf_llama3_8b.py)  [LLaMA3-Instruct]
+4fe0-bbba-39ffb77730be.png]- **\[2024.05.08\]** We supported the evaluation of
+4 MoE models: [Mixtral-8x22B-v0.1](configs/models/mixtral/
+hf_mixtral_8x22b_v0_1.py), [Mixtral-8x22B-Instruct-v0.1](configs/models/
+mixtral/hf_mixtral_8x22b_instruct_v0_1.py), [Qwen1.5-MoE-A2.7B](configs/models/
+qwen/hf_qwen1_5_moe_a2_7b.py), [Qwen1.5-MoE-A2.7B-Chat](configs/models/qwen/
+hf_qwen1_5_moe_a2_7b_chat.py). Try them out now! - **\[2024.04.30\]** We
+supported evaluating a model's compression efficiency by calculating its Bits
+per Character (BPC) metric on an [external corpora](configs/datasets/
+llm_compression/README.md) ([official paper](https://github.com/hkust-nlp/llm-
+compression-intelligence)). Check out the [llm-compression](configs/
+eval_llm_compression.py) evaluation config now!  - **\
+[2024.04.29\]** We report the performance of several famous LLMs on the common
+benchmarks, welcome to [documentation](https://opencompass.readthedocs.io/en/
+latest/user_guides/corebench.html) for more information! . - **\
+[2024.04.26\]** We deprecated the multi-madality evaluating function from
+OpenCompass, related implement has moved to [VLMEvalKit](https://github.com/
+open-compass/VLMEvalKit), welcome to use! . - **\[2024.04.26\]** We
+supported the evaluation of [ArenaHard](configs/eval_subjective_arena_hard.py)
+welcome to try!. - **\[2024.04.22\]** We supported the evaluation
+of [LLaMA3](configs/models/hf_llama/hf_llama3_8b.py)  [LLaMA3-Instruct]
 (configs/models/hf_llama/hf_llama3_8b_instruct.py), welcome to try!
  - **\[2024.02.29\]** We supported the MT-Bench, AlpacalEval and
 AlignBench, more information can be found [here](https://
 opencompass.readthedocs.io/en/latest/advanced_guides/
 subjective_evaluation.html) - **\[2024.01.30\]** We release OpenCompass 2.0.
 Click [CompassKit](https://github.com/open-compass), [CompassHub](https://
 hub.opencompass.org.cn/home), and [CompassRank](https://
@@ -93,15 +111,15 @@
 ```bash conda create --name opencompass python=3.10 pytorch torchvision
 pytorch-cuda -c nvidia -c pytorch -y conda activate opencompass git clone
 https://github.com/open-compass/opencompass opencompass cd opencompass pip
 install -e . ``` #### API Models with CPU-only ```bash conda create -
 n opencompass python=3.10 pytorch torchvision torchaudio cpuonly -c pytorch -
 y conda activate opencompass git clone https://github.com/open-compass/
 opencompass opencompass cd opencompass pip install -e . # also please install
-requiresments packages via `pip install -r requirements/api.txt` for API models
+requirements packages via `pip install -r requirements/api.txt` for API models
 if needed. ``` ###  Data Preparation ```bash # Download dataset to data/
 folder wget https://github.com/open-compass/opencompass/releases/download/
 0.2.2.rc1/OpenCompassData-core-20240207.zip unzip OpenCompassData-core-
 20240207.zip ``` Some third-party features, like Humaneval and Llama, may
 require additional steps to work properly, for detailed steps please refer to
 the [Installation Guide](https://opencompass.readthedocs.io/en/latest/
 get_started/installation.html).
@@ -112,27 +130,20 @@
 following command: ```bash python run.py --models hf_llama_7b --datasets
 mmlu_ppl ceval_ppl ``` OpenCompass has predefined configurations for many
 models and datasets. You can list all available model and dataset
 configurations using the [tools](./docs/en/tools.md#list-configs). ```bash #
 List all configurations python tools/list_configs.py # List all configurations
 related to llama and mmlu python tools/list_configs.py llama mmlu ``` You can
 also evaluate other HuggingFace models via command line. Taking LLaMA-7b as an
-example: ```bash python run.py --datasets ceval_ppl mmlu_ppl \ --hf-path
-huggyllama/llama-7b \ # HuggingFace model path --model-kwargs device_map='auto'
-\ # Arguments for model construction --tokenizer-kwargs padding_side='left'
-truncation='left' use_fast=False \ # Arguments for tokenizer construction --
-max-out-len 100 \ # Maximum number of tokens generated --max-seq-len 2048 \ #
-Maximum sequence length the model can accept --batch-size 8 \ # Batch size --
-no-batch-padding \ # Don't enable batch padding, infer through for loop to
-avoid performance loss --num-gpus 1 # Number of minimum required GPUs ``` >
-**Note**
-> To run the command above, you will need to remove the comments starting from
-`# ` first. Through the command line or configuration files, OpenCompass also
-supports evaluating APIs or custom models, as well as more diversified
-evaluation strategies. Please read the [Quick Start](https://
+example: ```bash python run.py --datasets ceval_ppl mmlu_ppl --hf-type base --
+hf-path huggyllama/llama-7b ``` > \[!TIP\] > > configuration with `_ppl` is
+designed for base model typically. > configuration with `_gen` can be used for
+both base model and chat model. Through the command line or configuration
+files, OpenCompass also supports evaluating APIs or custom models, as well as
+more diversified evaluation strategies. Please read the [Quick Start](https://
 opencompass.readthedocs.io/en/latest/get_started/quick_start.html) to learn how
 to run an evaluation task.
                                                                 _____B_a_c_k_ _t_o_ _t_o_p
 ##  Dataset Support
         LLaanngguuaaggee           KKnnoowwlleeddggee           RReeaassoonniinngg          EExxaammiinnaattiioonn
 WWoorrdd DDeeffiinniittiioonn -   KKnnoowwlleeddggee QQuueessttiioonn  TTeexxttuuaall EEnnttaaiillmmeenntt  JJuunniioorr HHiigghh,, HHiigghh
 WiC - SummEdits     AAnnsswweerriinngg - BoolQ - - CMNLI - OCNLI -   SScchhooooll,, UUnniivveerrssiittyy,,
```

### Comparing `opencompass-0.2.4/opencompass.egg-info/SOURCES.txt` & `opencompass-0.2.5/opencompass.egg-info/SOURCES.txt`

 * *Files 2% similar despite different names*

```diff
@@ -6,14 +6,15 @@
 opencompass.egg-info/SOURCES.txt
 opencompass.egg-info/dependency_links.txt
 opencompass.egg-info/entry_points.txt
 opencompass.egg-info/requires.txt
 opencompass.egg-info/top_level.txt
 opencompass/datasets/FinanceIQ.py
 opencompass/datasets/GaokaoBench.py
+opencompass/datasets/MMLUArabic.py
 opencompass/datasets/OpenFinData.py
 opencompass/datasets/QuALITY.py
 opencompass/datasets/__init__.py
 opencompass/datasets/advglue.py
 opencompass/datasets/afqmcd.py
 opencompass/datasets/anli.py
 opencompass/datasets/anthropics_evals.py
@@ -23,14 +24,15 @@
 opencompass/datasets/base.py
 opencompass/datasets/bbh.py
 opencompass/datasets/boolq.py
 opencompass/datasets/bustum.py
 opencompass/datasets/c3.py
 opencompass/datasets/cb.py
 opencompass/datasets/ceval.py
+opencompass/datasets/charm.py
 opencompass/datasets/chembench.py
 opencompass/datasets/chid.py
 opencompass/datasets/cibench.py
 opencompass/datasets/circular.py
 opencompass/datasets/civilcomments.py
 opencompass/datasets/clozeTest_maxmin.py
 opencompass/datasets/cluewsc.py
@@ -44,17 +46,19 @@
 opencompass/datasets/crowspairs.py
 opencompass/datasets/crowspairs_cn.py
 opencompass/datasets/csl.py
 opencompass/datasets/custom.py
 opencompass/datasets/cvalues.py
 opencompass/datasets/drcd.py
 opencompass/datasets/drop.py
+opencompass/datasets/drop_simple_eval.py
 opencompass/datasets/ds1000.py
 opencompass/datasets/ds1000_interpreter.py
 opencompass/datasets/eprstmt.py
+opencompass/datasets/flames.py
 opencompass/datasets/flores.py
 opencompass/datasets/game24.py
 opencompass/datasets/govrepcrs.py
 opencompass/datasets/gpqa.py
 opencompass/datasets/gsm8k.py
 opencompass/datasets/gsm_hard.py
 opencompass/datasets/hellaswag.py
@@ -65,35 +69,38 @@
 opencompass/datasets/hungarian_math.py
 opencompass/datasets/iwslt2017.py
 opencompass/datasets/jigsawmultilingual.py
 opencompass/datasets/jsonl.py
 opencompass/datasets/kaoshi.py
 opencompass/datasets/lambada.py
 opencompass/datasets/lcsts.py
+opencompass/datasets/llm_compression.py
 opencompass/datasets/lmeval.py
 opencompass/datasets/mastermath2024v1.py
 opencompass/datasets/math.py
 opencompass/datasets/math401.py
 opencompass/datasets/math_intern.py
 opencompass/datasets/mathbench.py
 opencompass/datasets/mbpp.py
+opencompass/datasets/mgsm.py
 opencompass/datasets/mmlu.py
 opencompass/datasets/multirc.py
 opencompass/datasets/narrativeqa.py
 opencompass/datasets/natural_question.py
 opencompass/datasets/natural_question_cn.py
 opencompass/datasets/obqa.py
 opencompass/datasets/piqa.py
 opencompass/datasets/py150.py
 opencompass/datasets/qasper.py
 opencompass/datasets/qaspercut.py
 opencompass/datasets/race.py
 opencompass/datasets/realtoxicprompts.py
 opencompass/datasets/record.py
 opencompass/datasets/rolebench.py
+opencompass/datasets/s3eval.py
 opencompass/datasets/safety.py
 opencompass/datasets/scibench.py
 opencompass/datasets/siqa.py
 opencompass/datasets/squad20.py
 opencompass/datasets/storycloze.py
 opencompass/datasets/strategyqa.py
 opencompass/datasets/summedits.py
@@ -221,15 +228,17 @@
 opencompass/datasets/medbench/medbench.py
 opencompass/datasets/medbench/post_process.py
 opencompass/datasets/medbench/utils.py
 opencompass/datasets/reasonbench/ReasonBenchDataset.py
 opencompass/datasets/reasonbench/__init__.py
 opencompass/datasets/subjective/__init__.py
 opencompass/datasets/subjective/alignbench.py
+opencompass/datasets/subjective/arena_hard.py
 opencompass/datasets/subjective/compass_arena.py
+opencompass/datasets/subjective/compassbench.py
 opencompass/datasets/subjective/corev2.py
 opencompass/datasets/subjective/creationbench.py
 opencompass/datasets/subjective/information_retrival.py
 opencompass/datasets/subjective/mtbench.py
 opencompass/datasets/subjective/multiround.py
 opencompass/datasets/subjective/subjective_cmp.py
 opencompass/datasets/teval/__init__.py
@@ -253,17 +262,19 @@
 opencompass/models/ai360_api.py
 opencompass/models/alaya.py
 opencompass/models/baichuan_api.py
 opencompass/models/baidu_api.py
 opencompass/models/base.py
 opencompass/models/base_api.py
 opencompass/models/bytedance_api.py
+opencompass/models/deepseek_api.py
 opencompass/models/gemini_api.py
 opencompass/models/glm.py
 opencompass/models/huggingface.py
+opencompass/models/huggingface_above_v4_33.py
 opencompass/models/hunyuan_api.py
 opencompass/models/intern_model.py
 opencompass/models/krgpt_api.py
 opencompass/models/lagent.py
 opencompass/models/langchain.py
 opencompass/models/lightllm_api.py
 opencompass/models/llama2.py
@@ -275,33 +286,38 @@
 opencompass/models/modelscope.py
 opencompass/models/moonshot_api.py
 opencompass/models/nanbeige_api.py
 opencompass/models/openai_api.py
 opencompass/models/pangu_api.py
 opencompass/models/qwen_api.py
 opencompass/models/sensetime_api.py
+opencompass/models/stepfun_api.py
 opencompass/models/turbomind.py
 opencompass/models/turbomind_api.py
 opencompass/models/turbomind_tis.py
+opencompass/models/turbomind_with_tf_above_v4_33.py
 opencompass/models/unigpt_api.py
 opencompass/models/vllm.py
+opencompass/models/vllm_with_tf_above_v4_33.py
 opencompass/models/xunfei_api.py
 opencompass/models/yayi_api.py
+opencompass/models/yi_api.py
 opencompass/models/zhipuai_api.py
 opencompass/models/zhipuai_v2_api.py
 opencompass/models/claude_api/__init__.py
 opencompass/models/claude_api/claude_api.py
 opencompass/models/claude_api/postprocessors.py
 opencompass/openicl/__init__.py
 opencompass/openicl/icl_dataset_reader.py
 opencompass/openicl/icl_prompt_template.py
 opencompass/openicl/icl_evaluator/__init__.py
 opencompass/openicl/icl_evaluator/icl_agent_evaluator.py
 opencompass/openicl/icl_evaluator/icl_aucroc_evaluator.py
 opencompass/openicl/icl_evaluator/icl_base_evaluator.py
+opencompass/openicl/icl_evaluator/icl_bpc_evaluator.py
 opencompass/openicl/icl_evaluator/icl_circular_evaluator.py
 opencompass/openicl/icl_evaluator/icl_em_evaluator.py
 opencompass/openicl/icl_evaluator/icl_hf_evaluator.py
 opencompass/openicl/icl_evaluator/icl_jieba_rouge_evaluator.py
 opencompass/openicl/icl_evaluator/icl_misc_evaluator.py
 opencompass/openicl/icl_evaluator/icl_plugin_evaluator.py
 opencompass/openicl/icl_evaluator/icl_toxic_evaluator.py
@@ -314,14 +330,15 @@
 opencompass/openicl/icl_inferencer/icl_clp_inferencer.py
 opencompass/openicl/icl_inferencer/icl_gen_inferencer.py
 opencompass/openicl/icl_inferencer/icl_ll_inferencer.py
 opencompass/openicl/icl_inferencer/icl_mink_percent_inferencer.py
 opencompass/openicl/icl_inferencer/icl_ppl_inferencer.py
 opencompass/openicl/icl_inferencer/icl_ppl_only_inferencer.py
 opencompass/openicl/icl_inferencer/icl_sc_inferencer.py
+opencompass/openicl/icl_inferencer/icl_sw_ce_loss_inferencer.py
 opencompass/openicl/icl_inferencer/icl_tot_inferencer.py
 opencompass/openicl/icl_retriever/__init__.py
 opencompass/openicl/icl_retriever/icl_base_retriever.py
 opencompass/openicl/icl_retriever/icl_bm25_retriever.py
 opencompass/openicl/icl_retriever/icl_dpp_retriever.py
 opencompass/openicl/icl_retriever/icl_fix_k_retriever.py
 opencompass/openicl/icl_retriever/icl_mdl_retriever.py
@@ -329,15 +346,14 @@
 opencompass/openicl/icl_retriever/icl_topk_retriever.py
 opencompass/openicl/icl_retriever/icl_votek_retriever.py
 opencompass/openicl/icl_retriever/icl_zero_retriever.py
 opencompass/openicl/utils/__init__.py
 opencompass/openicl/utils/logging.py
 opencompass/partitioners/__init__.py
 opencompass/partitioners/base.py
-opencompass/partitioners/mm_naive.py
 opencompass/partitioners/naive.py
 opencompass/partitioners/num_worker.py
 opencompass/partitioners/size.py
 opencompass/partitioners/sub_naive.py
 opencompass/partitioners/sub_size.py
 opencompass/runners/__init__.py
 opencompass/runners/base.py
@@ -345,32 +361,37 @@
 opencompass/runners/local.py
 opencompass/runners/local_api.py
 opencompass/runners/slurm.py
 opencompass/runners/slurm_sequential.py
 opencompass/summarizers/__init__.py
 opencompass/summarizers/circular.py
 opencompass/summarizers/default.py
+opencompass/summarizers/llm_compression.py
+opencompass/summarizers/multi_faceted.py
 opencompass/summarizers/multi_model.py
 opencompass/summarizers/needlebench.py
 opencompass/summarizers/summarizer_pretrain.py
 opencompass/summarizers/subjective/__init__.py
 opencompass/summarizers/subjective/alignmentbench.py
+opencompass/summarizers/subjective/all_obj.py
 opencompass/summarizers/subjective/alpacaeval.py
+opencompass/summarizers/subjective/arenahard.py
 opencompass/summarizers/subjective/compass_arena.py
+opencompass/summarizers/subjective/compassbench.py
 opencompass/summarizers/subjective/corev2.py
 opencompass/summarizers/subjective/creationbench.py
+opencompass/summarizers/subjective/flames.py
 opencompass/summarizers/subjective/information_retrival.py
 opencompass/summarizers/subjective/mtbench.py
 opencompass/summarizers/subjective/multiround.py
 opencompass/summarizers/subjective/subjective_post_process.py
 opencompass/summarizers/subjective/utils.py
 opencompass/tasks/__init__.py
 opencompass/tasks/base.py
 opencompass/tasks/llm_eval.py
-opencompass/tasks/mm_infer.py
 opencompass/tasks/openicl_attack.py
 opencompass/tasks/openicl_eval.py
 opencompass/tasks/openicl_infer.py
 opencompass/tasks/subjective_eval.py
 opencompass/utils/__init__.py
 opencompass/utils/abbr.py
 opencompass/utils/auxiliary.py
```

### Comparing `opencompass-0.2.4/setup.py` & `opencompass-0.2.5/setup.py`

 * *Files identical despite different names*

