# Comparing `tmp/onnxruntime_extensions-0.8.0-cp39-cp39-win_amd64.whl.zip` & `tmp/onnxruntime_extensions-0.9.0-cp39-cp39-win_amd64.whl.zip`

## zipinfo {}

```diff
@@ -1,48 +1,50 @@
-Zip file size: 1743650 bytes, number of entries: 46
-drwx---     6.3 fat        0 bx stor 23-May-23 21:44 onnxruntime_extensions/
-drwx---     6.3 fat        0 bx stor 23-May-23 21:44 onnxruntime_extensions-0.8.0.dist-info/
--rw-rw-rw-  2.0 fat     1162 b- defN 23-May-23 21:12 onnxruntime_extensions-0.8.0.dist-info/LICENSE
--rw-rw-rw-  2.0 fat     5224 b- defN 23-May-23 21:12 onnxruntime_extensions-0.8.0.dist-info/METADATA
--rw-rw-r--  2.0 fat     3935 b- defN 23-May-23 21:12 onnxruntime_extensions-0.8.0.dist-info/RECORD
--rw-rw-rw-  2.0 fat       23 b- defN 23-May-23 20:59 onnxruntime_extensions-0.8.0.dist-info/top_level.txt
--rw-rw-rw-  2.0 fat      100 b- defN 23-May-23 21:12 onnxruntime_extensions-0.8.0.dist-info/WHEEL
--rw-rw-rw-  2.0 fat     1639 b- defN 23-May-23 20:25 onnxruntime_extensions/cmake_helper.py
--rw-rw-rw-  2.0 fat     2124 b- defN 23-May-23 20:25 onnxruntime_extensions/cmd.py
--rw-rw-rw-  2.0 fat     2806 b- defN 23-May-23 20:25 onnxruntime_extensions/cvt.py
-drwx---     6.3 fat        0 bx stor 23-May-23 21:44 onnxruntime_extensions/onnxprocess/
--rw-rw-rw-  2.0 fat      859 b- defN 23-May-23 20:25 onnxruntime_extensions/onnxprocess/torch_wrapper.py
--rw-rw-rw-  2.0 fat     1844 b- defN 23-May-23 20:25 onnxruntime_extensions/onnxprocess/_builder.py
--rw-rw-rw-  2.0 fat    73255 b- defN 23-May-23 20:25 onnxruntime_extensions/onnxprocess/_onnx_ops.py
--rw-rw-rw-  2.0 fat    15158 b- defN 23-May-23 20:25 onnxruntime_extensions/onnxprocess/_session.py
--rw-rw-rw-  2.0 fat    25410 b- defN 23-May-23 20:25 onnxruntime_extensions/onnxprocess/_tensor.py
--rw-rw-rw-  2.0 fat      534 b- defN 23-May-23 20:25 onnxruntime_extensions/onnxprocess/__init__.py
-drwx---     6.3 fat        0 bx stor 23-May-23 21:44 onnxruntime_extensions/pnp/
--rw-rw-rw-  2.0 fat     3928 b- defN 23-May-23 20:25 onnxruntime_extensions/pnp/_base.py
--rw-rw-rw-  2.0 fat     2452 b- defN 23-May-23 20:25 onnxruntime_extensions/pnp/_imagenet.py
--rw-rw-rw-  2.0 fat     7420 b- defN 23-May-23 20:25 onnxruntime_extensions/pnp/_nlp.py
--rw-rw-rw-  2.0 fat    74398 b- defN 23-May-23 20:25 onnxruntime_extensions/pnp/_onnx_ops.py
--rw-rw-rw-  2.0 fat    11927 b- defN 23-May-23 20:25 onnxruntime_extensions/pnp/_torchext.py
--rw-rw-rw-  2.0 fat     1649 b- defN 23-May-23 20:25 onnxruntime_extensions/pnp/_unifier.py
--rw-rw-rw-  2.0 fat    13061 b- defN 23-May-23 20:25 onnxruntime_extensions/pnp/_utils.py
--rw-rw-rw-  2.0 fat      495 b- defN 23-May-23 20:25 onnxruntime_extensions/pnp/__init__.py
-drwx---     6.3 fat        0 bx stor 23-May-23 21:44 onnxruntime_extensions/tools/
--rw-rw-rw-  2.0 fat    23211 b- defN 23-May-23 20:25 onnxruntime_extensions/tools/add_pre_post_processing_to_model.py
-drwx---     6.3 fat        0 bx stor 23-May-23 21:44 onnxruntime_extensions/tools/pre_post_processing/
--rw-rw-rw-  2.0 fat    19445 b- defN 23-May-23 20:25 onnxruntime_extensions/tools/pre_post_processing/pre_post_processor.py
--rw-rw-rw-  2.0 fat     9130 b- defN 23-May-23 20:25 onnxruntime_extensions/tools/pre_post_processing/step.py
-drwx---     6.3 fat        0 bx stor 23-May-23 21:44 onnxruntime_extensions/tools/pre_post_processing/steps/
--rw-rw-rw-  2.0 fat     9380 b- defN 23-May-23 20:25 onnxruntime_extensions/tools/pre_post_processing/steps/general.py
--rw-rw-rw-  2.0 fat    14727 b- defN 23-May-23 20:25 onnxruntime_extensions/tools/pre_post_processing/steps/nlp.py
--rw-rw-rw-  2.0 fat    44443 b- defN 23-May-23 20:25 onnxruntime_extensions/tools/pre_post_processing/steps/vision.py
--rw-rw-rw-  2.0 fat      165 b- defN 23-May-23 20:25 onnxruntime_extensions/tools/pre_post_processing/steps/__init__.py
--rw-rw-rw-  2.0 fat     5698 b- defN 23-May-23 20:25 onnxruntime_extensions/tools/pre_post_processing/utils.py
--rw-rw-rw-  2.0 fat      125 b- defN 23-May-23 20:25 onnxruntime_extensions/tools/pre_post_processing/__init__.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-May-23 20:25 onnxruntime_extensions/tools/__init__.py
--rw-rw-rw-  2.0 fat     4419 b- defN 23-May-23 20:25 onnxruntime_extensions/util.py
--rw-rw-rw-  2.0 fat    13867 b- defN 23-May-23 20:25 onnxruntime_extensions/_cuops.py
--rw-a--     6.3 fat  4420016 bx defN 23-May-23 21:46 onnxruntime_extensions/_extensions_pydll.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat     6489 b- defN 23-May-23 20:25 onnxruntime_extensions/_ocos.py
--rw-rw-rw-  2.0 fat     6599 b- defN 23-May-23 20:25 onnxruntime_extensions/_ortapi2.py
--rw-rw-rw-  2.0 fat       75 b- defN 23-May-23 20:59 onnxruntime_extensions/_version.py
--rw-rw-rw-  2.0 fat      929 b- defN 23-May-23 20:25 onnxruntime_extensions/__init__.py
-46 files, 4828121 bytes uncompressed, 1735798 bytes compressed:  64.0%
+Zip file size: 2107806 bytes, number of entries: 48
+drwx---     6.3 fat        0 bx stor 23-Sep-13 05:11 onnxruntime_extensions/
+drwx---     6.3 fat        0 bx stor 23-Sep-13 05:11 onnxruntime_extensions-0.9.0.dist-info/
+-rw-rw-rw-  2.0 fat     1162 b- defN 23-Sep-13 03:51 onnxruntime_extensions-0.9.0.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat     4281 b- defN 23-Sep-13 03:51 onnxruntime_extensions-0.9.0.dist-info/METADATA
+-rw-rw-r--  2.0 fat     4158 b- defN 23-Sep-13 03:51 onnxruntime_extensions-0.9.0.dist-info/RECORD
+-rw-rw-rw-  2.0 fat       23 b- defN 23-Sep-13 03:28 onnxruntime_extensions-0.9.0.dist-info/top_level.txt
+-rw-rw-rw-  2.0 fat      100 b- defN 23-Sep-13 03:51 onnxruntime_extensions-0.9.0.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat     2428 b- defN 23-Sep-13 02:48 onnxruntime_extensions/cmd.py
+-rw-rw-rw-  2.0 fat     2951 b- defN 23-Sep-13 02:48 onnxruntime_extensions/cvt.py
+drwx---     6.3 fat        0 bx stor 23-Sep-13 05:11 onnxruntime_extensions/onnxprocess/
+-rw-rw-rw-  2.0 fat      859 b- defN 23-Sep-13 02:48 onnxruntime_extensions/onnxprocess/torch_wrapper.py
+-rw-rw-rw-  2.0 fat     1844 b- defN 23-Sep-13 02:48 onnxruntime_extensions/onnxprocess/_builder.py
+-rw-rw-rw-  2.0 fat    73255 b- defN 23-Sep-13 02:48 onnxruntime_extensions/onnxprocess/_onnx_ops.py
+-rw-rw-rw-  2.0 fat    15158 b- defN 23-Sep-13 02:48 onnxruntime_extensions/onnxprocess/_session.py
+-rw-rw-rw-  2.0 fat    25410 b- defN 23-Sep-13 02:48 onnxruntime_extensions/onnxprocess/_tensor.py
+-rw-rw-rw-  2.0 fat      534 b- defN 23-Sep-13 02:48 onnxruntime_extensions/onnxprocess/__init__.py
+drwx---     6.3 fat        0 bx stor 23-Sep-13 05:11 onnxruntime_extensions/pnp/
+-rw-rw-rw-  2.0 fat     3928 b- defN 23-Sep-13 02:48 onnxruntime_extensions/pnp/_base.py
+-rw-rw-rw-  2.0 fat     2452 b- defN 23-Sep-13 02:48 onnxruntime_extensions/pnp/_imagenet.py
+-rw-rw-rw-  2.0 fat     7420 b- defN 23-Sep-13 02:48 onnxruntime_extensions/pnp/_nlp.py
+-rw-rw-rw-  2.0 fat    74398 b- defN 23-Sep-13 02:48 onnxruntime_extensions/pnp/_onnx_ops.py
+-rw-rw-rw-  2.0 fat    11927 b- defN 23-Sep-13 02:48 onnxruntime_extensions/pnp/_torchext.py
+-rw-rw-rw-  2.0 fat     1649 b- defN 23-Sep-13 02:48 onnxruntime_extensions/pnp/_unifier.py
+-rw-rw-rw-  2.0 fat    13061 b- defN 23-Sep-13 02:48 onnxruntime_extensions/pnp/_utils.py
+-rw-rw-rw-  2.0 fat      495 b- defN 23-Sep-13 02:48 onnxruntime_extensions/pnp/__init__.py
+drwx---     6.3 fat        0 bx stor 23-Sep-13 05:11 onnxruntime_extensions/tools/
+-rw-rw-rw-  2.0 fat     6786 b- defN 23-Sep-13 02:48 onnxruntime_extensions/tools/add_HuggingFace_CLIPImageProcessor_to_model.py
+-rw-rw-rw-  2.0 fat    23211 b- defN 23-Sep-13 02:48 onnxruntime_extensions/tools/add_pre_post_processing_to_model.py
+drwx---     6.3 fat        0 bx stor 23-Sep-13 05:11 onnxruntime_extensions/tools/pre_post_processing/
+-rw-rw-rw-  2.0 fat    19445 b- defN 23-Sep-13 02:48 onnxruntime_extensions/tools/pre_post_processing/pre_post_processor.py
+-rw-rw-rw-  2.0 fat     9130 b- defN 23-Sep-13 02:48 onnxruntime_extensions/tools/pre_post_processing/step.py
+drwx---     6.3 fat        0 bx stor 23-Sep-13 05:11 onnxruntime_extensions/tools/pre_post_processing/steps/
+-rw-rw-rw-  2.0 fat     9380 b- defN 23-Sep-13 02:48 onnxruntime_extensions/tools/pre_post_processing/steps/general.py
+-rw-rw-rw-  2.0 fat    14727 b- defN 23-Sep-13 02:48 onnxruntime_extensions/tools/pre_post_processing/steps/nlp.py
+-rw-rw-rw-  2.0 fat    44542 b- defN 23-Sep-13 02:48 onnxruntime_extensions/tools/pre_post_processing/steps/vision.py
+-rw-rw-rw-  2.0 fat      165 b- defN 23-Sep-13 02:48 onnxruntime_extensions/tools/pre_post_processing/steps/__init__.py
+-rw-rw-rw-  2.0 fat     5698 b- defN 23-Sep-13 02:48 onnxruntime_extensions/tools/pre_post_processing/utils.py
+-rw-rw-rw-  2.0 fat      125 b- defN 23-Sep-13 02:48 onnxruntime_extensions/tools/pre_post_processing/__init__.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Sep-13 02:48 onnxruntime_extensions/tools/__init__.py
+-rw-rw-rw-  2.0 fat     7260 b- defN 23-Sep-13 02:48 onnxruntime_extensions/util.py
+-rw-rw-rw-  2.0 fat    15302 b- defN 23-Sep-13 02:48 onnxruntime_extensions/_cuops.py
+-rw-a--     6.3 fat  5158832 bx defN 23-Sep-13 05:13 onnxruntime_extensions/_extensions_pydll.cp39-win_amd64.pyd
+-rw-rw-rw-  2.0 fat    10808 b- defN 23-Sep-13 02:48 onnxruntime_extensions/_hf_cvt.py
+-rw-rw-rw-  2.0 fat     6530 b- defN 23-Sep-13 02:48 onnxruntime_extensions/_ocos.py
+-rw-rw-rw-  2.0 fat     6850 b- defN 23-Sep-13 02:48 onnxruntime_extensions/_ortapi2.py
+-rw-rw-rw-  2.0 fat    10725 b- defN 23-Sep-13 02:48 onnxruntime_extensions/_torch_cvt.py
+-rw-rw-rw-  2.0 fat       75 b- defN 23-Sep-13 03:28 onnxruntime_extensions/_version.py
+-rw-rw-rw-  2.0 fat     1634 b- defN 23-Sep-13 02:48 onnxruntime_extensions/__init__.py
+48 files, 5598718 bytes uncompressed, 2099590 bytes compressed:  62.5%
```

## zipnote {}

```diff
@@ -1,29 +1,26 @@
 Filename: onnxruntime_extensions/
 Comment: 
 
-Filename: onnxruntime_extensions-0.8.0.dist-info/
+Filename: onnxruntime_extensions-0.9.0.dist-info/
 Comment: 
 
-Filename: onnxruntime_extensions-0.8.0.dist-info/LICENSE
+Filename: onnxruntime_extensions-0.9.0.dist-info/LICENSE
 Comment: 
 
-Filename: onnxruntime_extensions-0.8.0.dist-info/METADATA
+Filename: onnxruntime_extensions-0.9.0.dist-info/METADATA
 Comment: 
 
-Filename: onnxruntime_extensions-0.8.0.dist-info/RECORD
+Filename: onnxruntime_extensions-0.9.0.dist-info/RECORD
 Comment: 
 
-Filename: onnxruntime_extensions-0.8.0.dist-info/top_level.txt
+Filename: onnxruntime_extensions-0.9.0.dist-info/top_level.txt
 Comment: 
 
-Filename: onnxruntime_extensions-0.8.0.dist-info/WHEEL
-Comment: 
-
-Filename: onnxruntime_extensions/cmake_helper.py
+Filename: onnxruntime_extensions-0.9.0.dist-info/WHEEL
 Comment: 
 
 Filename: onnxruntime_extensions/cmd.py
 Comment: 
 
 Filename: onnxruntime_extensions/cvt.py
 Comment: 
@@ -75,14 +72,17 @@
 
 Filename: onnxruntime_extensions/pnp/__init__.py
 Comment: 
 
 Filename: onnxruntime_extensions/tools/
 Comment: 
 
+Filename: onnxruntime_extensions/tools/add_HuggingFace_CLIPImageProcessor_to_model.py
+Comment: 
+
 Filename: onnxruntime_extensions/tools/add_pre_post_processing_to_model.py
 Comment: 
 
 Filename: onnxruntime_extensions/tools/pre_post_processing/
 Comment: 
 
 Filename: onnxruntime_extensions/tools/pre_post_processing/pre_post_processor.py
@@ -120,20 +120,26 @@
 
 Filename: onnxruntime_extensions/_cuops.py
 Comment: 
 
 Filename: onnxruntime_extensions/_extensions_pydll.cp39-win_amd64.pyd
 Comment: 
 
+Filename: onnxruntime_extensions/_hf_cvt.py
+Comment: 
+
 Filename: onnxruntime_extensions/_ocos.py
 Comment: 
 
 Filename: onnxruntime_extensions/_ortapi2.py
 Comment: 
 
+Filename: onnxruntime_extensions/_torch_cvt.py
+Comment: 
+
 Filename: onnxruntime_extensions/_version.py
 Comment: 
 
 Filename: onnxruntime_extensions/__init__.py
 Comment: 
 
 Zip file comment:
```

## onnxruntime_extensions/cmd.py

```diff
@@ -1,7 +1,16 @@
+# Copyright (c) Microsoft Corporation. All rights reserved.
+# Licensed under the MIT License. See License.txt in the project root for
+# license information.
+###############################################################################
+
+"""
+cmd.py: cli commands for onnxruntime_extensions
+"""
+
 import os
 import argparse
 import onnx
 import numpy
 
 from onnx import onnx_pb, save_tensor, numpy_helper
 from ._ortapi2 import OrtPyFunction
```

## onnxruntime_extensions/cvt.py

```diff
@@ -1,65 +1,71 @@
-import json
-from ._cuops import CustomOpConverter
-
-
-class HFTokenizerConverter(CustomOpConverter):
-    def __init__(self, tokenizer):
-        self.tokenizer = tokenizer
-
-    def bpe_tokenizer(self, **kwargs):
-        hf_gpt2_tokenizer = self.tokenizer
-        attrs = {'vocab': json.dumps(
-            hf_gpt2_tokenizer.encoder, separators=(',', ':'))}
-        sorted_merges = {v_: k_ for k_,
-                         v_ in hf_gpt2_tokenizer.bpe_ranks.items()}
-        attrs['merges'] = '\n'.join("{} {}".format(
-            *sorted_merges[n_]) for n_ in range(len(sorted_merges)))
-        attrs.update(**kwargs)
-        return attrs
-
-    def bpe_decoder(self, **kwargs):
-        decoder = self.tokenizer.decoder
-        id_vocab = "\n".join([decoder[_idx] for _idx in sorted(decoder)])
-        # with open("id_vocab.txt", "w", encoding="utf-8") as f:
-        #     f.write(id_vocab)
-        byte_decoder = self.tokenizer.byte_decoder
-        str_byte_decoder = "\n".join(["{}\t{}".format(
-            ord(_c), str(byte_decoder[_c])) for _c in byte_decoder])
-        # with open("byte_decoder.txt", "w", encoding="utf-8") as f:
-        #     f.write(str_byte_decoder)
-        all_special_ids = self.tokenizer.all_special_ids
-        added_tokens = self.tokenizer.added_tokens_decoder
-        str_all_special_ids = "\n".join([str(_id) for _id in all_special_ids])
-        str_added_tokens = "\n".join(
-            ["{}\t{}".format(str(_id), added_tokens[_id]) for _id in added_tokens])
-        kwargs.update({
-            "id_vocab": id_vocab,
-            "byte_decoder": str_byte_decoder,
-            "added_tokens": str_added_tokens,
-            "all_special_ids": str_all_special_ids,
-            "skip_special_tokens": kwargs.get("skip_special_tokens", False)
-        })
-
-        return kwargs
-
-    def clip_tokenizer(self, **kwargs):
-        hf_clip_tokenizer = self.tokenizer
-        attrs = {'vocab': json.dumps(
-            hf_clip_tokenizer.encoder, separators=(',', ':'))}
-        sorted_merges = {v_: k_ for k_,
-                         v_ in hf_clip_tokenizer.bpe_ranks.items()}
-        attrs['merges'] = '\n'.join("{} {}".format(
-            *sorted_merges[n_]) for n_ in range(len(sorted_merges)))
-        attrs.update(**kwargs)
-        return attrs
-
-    def roberta_tokenizer(self, **kwargs):
-        hf_roberta_tokenizer = self.tokenizer
-        attrs = {'vocab': json.dumps(
-            hf_roberta_tokenizer.encoder, separators=(',', ':'))}
-        sorted_merges = {v_: k_ for k_,
-                         v_ in hf_roberta_tokenizer.bpe_ranks.items()}
-        attrs['merges'] = '\n'.join("{} {}".format(
-            *sorted_merges[n_]) for n_ in range(len(sorted_merges)))
-        attrs.update(**kwargs)
-        return attrs
+# Copyright (c) Microsoft Corporation. All rights reserved.
+# Licensed under the MIT License. See License.txt in the project root for
+# license information.
+###############################################################################
+
+"""
+cvt.py: Processing Graph Converter and Generator
+"""
+
+from typing import Union
+
+from ._hf_cvt import HFTokenizerConverter, HFTokenizerOnnxGraph  # noqa
+from ._ortapi2 import make_onnx_model
+
+
+_is_torch_available = False
+try:
+    import torch    # noqa
+    _is_torch_available = True
+    from ._torch_cvt import WhisperDataProcGraph
+except ImportError:
+    WhisperDataProcGraph = None
+
+
+def gen_processing_models(processor: Union[str, object],
+                          pre_kwargs: dict = None,
+                          post_kwargs: dict = None,
+                          opset: int = None,
+                          **kwargs):
+    """
+    Generate the pre- and post-processing ONNX model, basing on the name or HF class.
+
+    Parameters
+    ----------
+    processor:
+        the HF processor/tokenizer instance, or the name (str) of a Data Processor
+        the instance is preferred, otherwise when name was given, the corresponding configuration for the processor
+        has to be provided in the kwargs
+    pre_kwargs: dict
+        Keyword arguments for generating the pre-processing model
+    post_kwargs: dict
+        Keyword arguments for generating the post-processing model
+    opset: int
+        the target opset version of the model
+    kwargs:
+        The additional arguments for generating models
+
+    Returns
+    -------
+    ONNX-Models
+        The pre- and post-processing ONNX models
+    """
+    if pre_kwargs is None and post_kwargs is None:
+        raise ValueError("Either pre_kwargs or post_kwargs should be provided. None means no processing")
+
+    cls_name = processor if isinstance(processor, str) else type(processor).__name__
+    if cls_name == "WhisperProcessor":
+        if WhisperDataProcGraph is None:
+            raise ValueError("The Whisper processor needs torch.onnx support, please install pytorch 2.0 and above")
+        _converter = WhisperDataProcGraph(processor, opset=opset, **kwargs)
+        pre_m = _converter.pre_processing(**pre_kwargs) if pre_kwargs is not None else None
+        post_m = _converter.post_processing(**post_kwargs) if post_kwargs is not None else None
+        return pre_m, post_m
+    elif HFTokenizerOnnxGraph.is_supported(processor):
+        _converter = HFTokenizerOnnxGraph(processor)
+        pre_g = _converter.pre_processing(**pre_kwargs) if pre_kwargs is not None else None
+        post_g = _converter.post_processing(**post_kwargs) if post_kwargs is not None else None
+        return make_onnx_model(pre_g) if pre_g else None, \
+            make_onnx_model(post_g) if post_g else None
+    else:
+        raise ValueError(f"Unsupported processor/tokenizer: {cls_name}")
```

## onnxruntime_extensions/tools/pre_post_processing/steps/vision.py

```diff
@@ -518,20 +518,21 @@
 # Utilities
 #
 class ImageBytesToFloat(Step):
     """
     Convert uint8 or float values in range 0..255 to floating point values in range 0..1
     """
 
-    def __init__(self, name: Optional[str] = None):
+    def __init__(self, rescale_factor: float = 1/255, name: Optional[str] = None):
         """
         Args:
             name: Optional step name. Defaults to 'ImageBytesToFloat'
         """
         super().__init__(["data"], ["float_data"], name)
+        self.rescale_factor_ = rescale_factor
 
     def _create_graph_for_step(self, graph: onnx.GraphProto, onnx_opset: int):
         input_type_str, input_shape_str = self._get_input_type_and_shape_strs(graph, 0)
         if input_type_str == "uint8":
             optional_cast = f"""\
                 input_f = Cast <to = 1> ({self.input_names[0]})
             """
@@ -540,18 +541,18 @@
             optional_cast = f"input_f = Identity ({self.input_names[0]})"
 
         byte_to_float_graph = onnx.parser.parse_graph(
             f"""\
             byte_to_float ({input_type_str}[{input_shape_str}] {self.input_names[0]}) 
                 => (float[{input_shape_str}] {self.output_names[0]})
             {{
-                f_255 = Constant <value = float[1] {{255.0}}>()
+                f_scale = Constant <value = float[1] {{{self.rescale_factor_}}}>()
 
                 {optional_cast}
-                {self.output_names[0]} = Div(input_f, f_255)
+                {self.output_names[0]} = Mul(input_f, f_scale)
             }}
             """
         )
 
         onnx.checker.check_graph(byte_to_float_graph)
         return byte_to_float_graph
```

## onnxruntime_extensions/util.py

```diff
@@ -1,9 +1,14 @@
 # Copyright (c) Microsoft Corporation.
 # Licensed under the MIT License.
+
+"""
+util.py: Miscellaneous utility functions
+"""
+
 import onnx
 import pathlib
 import inspect
 
 import numpy as np
 
 
@@ -18,15 +23,15 @@
     with open(str(path), mode) as file_content:
         return file_content.read()
 
 
 def mel_filterbank(
         n_fft: int, n_mels: int = 80, sr=16000, min_mel=0, max_mel=45.245640471924965, dtype=np.float32):
     """
-    Compute a Mel-filterbank. The filters are stored in the rows, the columns
+    Compute a Mel-filterbank. The filters are stored in the rows, the columns,
     and it is Slaney normalized mel-scale filterbank.
     """
     fbank = np.zeros((n_mels, n_fft // 2 + 1), dtype=dtype)
 
     # the centers of the frequency bins for the DFT
     freq_bins = np.fft.rfftfreq(n=n_fft, d=1.0 / sr)
 
@@ -51,15 +56,15 @@
     for i in range(n_mels):
         left = -ramps[i] / mel_spacing[i]
         right = ramps[i + 2] / mel_spacing[i + 1]
 
         # intersect them with each other and zero
         fbank[i] = np.maximum(0, np.minimum(left, right))
 
-    energy_norm = 2.0 / (mel_bins[2 : n_mels + 2] - mel_bins[:n_mels])
+    energy_norm = 2.0 / (mel_bins[2: n_mels + 2] - mel_bins[:n_mels])
     fbank *= energy_norm[:, np.newaxis]
     return fbank
 
 
 def remove_unused_constants(subgraph):
     nodes = [_n for _n in subgraph.node]
 
@@ -117,7 +122,59 @@
     for node in nodes:
         for attr in node.attribute:
             if attr.type == onnx.AttributeProto.GRAPH:
                 remove_unused_initializers(attr.g, top_level_initializers)
             elif attr.type == onnx.AttributeProto.GRAPHS:
                 for subgraph in attr.graphs:
                     remove_unused_initializers(subgraph, top_level_initializers)
+
+
+def quick_merge(*models, connection_indices=None):
+    """
+    This function merges multiple ONNX models into a single model, without performing any ONNX format checks.
+
+    Parameters:
+    *models (onnx.ModelProto): Varargs parameter representing the ONNX models to be merged.
+    connection_indices (List[List[int]], optional): A nested list specifying which outputs in one model should connect 
+                                                    to which inputs in the next model, based on their indices. 
+                                                    If not provided, it's assumed that the sequence of outputs in 
+                                                    one model exactly matches the sequence of inputs in the next model.
+
+    Returns:
+    merged_model (onnx.ModelProto): The merged ONNX model.
+
+    Raises:
+    ValueError: If there is any conflict in tensor names, either in initializers or in nodes, including subgraphs.
+                If there is any conflict in opset versions for the same domain.
+    """
+
+    merged_graph = models[0].graph
+
+    # Dictionary to store unique opsets
+    opset_imports = {opset.domain if opset.domain else "ai.onnx": opset for opset in models[0].opset_import}
+
+    # Iterate over all other models and merge
+    for model_idx, model in enumerate(models[1:], start=1):
+        if connection_indices is None:
+            io_map = [(out.name, in_.name) for out, in_ in zip(models[model_idx - 1].graph.output, model.graph.input)]
+        else:
+            io_map = [(models[model_idx - 1].graph.output[out_idx].name, model.graph.input[in_idx].name)
+                      for out_idx, in_idx in connection_indices[model_idx - 1]]
+
+        merged_graph = onnx.compose.merge_graphs(merged_graph, model.graph, io_map)
+
+        for opset in model.opset_import:
+            if not opset.domain:
+                opset.domain = "ai.onnx"
+            if opset.domain in opset_imports and opset_imports[opset.domain].version != opset.version:
+                raise ValueError(f"Conflict in opset versions for domain '{opset.domain}': " +
+                                 f"model {model_idx} has version {opset.version}, while previous model has version " +
+                                 f"{opset_imports[opset.domain].version}.")
+            else:
+                opset_imports[opset.domain] = opset
+
+    default_opset = opset_imports.pop("ai.onnx", None)
+    merged_model = onnx.helper.make_model_gen_version(merged_graph,
+                                                      opset_imports=[default_opset],
+                                                      producer_name='ONNX Model Merger')
+    merged_model.opset_import.extend(opset_imports.values())
+    return merged_model
```

## onnxruntime_extensions/_cuops.py

```diff
@@ -1,12 +1,16 @@
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License. See License.txt in the project root for
 # license information.
 ###############################################################################
 
+"""
+_cuops.py: Custom operators signatures for Python usage.
+"""
+
 import onnx
 import numpy
 from onnx import onnx_pb as onnx_proto
 from ._ocos import default_opset_domain, Opdef, PyCustomOpDef
 
 
 class CustomOp:
@@ -23,14 +27,18 @@
         return None
 
     @classmethod
     def get_outputs(cls):
         return None
 
     @classmethod
+    def input_default_values(cls):
+        return None
+
+    @classmethod
     def serialize_attr(cls, attrs):
         """
         Only support serialize the basic python type like list or dict,
         All other types needs to be serialized by the users
         :param attrs: the dict attributes
         :return: the dict of serialized data
         """
@@ -89,20 +97,20 @@
         ]
 
 
 class BpeDecoder(CustomOp):
     @classmethod
     def get_inputs(cls):
         return [
-            cls.io_def("ids", onnx.TensorProto.INT64, [])
+            cls.io_def("ids", onnx.TensorProto.INT64, None)
         ]
 
     @classmethod
     def get_outputs(cls):
-        return [cls.io_def('str', onnx_proto.TensorProto.STRING, [])]
+        return [cls.io_def('str', onnx_proto.TensorProto.STRING, None)]
 
 
 class VectorToString(CustomOp):
 
     @classmethod
     def get_inputs(cls):
         return [cls.io_def("token_ids", onnx.TensorProto.INT64, [])]
@@ -233,22 +241,25 @@
         return [cls.io_def("text", onnx.TensorProto.STRING, [None])]
 
     @classmethod
     def get_outputs(cls):
         return [
             cls.io_def('input_ids', onnx_proto.TensorProto.INT64, [None]),
             cls.io_def('token_type_ids', onnx_proto.TensorProto.INT64, [None]),
-            cls.io_def('attention_mask', onnx_proto.TensorProto.INT64, [None])
+            cls.io_def('attention_mask', onnx_proto.TensorProto.INT64, [None]),
+            cls.io_def('offset_mapping', onnx.TensorProto.INT64, [None, 2])
         ]
 
     @classmethod
     def serialize_attr(cls, attrs):
         attrs_data = {}
         for k_, v_ in attrs.items():
-            if k_ == 'vocab_file':
+            if k_ == 'vocab':
+                attrs_data['vocab_file'] = v_
+            elif k_ == 'vocab_file':
                 with open(v_, "r", encoding='utf-8') as model_file:
                     lines = model_file.readlines()
                     attrs_data[k_] = '\n'.join(lines)
             else:
                 attrs_data[k_] = v_
         return attrs_data
 
@@ -303,14 +314,25 @@
             cls.io_def('nbest_size', onnx_proto.TensorProto.INT64, [None]),
             cls.io_def('alpha', onnx_proto.TensorProto.FLOAT, [None]),
             cls.io_def('add_bos', onnx_proto.TensorProto.BOOL, [None]),
             cls.io_def('add_eos', onnx_proto.TensorProto.BOOL, [None]),
             cls.io_def('reverse', onnx_proto.TensorProto.BOOL, [None])
         ]
 
+    # beyond Python 3.7, the order of the dict is guaranteed to be insertion order
+    @classmethod
+    def input_default_values(cls):
+        return {
+            'nbest_size': [0],
+            'alpha': [0],
+            'add_bos': [False],
+            'add_eos': [False],
+            'reverse': [False]
+        }
+
     @classmethod
     def get_outputs(cls):
         return [
             cls.io_def('tokens', onnx_proto.TensorProto.INT32, [None]),
             cls.io_def('indices', onnx_proto.TensorProto.INT64, [None])
         ]
 
@@ -324,14 +346,34 @@
         ]
 
     @classmethod
     def get_outputs(cls):
         return [cls.io_def('str', onnx_proto.TensorProto.STRING, [None])]
 
 
+class TrieTokenizer(CustomOp):
+    @classmethod
+    def get_inputs(cls):
+        return [cls.io_def('str', onnx_proto.TensorProto.STRING, ['N'])]
+
+    @classmethod
+    def get_outputs(cls):
+        return [cls.io_def("ids", onnx.TensorProto.INT64, ['N', None])]
+
+
+class TrieDetokenizer(CustomOp):
+    @classmethod
+    def get_inputs(cls):
+        return [cls.io_def("ids", onnx.TensorProto.INT64, ['N', None])]
+
+    @classmethod
+    def get_outputs(cls):
+        return [cls.io_def('str', onnx_proto.TensorProto.STRING, [None])]
+
+
 class Inverse(CustomOp):
 
     @classmethod
     def get_inputs(cls):
         return [
             cls.io_def('input', onnx_proto.TensorProto.FLOAT, [None, None])
         ]
@@ -418,54 +460,61 @@
     @classmethod
     def get_outputs(cls):
         return [
             cls.io_def('stft_norm', onnx_proto.TensorProto.FLOAT, [1, None, None])
         ]
 
 
+# TODO: have a C++ impl.
+def _argsort_op(x, dim):
+    d = numpy.argsort(x, dim)
+    return d[:, ::-1]
+
+
+Opdef.create(_argsort_op,
+             op_type='ArgSort',
+             inputs=[PyCustomOpDef.dt_float, PyCustomOpDef.dt_int64],
+             outputs=[PyCustomOpDef.dt_int64])
+
+
+class CustomOpConverter:
+    pass
+
+
 class SingleOpGraph:
 
     @classmethod
     def get_next_id(cls):
         if not hasattr(cls, '_id_counter'):
             cls._id_counter = 0
         cls._id_counter += 1
         return cls._id_counter
 
     @classmethod
-    def build_my_graph(cls, op_class, *args, **kwargs):
+    def build_graph(cls, op_class, *args, **kwargs):
         if isinstance(op_class, str):
             op_class = cls.get_op_class(op_class)
 
+        cvt = kwargs.pop('cvt', None)
+        if cvt is None and len(args) > 0 and isinstance(args[0], CustomOpConverter):
+            cvt = args[0]
+            args = args[1:]
+
+        new_kwargs = kwargs if cvt is None else cvt(**kwargs)
+
         op_type = op_class.op_type()
         inputs = op_class.get_inputs()
         outputs = op_class.get_outputs()
-        attrs = op_class.serialize_attr(kwargs)
+        attrs = op_class.serialize_attr(new_kwargs)
         cuop = onnx.helper.make_node(op_type, [i_.name for i_ in inputs],
                                      [o_.name for o_ in outputs],
                                      "{}_{}".format(op_type,
                                                     cls.get_next_id()),
                                      **attrs,
                                      domain=default_opset_domain())
         graph = onnx.helper.make_graph([cuop], "og_{}_{}".format(
             op_type, cls.get_next_id()), inputs, outputs)
         return graph
 
     @staticmethod
     def get_op_class(op_type):
         return globals()[op_type]
-
-
-# TODO: have a C++ impl.
-def _argsort_op(x, dim):
-    d = numpy.argsort(x, dim)
-    return d[:, ::-1]
-
-
-Opdef.create(_argsort_op,
-             op_type='ArgSort',
-             inputs=[PyCustomOpDef.dt_float, PyCustomOpDef.dt_int64],
-             outputs=[PyCustomOpDef.dt_int64])
-
-
-class CustomOpConverter:
-    pass
```

## onnxruntime_extensions/_ocos.py

```diff
@@ -1,24 +1,27 @@
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License. See License.txt in the project root for
 # license information.
 ###############################################################################
+"""
+_ocos.py: PythonOp implementation
+"""
 
 import sys
 import copy
 import onnx
 from onnx import helper
 from ._extensions_pydll import (  # noqa
     PyCustomOpDef, enable_py_op, add_custom_op, hash_64, default_opset_domain)
 
 
 def get_library_path():
     """
     The custom operator library binary path
-    :return: A string of the this library path.
+    :return: A string of this library path.
     """
     mod = sys.modules['onnxruntime_extensions._extensions_pydll']
     return mod.__file__
 
 
 class Opdef:
```

## onnxruntime_extensions/_ortapi2.py

```diff
@@ -1,20 +1,25 @@
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License. See License.txt in the project root for
 # license information.
 ###############################################################################
 
+"""
+_ortapi2.py: ONNXRuntime-Extensions Python API
+"""
+
 import numpy as np
 from ._ocos import default_opset_domain, get_library_path  # noqa
-from ._cuops import onnx, onnx_proto, CustomOpConverter, SingleOpGraph
+from ._cuops import onnx, onnx_proto, SingleOpGraph
 
 _ort_check_passed = False
 try:
     from packaging import version as _ver
     import onnxruntime as _ort
+
     if _ver.parse(_ort.__version__) >= _ver.parse("1.10.0"):
         _ort_check_passed = True
 except ImportError:
     pass
 
 if not _ort_check_passed:
     raise RuntimeError("please install ONNXRuntime/ONNXRuntime-GPU >= 1.10.0")
@@ -28,14 +33,15 @@
         "1.8": 14,
         "1.9": 15,
         "1.10": 15,
         "1.11": 16,
         "1.12": 17,
         "1.13": 17,
         "1.14": 18,
+        "1.15": 18
     }
 
     ort_ver_string = '.'.join(_ort.__version__.split('.')[0:2])
     max_ver = max(_ORT_OPSET_SUPPORT_TABLE, key=_ORT_OPSET_SUPPORT_TABLE.get)
     if ort_ver_string > max_ver:
         ort_ver_string = max_ver
     return _ORT_OPSET_SUPPORT_TABLE.get(ort_ver_string, 11)
@@ -50,43 +56,48 @@
         onnx.helper.make_operatorsetid('ai.onnx', opset_version)])
     model.opset_import.extend(
         [onnx.helper.make_operatorsetid(extra_domain, extra_opset_version)])
     return model
 
 
 class OrtPyFunction:
+    """
+    OrtPyFunction is a convenience class that serves as a wrapper around the ONNXRuntime InferenceSession,
+    equipped with registered onnxruntime-extensions. This allows execution of an ONNX model as if it were a 
+    standard Python function. The order of the function arguments correlates directly with
+    the sequence of the input/output in the ONNX graph.
+    """
 
-    @classmethod
-    def get_ort_session_options(cls):
-        # ONNXRuntime has an issue to support reusing the SessionOptions object.
-        # Create a new one every time here
+    def get_ort_session_options(self):
         so = _ort.SessionOptions()
+        for k, v in self.extra_session_options.items():
+            so.__setattr__(k, v)
         so.register_custom_ops_library(get_library_path())
         return so
 
-    def __init__(self, cpu_only=None):
+    def __init__(self, path_or_model=None, cpu_only=None):
         self._onnx_model = None
         self.ort_session = None
         self.default_inputs = {}
         self.execution_providers = ['CPUExecutionProvider']
         if not cpu_only:
             if _ort.get_device() == 'GPU':
                 self.execution_providers = ['CUDAExecutionProvider']
-
-    def create_from_customop(self, op_type, *args, **kwargs):
-        cvt = kwargs.get('cvt', None)
-        if cvt is None:
-            cvt = args[0] if len(args) > 0 and isinstance(
-                args[0], CustomOpConverter) else None
-            args = args[1:]
+        self.extra_session_options = {}
+        mpath = None
+        if isinstance(path_or_model, str):
+            oxml = onnx.load_model(path_or_model)
+            mpath = path_or_model
         else:
-            del kwargs['cvt']
+            oxml = path_or_model
+        if path_or_model is not None:
+            self._bind(oxml, mpath)
 
-        new_kwargs = kwargs if cvt is None else cvt(**kwargs)
-        graph = SingleOpGraph.build_my_graph(op_type, *args, **new_kwargs)
+    def create_from_customop(self, op_type, *args, **kwargs):
+        graph = SingleOpGraph.build_graph(op_type, *args, **kwargs)
         self._bind(make_onnx_model(graph))
         return self
 
     def add_default_input(self, **kwargs):
         inputs = {
             ky_: val_ if isinstance(val_, (np.ndarray, np.generic)) else
             np.asarray(list(val_), dtype=np.uint8) for ky_, val_ in kwargs.items()
@@ -131,25 +142,21 @@
         cpuonly = kwargs.get('cpu_only', None)
         if cpuonly is not None:
             del kwargs['cpu_only']
         return cpuonly
 
     @classmethod
     def from_customop(cls, op_type, *args, **kwargs):
-        return cls(cls._get_kwarg_device(kwargs)).create_from_customop(op_type, *args, **kwargs)
+        return (cls(cpu_only=cls._get_kwarg_device(kwargs))
+                .create_from_customop(op_type, *args, **kwargs))
 
     @classmethod
     def from_model(cls, path_or_model, *args, **kwargs):
-        mpath = None
-        if isinstance(path_or_model, str):
-            oxml = onnx.load_model(path_or_model)
-            mpath = path_or_model
-        else:
-            oxml = path_or_model
-        return cls(cls._get_kwarg_device(kwargs))._bind(oxml, mpath)
+        fn = cls(path_or_model, cls._get_kwarg_device(kwargs))
+        return fn
 
     def _argument_map(self, *args, **kwargs):
         idx = 0
         feed = {}
         for i_ in self.inputs:
             if i_.name in self.default_inputs:
                 feed[i_.name] = self.default_inputs[i_.name]
@@ -170,15 +177,15 @@
         self._ensure_ort_session()
         outputs = self.ort_session.run(
             None, self._argument_map(*args, **kwargs))
         return outputs[0] if len(outputs) == 1 else tuple(outputs)
 
 
 def optimize_model(model_or_file, output_file):
-    sess_options = OrtPyFunction.get_ort_session_options()
+    sess_options = OrtPyFunction().get_ort_session_options()
     sess_options.graph_optimization_level = _ort.GraphOptimizationLevel.ORT_ENABLE_BASIC
     sess_options.optimized_model_filepath = output_file
     _ort.InferenceSession(model_or_file if isinstance(model_or_file, str)
                           else model_or_file.SerializeToString(), sess_options)
 
 
 ONNXRuntimeError = _ort.capi.onnxruntime_pybind11_state.Fail
```

## onnxruntime_extensions/_version.py

```diff
@@ -1,2 +1,2 @@
 # Generated by setup.py, DON'T MANUALLY UPDATE IT!
-__version__ = "0.8.0"
+__version__ = "0.9.0"
```

## onnxruntime_extensions/__init__.py

```diff
@@ -1,27 +1,45 @@
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # Licensed under the MIT License. See License.txt in the project root for
 # license information.
 ###############################################################################
 
 """
-The entry point to onnxruntime-extensions package.
+The `onnxruntime-extensions` Python package offers an API that allows users to generate models for pre-processing and
+post-processing tasks. In addition, it also provides an API to register custom operations implemented in Python.
+This enables more flexibility and control over model execution, thus expanding the functionality of the ONNX Runtime.
 """
 
 __author__ = "Microsoft"
 
+__all__ = [
+    'gen_processing_models',
+    'get_library_path',
+    'Opdef', 'onnx_op', 'PyCustomOpDef', 'PyOp',
+    'enable_py_op',
+    'expand_onnx_inputs',
+    'hook_model_op',
+    'default_opset_domain',
+    'OrtPyFunction', 'PyOrtFunction',
+    'optimize_model',
+    'make_onnx_model',
+    'ONNXRuntimeError',
+    'hash_64',
+    '__version__',
+]
 
 from ._version import __version__
-from ._ocos import get_library_path  # noqa
-from ._ocos import Opdef, PyCustomOpDef # noqa
-from ._ocos import hash_64 # noqa
-from ._ocos import enable_py_op  # noqa
-from ._ocos import expand_onnx_inputs  # noqa
-from ._ocos import hook_model_op  # noqa
-from ._ocos import default_opset_domain  # noqa
-from ._cuops import *  # noqa
+from ._ocos import get_library_path
+from ._ocos import Opdef, PyCustomOpDef
+from ._ocos import hash_64
+from ._ocos import enable_py_op
+from ._ocos import expand_onnx_inputs
+from ._ocos import hook_model_op
+from ._ocos import default_opset_domain
+from ._cuops import *   # noqa
 from ._ortapi2 import OrtPyFunction as PyOrtFunction  # backward compatibility
 from ._ortapi2 import OrtPyFunction, optimize_model, make_onnx_model, ONNXRuntimeError
+from .cvt import gen_processing_models
 
-
+# rename the implementation with a more formal name
 onnx_op = Opdef.declare
 PyOp = PyCustomOpDef
```

## Comparing `onnxruntime_extensions-0.8.0.dist-info/LICENSE` & `onnxruntime_extensions-0.9.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `onnxruntime_extensions-0.8.0.dist-info/METADATA` & `onnxruntime_extensions-0.9.0.dist-info/METADATA`

 * *Files 18% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: onnxruntime-extensions
-Version: 0.8.0
+Version: 0.9.0
 Summary: ONNXRuntime Extensions
 Home-page: https://github.com/microsoft/onnxruntime-extensions
 Author: Microsoft Corporation
 Author-email: onnxruntime@microsoft.com
 License: MIT License
 Classifier: Development Status :: 4 - Beta
 Classifier: Environment :: Console
@@ -14,55 +14,59 @@
 Classifier: Operating System :: POSIX :: Linux
 Classifier: Programming Language :: C++
 Classifier: Programming Language :: Python
 Classifier: Programming Language :: Python :: Implementation :: CPython
 Classifier: License :: OSI Approved :: MIT License
 Description-Content-Type: text/markdown
 License-File: LICENSE
-Requires-Dist: onnx (>=1.9.0)
+Requires-Dist: onnx >=1.9.0
 
 # ONNXRuntime-Extensions
 
-[![Build Status](https://aiinfra.visualstudio.com/Lotus/_apis/build/status/onnxruntime-extensions/extensions.wheel?branchName=main)](https://aiinfra.visualstudio.com/Lotus/_build/latest?definitionId=1085&branchName=main)
+[![Build Status](https://dev.azure.com/onnxruntime/onnxruntime/_apis/build/status%2Fmicrosoft.onnxruntime-extensions?branchName=main)](https://dev.azure.com/onnxruntime/onnxruntime/_build/latest?definitionId=209&branchName=main)
 
 ## What's ONNXRuntime-Extensions
 
 Introduction: ONNXRuntime-Extensions is a library that extends the capability of the ONNX models and inference with ONNX Runtime, via ONNX Runtime Custom Operator ABIs. It includes a set of [ONNX Runtime Custom Operator](https://onnxruntime.ai/docs/reference/operators/add-custom-op.html) to support the common pre- and post-processing operators for vision, text, and nlp models. And it supports multiple languages and platforms, like Python on Windows/Linux/macOS, some mobile platforms like Android and iOS, and Web-Assembly etc. The basic workflow is to enhance a ONNX model firstly and then do the model inference with ONNX Runtime and ONNXRuntime-Extensions package.
 
 
 ## Quickstart
 
 ### **Python installation**
 ```bash
 pip install onnxruntime-extensions
 ````
 
 
-### **nightly build**
+### **Nightly Build**
 
 #### <strong>on Windows</strong>
 ```cmd
 pip install --index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/ORT-Nightly/pypi/simple/ onnxruntime-extensions
 ```
 Please ensure that you have met the prerequisites of onnxruntime-extensions (e.g., onnx and onnxruntime) in your Python environment.
 #### <strong>on Linux/macOS</strong>
-the packages are not ready yet, so it could be installed from source. Please make sure the compiler toolkit like gcc(later than g++ 8.0) or clang, and the tool <strong>cmake</strong> are installed before the following command
+Please make sure the compiler toolkit like gcc(later than g++ 8.0) or clang are installed before the following command
 ```bash
 python -m pip install git+https://github.com/microsoft/onnxruntime-extensions.git
 ```
 
 
 ## Usage
 
-## 1. Augment an ONNX model with a pre- and post-processing pipeline
-check [tutorial](./tutorials) for a couple of examples on how to do it.
+## 1. Generate the pre-/post- processing ONNX model
+With onnxruntime-extensions Python package, you can easily get the ONNX processing graph by converting them from Huggingface transformer data processing classes, check the following API for details.
+```python
+help(onnxruntime_extensions.gen_processing_models)
+```
+### NOTE: These data processing model can be merged into other model [onnx.compose](https://onnx.ai/onnx/api/compose.html) if needed.
 ## 2. Using Extensions for ONNX Runtime inference
 
 ### Python
-
+There are individual packages for the following languages, please install it for the build.
 ```python
 import onnxruntime as _ort
 from onnxruntime_extensions import get_library_path as _lib_path
 
 so = _ort.SessionOptions()
 so.register_custom_ops_library(_lib_path())
 
@@ -85,37 +89,16 @@
 var env = OrtEnvironment.getEnvironment();
 var sess_opt = new OrtSession.SessionOptions();
 
 /* Register the custom ops from onnxruntime-extensions */
 sess_opt.registerCustomOpLibrary(OrtxPackage.getLibraryPath());
 ```
 
-## Use exporters to generate graphs with custom operators
-
-The PyTorch and TensorFlow converters support custom operator generation if the operation from the original framework cannot be interpreted as a standard ONNX operators. Check the following two examples on how to do this.
-
-1. [CustomOp conversion by pytorch.onnx.exporter](https://github.com/microsoft/onnxruntime-extensions/blob/main/tutorials/pytorch_custom_ops_tutorial.ipynb)
-2. [CustomOp conversion by tf2onnx](https://github.com/microsoft/onnxruntime-extensions/blob/main/tutorials/tf2onnx_custom_ops_tutorial.ipynb)
-
-
-## Add a new custom operator to onnxruntime-extensions
-
-You can contribute customop C++ implementations directly in this repository if they have general applicability to other users. In addition, if you want to quickly verify the ONNX model with Python, you can wrap the custom operator with **[PyOp](docs/pyop.md)**.
-
-```python
-import numpy
-from onnxruntime_extensions import PyOp, onnx_op
-
-# Implement the CustomOp by decorating a function with onnx_op
-@onnx_op(op_type="Inverse", inputs=[PyOp.dt_float])
-def inverse(x):
-    # the user custom op implementation here:
-    return numpy.linalg.inv(x)
-
-# Run the model with this custom op
-# model_func = PyOrtFunction(model_path)
-# outputs = model_func(inputs)
-# ...
+### C#
+```C#
+SessionOptions options = new SessionOptions()
+options.RegisterOrtExtensions()
+session = new InferenceSession(model, options)
 ```
-Check [development.md](./docs/development.md) for build and test
+
 
 #
```

## Comparing `onnxruntime_extensions-0.8.0.dist-info/RECORD` & `onnxruntime_extensions-0.9.0.dist-info/RECORD`

 * *Files 9% similar despite different names*

```diff
@@ -1,17 +1,18 @@
-onnxruntime_extensions/__init__.py,sha256=swmC9_r2HdBj4gcm3_Pg9YmWCXTVbzpaiG75_2e_6eE,929
-onnxruntime_extensions/_cuops.py,sha256=Xsuh7liVYxw3-xrz8az7sA7acr_eco5HzYYcHYCibs8,13867
-onnxruntime_extensions/_extensions_pydll.cp39-win_amd64.pyd,sha256=NNuAz5i0yGCGEZtbnulKHFIsl4AGwZ1B4HC_1pgGSww,4409856
-onnxruntime_extensions/_ocos.py,sha256=VNpaQuRZ4jrPl-PZ_rFdVQAWulsTPiybVX_286pGOZw,6489
-onnxruntime_extensions/_ortapi2.py,sha256=bZfk3C0cRiaXq_i-MCo1ghMRIbrLQ-6L8sgiYmRrogw,6599
-onnxruntime_extensions/_version.py,sha256=6NOfYujzAt6PDNfc4W5EqVPkylNawZdE9tUbM9z7zHk,75
-onnxruntime_extensions/cmake_helper.py,sha256=ms5eXDjEbbGLw3s8cYjYA_BR7CYdP1SIIvy0rxxryEs,1639
-onnxruntime_extensions/cmd.py,sha256=GIWnaW-QNe4436DqoYaUi_5hcMtL5ccQ8GygdFJM_RI,2124
-onnxruntime_extensions/cvt.py,sha256=uyq2M9WTLPxltk12UKCsqsff_hFer6OCqI5ioRVhFYU,2806
-onnxruntime_extensions/util.py,sha256=XfEpi3_m63-2w7nabNLmEf4sGy63i3qPlfBrTe7zyyA,4419
+onnxruntime_extensions/__init__.py,sha256=TIv1viYprQD6ZbPhd5iu7-u9StGEWa1PQzkoEc9BpEE,1634
+onnxruntime_extensions/_cuops.py,sha256=Wta2dHDdYfEgJm20gAgbYW-zdxtrSiO7pdtqlxNAeWs,15302
+onnxruntime_extensions/_extensions_pydll.cp39-win_amd64.pyd,sha256=pW2W_raar5JaNsuJRkcBjsKKv-B0B2YGOsbic2ATg9w,5148672
+onnxruntime_extensions/_hf_cvt.py,sha256=wQ2kYQ49q1eOIws9IMU1NzZ9pmHn-Sn4wc3VCiYAYgY,10808
+onnxruntime_extensions/_ocos.py,sha256=bSyvLrDrGh1lj5BdjpvT5fNCM0wq-3A5TQj2gZgxEFQ,6530
+onnxruntime_extensions/_ortapi2.py,sha256=rSNSS_VRKqFKOZVXvLW3PelcC5C6ct95RcVMFRpWm44,6850
+onnxruntime_extensions/_torch_cvt.py,sha256=pLGM-yjBEaGAtpyQ589sBODtM_dS53csDxl_CZi7zVs,10725
+onnxruntime_extensions/_version.py,sha256=gOg4XW81SqwhHggnJ3aUVKEviXwpITaavHP7dYlVJWk,75
+onnxruntime_extensions/cmd.py,sha256=eIiNNY0ohbUCPgmr9RwOfi0Gzw7nWL17i625L-ZKezI,2428
+onnxruntime_extensions/cvt.py,sha256=s22ykCTHHaIw-sEN0S4_zYW8IHlI03YYP3o0J6Tm7LE,2951
+onnxruntime_extensions/util.py,sha256=9VCyPZuDaofUkwLnrM6DD_JFPNkbMP-EIU4wpEs9bsg,7260
 onnxruntime_extensions/onnxprocess/__init__.py,sha256=BnveHXnu2nTQNbCLeZujZgZwO9A3yWFbQGTDthCFbIc,534
 onnxruntime_extensions/onnxprocess/_builder.py,sha256=L_afKeE7Wc4mWJ47eVXQ2stvmal_37QVTQZgKmt0ZK8,1844
 onnxruntime_extensions/onnxprocess/_onnx_ops.py,sha256=CYa1_yOEqpnfyvIea_gYpyRzjh82LVH6QB-eKvoEa-w,73255
 onnxruntime_extensions/onnxprocess/_session.py,sha256=lVm8U2UxF_FONBIAsRT87noBfVYz53CIx1FReaCTa0o,15158
 onnxruntime_extensions/onnxprocess/_tensor.py,sha256=COboqrHcfyCNL-H4byQNu0G0F7bV8vOlRe21lUKC2OU,25410
 onnxruntime_extensions/onnxprocess/torch_wrapper.py,sha256=nFF6wqiXUQy_PoZn2SunnoErBRo-2FD-R-1kHLavXVE,859
 onnxruntime_extensions/pnp/__init__.py,sha256=IKR_f0Ts-XtByS9jynv5-ShV-3PbN0mHmx4jGbtT2sQ,495
@@ -19,21 +20,22 @@
 onnxruntime_extensions/pnp/_imagenet.py,sha256=LcRzsr3uCURwUIfAjY8iDWExpG3FUqj_jOWmuW0bp6U,2452
 onnxruntime_extensions/pnp/_nlp.py,sha256=lhPGkGLZuKWCyp9ZEBvzxw1jOv85VBOK-5XMoXvY2bE,7420
 onnxruntime_extensions/pnp/_onnx_ops.py,sha256=836Bc_RUFJOVt4iS2c_vX1Un2LKmUlYemz3-2-aNFeg,74398
 onnxruntime_extensions/pnp/_torchext.py,sha256=SVYut2bBKvEbwl4luEF6IaLpU5FFJ2IfwMHIbGiP3TI,11927
 onnxruntime_extensions/pnp/_unifier.py,sha256=FPQYL1Z6f1Tv2qRsnhW_is9k7-GmCYhf6ZIXPU95gmU,1649
 onnxruntime_extensions/pnp/_utils.py,sha256=xBh7-_VstgqXlhBaQ_6E5GV6341ywCRQsrJZZZtYaCc,13061
 onnxruntime_extensions/tools/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+onnxruntime_extensions/tools/add_HuggingFace_CLIPImageProcessor_to_model.py,sha256=iNGAd9Ym0iKDQkXdWdka-R3S47TT3hMTihdGXg0uHL0,6786
 onnxruntime_extensions/tools/add_pre_post_processing_to_model.py,sha256=6J65GBk1bn_j25VJX4L7CFtDNWg4dVuE1M-iQwPQdY4,23211
 onnxruntime_extensions/tools/pre_post_processing/__init__.py,sha256=YKxCtG2McBExYYmcf1tbqDquqIS1iTs4iPx86MBcfRo,125
 onnxruntime_extensions/tools/pre_post_processing/pre_post_processor.py,sha256=xbLAVoQ9r7sofn0rFQHKqE3KIh5VI7cKx9REtlR55fQ,19445
 onnxruntime_extensions/tools/pre_post_processing/step.py,sha256=4hH-Ve53sqUVl5xyuHkFQeasmD2-WnDLURaG6zgmmHc,9130
 onnxruntime_extensions/tools/pre_post_processing/utils.py,sha256=-0soK4Md5EAp5fQ6tTXhI9EC-VOz-J6lST9arWwvXx0,5698
 onnxruntime_extensions/tools/pre_post_processing/steps/__init__.py,sha256=pdVRZBE7jCym0V8RpyVoiFeq1d4CwQJAh589pvbff-g,165
 onnxruntime_extensions/tools/pre_post_processing/steps/general.py,sha256=9rNSdcquTMy5YQaLCppbKgbit8u1CD3vvMWd09efITs,9380
 onnxruntime_extensions/tools/pre_post_processing/steps/nlp.py,sha256=ZHHv8LoikPALJ-tBLHmJxk-ac8kLInNnrw8EmZS4JxQ,14727
-onnxruntime_extensions/tools/pre_post_processing/steps/vision.py,sha256=HSl43Voswo6GkZTosx5oFCLLdrMTzdCsFpCIqc1Faik,44443
-onnxruntime_extensions-0.8.0.dist-info/LICENSE,sha256=mQaUD2Gx8LUz-n2ZuvVReLKAj74RPqUd-_rYVyzNXys,1162
-onnxruntime_extensions-0.8.0.dist-info/METADATA,sha256=le57yFFfYSiIM8nfH22sEAznATNQzPZaY2_76Kd2gzE,5224
-onnxruntime_extensions-0.8.0.dist-info/WHEEL,sha256=eep6QWEFiQfg2wcclssb_WY-D33AnLYLnEKGA9Rn-VU,100
-onnxruntime_extensions-0.8.0.dist-info/top_level.txt,sha256=XyAgQDKyXsf6_0MJb58kRdHwigpTn7A7kl9diBEjs8M,23
-onnxruntime_extensions-0.8.0.dist-info/RECORD,,
+onnxruntime_extensions/tools/pre_post_processing/steps/vision.py,sha256=ikJR8lH44Qpehu5Rn98UkD88H9YQkLmRqNuXBD63BZg,44542
+onnxruntime_extensions-0.9.0.dist-info/LICENSE,sha256=mQaUD2Gx8LUz-n2ZuvVReLKAj74RPqUd-_rYVyzNXys,1162
+onnxruntime_extensions-0.9.0.dist-info/METADATA,sha256=1EzNO8DzuzPg6qhlj3j8qwjKx3K4HAV1tBAJpP0L7jo,4281
+onnxruntime_extensions-0.9.0.dist-info/WHEEL,sha256=2xSj8c4s_gFbWpcImYQptdXS3JLuzC2uK5Om8I_SEKg,100
+onnxruntime_extensions-0.9.0.dist-info/top_level.txt,sha256=XyAgQDKyXsf6_0MJb58kRdHwigpTn7A7kl9diBEjs8M,23
+onnxruntime_extensions-0.9.0.dist-info/RECORD,,
```

