# Comparing `tmp/ragfmk-0.1.4.11-py3-none-any.whl.zip` & `tmp/ragfmk-0.1.5.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,41 +1,41 @@
-Zip file size: 32688 bytes, number of entries: 39
+Zip file size: 33691 bytes, number of entries: 39
 -rw-rw-rw-  2.0 fat     3665 b- defN 24-May-27 13:59 RagAdhocQueryDoc.py
 -rw-rw-rw-  2.0 fat     1731 b- defN 24-May-27 14:37 RagPrompt.py
 -rw-rw-rw-  2.0 fat     2229 b- defN 24-May-27 14:37 RagTest.py
 -rw-rw-rw-  2.0 fat        0 b- defN 23-Dec-21 09:21 __init__.py
 -rw-rw-rw-  2.0 fat        0 b- defN 23-Dec-21 09:21 ragfmk/__init__.py
--rw-rw-rw-  2.0 fat     8348 b- defN 24-May-28 09:14 ragfmk/rag.py
+-rw-rw-rw-  2.0 fat    10584 b- defN 24-May-29 09:53 ragfmk/rag.py
 -rw-rw-rw-  2.0 fat     2641 b- defN 24-May-27 14:37 ragfmk/ragChromaDB.py
 -rw-rw-rw-  2.0 fat     3740 b- defN 24-May-27 14:37 ragfmk/ragFAISS.py
 -rw-rw-rw-  2.0 fat        0 b- defN 24-Mar-25 16:16 ragfmk/elements/__init__.py
 -rw-rw-rw-  2.0 fat        0 b- defN 23-Dec-21 09:21 ragfmk/elements/embeddings/__init__.py
 -rw-rw-rw-  2.0 fat     1586 b- defN 24-May-08 13:35 ragfmk/elements/embeddings/embedding.py
 -rw-rw-rw-  2.0 fat     3333 b- defN 24-May-08 16:07 ragfmk/elements/embeddings/embeddings.py
 -rw-rw-rw-  2.0 fat     2528 b- defN 24-May-08 15:44 ragfmk/elements/embeddings/ollamaEmbeddings.py
 -rw-rw-rw-  2.0 fat     1833 b- defN 24-May-08 14:39 ragfmk/elements/embeddings/stEmbeddings.py
 -rw-rw-rw-  2.0 fat        0 b- defN 23-Dec-21 09:21 ragfmk/elements/llms/__init__.py
--rw-rw-rw-  2.0 fat     1233 b- defN 24-May-27 13:48 ragfmk/elements/llms/ollama.py
+-rw-rw-rw-  2.0 fat     1572 b- defN 24-May-28 15:24 ragfmk/elements/llms/ollama.py
 -rw-rw-rw-  2.0 fat     3390 b- defN 24-May-08 16:02 ragfmk/elements/simsearchengines/ChromaDBWrapper.py
 -rw-rw-rw-  2.0 fat     4814 b- defN 24-May-06 08:12 ragfmk/elements/simsearchengines/FAISSWrapper.py
 -rw-rw-rw-  2.0 fat        0 b- defN 24-Mar-25 16:16 ragfmk/elements/simsearchengines/__init__.py
 -rw-rw-rw-  2.0 fat        0 b- defN 23-Dec-21 09:21 ragfmk/elements/wrappers/__init__.py
 -rw-rw-rw-  2.0 fat     3203 b- defN 24-May-08 13:09 ragfmk/elements/wrappers/chunks.py
 -rw-rw-rw-  2.0 fat     7519 b- defN 24-May-06 08:12 ragfmk/elements/wrappers/document.py
 -rw-rw-rw-  2.0 fat     3128 b- defN 24-May-06 08:12 ragfmk/elements/wrappers/nearest.py
 -rw-rw-rw-  2.0 fat     1850 b- defN 24-May-06 08:12 ragfmk/elements/wrappers/prompt.py
 -rw-rw-rw-  2.0 fat      740 b- defN 24-May-01 15:19 ragfmk/interfaces/IChunks.py
 -rw-rw-rw-  2.0 fat      425 b- defN 24-May-01 15:19 ragfmk/interfaces/IDocument.py
 -rw-rw-rw-  2.0 fat      916 b- defN 24-May-08 16:07 ragfmk/interfaces/IEmbeddings.py
 -rw-rw-rw-  2.0 fat      895 b- defN 24-May-02 09:04 ragfmk/interfaces/INearest.py
 -rw-rw-rw-  2.0 fat      581 b- defN 24-May-01 15:19 ragfmk/interfaces/IPrompt.py
 -rw-rw-rw-  2.0 fat     1061 b- defN 24-May-27 10:00 ragfmk/interfaces/IRag.py
--rw-rw-rw-  2.0 fat     1842 b- defN 24-May-27 14:05 ragfmk/utils/CONST.py
+-rw-rw-rw-  2.0 fat     2122 b- defN 24-May-29 09:52 ragfmk/utils/CONST.py
 -rw-rw-rw-  2.0 fat        0 b- defN 24-Mar-25 16:16 ragfmk/utils/__init__.py
 -rw-rw-rw-  2.0 fat     2097 b- defN 24-May-27 08:57 ragfmk/utils/trace.py
--rw-rw-rw-  2.0 fat     1091 b- defN 24-May-28 09:17 ragfmk-0.1.4.11.dist-info/LICENSE
--rw-rw-rw-  2.0 fat     8714 b- defN 24-May-28 09:17 ragfmk-0.1.4.11.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 24-May-28 09:17 ragfmk-0.1.4.11.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       86 b- defN 24-May-28 09:17 ragfmk-0.1.4.11.dist-info/entry_points.txt
--rw-rw-rw-  2.0 fat       51 b- defN 24-May-28 09:17 ragfmk-0.1.4.11.dist-info/top_level.txt
--rw-rw-r--  2.0 fat     3317 b- defN 24-May-28 09:17 ragfmk-0.1.4.11.dist-info/RECORD
-39 files, 78679 bytes uncompressed, 27336 bytes compressed:  65.3%
+-rw-rw-rw-  2.0 fat     1091 b- defN 24-May-29 10:52 ragfmk-0.1.5.0.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat     8713 b- defN 24-May-29 10:52 ragfmk-0.1.5.0.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 24-May-29 10:52 ragfmk-0.1.5.0.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       86 b- defN 24-May-29 10:52 ragfmk-0.1.5.0.dist-info/entry_points.txt
+-rw-rw-rw-  2.0 fat       51 b- defN 24-May-29 10:52 ragfmk-0.1.5.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat     3312 b- defN 24-May-29 10:52 ragfmk-0.1.5.0.dist-info/RECORD
+39 files, 81528 bytes uncompressed, 28351 bytes compressed:  65.2%
```

## zipnote {}

```diff
@@ -93,26 +93,26 @@
 
 Filename: ragfmk/utils/__init__.py
 Comment: 
 
 Filename: ragfmk/utils/trace.py
 Comment: 
 
-Filename: ragfmk-0.1.4.11.dist-info/LICENSE
+Filename: ragfmk-0.1.5.0.dist-info/LICENSE
 Comment: 
 
-Filename: ragfmk-0.1.4.11.dist-info/METADATA
+Filename: ragfmk-0.1.5.0.dist-info/METADATA
 Comment: 
 
-Filename: ragfmk-0.1.4.11.dist-info/WHEEL
+Filename: ragfmk-0.1.5.0.dist-info/WHEEL
 Comment: 
 
-Filename: ragfmk-0.1.4.11.dist-info/entry_points.txt
+Filename: ragfmk-0.1.5.0.dist-info/entry_points.txt
 Comment: 
 
-Filename: ragfmk-0.1.4.11.dist-info/top_level.txt
+Filename: ragfmk-0.1.5.0.dist-info/top_level.txt
 Comment: 
 
-Filename: ragfmk-0.1.4.11.dist-info/RECORD
+Filename: ragfmk-0.1.5.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## ragfmk/rag.py

```diff
@@ -8,14 +8,18 @@
 from ragfmk.utils.trace import trace
 from ragfmk.elements.embeddings.embeddings import embeddings
 from ragfmk.elements.embeddings.stEmbeddings import stEmbeddings
 from ragfmk.elements.wrappers.chunks import chunks
 from ragfmk.interfaces.IRag import IRag
 import ragfmk.utils.CONST as C
 
+import requests
+import os
+import time
+
 class rag(IRag):
     def __init__(self):
         self.__trace = trace()
         self.__trace.start()
 
     def initSearchEngine(self):
         # No search engine for the high level class
@@ -150,27 +154,27 @@
                 raise Exception("Error while creating the prompt")
             self.addMilestone("PROMPT", "Prompt built successfully", customPrompt)
             return customPrompt
         except Exception as e:
             self.trace.addlog("ERROR", "Error while building the LLM prompt {}".format(str(e)))
             return ""
 
-    def promptLLM(self, question, urlOllama, model, temperature):
+    def promptLLM(self, question, urlOllama, model, temperature, contextwindow=C.OLLAMA_DEFAULT_CTXWIN):
         """ send a prompt to the LLM
         Args:
             question (str): prompt
             urlOllama (str): Ollama URL
             model (str): Ollama Model
             temperature (str): Ollama Model LLM Temperature
         Returns:
             str: LLM response
         """
         try:
             self.trace.addlog("INFO", "Send the prompt to the LLM ...")
-            myllm = ollama(urlOllama, model, temperature)
+            myllm = ollama(urlOllama, model, temperature, contextwindow)
             resp = myllm.prompt(question)
             try:
                 token_used = resp["prompt_eval_count"]
             except:
                 token_used = 0
             self.addMilestone("LLMPT", "LLM Reponse\n {}\n".format(resp["response"]))
             return resp["response"], token_used
@@ -192,8 +196,48 @@
             self.trace.addlog("INFO", "Create embeddings for list of texts/chunks ...")
             if (not embds.create(cks)):
                 raise Exception("Error while creating the chunks embeddings")
             self.addMilestone("DOCEMBEDDGS", "Embeddings created from chunks successfully")
             return embds
         except Exception as e:
             self.trace.addlog("ERROR", "Error while creating the list of texts/chunks embeddings {}".format(str(e)))
-            return None
+            return None
+        
+    def HallucinationDetector(self, response1, response2) -> float:
+        """ The HHEM model is an open source model, created by Vectara, for detecting hallucinations in LLMs. 
+            It is particularly useful in the context of building retrieval-augmented-generation (RAG) applications where 
+            a set of facts is summarized by an LLM, but the model can also be used in other contexts.
+            Cf. https://huggingface.co/vectara/hallucination_evaluation_model
+            Note: the environment variable HUGGINGFACE_API_KEY must be set with a valid HF token
+        Args:
+            response1 (_type_): Response 1 to check
+            response2 (_type_): Response 2 to check
+        Returns:
+            float: return the confidence score (under 0,5 it's probably an hallucination)
+        """
+        try:
+            # Get the API Hugging Face key first
+            try:
+                huggingFaceKey = os.environ[C.HUGGINGFACE_API_KEY]
+            except:
+                raise Exception ("The {} environment variable needs to be defined to use Hugging Face.".format(C.HUGGINGFACE_API_KEY))
+
+            headers = {"Authorization": "Bearer " + huggingFaceKey}
+            payload = { "inputs": response1 + "[SEP]" + response2 }
+            
+            # May need some time to load the model especially the first time
+            iteration = 1
+            while True:
+                response = requests.post(C.HHEM_HF_MODEL, headers=headers, json=payload)
+                if response.status_code == 200:
+                    break
+                time.sleep(C.HF_API_WAITSEC)
+                if (iteration >= C.HF_ITERATION_MAX):
+                    raise Exception ("Llamaindex seems not responsive or not responsive enough, please retry again.")
+                iteration += 1
+            
+            hallucinationScore = response.json()[0][0]["score"]
+            return float(hallucinationScore)
+        except Exception as e:
+            self.trace.addlog("ERROR", "Error while checking hallucinations {}".format(str(e)))
+            return None
+
```

## ragfmk/elements/llms/ollama.py

```diff
@@ -1,37 +1,48 @@
 __author__ = "Benoit CAYLA"
 __email__ = "benoit@datacorner.fr"
 __license__ = "MIT"
 
 import requests
 import json
+import ragfmk.utils.CONST as C
 
 class ollama:
-    def __init__(self, urlbase, model, temperature):
+    def __init__(self, 
+                 urlbase, 
+                 model, 
+                 temperature, 
+                 contextwindow=C.OLLAMA_DEFAULT_CTXWIN):
         self.__modelName = model
         self.__urlbase = urlbase
         self.__temperature = temperature
-    
+        self.__contextwindow = contextwindow
+        
     @property
     def model(self):
         return self.__modelName
     @property
     def urlbase(self):
         return self.__urlbase
     @property
     def temperature(self):
         return self.__temperature
     
+    @property
+    def contextwindow(self):
+        return self.__contextwindow
+    
     def prompt(self, prompt):
         try:
             url = self.urlbase + "/generate"
             params = {"model": self.model,
                       "prompt": prompt, 
                       "stream": False,
-                      "temperature": self.temperature}
+                      "temperature": self.temperature,
+                      "num_ctx": self.contextwindow}
             response = requests.post(url, json=params)
             if (response.status_code == 200):
                 response_text = response.text
                 data = json.loads(response_text)
                 return data
             else:
                 raise Exception("Error while reaching out to the Web Service: {}", str(response.status_code, response.text))
```

## ragfmk/utils/CONST.py

```diff
@@ -22,14 +22,15 @@
 
 # LLM stuff
 SEMCHUNK_EMBEDDING_MODEL = "sentence-transformers/all-mpnet-base-v2"
 EMBEDDING_MODEL = "paraphrase-mpnet-base-v2"
 OLLAMA_LOCAL_URL = "http://localhost:11434/api"
 OLLAMA_DEFAULT_LLM = "tinydolphin"
 OLLAMA_DEFAULT_EMB = "all-minilm"
+OLLAMA_DEFAULT_CTXWIN = 2048
 LLM_DEFAULT_TEMPERATURE = 0.9
 SM_DEFAULT_NEAREST = 3
 CHKS_DEFAULT_SIZE = 500
 CHKS_DEFAULT_OVERLAP = 50
 CHKS_DEFAULT_SEP = "."
 
 # JSON "tags" for chunks & embeddings
@@ -56,12 +57,19 @@
 LLAMAPARSE_API_URL = "https://api.cloud.llamaindex.ai/api/parsing"
 LLAMAPARSE_API_WAITSEC = 2
 LLAMAPARSE_ITERATION_MAX = 20
 LLAMAINDEX_API_KEY = "LLAMAINDEX_API_KEY"
 READER_VALPYPDF = "pymupdf"
 READER_VALLLAMAPARSE = "llamaparse"
 
+# Hugging Face
+HUGGINGFACE_API_KEY = "HUGGINGFACE_API_KEY"
+HUGGING_FACE_URL = "https://api-inference.huggingface.co"
+HHEM_HF_MODEL = HUGGING_FACE_URL + "/models/vectara/hallucination_evaluation_model"
+HF_API_WAITSEC = 2
+HF_ITERATION_MAX = 20
+
 # Chroma DB
 CDB_DEFAULT_HOST = "localhost"
 CDB_DEFAULT_PORT = 8000
 CDB_DEFAULT_COLLECTION = "default"
 CDB_DEFAULT_EMBEDDINGSMODEL_ST = "all-MiniLM-L6-v2"
```

## Comparing `ragfmk-0.1.4.11.dist-info/LICENSE` & `ragfmk-0.1.5.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `ragfmk-0.1.4.11.dist-info/METADATA` & `ragfmk-0.1.5.0.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: ragfmk
-Version: 0.1.4.11
+Version: 0.1.5.0
 Summary: Lightweight RAG Framework: Simple and Scalable Framework with Efficient Embeddings. Leverage: FAISS, ChromaDB, and Ollama.
 Author-email: Benoit Cayla <benoit@datacorner.fr>
 License: MIT License
         
         Copyright (c) 2023 Benoît Cayla
         
         Permission is hereby granted, free of charge, to any person obtaining a copy
```

## Comparing `ragfmk-0.1.4.11.dist-info/RECORD` & `ragfmk-0.1.5.0.dist-info/RECORD`

 * *Files 11% similar despite different names*

```diff
@@ -1,39 +1,39 @@
 RagAdhocQueryDoc.py,sha256=gzuNzXJBN1gIh_e6qd4tv5ntH6a1n3yYXmWkBg3ZfpE,3665
 RagPrompt.py,sha256=WlBntCbi2iSZQeaNW_GzdSg65w24Lp3EpQwR40AulYI,1731
 RagTest.py,sha256=E55p9LoB5cQ9PQrwJc8syLnEeLJ6Ks9KQTjYkTDbzdI,2229
 __init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 ragfmk/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-ragfmk/rag.py,sha256=hgCVYbGbeOBlw7j2-HkwCXxCVWQSnJ9XEGEq-uW6GfY,8348
+ragfmk/rag.py,sha256=YN8qd2vPt42SO6vxfeTrNoR3Ko3Ue0RwFN0xW8ontRU,10584
 ragfmk/ragChromaDB.py,sha256=LSyPDjYeTyDsmEmHPKJ-8GT136qFJO5hz1wizr9CGDk,2641
 ragfmk/ragFAISS.py,sha256=943uAkMw-VWP4ZQsdpSZSCeEHm70r0cpPIylzsBPDpo,3740
 ragfmk/elements/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 ragfmk/elements/embeddings/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 ragfmk/elements/embeddings/embedding.py,sha256=cmVbqR1-8dwcktghdctmTDhEn8QeiId2EkBrB8Jyl5E,1586
 ragfmk/elements/embeddings/embeddings.py,sha256=8qrLKBOfdXqbbeyWEl2SK7_Ug_dBISGwjM5rCxfR9Lo,3333
 ragfmk/elements/embeddings/ollamaEmbeddings.py,sha256=z-Bxq6zKHPkUoQUVY4oo98EKCM3Yrf37O2vWextDx_8,2528
 ragfmk/elements/embeddings/stEmbeddings.py,sha256=ckR-gL2mHMKId9I8DfGPoWrQeLhNctaZJpMq6iDnhQA,1833
 ragfmk/elements/llms/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-ragfmk/elements/llms/ollama.py,sha256=9FUkoHiWZk-KRe7QDXFtC828ghUHJrZmKbMiTrpWHE0,1233
+ragfmk/elements/llms/ollama.py,sha256=_a48jMUinC0q0kxZ2NxEl8_1lrKxPxXdwW_H1MgcCcw,1572
 ragfmk/elements/simsearchengines/ChromaDBWrapper.py,sha256=mtNMIWED5V1aY36vIvGk4pnqo0gTQoMEN_dDzd3mv-Q,3390
 ragfmk/elements/simsearchengines/FAISSWrapper.py,sha256=CCRRVoFfturN4-QqC_vmBzZjrlkTj6GC1SBtx1ISKuc,4814
 ragfmk/elements/simsearchengines/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 ragfmk/elements/wrappers/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 ragfmk/elements/wrappers/chunks.py,sha256=uBiTw2LP6waZe_uEpIGIfkuZUubwVf2vX6JbaCZC-3k,3203
 ragfmk/elements/wrappers/document.py,sha256=94to3cUwiypdcLwlDHwEtcy1d4W6EdTDkOf79juWRUE,7519
 ragfmk/elements/wrappers/nearest.py,sha256=ZDsjrOTFwkleFVMBnRe0vclaiypzG4pNyqCaICXmkBM,3128
 ragfmk/elements/wrappers/prompt.py,sha256=tsOH_Z8XRxuHhz7EmzEmfJdYxuYd99D4Eo0EgBM2-kE,1850
 ragfmk/interfaces/IChunks.py,sha256=1dBqYCEXgWiNQAiteQHcG-ZQ_uCluUPaGXCyBSMXZto,740
 ragfmk/interfaces/IDocument.py,sha256=0KrP-0szl8qHDH1TyC5opBqxTJV1dAIe9nTeNetiUR0,425
 ragfmk/interfaces/IEmbeddings.py,sha256=To1PGoNXFIplJf40CVrEUDoXcMRXkBNwtsYYlfI_nF4,916
 ragfmk/interfaces/INearest.py,sha256=DvPacdOW7lRzzX7MpdyvF3SlczjIlyxYrrQ-HcSV4bg,895
 ragfmk/interfaces/IPrompt.py,sha256=pZf8xQlKUw5s2Q4MSyxfpIQyNXGb6fjDwf9S1KAK9jg,581
 ragfmk/interfaces/IRag.py,sha256=unEm0bEoGOwuhgAwNdI4h-gF1vkxC4nGHTAaeUC-PR8,1061
-ragfmk/utils/CONST.py,sha256=ftYmVGuexCo7hF5_-llP2NBvZ_T_sFwpTPMiKhhPlYI,1842
+ragfmk/utils/CONST.py,sha256=-WjSenakXjnKqkZBtDgjSo2_Zf9ZsExDhxD7UZbv6zI,2122
 ragfmk/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 ragfmk/utils/trace.py,sha256=SvAuKCuXLsxs0jIgEjQtDAPWMYP0rXtbvKkK7EWGfMQ,2097
-ragfmk-0.1.4.11.dist-info/LICENSE,sha256=BvMmeujpLDerNqzsA1gOET7mphtef9ebt_u8-bp4bNA,1091
-ragfmk-0.1.4.11.dist-info/METADATA,sha256=-aHUdX55r8XV-L8GrfGFlVbTPvlPp10JTeOjtw3rgNI,8714
-ragfmk-0.1.4.11.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
-ragfmk-0.1.4.11.dist-info/entry_points.txt,sha256=EDEdK3AmGTz0ZLfXx10aBUy50jxn6Uy3isBcmPkE3jg,86
-ragfmk-0.1.4.11.dist-info/top_level.txt,sha256=LlNlUm1ce3g4y9ARq9Q4C-19JnDcERKf-aDueiUfWG8,51
-ragfmk-0.1.4.11.dist-info/RECORD,,
+ragfmk-0.1.5.0.dist-info/LICENSE,sha256=BvMmeujpLDerNqzsA1gOET7mphtef9ebt_u8-bp4bNA,1091
+ragfmk-0.1.5.0.dist-info/METADATA,sha256=fpDDbeUXiivoPBHoRuOVHkWDVC019J1z9wK3MVVDfIU,8713
+ragfmk-0.1.5.0.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+ragfmk-0.1.5.0.dist-info/entry_points.txt,sha256=EDEdK3AmGTz0ZLfXx10aBUy50jxn6Uy3isBcmPkE3jg,86
+ragfmk-0.1.5.0.dist-info/top_level.txt,sha256=LlNlUm1ce3g4y9ARq9Q4C-19JnDcERKf-aDueiUfWG8,51
+ragfmk-0.1.5.0.dist-info/RECORD,,
```

